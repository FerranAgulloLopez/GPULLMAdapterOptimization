INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.274846740998328,
    "estimated_duration": 3600.0922476045907,
    "input_throughput": 4853.730070841009,
    "output_throughput": 4252.432700908238,
    "total_throughput": 9106.162771749248,
    "itl": 129.71004286619927,
    "ttft": 2105721.6490514292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2318,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.327564136060381,
    "arrivals": 572657,
    "finished_requests": 71087,
    "scheduler_time": 73.75178138895224
}
#Debug simulation 
Total elapsed time: 5.274960771668702. Arrivals time: 0.28666333854198456 Scheduler time: 4.8363022278063 Scheduler overhead time: 0.041166000533849 Adapter cache time: 0.04822722123935819 Engine time: 0.04280426958575845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.232293108012527,
    "estimated_duration": 3600.0011641413134,
    "input_throughput": 4748.491520023568,
    "output_throughput": 4161.8197652926865,
    "total_throughput": 8910.311285316255,
    "itl": 116.50115023687077,
    "ttft": 2119577.4910384114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.60521337120761,
    "arrivals": 572657,
    "finished_requests": 69550,
    "scheduler_time": 73.72403042969366
}
#Debug simulation 
Total elapsed time: 5.232408370357007. Arrivals time: 0.23559832479804754 Scheduler time: 4.830635801889002 Scheduler overhead time: 0.0449772197753191 Adapter cache time: 0.05274425586685538 Engine time: 0.04684249730780721 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 4.959768868051469,
    "estimated_duration": 3600.076497812136,
    "input_throughput": 4420.322459723026,
    "output_throughput": 3874.8051627451655,
    "total_throughput": 8295.127622468191,
    "itl": 89.49812526554189,
    "ttft": 2162486.614920046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.924806145359431,
    "arrivals": 572657,
    "finished_requests": 64668,
    "scheduler_time": 73.31261288595002
}
#Debug simulation 
Total elapsed time: 4.9598563788458705. Arrivals time: 0.27915692795068026 Scheduler time: 4.473874505609274 Scheduler overhead time: 0.05621861945837736 Adapter cache time: 0.0648265928030014 Engine time: 0.05876131821423769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.252504700329155,
    "estimated_duration": 3600.099447492435,
    "input_throughput": 4749.894620802953,
    "output_throughput": 4163.261103923018,
    "total_throughput": 8913.15572472597,
    "itl": 116.46467378594461,
    "ttft": 2119271.1884756433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.392739277580214,
    "arrivals": 572657,
    "finished_requests": 69568,
    "scheduler_time": 73.75012695789847
}
#Debug simulation 
Total elapsed time: 5.252593121025711. Arrivals time: 0.2891381699591875 Scheduler time: 4.796719784848392 Scheduler overhead time: 0.04520406713709235 Adapter cache time: 0.05283297225832939 Engine time: 0.047020518220961094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 4.952583075966686,
    "estimated_duration": 3600.002858134995,
    "input_throughput": 4420.413157184017,
    "output_throughput": 3875.0308123996742,
    "total_throughput": 8295.443969583692,
    "itl": 89.49390302584568,
    "ttft": 2162420.0268996097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.755177177711996,
    "arrivals": 572657,
    "finished_requests": 64669,
    "scheduler_time": 73.3146105769467
}
#Debug simulation 
Total elapsed time: 4.952676249668002. Arrivals time: 0.28304103668779135 Scheduler time: 4.463036878500134 Scheduler overhead time: 0.05590302310883999 Adapter cache time: 0.06442088400945067 Engine time: 0.0592613760381937 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.556649523321539,
    "estimated_duration": 3600.10685292337,
    "input_throughput": 4751.195089143118,
    "output_throughput": 4164.509724989301,
    "total_throughput": 8915.70481413242,
    "itl": 116.43449446533926,
    "ttft": 2119169.4483949067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.478732693214793,
    "arrivals": 572657,
    "finished_requests": 69590,
    "scheduler_time": 73.76826970902941
}
#Debug simulation 
Total elapsed time: 5.556712923105806. Arrivals time: 0.303407680708915 Scheduler time: 5.086945618502796 Scheduler overhead time: 0.04516539815813303 Adapter cache time: 0.05280797742307186 Engine time: 0.04681011009961367 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382489721 . Total output tokens: 337254190
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.273887445218861,
    "estimated_duration": 3600.0679998217847,
    "input_throughput": 4420.4884465481755,
    "output_throughput": 3875.0401383225517,
    "total_throughput": 8295.528584870728,
    "itl": 89.49069447046385,
    "ttft": 2162367.0218772735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.628421465623802,
    "arrivals": 572657,
    "finished_requests": 64670,
    "scheduler_time": 73.31867240392155
}
#Debug simulation 
Total elapsed time: 5.273952126968652. Arrivals time: 0.22910890774801373 Scheduler time: 4.839465206488967 Scheduler overhead time: 0.05583705799654126 Adapter cache time: 0.06435114750638604 Engine time: 0.05817954195663333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.435950484126806,
    "estimated_duration": 3600.046500890215,
    "input_throughput": 5015.804655727323,
    "output_throughput": 4361.203388933335,
    "total_throughput": 9377.008044660657,
    "itl": 126.70679340833294,
    "ttft": 2082635.0314144979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.179549972522269,
    "arrivals": 564989,
    "finished_requests": 73232,
    "scheduler_time": 75.59141628702768
}
#Debug simulation 
Total elapsed time: 5.436044744215906. Arrivals time: 0.29961296916007996 Scheduler time: 4.990094688721001 Scheduler overhead time: 0.04217458190396428 Adapter cache time: 0.04043060168623924 Engine time: 0.043541951570659876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.359496758785099,
    "estimated_duration": 3600.0369210357994,
    "input_throughput": 4898.798647577711,
    "output_throughput": 4259.652147008496,
    "total_throughput": 9158.450794586208,
    "itl": 114.01897767090594,
    "ttft": 2097586.046500407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.742969781625993,
    "arrivals": 564989,
    "finished_requests": 71541,
    "scheduler_time": 75.38498495348759
}
#Debug simulation 
Total elapsed time: 5.359587186947465. Arrivals time: 0.24547284422442317 Scheduler time: 4.952237159945071 Scheduler overhead time: 0.04613937716931105 Adapter cache time: 0.04579435056075454 Engine time: 0.04775764932855964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.02146870829165,
    "estimated_duration": 3600.089425510636,
    "input_throughput": 4524.873155807633,
    "output_throughput": 3943.1564947824822,
    "total_throughput": 8468.029650590115,
    "itl": 87.98276122988148,
    "ttft": 2140975.982374731,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.517464807764613,
    "arrivals": 564989,
    "finished_requests": 66022,
    "scheduler_time": 74.57663743897136
}
#Debug simulation 
Total elapsed time: 5.021556654013693. Arrivals time: 0.2823955016210675 Scheduler time: 4.5394151927903295 Scheduler overhead time: 0.05658723600208759 Adapter cache time: 0.056229932233691216 Engine time: 0.05931848706677556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.350745470728725,
    "estimated_duration": 3600.037929715896,
    "input_throughput": 4899.612821966017,
    "output_throughput": 4260.570388271005,
    "total_throughput": 9160.183210237023,
    "itl": 114.0008648727519,
    "ttft": 2097394.7832336663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.110772054842665,
    "arrivals": 564989,
    "finished_requests": 71552,
    "scheduler_time": 75.39795960561032
}
#Debug simulation 
Total elapsed time: 5.350845853798091. Arrivals time: 0.24330335715785623 Scheduler time: 4.945596213918179 Scheduler overhead time: 0.04613603884354234 Adapter cache time: 0.04555964469909668 Engine time: 0.04807098116725683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.039557857904583,
    "estimated_duration": 3600.0957245469854,
    "input_throughput": 4524.945236574194,
    "output_throughput": 3943.1659839506938,
    "total_throughput": 8468.111220524886,
    "itl": 87.97944980440442,
    "ttft": 2141007.2377757956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.42654036723076,
    "arrivals": 564989,
    "finished_requests": 66025,
    "scheduler_time": 74.5786224138304
}
#Debug simulation 
Total elapsed time: 5.039647137746215. Arrivals time: 0.28462924528867006 Scheduler time: 4.553903933614492 Scheduler overhead time: 0.05655971588566899 Adapter cache time: 0.05735959857702255 Engine time: 0.059510042890906334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.349887135904282,
    "estimated_duration": 3600.0801943321235,
    "input_throughput": 4900.191953438616,
    "output_throughput": 4261.133411458883,
    "total_throughput": 9161.325364897499,
    "itl": 113.98679283051585,
    "ttft": 2097408.1930206614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.641553365862889,
    "arrivals": 564989,
    "finished_requests": 71563,
    "scheduler_time": 75.4082955402874
}
#Debug simulation 
Total elapsed time: 5.349980887956917. Arrivals time: 0.24312988109886646 Scheduler time: 4.944708288647234 Scheduler overhead time: 0.04612029017880559 Adapter cache time: 0.04569378262385726 Engine time: 0.048118894919753075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 377365268 . Total output tokens: 332810037
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.086984144989401,
    "estimated_duration": 3600.029382741248,
    "input_throughput": 4525.028622848566,
    "output_throughput": 3943.2386491219704,
    "total_throughput": 8468.267271970535,
    "itl": 87.97779852676379,
    "ttft": 2140980.180654389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.36026287071406,
    "arrivals": 564989,
    "finished_requests": 66025,
    "scheduler_time": 74.57855810461069
}
#Debug simulation 
Total elapsed time: 5.087077075149864. Arrivals time: 0.27240802999585867 Scheduler time: 4.614225805271417 Scheduler overhead time: 0.056682961992919445 Adapter cache time: 0.056670304387807846 Engine time: 0.059513701125979424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.537612162064761,
    "estimated_duration": 3600.013613592435,
    "input_throughput": 5091.108803255475,
    "output_throughput": 4460.02007864011,
    "total_throughput": 9551.128881895585,
    "itl": 123.33196862205446,
    "ttft": 2070997.510866609,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6764994217642166,
    "arrivals": 561196,
    "finished_requests": 74356,
    "scheduler_time": 77.35873225973906
}
#Debug simulation 
Total elapsed time: 5.537698839791119. Arrivals time: 0.303994320333004 Scheduler time: 5.088101907167584 Scheduler overhead time: 0.04316908912733197 Adapter cache time: 0.036842904053628445 Engine time: 0.044822868425399065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.463441404048353,
    "estimated_duration": 3600.09171651065,
    "input_throughput": 4963.130499719074,
    "output_throughput": 4350.3247231656105,
    "total_throughput": 9313.455222884684,
    "itl": 111.11123558425493,
    "ttft": 2085666.1304809414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9901022307900766,
    "arrivals": 561196,
    "finished_requests": 72483,
    "scheduler_time": 77.05248109850923
}
#Debug simulation 
Total elapsed time: 5.463531950954348. Arrivals time: 0.24723932147026062 Scheduler time: 5.056416776031256 Scheduler overhead time: 0.046986366622149944 Adapter cache time: 0.041182335931807756 Engine time: 0.04902686830610037 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.105988847091794,
    "estimated_duration": 3600.04906580642,
    "input_throughput": 4553.7059913175935,
    "output_throughput": 4004.798750366601,
    "total_throughput": 8558.504741684194,
    "itl": 86.38263429681828,
    "ttft": 2132859.1924189413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7692861242825497,
    "arrivals": 561196,
    "finished_requests": 66616,
    "scheduler_time": 75.77913713689092
}
#Debug simulation 
Total elapsed time: 5.106079683173448. Arrivals time: 0.28659651800990105 Scheduler time: 4.619592402130365 Scheduler overhead time: 0.058510547038167715 Adapter cache time: 0.05153352813795209 Engine time: 0.06111606629565358 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.493852554820478,
    "estimated_duration": 3600.031641220798,
    "input_throughput": 4963.508874588824,
    "output_throughput": 4350.582595070975,
    "total_throughput": 9314.0914696598,
    "itl": 111.10117305605021,
    "ttft": 2085560.6480174838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.680528953741296,
    "arrivals": 561196,
    "finished_requests": 72487,
    "scheduler_time": 77.05746787177364
}
#Debug simulation 
Total elapsed time: 5.493946956004947. Arrivals time: 0.24604589492082596 Scheduler time: 5.087690947577357 Scheduler overhead time: 0.047184376046061516 Adapter cache time: 0.041073767468333244 Engine time: 0.049177004024386406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.1166538670659065,
    "estimated_duration": 3600.006974417549,
    "input_throughput": 4553.759233383803,
    "output_throughput": 4004.8455745929846,
    "total_throughput": 8558.604807976786,
    "itl": 86.38157536877922,
    "ttft": 2132840.9317374113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7272413374297644,
    "arrivals": 561196,
    "finished_requests": 66616,
    "scheduler_time": 75.77909053487323
}
#Debug simulation 
Total elapsed time: 5.116746335756034. Arrivals time: 0.23798681516200304 Scheduler time: 4.6814929456450045 Scheduler overhead time: 0.0574166439473629 Adapter cache time: 0.05120668327435851 Engine time: 0.060475002974271774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.4434134610928595,
    "estimated_duration": 3600.052284804826,
    "input_throughput": 4963.480412609825,
    "output_throughput": 4350.557647761806,
    "total_throughput": 9314.038060371631,
    "itl": 111.09631261286592,
    "ttft": 2085469.3850909239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 543,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.466469070729784,
    "arrivals": 561196,
    "finished_requests": 72487,
    "scheduler_time": 77.06144313910208
}
#Debug simulation 
Total elapsed time: 5.443508401978761. Arrivals time: 0.24618329340592027 Scheduler time: 5.037485962267965 Scheduler overhead time: 0.04705813294276595 Adapter cache time: 0.04054715856909752 Engine time: 0.04935214715078473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374829532 . Total output tokens: 330624068
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.097415558993816,
    "estimated_duration": 3600.078958319029,
    "input_throughput": 4553.690402294571,
    "output_throughput": 4004.8857724865284,
    "total_throughput": 8558.5761747811,
    "itl": 86.38123113238221,
    "ttft": 2132849.705199994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6990734014101645,
    "arrivals": 561196,
    "finished_requests": 66618,
    "scheduler_time": 75.78109925435021
}
#Debug simulation 
Total elapsed time: 5.09750714013353. Arrivals time: 0.23594167875126004 Scheduler time: 4.664233370218426 Scheduler overhead time: 0.057575492188334465 Adapter cache time: 0.05124227935448289 Engine time: 0.06054931366816163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.589362381957471,
    "estimated_duration": 3600.1206706674675,
    "input_throughput": 5165.431023330738,
    "output_throughput": 4510.856853303517,
    "total_throughput": 9676.287876634255,
    "itl": 122.12848916791755,
    "ttft": 2063367.342535625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7456759844347813,
    "arrivals": 559351,
    "finished_requests": 75412,
    "scheduler_time": 78.23784527296463
}
#Debug simulation 
Total elapsed time: 5.589460001327097. Arrivals time: 0.30704140290617943 Scheduler time: 5.140722755808383 Scheduler overhead time: 0.04339832533150911 Adapter cache time: 0.0322383395396173 Engine time: 0.04517431044951081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.468417020048946,
    "estimated_duration": 3600.0606404764007,
    "input_throughput": 5027.821419587183,
    "output_throughput": 4393.5662700134135,
    "total_throughput": 9421.387689600597,
    "itl": 110.19071906673231,
    "ttft": 2079761.1981395972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8441884585609678,
    "arrivals": 559351,
    "finished_requests": 73403,
    "scheduler_time": 77.81465904620455
}
#Debug simulation 
Total elapsed time: 5.468513580970466. Arrivals time: 0.2987191234715283 Scheduler time: 5.013253767509013 Scheduler overhead time: 0.047223552130162716 Adapter cache time: 0.03708157129585743 Engine time: 0.049406621139496565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.1608963049948215,
    "estimated_duration": 3600.082319415274,
    "input_throughput": 4618.277451694612,
    "output_throughput": 4039.4979085817126,
    "total_throughput": 8657.775360276324,
    "itl": 85.63373904879917,
    "ttft": 2129240.40494938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.788350605554882,
    "arrivals": 559351,
    "finished_requests": 67381,
    "scheduler_time": 76.42132436883945
}
#Debug simulation 
Total elapsed time: 5.160986657254398. Arrivals time: 0.31795361498370767 Scheduler time: 4.647562832105905 Scheduler overhead time: 0.05823782039806247 Adapter cache time: 0.048284714575856924 Engine time: 0.06073302123695612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.5153813869692385,
    "estimated_duration": 3600.0597916918673,
    "input_throughput": 5028.606758637268,
    "output_throughput": 4394.21201739696,
    "total_throughput": 9422.818776034228,
    "itl": 110.18809446781455,
    "ttft": 2079711.4706948546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.722025012909432,
    "arrivals": 559351,
    "finished_requests": 73413,
    "scheduler_time": 77.81706936199473
}
#Debug simulation 
Total elapsed time: 5.515475039370358. Arrivals time: 0.24638505605980754 Scheduler time: 5.111941623035818 Scheduler overhead time: 0.04761319234967232 Adapter cache time: 0.03728477703407407 Engine time: 0.049263641238212585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.138406376820058,
    "estimated_duration": 3600.065291577904,
    "input_throughput": 4618.299295542156,
    "output_throughput": 4039.5170148778134,
    "total_throughput": 8657.816310419968,
    "itl": 85.6333427237018,
    "ttft": 2129231.8156390716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7713669970724764,
    "arrivals": 559351,
    "finished_requests": 67381,
    "scheduler_time": 76.42128013995192
}
#Debug simulation 
Total elapsed time: 5.1385083720088005. Arrivals time: 0.2927914229221642 Scheduler time: 4.650158885866404 Scheduler overhead time: 0.05787566024810076 Adapter cache time: 0.04844238702207804 Engine time: 0.061066349036991596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.483149507082999,
    "estimated_duration": 3600.0488284350313,
    "input_throughput": 5028.90290181705,
    "output_throughput": 4394.379285926815,
    "total_throughput": 9423.282187743866,
    "itl": 110.18792318472143,
    "ttft": 2079734.4136260527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6151319979643395,
    "arrivals": 559351,
    "finished_requests": 73417,
    "scheduler_time": 77.81903224466718
}
#Debug simulation 
Total elapsed time: 5.4832688719034195. Arrivals time: 0.3032905817963183 Scheduler time: 5.023313782643527 Scheduler overhead time: 0.04735245695337653 Adapter cache time: 0.0369066558778286 Engine time: 0.049443022813647985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373612207 . Total output tokens: 329507681
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.161034737247974,
    "estimated_duration": 3600.049912228561,
    "input_throughput": 4618.3190248347955,
    "output_throughput": 4039.5342716228206,
    "total_throughput": 8657.853296457617,
    "itl": 85.63301966594365,
    "ttft": 2129223.7721651793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7560403260029884,
    "arrivals": 559351,
    "finished_requests": 67381,
    "scheduler_time": 76.42122746167786
}
#Debug simulation 
Total elapsed time: 5.16112838499248. Arrivals time: 0.32380169443786144 Scheduler time: 4.642668582033366 Scheduler overhead time: 0.05795433558523655 Adapter cache time: 0.04803191823884845 Engine time: 0.0604735785163939 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.615033938083798,
    "estimated_duration": 3600.0356594190357,
    "input_throughput": 5196.586581317095,
    "output_throughput": 4533.778702245956,
    "total_throughput": 9730.365283563051,
    "itl": 121.59290245284366,
    "ttft": 2059119.1441721902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3687686696136354,
    "arrivals": 558425,
    "finished_requests": 75800,
    "scheduler_time": 78.61304414966558
}
#Debug simulation 
Total elapsed time: 5.615130033809692. Arrivals time: 0.30751598067581654 Scheduler time: 5.164607574697584 Scheduler overhead time: 0.04375582840293646 Adapter cache time: 0.03285751072689891 Engine time: 0.04537778394296765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.820403296966106,
    "estimated_duration": 3600.0805202496663,
    "input_throughput": 5058.589078095687,
    "output_throughput": 4414.529039172113,
    "total_throughput": 9473.1181172678,
    "itl": 109.77027127632763,
    "ttft": 2075578.9261268233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4605865405173988,
    "arrivals": 558425,
    "finished_requests": 73781,
    "scheduler_time": 78.15453384780191
}
#Debug simulation 
Total elapsed time: 5.820468944031745. Arrivals time: 0.6019581356085837 Scheduler time: 5.061344056390226 Scheduler overhead time: 0.047506514471024275 Adapter cache time: 0.0370601792819798 Engine time: 0.049719061236828566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.146288244985044,
    "estimated_duration": 3600.0223674644226,
    "input_throughput": 4643.13087359316,
    "output_throughput": 4055.338414545081,
    "total_throughput": 8698.469288138242,
    "itl": 85.49443226561608,
    "ttft": 2125380.384124996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.446368897673677,
    "arrivals": 558425,
    "finished_requests": 67752,
    "scheduler_time": 76.6811015404262
}
#Debug simulation 
Total elapsed time: 5.146383213810623. Arrivals time: 0.29069404816254973 Scheduler time: 4.659652494825423 Scheduler overhead time: 0.05796757573261857 Adapter cache time: 0.04853398725390434 Engine time: 0.06119986204430461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.497093711048365,
    "estimated_duration": 3600.006653313234,
    "input_throughput": 5058.555928845249,
    "output_throughput": 4414.523785775132,
    "total_throughput": 9473.07971462038,
    "itl": 109.77284179112131,
    "ttft": 2075588.5456144693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3564699675189311,
    "arrivals": 558425,
    "finished_requests": 73779,
    "scheduler_time": 78.15364103210389
}
#Debug simulation 
Total elapsed time: 5.497193395160139. Arrivals time: 0.29941842844709754 Scheduler time: 5.039660184178501 Scheduler overhead time: 0.0474417470395565 Adapter cache time: 0.03807101026177406 Engine time: 0.049590958282351494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.156402149237692,
    "estimated_duration": 3600.0785335413716,
    "input_throughput": 4643.019546453434,
    "output_throughput": 4055.0734835330045,
    "total_throughput": 8698.093029986438,
    "itl": 85.49412771428536,
    "ttft": 2125461.5522210263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4316635781340332,
    "arrivals": 558425,
    "finished_requests": 67750,
    "scheduler_time": 76.6847049248923
}
#Debug simulation 
Total elapsed time: 5.156491428148001. Arrivals time: 0.28903438756242394 Scheduler time: 4.672380439471453 Scheduler overhead time: 0.05793249560520053 Adapter cache time: 0.04818312404677272 Engine time: 0.060812536627054214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.511014200747013,
    "estimated_duration": 3600.052288156636,
    "input_throughput": 5058.712358126622,
    "output_throughput": 4414.584769305583,
    "total_throughput": 9473.297127432206,
    "itl": 109.76913031317238,
    "ttft": 2075533.8165244097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2704002671735326,
    "arrivals": 558425,
    "finished_requests": 73782,
    "scheduler_time": 78.15595994508443
}
#Debug simulation 
Total elapsed time: 5.511109406594187. Arrivals time: 0.2499246490187943 Scheduler time: 5.104037416167557 Scheduler overhead time: 0.04741589352488518 Adapter cache time: 0.037071784026920795 Engine time: 0.04973331559449434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372965049 . Total output tokens: 328937586
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.176099989097565,
    "estimated_duration": 3600.0653093918368,
    "input_throughput": 4643.036601695352,
    "output_throughput": 4055.088379067811,
    "total_throughput": 8698.124980763163,
    "itl": 85.49382396886257,
    "ttft": 2125454.080173552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.419236547537151,
    "arrivals": 558425,
    "finished_requests": 67750,
    "scheduler_time": 76.68463424989108
}
#Debug simulation 
Total elapsed time: 5.176192712970078. Arrivals time: 0.325279432348907 Scheduler time: 4.656016908120364 Scheduler overhead time: 0.05804496677592397 Adapter cache time: 0.04790077731013298 Engine time: 0.06075905729085207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.646133872214705,
    "estimated_duration": 3600.1006279803137,
    "input_throughput": 5231.63015322887,
    "output_throughput": 4541.151675855156,
    "total_throughput": 9772.781829084026,
    "itl": 121.44639351636722,
    "ttft": 2055739.6408628698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0116985818883393,
    "arrivals": 557951,
    "finished_requests": 76291,
    "scheduler_time": 78.74486526552495
}
#Debug simulation 
Total elapsed time: 5.6462288252078. Arrivals time: 0.3084897887893021 Scheduler time: 5.195683050435036 Scheduler overhead time: 0.04394775256514549 Adapter cache time: 0.0307783130556345 Engine time: 0.0462436405941844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.470763240009546,
    "estimated_duration": 3600.068126302978,
    "input_throughput": 5086.445410910794,
    "output_throughput": 4418.700547296597,
    "total_throughput": 9505.14595820739,
    "itl": 109.74998334839813,
    "ttft": 2070790.6193790417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0908573545236144,
    "arrivals": 557951,
    "finished_requests": 74167,
    "scheduler_time": 78.22549189679314
}
#Debug simulation 
Total elapsed time: 5.470855970866978. Arrivals time: 0.24743899377062917 Scheduler time: 5.068962126970291 Scheduler overhead time: 0.04729491984471679 Adapter cache time: 0.035106063820421696 Engine time: 0.049188893754035234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.153058758936822,
    "estimated_duration": 3600.0517162450533,
    "input_throughput": 4668.993204778686,
    "output_throughput": 4058.04559253325,
    "total_throughput": 8727.038797311936,
    "itl": 85.45879996409698,
    "ttft": 2122409.6335135465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0902487800363456,
    "arrivals": 557951,
    "finished_requests": 68109,
    "scheduler_time": 76.70764265093989
}
#Debug simulation 
Total elapsed time: 5.153153134975582. Arrivals time: 0.29242362082004547 Scheduler time: 4.665886907372624 Scheduler overhead time: 0.05788381537422538 Adapter cache time: 0.04788253549486399 Engine time: 0.060837734024971724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.518722470849752,
    "estimated_duration": 3600.110407803393,
    "input_throughput": 5086.652053866696,
    "output_throughput": 4418.775870184024,
    "total_throughput": 9505.42792405072,
    "itl": 109.74738403786526,
    "ttft": 2070781.9612304529,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.020058084884657,
    "arrivals": 557951,
    "finished_requests": 74170,
    "scheduler_time": 78.22779906093396
}
#Debug simulation 
Total elapsed time: 5.518811984919012. Arrivals time: 0.3046757164411247 Scheduler time: 5.058273582253605 Scheduler overhead time: 0.0475139357149601 Adapter cache time: 0.0354770733974874 Engine time: 0.0498853144235909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.160244742408395,
    "estimated_duration": 3600.041524500194,
    "input_throughput": 4669.006422733859,
    "output_throughput": 4058.0570808910993,
    "total_throughput": 8727.063503624959,
    "itl": 85.45853598491354,
    "ttft": 2122403.9985057176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0801000383822252,
    "arrivals": 557951,
    "finished_requests": 68109,
    "scheduler_time": 76.70759964773498
}
#Debug simulation 
Total elapsed time: 5.160339078400284. Arrivals time: 0.2910387758165598 Scheduler time: 4.674317019525915 Scheduler overhead time: 0.057802464347332716 Adapter cache time: 0.04824153706431389 Engine time: 0.060761071275919676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.479942525736988,
    "estimated_duration": 3600.04764364361,
    "input_throughput": 5086.740735871456,
    "output_throughput": 4418.85290826302,
    "total_throughput": 9505.593644134477,
    "itl": 109.74608985567154,
    "ttft": 2070741.0172051436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9575881410855781,
    "arrivals": 557951,
    "finished_requests": 74170,
    "scheduler_time": 78.22750484494983
}
#Debug simulation 
Total elapsed time: 5.480035289656371. Arrivals time: 0.24912988068535924 Scheduler time: 5.075658108107746 Scheduler overhead time: 0.04736954113468528 Adapter cache time: 0.03557888278737664 Engine time: 0.0494698672555387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372650759 . Total output tokens: 328656479
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.183103675954044,
    "estimated_duration": 3600.0321476372196,
    "input_throughput": 4669.018583912331,
    "output_throughput": 4058.0676507537087,
    "total_throughput": 8727.08623466604,
    "itl": 85.45834626816003,
    "ttft": 2122398.0821057595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0707797654345637,
    "arrivals": 557951,
    "finished_requests": 68109,
    "scheduler_time": 76.7075430577079
}
#Debug simulation 
Total elapsed time: 5.183191989082843. Arrivals time: 0.31813047267496586 Scheduler time: 4.669053886085749 Scheduler overhead time: 0.05829175189137459 Adapter cache time: 0.04823020752519369 Engine time: 0.06120153749361634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.223628530744463,
    "estimated_duration": 3600.1230566514832,
    "input_throughput": 5495.852971871177,
    "output_throughput": 4814.07822101504,
    "total_throughput": 10309.931192886217,
    "itl": 114.87040714513115,
    "ttft": 2007666.7593348105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.734992331206081,
    "arrivals": 518627,
    "finished_requests": 80434,
    "scheduler_time": 83.37413458512404
}
#Debug simulation 
Total elapsed time: 6.223695464897901. Arrivals time: 0.2630663919262588 Scheduler time: 5.805028660222888 Scheduler overhead time: 0.046063289511948824 Adapter cache time: 0.03925257921218872 Engine time: 0.04799811355769634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.841302439570427,
    "estimated_duration": 3600.099323635012,
    "input_throughput": 5369.967398699463,
    "output_throughput": 4706.921803171332,
    "total_throughput": 10076.889201870794,
    "itl": 103.3357259862272,
    "ttft": 2022979.3405614917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.411638361657921,
    "arrivals": 518627,
    "finished_requests": 78610,
    "scheduler_time": 83.20861410431723
}
#Debug simulation 
Total elapsed time: 5.841402984689921. Arrivals time: 0.30931906029582024 Scheduler time: 5.363442477304488 Scheduler overhead time: 0.05064061935991049 Adapter cache time: 0.041145621333271265 Engine time: 0.052532837726175785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.444097755011171,
    "estimated_duration": 3600.059656717939,
    "input_throughput": 4952.101548298536,
    "output_throughput": 4339.55892115485,
    "total_throughput": 9291.660469453385,
    "itl": 80.07548005591202,
    "ttft": 2074386.4225650856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.842358451699816,
    "arrivals": 518627,
    "finished_requests": 72405,
    "scheduler_time": 81.95512853976128
}
#Debug simulation 
Total elapsed time: 5.444188821129501. Arrivals time: 0.2960555227473378 Scheduler time: 4.946972261648625 Scheduler overhead time: 0.0618573990650475 Adapter cache time: 0.04450771398842335 Engine time: 0.06473969342187047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.843284485861659,
    "estimated_duration": 3600.0757218892263,
    "input_throughput": 5371.033970877952,
    "output_throughput": 4707.6101474627485,
    "total_throughput": 10078.6441183407,
    "itl": 103.31520826943023,
    "ttft": 2022876.1803060914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.726140531646113,
    "arrivals": 518627,
    "finished_requests": 78621,
    "scheduler_time": 83.22309835365841
}
#Debug simulation 
Total elapsed time: 5.843379735946655. Arrivals time: 0.2608597204089165 Scheduler time: 5.41409991402179 Scheduler overhead time: 0.050423738081008196 Adapter cache time: 0.041093433275818825 Engine time: 0.05261931847780943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.449774182867259,
    "estimated_duration": 3600.00434326438,
    "input_throughput": 4952.278191929591,
    "output_throughput": 4339.522820084752,
    "total_throughput": 9291.801012014344,
    "itl": 80.0764467724761,
    "ttft": 2074413.3691996718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.754385520620255,
    "arrivals": 518627,
    "finished_requests": 72405,
    "scheduler_time": 81.95527911628257
}
#Debug simulation 
Total elapsed time: 5.449866264127195. Arrivals time: 0.24674342712387443 Scheduler time: 5.00187000259757 Scheduler overhead time: 0.06168442498892546 Adapter cache time: 0.0446040122769773 Engine time: 0.06495174346491694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.8416501437313855,
    "estimated_duration": 3600.044276697494,
    "input_throughput": 5371.953374345971,
    "output_throughput": 4708.342925034614,
    "total_throughput": 10080.296299380585,
    "itl": 103.29707126728897,
    "ttft": 2022759.4113556084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.235258013335978,
    "arrivals": 518627,
    "finished_requests": 78630,
    "scheduler_time": 83.2313833424387
}
#Debug simulation 
Total elapsed time: 5.84176927478984. Arrivals time: 0.25811476772651076 Scheduler time: 5.414213033393025 Scheduler overhead time: 0.05053520994260907 Adapter cache time: 0.04165473207831383 Engine time: 0.05285860877484083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346294710 . Total output tokens: 305460991
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.469015066046268,
    "estimated_duration": 3600.02827647233,
    "input_throughput": 4952.548599832376,
    "output_throughput": 4339.79313498875,
    "total_throughput": 9292.341734821126,
    "itl": 80.07485771396773,
    "ttft": 2074420.042051804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.68355144621803,
    "arrivals": 518627,
    "finished_requests": 72410,
    "scheduler_time": 81.95740031336061
}
#Debug simulation 
Total elapsed time: 5.469107528217137. Arrivals time: 0.24553330289199948 Scheduler time: 5.0214589587412775 Scheduler overhead time: 0.06195254111662507 Adapter cache time: 0.04497190844267607 Engine time: 0.06506327074021101 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.075990880839527,
    "estimated_duration": 3600.0673750540723,
    "input_throughput": 5665.174530155473,
    "output_throughput": 4952.193707133668,
    "total_throughput": 10617.368237289142,
    "itl": 111.51872874084061,
    "ttft": 1998210.5018585343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 767,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.071717727505698,
    "arrivals": 514902,
    "finished_requests": 83043,
    "scheduler_time": 85.70893134479446
}
#Debug simulation 
Total elapsed time: 6.076087743975222. Arrivals time: 0.26555809332057834 Scheduler time: 5.657125431578606 Scheduler overhead time: 0.04716232977807522 Adapter cache time: 0.03418965544551611 Engine time: 0.04922068910673261 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.273301235865802,
    "estimated_duration": 3600.098486539371,
    "input_throughput": 5515.041900724642,
    "output_throughput": 4826.249355389679,
    "total_throughput": 10341.29125611432,
    "itl": 100.60348056608908,
    "ttft": 2014253.988648499,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 748,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.456789361443377,
    "arrivals": 514902,
    "finished_requests": 80838,
    "scheduler_time": 85.27243693143167
}
#Debug simulation 
Total elapsed time: 6.273367797024548. Arrivals time: 0.5473394887521863 Scheduler time: 5.559934187680483 Scheduler overhead time: 0.05167162185534835 Adapter cache time: 0.03542314749211073 Engine time: 0.05408034520223737 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.538040528073907,
    "estimated_duration": 3600.079027773724,
    "input_throughput": 5035.227243666872,
    "output_throughput": 4417.178311175535,
    "total_throughput": 9452.405554842406,
    "itl": 78.48779075431169,
    "ttft": 2067500.7241212064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.000082736457727,
    "arrivals": 514902,
    "finished_requests": 73807,
    "scheduler_time": 83.43434070774646
}
#Debug simulation 
Total elapsed time: 5.538131498731673. Arrivals time: 0.2948714345693588 Scheduler time: 5.045064014848322 Scheduler overhead time: 0.06307208305224776 Adapter cache time: 0.03867805935442448 Engine time: 0.06585894338786602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.004083393141627,
    "estimated_duration": 3600.0857191914547,
    "input_throughput": 5515.435894804049,
    "output_throughput": 4826.942288447177,
    "total_throughput": 10342.378183251225,
    "itl": 100.59108090062968,
    "ttft": 2014169.436167982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 749,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.071694967909705,
    "arrivals": 514902,
    "finished_requests": 80845,
    "scheduler_time": 85.2796097939219
}
#Debug simulation 
Total elapsed time: 6.004189240280539. Arrivals time: 0.27347614988684654 Scheduler time: 5.563740401994437 Scheduler overhead time: 0.051908817142248154 Adapter cache time: 0.035905192606151104 Engine time: 0.054063564632087946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.536075370386243,
    "estimated_duration": 3600.048260999037,
    "input_throughput": 5035.475828584968,
    "output_throughput": 4417.375225849577,
    "total_throughput": 9452.851054434544,
    "itl": 78.48746896737497,
    "ttft": 2067463.382189372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.93960601740054,
    "arrivals": 514902,
    "finished_requests": 73812,
    "scheduler_time": 83.43462107860643
}
#Debug simulation 
Total elapsed time: 5.536169117316604. Arrivals time: 0.248998305760324 Scheduler time: 5.08900395873934 Scheduler overhead time: 0.06302613578736782 Adapter cache time: 0.038402105681598186 Engine time: 0.0661227977834642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.269181335344911,
    "estimated_duration": 3600.1078740875632,
    "input_throughput": 5515.649445652425,
    "output_throughput": 4827.182853348388,
    "total_throughput": 10342.832299000813,
    "itl": 100.58449921747936,
    "ttft": 2014147.939037313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 748,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.775172863546736,
    "arrivals": 514902,
    "finished_requests": 80849,
    "scheduler_time": 85.28617759504124
}
#Debug simulation 
Total elapsed time: 6.269245070405304. Arrivals time: 0.5467502544634044 Scheduler time: 5.556331800296903 Scheduler overhead time: 0.0515175829641521 Adapter cache time: 0.035954358987510204 Engine time: 0.053789978846907616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343731507 . Total output tokens: 303266248
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.510914919897914,
    "estimated_duration": 3600.0681554235985,
    "input_throughput": 5035.322448740431,
    "output_throughput": 4417.349703794376,
    "total_throughput": 9452.672152534808,
    "itl": 78.48758875353897,
    "ttft": 2067513.631575111,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 666,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.895336307100981,
    "arrivals": 514902,
    "finished_requests": 73810,
    "scheduler_time": 83.4359374599302
}
#Debug simulation 
Total elapsed time: 5.511007247958332. Arrivals time: 0.2969314050860703 Scheduler time: 5.016693697310984 Scheduler overhead time: 0.06262364657595754 Adapter cache time: 0.038280952256172895 Engine time: 0.06598573736846447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.141081092879176,
    "estimated_duration": 3600.0235068706806,
    "input_throughput": 5768.444000536903,
    "output_throughput": 5002.584001362447,
    "total_throughput": 10771.02800189935,
    "itl": 110.10306813463,
    "ttft": 1984299.2795103486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9623592463136004,
    "arrivals": 512974,
    "finished_requests": 83942,
    "scheduler_time": 86.63393600063127
}
#Debug simulation 
Total elapsed time: 6.1411713026463985. Arrivals time: 0.3164060520939529 Scheduler time: 5.671494355425239 Scheduler overhead time: 0.047733211889863014 Adapter cache time: 0.032490589655935764 Engine time: 0.049917551688849926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.989791638217866,
    "estimated_duration": 3600.04241989386,
    "input_throughput": 5610.932218014127,
    "output_throughput": 4867.565977324412,
    "total_throughput": 10478.49819533854,
    "itl": 99.43459702502099,
    "ttft": 2000171.185203587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.196522196209994,
    "arrivals": 512974,
    "finished_requests": 81610,
    "scheduler_time": 86.07486427901623
}
#Debug simulation 
Total elapsed time: 5.98991517489776. Arrivals time: 0.26023302413523197 Scheduler time: 5.564546083565801 Scheduler overhead time: 0.052139539271593094 Adapter cache time: 0.03315255418419838 Engine time: 0.05462155304849148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.574781707953662,
    "estimated_duration": 3600.027959737634,
    "input_throughput": 5130.176822667228,
    "output_throughput": 4444.996033076981,
    "total_throughput": 9575.17285574421,
    "itl": 77.81360601389433,
    "ttft": 2057252.6122466915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.015892632943599,
    "arrivals": 512974,
    "finished_requests": 74569,
    "scheduler_time": 83.9991089040395
}
#Debug simulation 
Total elapsed time: 5.574900719802827. Arrivals time: 0.25092391297221184 Scheduler time: 5.125058939680457 Scheduler overhead time: 0.06374332075938582 Adapter cache time: 0.036751268431544304 Engine time: 0.067398551851511 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.028170384000987,
    "estimated_duration": 3600.070882904076,
    "input_throughput": 5611.197295011219,
    "output_throughput": 4867.722211587058,
    "total_throughput": 10478.919506598277,
    "itl": 99.4316754342014,
    "ttft": 2000103.8199476849,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9710732146073076,
    "arrivals": 512974,
    "finished_requests": 81613,
    "scheduler_time": 86.08068895537652
}
#Debug simulation 
Total elapsed time: 6.0282877259887755. Arrivals time: 0.2625864716246724 Scheduler time: 5.600336394738406 Scheduler overhead time: 0.05218540085479617 Adapter cache time: 0.033029531594365835 Engine time: 0.054883251432329416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.589079758152366,
    "estimated_duration": 3600.01998645632,
    "input_throughput": 5130.188184921647,
    "output_throughput": 4445.005877801162,
    "total_throughput": 9575.194062722809,
    "itl": 77.81298011240644,
    "ttft": 2057222.8110136015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.984617939274779,
    "arrivals": 512974,
    "finished_requests": 74569,
    "scheduler_time": 84.00060106678222
}
#Debug simulation 
Total elapsed time: 5.58917159633711. Arrivals time: 0.29494354873895645 Scheduler time: 5.096939025446773 Scheduler overhead time: 0.06352570839226246 Adapter cache time: 0.03624363476410508 Engine time: 0.0666652019135654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.012370329815894,
    "estimated_duration": 3600.0107008252453,
    "input_throughput": 5611.534431097415,
    "output_throughput": 4867.819419531963,
    "total_throughput": 10479.353850629377,
    "itl": 99.42811830949633,
    "ttft": 2000056.4567841599,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7961573719698816,
    "arrivals": 512974,
    "finished_requests": 81615,
    "scheduler_time": 86.08254007352134
}
#Debug simulation 
Total elapsed time: 6.012467821128666. Arrivals time: 0.30981140676885843 Scheduler time: 5.536940633319318 Scheduler overhead time: 0.05243736878037453 Adapter cache time: 0.033102250192314386 Engine time: 0.05466359807178378 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342451317 . Total output tokens: 302156828
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.5861455369740725,
    "estimated_duration": 3600.073007185597,
    "input_throughput": 5130.445955716644,
    "output_throughput": 4445.212629871254,
    "total_throughput": 9575.658585587897,
    "itl": 77.81356831890801,
    "ttft": 2056982.9916683508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9542250798084044,
    "arrivals": 512974,
    "finished_requests": 74573,
    "scheduler_time": 84.00248618833793
}
#Debug simulation 
Total elapsed time: 5.586234568618238. Arrivals time: 0.29681304283440113 Scheduler time: 5.091573144309223 Scheduler overhead time: 0.06339111411944032 Adapter cache time: 0.03654427453875542 Engine time: 0.0668951841071248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.2111757081002,
    "estimated_duration": 3600.0143003693347,
    "input_throughput": 5757.487962721026,
    "output_throughput": 5040.981086696847,
    "total_throughput": 10798.469049417874,
    "itl": 109.51018581529415,
    "ttft": 1981455.1550424953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6464898489555324,
    "arrivals": 512003,
    "finished_requests": 84268,
    "scheduler_time": 87.26679118034517
}
#Debug simulation 
Total elapsed time: 6.211285405326635. Arrivals time: 0.3283020304515958 Scheduler time: 5.731381755787879 Scheduler overhead time: 0.04801708273589611 Adapter cache time: 0.029975940473377705 Engine time: 0.05022219009697437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.038876422215253,
    "estimated_duration": 3600.0724157011514,
    "input_throughput": 5592.600002208217,
    "output_throughput": 4899.315614617946,
    "total_throughput": 10491.915616826163,
    "itl": 99.02347584063517,
    "ttft": 1997415.0752804212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8078303378168519,
    "arrivals": 512003,
    "finished_requests": 81893,
    "scheduler_time": 86.6197675723554
}
#Debug simulation 
Total elapsed time: 6.038998813834041. Arrivals time: 0.26450764667242765 Scheduler time: 5.610788999125361 Scheduler overhead time: 0.052176440600305796 Adapter cache time: 0.031076074112206697 Engine time: 0.05501345777884126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.605816242285073,
    "estimated_duration": 3600.0276907801403,
    "input_throughput": 5095.815525803511,
    "output_throughput": 4469.903395802172,
    "total_throughput": 9565.718921605683,
    "itl": 77.63659542093741,
    "ttft": 2054172.9238382336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7858689099643423,
    "arrivals": 512003,
    "finished_requests": 74675,
    "scheduler_time": 84.38171318622472
}
#Debug simulation 
Total elapsed time: 5.605908565223217. Arrivals time: 0.3013129401952028 Scheduler time: 5.108249023091048 Scheduler overhead time: 0.0636314763687551 Adapter cache time: 0.03490525996312499 Engine time: 0.0667402190156281 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.058469383977354,
    "estimated_duration": 3600.098100819475,
    "input_throughput": 5592.839816064125,
    "output_throughput": 4899.38537952204,
    "total_throughput": 10492.225195586167,
    "itl": 99.02028884659117,
    "ttft": 1997376.9055720088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6801140082720643,
    "arrivals": 512003,
    "finished_requests": 81896,
    "scheduler_time": 86.62254021308986
}
#Debug simulation 
Total elapsed time: 6.05856161005795. Arrivals time: 0.2638998394832015 Scheduler time: 5.631246702745557 Scheduler overhead time: 0.052338764537125826 Adapter cache time: 0.030972065404057503 Engine time: 0.05481470888480544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.587428102269769,
    "estimated_duration": 3600.0798164209355,
    "input_throughput": 5096.06312513347,
    "output_throughput": 4470.1867238041095,
    "total_throughput": 9566.24984893758,
    "itl": 77.63425731274042,
    "ttft": 2054075.1482243654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7676425984222488,
    "arrivals": 512003,
    "finished_requests": 74680,
    "scheduler_time": 84.38251430721331
}
#Debug simulation 
Total elapsed time: 5.587561368942261. Arrivals time: 0.2503606891259551 Scheduler time: 5.14119873335585 Scheduler overhead time: 0.06343102548271418 Adapter cache time: 0.034467312041670084 Engine time: 0.0672396975569427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.019864603877068,
    "estimated_duration": 3600.025552409151,
    "input_throughput": 5592.710859101074,
    "output_throughput": 4899.391613539139,
    "total_throughput": 10492.102472640214,
    "itl": 99.01910255185932,
    "ttft": 1997310.4981335122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.570444551380346,
    "arrivals": 512003,
    "finished_requests": 81894,
    "scheduler_time": 86.62387177090774
}
#Debug simulation 
Total elapsed time: 6.019958260003477. Arrivals time: 0.26446445751935244 Scheduler time: 5.591983463615179 Scheduler overhead time: 0.05221554730087519 Adapter cache time: 0.03127800580114126 Engine time: 0.05468086013570428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341808377 . Total output tokens: 301573848
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.588400446809828,
    "estimated_duration": 3600.0779864029055,
    "input_throughput": 5095.64711355865,
    "output_throughput": 4469.84039256284,
    "total_throughput": 9565.48750612149,
    "itl": 77.63631849009356,
    "ttft": 2054112.538989295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7523159273527606,
    "arrivals": 512003,
    "finished_requests": 74674,
    "scheduler_time": 84.38294483647137
}
#Debug simulation 
Total elapsed time: 5.58849470783025. Arrivals time: 0.2973387655802071 Scheduler time: 5.095082635991275 Scheduler overhead time: 0.06351000256836414 Adapter cache time: 0.034659835044294596 Engine time: 0.06679477635771036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.146618579979986,
    "estimated_duration": 3600.101851072144,
    "input_throughput": 5758.345696199185,
    "output_throughput": 5045.1703177761065,
    "total_throughput": 10803.516013975292,
    "itl": 109.48846481678075,
    "ttft": 1985723.5598391825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 164,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0844350812397885,
    "arrivals": 511517,
    "finished_requests": 84314,
    "scheduler_time": 87.33475683946763
}
#Debug simulation 
Total elapsed time: 6.146726626902819. Arrivals time: 0.2855223356746137 Scheduler time: 5.710821265820414 Scheduler overhead time: 0.04785628756508231 Adapter cache time: 0.028495939914137125 Engine time: 0.05068268580362201 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.018605187069625,
    "estimated_duration": 3600.0842750975266,
    "input_throughput": 5598.390054203386,
    "output_throughput": 4904.781846953195,
    "total_throughput": 10503.17190115658,
    "itl": 98.9519367386113,
    "ttft": 2003425.818958033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1846802414162096,
    "arrivals": 511517,
    "finished_requests": 81968,
    "scheduler_time": 86.69088519797569
}
#Debug simulation 
Total elapsed time: 6.018697868101299. Arrivals time: 0.2898330823518336 Scheduler time: 5.5665083816275 Scheduler overhead time: 0.05239246552810073 Adapter cache time: 0.029992814175784588 Engine time: 0.05450813099741936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.570340421050787,
    "estimated_duration": 3600.0804781344773,
    "input_throughput": 5097.380492313127,
    "output_throughput": 4471.276988880334,
    "total_throughput": 9568.65748119346,
    "itl": 77.57270556833835,
    "ttft": 2060916.2997843365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1685278181824845,
    "arrivals": 511517,
    "finished_requests": 74634,
    "scheduler_time": 84.42086816878265
}
#Debug simulation 
Total elapsed time: 5.5704329130239785. Arrivals time: 0.25377320498228073 Scheduler time: 5.121769695542753 Scheduler overhead time: 0.06380243971943855 Adapter cache time: 0.033180218655616045 Engine time: 0.06682107830420136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.9665230880491436,
    "estimated_duration": 3600.0220160467993,
    "input_throughput": 5598.749927127663,
    "output_throughput": 4905.059447217127,
    "total_throughput": 10503.80937434479,
    "itl": 98.94908051581972,
    "ttft": 2003304.7347780918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.104163424964062,
    "arrivals": 511517,
    "finished_requests": 81972,
    "scheduler_time": 86.6903961416717
}
#Debug simulation 
Total elapsed time: 5.966629763133824. Arrivals time: 0.27620353316888213 Scheduler time: 5.528367448598146 Scheduler overhead time: 0.05222764890640974 Adapter cache time: 0.029841286595910788 Engine time: 0.05466090841218829 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.511283248197287,
    "estimated_duration": 3600.0132450480537,
    "input_throughput": 5097.395690208093,
    "output_throughput": 4471.334382489234,
    "total_throughput": 9568.730072697326,
    "itl": 77.5741028649678,
    "ttft": 2060867.9213284717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1567221391154465,
    "arrivals": 511517,
    "finished_requests": 74632,
    "scheduler_time": 84.41970446140405
}
#Debug simulation 
Total elapsed time: 5.511402411852032. Arrivals time: 0.2510170675814152 Scheduler time: 5.0666464027017355 Scheduler overhead time: 0.06314819864928722 Adapter cache time: 0.03292142599821091 Engine time: 0.06674391031265259 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.999855981674045,
    "estimated_duration": 3600.049818234729,
    "input_throughput": 5598.542802911251,
    "output_throughput": 4904.709626673483,
    "total_throughput": 10503.252429584734,
    "itl": 98.95093147046536,
    "ttft": 2003343.0479867526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0278112714318537,
    "arrivals": 511517,
    "finished_requests": 81968,
    "scheduler_time": 86.6918094432424
}
#Debug simulation 
Total elapsed time: 5.9999511279165745. Arrivals time: 0.2692180476151407 Scheduler time: 5.567857301328331 Scheduler overhead time: 0.052313366904854774 Adapter cache time: 0.030232053250074387 Engine time: 0.05485521536320448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341488875 . Total output tokens: 301308540
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.597421175800264,
    "estimated_duration": 3600.059494337526,
    "input_throughput": 5097.469924834379,
    "output_throughput": 4471.339994608103,
    "total_throughput": 9568.809919442481,
    "itl": 77.57199973964698,
    "ttft": 2060802.6047536659,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1457449287548673,
    "arrivals": 511517,
    "finished_requests": 74633,
    "scheduler_time": 84.42182470128327
}
#Debug simulation 
Total elapsed time: 5.597519378643483. Arrivals time: 0.2536480384878814 Scheduler time: 5.149822061881423 Scheduler overhead time: 0.06352326646447182 Adapter cache time: 0.032697767950594425 Engine time: 0.06696450430899858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.297106551006436,
    "estimated_duration": 3600.111291740858,
    "input_throughput": 5915.727674546849,
    "output_throughput": 5149.841906980286,
    "total_throughput": 11065.569581527134,
    "itl": 107.42889621449069,
    "ttft": 1956961.2016383298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.589011868173337,
    "arrivals": 507129,
    "finished_requests": 86548,
    "scheduler_time": 89.10510239806067
}
#Debug simulation 
Total elapsed time: 6.297234528698027. Arrivals time: 0.3238586541265249 Scheduler time: 5.821194775868207 Scheduler overhead time: 0.048905638977885246 Adapter cache time: 0.02868064120411873 Engine time: 0.05088404379785061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.165448608342558,
    "estimated_duration": 3600.082718504135,
    "input_throughput": 5752.082554537619,
    "output_throughput": 5011.1790229909075,
    "total_throughput": 10763.261577528527,
    "itl": 97.11666063477801,
    "ttft": 1975887.8760249254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 666,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.852791027864447,
    "arrivals": 507129,
    "finished_requests": 84194,
    "scheduler_time": 88.49167466368861
}
#Debug simulation 
Total elapsed time: 6.165567649062723. Arrivals time: 0.3201503190211952 Scheduler time: 5.681369382888079 Scheduler overhead time: 0.05319092469289899 Adapter cache time: 0.029402079060673714 Engine time: 0.055637261364609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.955223672091961,
    "estimated_duration": 3600.0530775327784,
    "input_throughput": 5227.440705651277,
    "output_throughput": 4556.453654078284,
    "total_throughput": 9783.894359729562,
    "itl": 76.30596867079446,
    "ttft": 2034360.6029275237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.57050506926145,
    "arrivals": 507129,
    "finished_requests": 76508,
    "scheduler_time": 85.94416401338978
}
#Debug simulation 
Total elapsed time: 5.955322857014835. Arrivals time: 0.5444575375877321 Scheduler time: 5.215921108610928 Scheduler overhead time: 0.06552421255037189 Adapter cache time: 0.030796115286648273 Engine time: 0.06738879671320319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.1574769909493625,
    "estimated_duration": 3600.087499110534,
    "input_throughput": 5752.545460385809,
    "output_throughput": 5011.688189372544,
    "total_throughput": 10764.233649758353,
    "itl": 97.1061135176922,
    "ttft": 1975899.365665269,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 666,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.515453331349408,
    "arrivals": 507129,
    "finished_requests": 84202,
    "scheduler_time": 88.49933791862489
}
#Debug simulation 
Total elapsed time: 6.157594913151115. Arrivals time: 0.2835408099927008 Scheduler time: 5.709778115153313 Scheduler overhead time: 0.05329892970621586 Adapter cache time: 0.029623860958963633 Engine time: 0.055579131469130516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.6813460271805525,
    "estimated_duration": 3600.0012017955823,
    "input_throughput": 5227.485477119735,
    "output_throughput": 4556.339312280984,
    "total_throughput": 9783.82478940072,
    "itl": 76.30277330654948,
    "ttft": 2034288.4268530775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.518364906250532,
    "arrivals": 507129,
    "finished_requests": 76507,
    "scheduler_time": 85.94514162508361
}
#Debug simulation 
Total elapsed time: 5.68143851403147. Arrivals time: 0.2545802239328623 Scheduler time: 5.232490656431764 Scheduler overhead time: 0.06450276682153344 Adapter cache time: 0.030858897138386965 Engine time: 0.06751052057370543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.179636110085994,
    "estimated_duration": 3600.0033703436757,
    "input_throughput": 5752.742392022517,
    "output_throughput": 5011.914752256892,
    "total_throughput": 10764.657144279408,
    "itl": 97.09732696403898,
    "ttft": 1975856.9772866587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2580752673605256,
    "arrivals": 507129,
    "finished_requests": 84204,
    "scheduler_time": 88.5034801532611
}
#Debug simulation 
Total elapsed time: 6.1797312828712165. Arrivals time: 0.2701729633845389 Scheduler time: 5.745299004949629 Scheduler overhead time: 0.05335449753329158 Adapter cache time: 0.02933777030557394 Engine time: 0.05577991297468543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338506703 . Total output tokens: 298681055
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.687764980830252,
    "estimated_duration": 3600.0187675086413,
    "input_throughput": 5227.4905258406625,
    "output_throughput": 4556.497079417136,
    "total_throughput": 9783.987605257798,
    "itl": 76.30213982918113,
    "ttft": 2034328.679461692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 609,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.475337899010662,
    "arrivals": 507129,
    "finished_requests": 76508,
    "scheduler_time": 85.94645168468504
}
#Debug simulation 
Total elapsed time: 5.687858120072633. Arrivals time: 0.2983973170630634 Scheduler time: 5.195055103395134 Scheduler overhead time: 0.06447943905368447 Adapter cache time: 0.03080719616264105 Engine time: 0.067649164237082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.365120606031269,
    "estimated_duration": 3600.02049995149,
    "input_throughput": 5957.7191297352365,
    "output_throughput": 5214.107808623016,
    "total_throughput": 11171.826938358252,
    "itl": 105.68851746745817,
    "ttft": 1965088.3408279272,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.810273838578747,
    "arrivals": 505238,
    "finished_requests": 87006,
    "scheduler_time": 90.2496256566574
}
#Debug simulation 
Total elapsed time: 6.365211280994117. Arrivals time: 0.32106358744204044 Scheduler time: 5.89366654958576 Scheduler overhead time: 0.04960614163428545 Adapter cache time: 0.025038059800863266 Engine time: 0.05190487019717693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.194419633131474,
    "estimated_duration": 3600.083136580205,
    "input_throughput": 5770.678123765369,
    "output_throughput": 5058.878450596096,
    "total_throughput": 10829.556574361464,
    "itl": 95.67722962158336,
    "ttft": 1982034.0269758783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.999716059537607,
    "arrivals": 505238,
    "finished_requests": 84262,
    "scheduler_time": 89.43208190898885
}
#Debug simulation 
Total elapsed time: 6.194516073912382. Arrivals time: 0.3165259175002575 Scheduler time: 5.715697249863297 Scheduler overhead time: 0.05384190287441015 Adapter cache time: 0.026047394145280123 Engine time: 0.05629612412303686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.7096280520781875,
    "estimated_duration": 3600.0833483420547,
    "input_throughput": 5227.758131951955,
    "output_throughput": 4585.915214325023,
    "total_throughput": 9813.673346276979,
    "itl": 75.45720216343925,
    "ttft": 2042988.8635429244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 373,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7748512795288285,
    "arrivals": 505238,
    "finished_requests": 76243,
    "scheduler_time": 86.59123587681385
}
#Debug simulation 
Total elapsed time: 5.709723245818168. Arrivals time: 0.3010626873001456 Scheduler time: 5.215222331229597 Scheduler overhead time: 0.06532904179766774 Adapter cache time: 0.027759112883359194 Engine time: 0.0685740178450942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.174467775039375,
    "estimated_duration": 3600.0623105084146,
    "input_throughput": 5771.151776832958,
    "output_throughput": 5059.121600989142,
    "total_throughput": 10830.273377822099,
    "itl": 95.6727441516451,
    "ttft": 1981970.8267119182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7998122393805476,
    "arrivals": 505238,
    "finished_requests": 84267,
    "scheduler_time": 89.43615711171037
}
#Debug simulation 
Total elapsed time: 6.174562914296985. Arrivals time: 0.2680288436822593 Scheduler time: 5.744696643203497 Scheduler overhead time: 0.05377210071310401 Adapter cache time: 0.02571105770766735 Engine time: 0.05623316019773483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.7148373778909445,
    "estimated_duration": 3600.056999214681,
    "input_throughput": 5227.796394364169,
    "output_throughput": 4585.948779033619,
    "total_throughput": 9813.745173397787,
    "itl": 75.45662382453219,
    "ttft": 2042976.8552981878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 373,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.748547398098761,
    "arrivals": 505238,
    "finished_requests": 76243,
    "scheduler_time": 86.59119063087053
}
#Debug simulation 
Total elapsed time: 5.714962631929666. Arrivals time: 0.2518388545140624 Scheduler time: 5.268160250969231 Scheduler overhead time: 0.06573710637167096 Adapter cache time: 0.0274036955088377 Engine time: 0.07006609765812755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.214775355067104,
    "estimated_duration": 3600.0459280500445,
    "input_throughput": 5771.452202348451,
    "output_throughput": 5059.247677394302,
    "total_throughput": 10830.699879742751,
    "itl": 95.66939214467358,
    "ttft": 1981924.42638586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6429432693961896,
    "arrivals": 505238,
    "finished_requests": 84268,
    "scheduler_time": 89.43807325213862
}
#Debug simulation 
Total elapsed time: 6.214868258219212. Arrivals time: 0.3176910290494561 Scheduler time: 5.734304268844426 Scheduler overhead time: 0.05412423238158226 Adapter cache time: 0.026158601511269808 Engine time: 0.05640251701697707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337255034 . Total output tokens: 297546279
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.991688983980566,
    "estimated_duration": 3600.0124734479114,
    "input_throughput": 5227.800219807296,
    "output_throughput": 4585.953277041855,
    "total_throughput": 9813.75349684915,
    "itl": 75.45486681881808,
    "ttft": 2042913.5230354934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.720018593221919,
    "arrivals": 505238,
    "finished_requests": 76241,
    "scheduler_time": 86.59017895127747
}
#Debug simulation 
Total elapsed time: 5.99175616633147. Arrivals time: 0.5906769325956702 Scheduler time: 5.208264775574207 Scheduler overhead time: 0.06529754726216197 Adapter cache time: 0.027478057891130447 Engine time: 0.06849453831091523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.445337688084692,
    "estimated_duration": 3600.0896083535376,
    "input_throughput": 6093.067225077213,
    "output_throughput": 5261.929857535843,
    "total_throughput": 11354.997082613056,
    "itl": 104.85838892117467,
    "ttft": 1948669.3347981616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8580869379779301,
    "arrivals": 504263,
    "finished_requests": 88554,
    "scheduler_time": 91.05578318924485
}
#Debug simulation 
Total elapsed time: 6.445445311255753. Arrivals time: 0.3385632471181452 Scheduler time: 5.957012999802828 Scheduler overhead time: 0.050168875604867935 Adapter cache time: 0.023422360885888338 Engine time: 0.05199654260650277 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.259990071412176,
    "estimated_duration": 3600.0613636282956,
    "input_throughput": 5907.254030405397,
    "output_throughput": 5101.5410974801,
    "total_throughput": 11008.795127885498,
    "itl": 95.00527275640309,
    "ttft": 1967840.9652498483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0060152177792068,
    "arrivals": 504263,
    "finished_requests": 85795,
    "scheduler_time": 90.13816315059103
}
#Debug simulation 
Total elapsed time: 6.26010866323486. Arrivals time: 0.27398156048730016 Scheduler time: 5.824362702667713 Scheduler overhead time: 0.05435724463313818 Adapter cache time: 0.023872571531683207 Engine time: 0.057057961355894804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.754277840722352,
    "estimated_duration": 3600.073973371845,
    "input_throughput": 5331.595445529883,
    "output_throughput": 4617.836778623016,
    "total_throughput": 9949.432224152899,
    "itl": 75.0532865814637,
    "ttft": 2030460.5832986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9337199262809055,
    "arrivals": 504263,
    "finished_requests": 77512,
    "scheduler_time": 87.11999973593207
}
#Debug simulation 
Total elapsed time: 5.754371454939246. Arrivals time: 0.30353460693731904 Scheduler time: 5.259048679843545 Scheduler overhead time: 0.06541500007733703 Adapter cache time: 0.025672879070043564 Engine time: 0.0687755779363215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.273242832161486,
    "estimated_duration": 3600.0962539730112,
    "input_throughput": 5907.595936227832,
    "output_throughput": 5101.56526502074,
    "total_throughput": 11009.161201248571,
    "itl": 95.00322413388847,
    "ttft": 1967734.2125639408,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.863028457527977,
    "arrivals": 504263,
    "finished_requests": 85798,
    "scheduler_time": 90.14178918264744
}
#Debug simulation 
Total elapsed time: 6.273336539976299. Arrivals time: 0.2769361878745258 Scheduler time: 5.833453867118806 Scheduler overhead time: 0.05426452960819006 Adapter cache time: 0.024199591483920813 Engine time: 0.05763748101890087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.767353934701532,
    "estimated_duration": 3600.0292756715985,
    "input_throughput": 5331.170257336197,
    "output_throughput": 4617.690781666981,
    "total_throughput": 9948.861039003179,
    "itl": 75.05504749988143,
    "ttft": 2030460.8664296928,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.91321532579605,
    "arrivals": 504263,
    "finished_requests": 77507,
    "scheduler_time": 87.1186222626284
}
#Debug simulation 
Total elapsed time: 5.767483236733824. Arrivals time: 0.30566170951351523 Scheduler time: 5.269610149320215 Scheduler overhead time: 0.06549638137221336 Adapter cache time: 0.02600859198719263 Engine time: 0.06887309951707721 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.228679020423442,
    "estimated_duration": 3600.086167695789,
    "input_throughput": 5907.346382659439,
    "output_throughput": 5101.508448547762,
    "total_throughput": 11008.8548312072,
    "itl": 95.00139318770513,
    "ttft": 1967805.4590107573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.74919433771632,
    "arrivals": 504263,
    "finished_requests": 85796,
    "scheduler_time": 90.14332452315479
}
#Debug simulation 
Total elapsed time: 6.228776475414634. Arrivals time: 0.32027521869167686 Scheduler time: 5.747139067854732 Scheduler overhead time: 0.05439168680459261 Adapter cache time: 0.024155728053301573 Engine time: 0.05665656318888068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336603964 . Total output tokens: 296965162
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.727890694048256,
    "estimated_duration": 3600.053605707134,
    "input_throughput": 5331.209782424926,
    "output_throughput": 4617.67151845915,
    "total_throughput": 9948.881300884075,
    "itl": 75.05168784260611,
    "ttft": 2030499.3475527419,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9055504937470007,
    "arrivals": 504263,
    "finished_requests": 77507,
    "scheduler_time": 87.1193275009142
}
#Debug simulation 
Total elapsed time: 5.727983204182237. Arrivals time: 0.25576590932905674 Scheduler time: 5.280492211692035 Scheduler overhead time: 0.06535891816020012 Adapter cache time: 0.02585517056286335 Engine time: 0.06872055260464549 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.393207817804068,
    "estimated_duration": 3600.051528990323,
    "input_throughput": 6012.364774698252,
    "output_throughput": 5270.7159459247305,
    "total_throughput": 11283.080720622982,
    "itl": 104.38885747002116,
    "ttft": 1951786.6236482246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 169,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1174971263995381,
    "arrivals": 503772,
    "finished_requests": 87924,
    "scheduler_time": 91.26155443176242
}
#Debug simulation 
Total elapsed time: 6.39329937286675. Arrivals time: 0.281057215295732 Scheduler time: 5.963379837572575 Scheduler overhead time: 0.050053165294229984 Adapter cache time: 0.022193825338035822 Engine time: 0.05240025697275996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.233343153260648,
    "estimated_duration": 3600.0545313198736,
    "input_throughput": 5830.406405623139,
    "output_throughput": 5108.037625546199,
    "total_throughput": 10938.444031169338,
    "itl": 94.58966220752617,
    "ttft": 1971256.7857090211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1977219364186753,
    "arrivals": 503772,
    "finished_requests": 85220,
    "scheduler_time": 90.33132152835825
}
#Debug simulation 
Total elapsed time: 6.233469035942107. Arrivals time: 0.2946275086142123 Scheduler time: 5.778545797336847 Scheduler overhead time: 0.0544991223141551 Adapter cache time: 0.022728149313479662 Engine time: 0.05669673951342702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.710995167028159,
    "estimated_duration": 3600.031958906724,
    "input_throughput": 5272.754024596097,
    "output_throughput": 4619.697044314741,
    "total_throughput": 9892.451068910837,
    "itl": 74.89025235859557,
    "ttft": 2035568.1409742031,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1450724872574216,
    "arrivals": 503772,
    "finished_requests": 77142,
    "scheduler_time": 87.20408546118854
}
#Debug simulation 
Total elapsed time: 5.7110961130820215. Arrivals time: 0.251785181928426 Scheduler time: 5.268398021347821 Scheduler overhead time: 0.06549474550411105 Adapter cache time: 0.02462794678285718 Engine time: 0.0688479314558208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.510494157206267,
    "estimated_duration": 3600.003383924385,
    "input_throughput": 5830.5375749727,
    "output_throughput": 5108.05380964782,
    "total_throughput": 10938.59138462052,
    "itl": 94.58699935160512,
    "ttft": 1971241.4323821014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1199815619131535,
    "arrivals": 503772,
    "finished_requests": 85220,
    "scheduler_time": 90.33143246459927
}
#Debug simulation 
Total elapsed time: 6.510558429174125. Arrivals time: 0.27213727636262774 Scheduler time: 6.077304259408265 Scheduler overhead time: 0.05461678048595786 Adapter cache time: 0.022807552479207516 Engine time: 0.05715275043621659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.717624010052532,
    "estimated_duration": 3600.0306742960893,
    "input_throughput": 5272.755906090036,
    "output_throughput": 4619.698692776237,
    "total_throughput": 9892.454598866274,
    "itl": 74.89034101194731,
    "ttft": 2035591.4668210868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1340952768968424,
    "arrivals": 503772,
    "finished_requests": 77142,
    "scheduler_time": 87.20502594991142
}
#Debug simulation 
Total elapsed time: 5.717713796067983. Arrivals time: 0.26302073895931244 Scheduler time: 5.263935937546194 Scheduler overhead time: 0.06560737080872059 Adapter cache time: 0.02436194848269224 Engine time: 0.06884286552667618 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.227173232939094,
    "estimated_duration": 3600.0860498679344,
    "input_throughput": 5830.58313308095,
    "output_throughput": 5108.314286175086,
    "total_throughput": 10938.897419256034,
    "itl": 94.58548209572419,
    "ttft": 1971275.5865148578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 165,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0533469551941357,
    "arrivals": 503772,
    "finished_requests": 85224,
    "scheduler_time": 90.33486441999372
}
#Debug simulation 
Total elapsed time: 6.227298077195883. Arrivals time: 0.2724321153946221 Scheduler time: 5.79407309833914 Scheduler overhead time: 0.05454486096277833 Adapter cache time: 0.02287404192611575 Engine time: 0.056858816649764776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336298056 . Total output tokens: 296684552
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.686132661998272,
    "estimated_duration": 3600.0719152357624,
    "input_throughput": 5272.579117008257,
    "output_throughput": 4619.642438145812,
    "total_throughput": 9892.221555154068,
    "itl": 74.89035695966201,
    "ttft": 2035675.352634004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1243607695959514,
    "arrivals": 503772,
    "finished_requests": 77142,
    "scheduler_time": 87.2059939161209
}
#Debug simulation 
Total elapsed time: 5.686249735765159. Arrivals time: 0.24972081929445267 Scheduler time: 5.245456242933869 Scheduler overhead time: 0.06570591917261481 Adapter cache time: 0.0247634700499475 Engine time: 0.06858700746670365 
