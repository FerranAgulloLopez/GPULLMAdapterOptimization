INFO 05-31 19:30:51 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 621175011 . Total output tokens: 547344786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.374548499938101,
    "estimated_duration": 3600.1317401061297,
    "input_throughput": 4542.833201853294,
    "output_throughput": 3946.273921515212,
    "total_throughput": 8489.107123368505,
    "itl": 122.32579757158734,
    "ttft": 2193538.7743003992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.491598677048454,
    "arrivals": 927414,
    "finished_requests": 66342,
    "scheduler_time": 96.97273555851298
}
#Debug simulation 
Total elapsed time: 5.374702795874327. Arrivals time: 0.3350840322673321 Scheduler time: 4.911711774766445 Scheduler overhead time: 0.04458037856966257 Adapter cache time: 0.017704174853861332 Engine time: 0.04495268641039729 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 621175011 . Total output tokens: 547344786
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.015264692250639,
    "estimated_duration": 3600.075926575106,
    "input_throughput": 4033.340767291473,
    "output_throughput": 3501.6186483579727,
    "total_throughput": 7534.959415649446,
    "itl": 98.18858868847228,
    "ttft": 2255772.985948924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1571,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.759790385961363,
    "arrivals": 927414,
    "finished_requests": 58925,
    "scheduler_time": 100.00798410736778
}
#Debug simulation 
Total elapsed time: 5.015336439013481. Arrivals time: 0.6629184116609395 Scheduler time: 4.187571218702942 Scheduler overhead time: 0.052415572572499514 Adapter cache time: 0.0342705394141376 Engine time: 0.053641941864043474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 621175011 . Total output tokens: 547344786
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.3473879736848176,
    "estimated_duration": 3600.0876261386256,
    "input_throughput": 4543.36885614744,
    "output_throughput": 3946.835876114552,
    "total_throughput": 8490.204732261991,
    "itl": 122.3166589500513,
    "ttft": 2193525.2676764484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.175084295133109,
    "arrivals": 927414,
    "finished_requests": 66349,
    "scheduler_time": 96.97944359861036
}
#Debug simulation 
Total elapsed time: 5.347506806720048. Arrivals time: 0.3283071853220463 Scheduler time: 4.891227726824582 Scheduler overhead time: 0.04461040161550045 Adapter cache time: 0.017686347011476755 Engine time: 0.04503043135628104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 621175011 . Total output tokens: 547344786
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.629861883819103,
    "estimated_duration": 3600.068387391157,
    "input_throughput": 4033.476989175394,
    "output_throughput": 3501.7037576709467,
    "total_throughput": 7535.18074684634,
    "itl": 98.18413801675192,
    "ttft": 2255863.5577238696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1571,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.641319360937759,
    "arrivals": 927414,
    "finished_requests": 58928,
    "scheduler_time": 100.01096209949773
}
#Debug simulation 
Total elapsed time: 4.629986269865185. Arrivals time: 0.3085333192721009 Scheduler time: 4.156932110432535 Scheduler overhead time: 0.052389557007700205 Adapter cache time: 0.03440134786069393 Engine time: 0.053270791191607714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.6961727435700595,
    "estimated_duration": 3600.12970036027,
    "input_throughput": 4705.112706996331,
    "output_throughput": 4099.913677699702,
    "total_throughput": 8805.026384696033,
    "itl": 133.7094272729898,
    "ttft": 2178138.146167205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.863173110834348,
    "arrivals": 926784,
    "finished_requests": 68673,
    "scheduler_time": 95.8861678521156
}
#Debug simulation 
Total elapsed time: 5.696305490564555. Arrivals time: 0.3380280421115458 Scheduler time: 5.241598154883832 Scheduler overhead time: 0.04166157590225339 Adapter cache time: 0.013850812334567308 Engine time: 0.041944191325455904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.323162842076272,
    "estimated_duration": 3600.074250113732,
    "input_throughput": 4525.776933485546,
    "output_throughput": 3943.2086711965817,
    "total_throughput": 8468.985604682128,
    "itl": 122.07844152691696,
    "ttft": 2197683.300222369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.503620781032376,
    "arrivals": 926784,
    "finished_requests": 66022,
    "scheduler_time": 96.98616737129083
}
#Debug simulation 
Total elapsed time: 5.323263004422188. Arrivals time: 0.3266213866882026 Scheduler time: 4.868939320091158 Scheduler overhead time: 0.04466059897094965 Adapter cache time: 0.01727266889065504 Engine time: 0.04511506576091051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.654327539727092,
    "estimated_duration": 3600.077262711133,
    "input_throughput": 4012.2461119410164,
    "output_throughput": 3496.1360775133785,
    "total_throughput": 7508.382189454395,
    "itl": 98.04399454508969,
    "ttft": 2259512.6419199985,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.470783716118733,
    "arrivals": 926784,
    "finished_requests": 58553,
    "scheduler_time": 100.02009953775384
}
#Debug simulation 
Total elapsed time: 4.654439338948578. Arrivals time: 0.31735152285546064 Scheduler time: 4.173575344961137 Scheduler overhead time: 0.052717012353241444 Adapter cache time: 0.032801459077745676 Engine time: 0.05354205099865794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.308214641176164,
    "estimated_duration": 3600.063941962901,
    "input_throughput": 4526.049054316467,
    "output_throughput": 3943.595788540109,
    "total_throughput": 8469.644842856576,
    "itl": 122.0708003107196,
    "ttft": 2197587.2065856997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.220423702476543,
    "arrivals": 926784,
    "finished_requests": 66028,
    "scheduler_time": 96.99285763985965
}
#Debug simulation 
Total elapsed time: 5.308380180969834. Arrivals time: 0.23385515855625272 Scheduler time: 4.947281722910702 Scheduler overhead time: 0.04462493397295475 Adapter cache time: 0.017109527252614498 Engine time: 0.04492715606465936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.653054013848305,
    "estimated_duration": 3600.0816947703674,
    "input_throughput": 4012.363669686493,
    "output_throughput": 3496.153717368025,
    "total_throughput": 7508.517387054518,
    "itl": 98.0413339237794,
    "ttft": 2259485.514353743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.36204719839602,
    "arrivals": 926784,
    "finished_requests": 58554,
    "scheduler_time": 100.02312036477181
}
#Debug simulation 
Total elapsed time: 4.6531513961963356. Arrivals time: 0.31321374885737896 Scheduler time: 4.175735996104777 Scheduler overhead time: 0.05256736604496837 Adapter cache time: 0.03356084832921624 Engine time: 0.053510460536926985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.387742746155709,
    "estimated_duration": 3600.0100814709763,
    "input_throughput": 4526.779823166832,
    "output_throughput": 3944.258121132286,
    "total_throughput": 8471.037944299118,
    "itl": 122.06101010838329,
    "ttft": 2197436.1185408705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.894191773748007,
    "arrivals": 926784,
    "finished_requests": 66035,
    "scheduler_time": 96.99953670234879
}
#Debug simulation 
Total elapsed time: 5.387842059135437. Arrivals time: 0.39518502028658986 Scheduler time: 4.865599078591913 Scheduler overhead time: 0.04462254047393799 Adapter cache time: 0.016876025591045618 Engine time: 0.045094477478414774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 620796555 . Total output tokens: 546997204
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.709487560205162,
    "estimated_duration": 3600.0861593499767,
    "input_throughput": 4012.4086926320006,
    "output_throughput": 3496.1943806013264,
    "total_throughput": 7508.603073233327,
    "itl": 98.03866844850674,
    "ttft": 2259443.115509455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.253517797849922,
    "arrivals": 926784,
    "finished_requests": 58555,
    "scheduler_time": 100.02613535660322
}
#Debug simulation 
Total elapsed time: 4.709610504098237. Arrivals time: 0.3707928112708032 Scheduler time: 4.1747587798163295 Scheduler overhead time: 0.052630501333624125 Adapter cache time: 0.033268977887928486 Engine time: 0.05347790848463774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.696312956046313,
    "estimated_duration": 3600.0463410144703,
    "input_throughput": 4674.287052443351,
    "output_throughput": 4100.676658467675,
    "total_throughput": 8774.963710911026,
    "itl": 134.54721092965983,
    "ttft": 2167190.7826859774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.3662236615924,
    "arrivals": 850231,
    "finished_requests": 68491,
    "scheduler_time": 95.69279646548866
}
#Debug simulation 
Total elapsed time: 6.696413293946534. Arrivals time: 0.33805660577490926 Scheduler time: 6.2313119121827185 Scheduler overhead time: 0.042004793882369995 Adapter cache time: 0.023459450341761112 Engine time: 0.04224647535011172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.168455189093947,
    "estimated_duration": 3600.03579266239,
    "input_throughput": 4492.974773464113,
    "output_throughput": 3943.2796832004415,
    "total_throughput": 8436.254456664554,
    "itl": 123.0422208412974,
    "ttft": 2186547.3692309908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.647106639793995,
    "arrivals": 850231,
    "finished_requests": 65869,
    "scheduler_time": 96.7309914046326
}
#Debug simulation 
Total elapsed time: 6.168583982158452. Arrivals time: 0.34229771653190255 Scheduler time: 5.686651920899749 Scheduler overhead time: 0.044962539337575436 Adapter cache time: 0.028850627597421408 Engine time: 0.0451462771743536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.082401850260794,
    "estimated_duration": 3600.097289360278,
    "input_throughput": 3991.053531376416,
    "output_throughput": 3500.6692839235616,
    "total_throughput": 7491.722815299978,
    "itl": 99.02237393437865,
    "ttft": 2248844.9626306295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2758,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.63506503215548,
    "arrivals": 850231,
    "finished_requests": 58514,
    "scheduler_time": 99.63722812659755
}
#Debug simulation 
Total elapsed time: 5.082501796074212. Arrivals time: 0.31531751714646816 Scheduler time: 4.585451297927648 Scheduler overhead time: 0.05275140469893813 Adapter cache time: 0.05112688662484288 Engine time: 0.053401298355311155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.19945165887475,
    "estimated_duration": 3600.01838271823,
    "input_throughput": 4491.20373318535,
    "output_throughput": 3941.467651419653,
    "total_throughput": 8432.671384605002,
    "itl": 122.79251088533945,
    "ttft": 2186078.7306072786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.804239521971963,
    "arrivals": 850231,
    "finished_requests": 65839,
    "scheduler_time": 96.77643211318201
}
#Debug simulation 
Total elapsed time: 6.1995609742589295. Arrivals time: 0.3524406519718468 Scheduler time: 5.70730561716482 Scheduler overhead time: 0.04516090918332338 Adapter cache time: 0.028543821070343256 Engine time: 0.04540440300479531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.079534841701388,
    "estimated_duration": 3600.030420837149,
    "input_throughput": 3991.4234937655287,
    "output_throughput": 3500.91374979804,
    "total_throughput": 7492.337243563569,
    "itl": 99.01859346061175,
    "ttft": 2248766.2726496514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2758,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.455908674383743,
    "arrivals": 850231,
    "finished_requests": 58519,
    "scheduler_time": 99.64021201937562
}
#Debug simulation 
Total elapsed time: 5.079705946613103. Arrivals time: 0.3059868412092328 Scheduler time: 4.592813639435917 Scheduler overhead time: 0.05271279672160745 Adapter cache time: 0.05056425230577588 Engine time: 0.05301185278221965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.548984314780682,
    "estimated_duration": 3600.1208917006047,
    "input_throughput": 4491.667220753093,
    "output_throughput": 3941.7345769454614,
    "total_throughput": 8433.401797698554,
    "itl": 122.7706393609208,
    "ttft": 2185901.4296316747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.1226230240754,
    "arrivals": 850231,
    "finished_requests": 65847,
    "scheduler_time": 96.79711909823554
}
#Debug simulation 
Total elapsed time: 6.549048081040382. Arrivals time: 0.599837249610573 Scheduler time: 5.809805128257722 Scheduler overhead time: 0.045014345552772284 Adapter cache time: 0.028474872931838036 Engine time: 0.045279486104846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 569312226 . Total output tokens: 501578098
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.142046983819455,
    "estimated_duration": 3600.061014064889,
    "input_throughput": 3991.6087377015183,
    "output_throughput": 3501.0059414989746,
    "total_throughput": 7492.614679200493,
    "itl": 99.01321794922073,
    "ttft": 2248781.168240576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2758,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.26246123142559,
    "arrivals": 850231,
    "finished_requests": 58523,
    "scheduler_time": 99.64631985280494
}
#Debug simulation 
Total elapsed time: 5.142150177620351. Arrivals time: 0.3638172270730138 Scheduler time: 4.596840348094702 Scheduler overhead time: 0.0528941135853529 Adapter cache time: 0.050814647693187 Engine time: 0.05330751230940223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.682706652209163,
    "estimated_duration": 3600.0299253737776,
    "input_throughput": 4727.06626132646,
    "output_throughput": 4097.592882778165,
    "total_throughput": 8824.659144104626,
    "itl": 134.35991687804938,
    "ttft": 2157174.1702240678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.250760235698193,
    "arrivals": 793158,
    "finished_requests": 68919,
    "scheduler_time": 95.56848462898388
}
#Debug simulation 
Total elapsed time: 5.68280122615397. Arrivals time: 0.2425908767618239 Scheduler time: 5.309885829221457 Scheduler overhead time: 0.0417800541035831 Adapter cache time: 0.027511979918926954 Engine time: 0.041858484502881765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.343462760094553,
    "estimated_duration": 3600.0315075471576,
    "input_throughput": 4544.635502689607,
    "output_throughput": 3942.14549796245,
    "total_throughput": 8486.781000652058,
    "itl": 122.7199896086091,
    "ttft": 2177723.753802713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1734,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.618973517166312,
    "arrivals": 793158,
    "finished_requests": 66247,
    "scheduler_time": 96.610971926269
}
#Debug simulation 
Total elapsed time: 5.343564706854522. Arrivals time: 0.23626427771523595 Scheduler time: 4.964301464147866 Scheduler overhead time: 0.04460395872592926 Adapter cache time: 0.032825673930346966 Engine time: 0.044887087773531675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.66813622508198,
    "estimated_duration": 3600.0723749062677,
    "input_throughput": 4038.260480910058,
    "output_throughput": 3498.2624482175584,
    "total_throughput": 7536.522929127616,
    "itl": 98.8820826091623,
    "ttft": 2240774.821601791,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.398280722927808,
    "arrivals": 793158,
    "finished_requests": 58840,
    "scheduler_time": 99.48533224274482
}
#Debug simulation 
Total elapsed time: 4.668233548756689. Arrivals time: 0.2698372551240027 Scheduler time: 4.213710502721369 Scheduler overhead time: 0.052371669095009565 Adapter cache time: 0.05455962102860212 Engine time: 0.053287952207028866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.313897160813212,
    "estimated_duration": 3600.123197698142,
    "input_throughput": 4545.594164795058,
    "output_throughput": 3943.0336742576765,
    "total_throughput": 8488.627839052735,
    "itl": 122.69564934299967,
    "ttft": 2177765.373112665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1734,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.870722412550972,
    "arrivals": 793158,
    "finished_requests": 66265,
    "scheduler_time": 96.63290961867766
}
#Debug simulation 
Total elapsed time: 5.313996284734458. Arrivals time: 0.2331922766752541 Scheduler time: 4.9382311552762985 Scheduler overhead time: 0.04459650907665491 Adapter cache time: 0.03235241398215294 Engine time: 0.045022692531347275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.625774700194597,
    "estimated_duration": 3600.0907043394254,
    "input_throughput": 4038.521580157727,
    "output_throughput": 3498.5793510197327,
    "total_throughput": 7537.10093117746,
    "itl": 98.87537513207248,
    "ttft": 2240688.5946969367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.189299491723553,
    "arrivals": 793158,
    "finished_requests": 58844,
    "scheduler_time": 99.49150359915329
}
#Debug simulation 
Total elapsed time: 4.6258753831498325. Arrivals time: 0.218562301248312 Scheduler time: 4.2221568138338625 Scheduler overhead time: 0.05244468944147229 Adapter cache time: 0.05482668336480856 Engine time: 0.053382336627691984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.37625826196745,
    "estimated_duration": 3600.0202602791474,
    "input_throughput": 4547.222742221638,
    "output_throughput": 3944.408634772219,
    "total_throughput": 8491.631376993857,
    "itl": 122.7146142622195,
    "ttft": 2177681.756276174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1750,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.171861645998815,
    "arrivals": 793158,
    "finished_requests": 66288,
    "scheduler_time": 96.64374017481937
}
#Debug simulation 
Total elapsed time: 5.376415472012013. Arrivals time: 0.28196566877886653 Scheduler time: 4.951030177529901 Scheduler overhead time: 0.044830364640802145 Adapter cache time: 0.03272327268496156 Engine time: 0.04511460196226835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 531128185 . Total output tokens: 468037129
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.612413915805519,
    "estimated_duration": 3600.1025379466387,
    "input_throughput": 4038.613024698105,
    "output_throughput": 3498.829232565237,
    "total_throughput": 7537.442257263342,
    "itl": 98.86936180236687,
    "ttft": 2240725.0910250116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.972447807807946,
    "arrivals": 793158,
    "finished_requests": 58848,
    "scheduler_time": 99.49769948940877
}
#Debug simulation 
Total elapsed time: 4.61251130187884. Arrivals time: 0.21657726215198636 Scheduler time: 4.2112247119657695 Scheduler overhead time: 0.05217727739363909 Adapter cache time: 0.0549668432213366 Engine time: 0.053120802622288465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.5280347019433975,
    "estimated_duration": 3600.0230403282317,
    "input_throughput": 4745.024631409729,
    "output_throughput": 4093.571856322442,
    "total_throughput": 8838.596487732171,
    "itl": 133.78726681636059,
    "ttft": 2156616.5951286335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1301,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.602744150567078,
    "arrivals": 783512,
    "finished_requests": 68958,
    "scheduler_time": 95.63210973950493
}
#Debug simulation 
Total elapsed time: 5.528134077787399. Arrivals time: 0.3228325257077813 Scheduler time: 5.076560785062611 Scheduler overhead time: 0.04169429000467062 Adapter cache time: 0.02594952192157507 Engine time: 0.0418785298243165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.265378715004772,
    "estimated_duration": 3600.0761743722956,
    "input_throughput": 4556.599139977975,
    "output_throughput": 3931.457645467125,
    "total_throughput": 8488.0567854451,
    "itl": 121.84990654313103,
    "ttft": 2178635.6539043537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.791264097546089,
    "arrivals": 783512,
    "finished_requests": 66220,
    "scheduler_time": 96.7219759106248
}
#Debug simulation 
Total elapsed time: 5.265471901744604. Arrivals time: 0.3176848702132702 Scheduler time: 4.805726243183017 Scheduler overhead time: 0.045078197959810495 Adapter cache time: 0.03090641088783741 Engine time: 0.04529796214774251 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.620473613962531,
    "estimated_duration": 3600.0408828098543,
    "input_throughput": 4054.2339587566553,
    "output_throughput": 3493.9553214691537,
    "total_throughput": 7548.189280225809,
    "itl": 98.49184965786395,
    "ttft": 2240005.1302004633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.281359346844866,
    "arrivals": 783512,
    "finished_requests": 58843,
    "scheduler_time": 99.62635518042626
}
#Debug simulation 
Total elapsed time: 4.620600639842451. Arrivals time: 0.29757740860804915 Scheduler time: 4.1342321573756635 Scheduler overhead time: 0.05236758152022958 Adapter cache time: 0.058422716334462166 Engine time: 0.05341296782717109 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.259945849888027,
    "estimated_duration": 3600.080943468618,
    "input_throughput": 4565.080968475525,
    "output_throughput": 3938.221174088329,
    "total_throughput": 8503.302142563854,
    "itl": 122.32461937766466,
    "ttft": 2177488.8397816266,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1661,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.38803753220953,
    "arrivals": 783512,
    "finished_requests": 66353,
    "scheduler_time": 96.67730166945447
}
#Debug simulation 
Total elapsed time: 5.260047948919237. Arrivals time: 0.3133336971513927 Scheduler time: 4.8044761386699975 Scheduler overhead time: 0.044783253222703934 Adapter cache time: 0.03174631483852863 Engine time: 0.04500226862728596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.614149398170412,
    "estimated_duration": 3600.0358551462646,
    "input_throughput": 4054.857392359754,
    "output_throughput": 3494.213809570216,
    "total_throughput": 7549.07120192997,
    "itl": 98.48476440506734,
    "ttft": 2239991.3467147797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.065129014459103,
    "arrivals": 783512,
    "finished_requests": 58848,
    "scheduler_time": 99.6322087205827
}
#Debug simulation 
Total elapsed time: 4.614242638926953. Arrivals time: 0.2958919066004455 Scheduler time: 4.129644560627639 Scheduler overhead time: 0.052429532166570425 Adapter cache time: 0.058268093038350344 Engine time: 0.05345004703849554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.26749780215323,
    "estimated_duration": 3600.103174508422,
    "input_throughput": 4566.006362371698,
    "output_throughput": 3939.2446028818176,
    "total_throughput": 8505.250965253515,
    "itl": 122.29947371683235,
    "ttft": 2177342.360956705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1661,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.603692682287962,
    "arrivals": 783512,
    "finished_requests": 66368,
    "scheduler_time": 96.69844498767567
}
#Debug simulation 
Total elapsed time: 5.26760898437351. Arrivals time: 0.32586760073900223 Scheduler time: 4.79945530788973 Scheduler overhead time: 0.044754582457244396 Adapter cache time: 0.03165621170774102 Engine time: 0.045138933695852757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524683070 . Total output tokens: 462447116
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.6131173321045935,
    "estimated_duration": 3600.0273596058228,
    "input_throughput": 4055.3997349616106,
    "output_throughput": 3494.2323331057537,
    "total_throughput": 7549.632068067364,
    "itl": 98.47571751616412,
    "ttft": 2240054.6261014976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.842838918455517,
    "arrivals": 783512,
    "finished_requests": 58853,
    "scheduler_time": 99.63810615707723
}
#Debug simulation 
Total elapsed time: 4.613237952347845. Arrivals time: 0.2945031188428402 Scheduler time: 4.130732622928917 Scheduler overhead time: 0.05232497025281191 Adapter cache time: 0.05754199670627713 Engine time: 0.053551297169178724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.380577097181231,
    "estimated_duration": 3600.0093222595547,
    "input_throughput": 4697.517280145737,
    "output_throughput": 4094.929673889645,
    "total_throughput": 8792.446954035382,
    "itl": 133.9952507135639,
    "ttft": 2160094.3625457925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.093588655106917,
    "arrivals": 778725,
    "finished_requests": 68768,
    "scheduler_time": 95.5989523821441
}
#Debug simulation 
Total elapsed time: 5.380673598032445. Arrivals time: 0.3197254170663655 Scheduler time: 4.93353156466037 Scheduler overhead time: 0.04157887026667595 Adapter cache time: 0.025040949694812298 Engine time: 0.04166145855560899 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.502491146791726,
    "estimated_duration": 3600.0031858190846,
    "input_throughput": 4512.875450776462,
    "output_throughput": 3938.4909590778716,
    "total_throughput": 8451.366409854334,
    "itl": 122.35607804861273,
    "ttft": 2181092.152433783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.287472571800325,
    "arrivals": 778725,
    "finished_requests": 66074,
    "scheduler_time": 96.65438942273865
}
#Debug simulation 
Total elapsed time: 5.5025546117685735. Arrivals time: 0.6543838945217431 Scheduler time: 4.706954826135188 Scheduler overhead time: 0.04476278834044933 Adapter cache time: 0.03078472474589944 Engine time: 0.045105559285730124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.655969504266977,
    "estimated_duration": 3600.000435668672,
    "input_throughput": 4012.6739588343517,
    "output_throughput": 3506.5287423098002,
    "total_throughput": 7519.202701144151,
    "itl": 98.17226040244341,
    "ttft": 2240999.488079402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3017,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.685756682465275,
    "arrivals": 778725,
    "finished_requests": 58718,
    "scheduler_time": 99.96032401643565
}
#Debug simulation 
Total elapsed time: 4.656077458988875. Arrivals time: 0.34233822161331773 Scheduler time: 4.126003379933536 Scheduler overhead time: 0.05218153307214379 Adapter cache time: 0.05769560765475035 Engine time: 0.053410294465720654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.193007015157491,
    "estimated_duration": 3600.000640269669,
    "input_throughput": 4513.850586088439,
    "output_throughput": 3938.956799445954,
    "total_throughput": 8452.807385534392,
    "itl": 122.3307933701287,
    "ttft": 2181067.0933060804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1545,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.61140895779719,
    "arrivals": 778725,
    "finished_requests": 66087,
    "scheduler_time": 96.67186148987194
}
#Debug simulation 
Total elapsed time: 5.193106517195702. Arrivals time: 0.2811054792255163 Scheduler time: 4.770653899293393 Scheduler overhead time: 0.04461747547611594 Adapter cache time: 0.030799183063209057 Engine time: 0.04515778785571456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.602932225912809,
    "estimated_duration": 3600.0254860354694,
    "input_throughput": 4012.7263143096734,
    "output_throughput": 3506.5715642757605,
    "total_throughput": 7519.297878585434,
    "itl": 98.16613549312726,
    "ttft": 2240983.595959561,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3017,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.482988966559464,
    "arrivals": 778725,
    "finished_requests": 58720,
    "scheduler_time": 99.96654516995162
}
#Debug simulation 
Total elapsed time: 4.603031413629651. Arrivals time: 0.29348698863759637 Scheduler time: 4.121584170497954 Scheduler overhead time: 0.05215585371479392 Adapter cache time: 0.05795001331716776 Engine time: 0.053429434541612864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.1318760509602726,
    "estimated_duration": 3600.0256693331366,
    "input_throughput": 4516.304463743381,
    "output_throughput": 3941.129398287928,
    "total_throughput": 8457.43386203131,
    "itl": 122.40495778368071,
    "ttft": 2180573.0423696404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.831238248478824,
    "arrivals": 778725,
    "finished_requests": 66118,
    "scheduler_time": 96.6846014153029
}
#Debug simulation 
Total elapsed time: 5.131968141067773. Arrivals time: 0.23508833348751068 Scheduler time: 4.756551384460181 Scheduler overhead time: 0.04445868171751499 Adapter cache time: 0.030326625797897577 Engine time: 0.04500187700614333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 521511585 . Total output tokens: 459598958
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.5937505583278835,
    "estimated_duration": 3600.033959480072,
    "input_throughput": 4012.8288128944155,
    "output_throughput": 3506.664143196919,
    "total_throughput": 7519.492956091335,
    "itl": 98.16031468531206,
    "ttft": 2240904.555396904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3017,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.26448034523093,
    "arrivals": 778725,
    "finished_requests": 58721,
    "scheduler_time": 99.97276566097071
}
#Debug simulation 
Total elapsed time: 4.593848877120763. Arrivals time: 0.29560366133227944 Scheduler time: 4.110888258554041 Scheduler overhead time: 0.05202042357996106 Adapter cache time: 0.0576554024592042 Engine time: 0.05316885141655803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.28529443172738,
    "estimated_duration": 3600.066625965364,
    "input_throughput": 4668.96303495293,
    "output_throughput": 4094.8322716241387,
    "total_throughput": 8763.795306577069,
    "itl": 134.1529406603203,
    "ttft": 2159512.33342718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1056,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.982703937739291,
    "arrivals": 776317,
    "finished_requests": 68245,
    "scheduler_time": 95.65230926947751
}
#Debug simulation 
Total elapsed time: 5.285392814781517. Arrivals time: 0.23748488537967205 Scheduler time: 4.9229855025187135 Scheduler overhead time: 0.04148660087957978 Adapter cache time: 0.02254063542932272 Engine time: 0.041772868018597364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.050862970761955,
    "estimated_duration": 3600.012841098023,
    "input_throughput": 4487.137883395166,
    "output_throughput": 3938.212896950599,
    "total_throughput": 8425.350780345765,
    "itl": 122.64681229137672,
    "ttft": 2179758.9120090916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1359,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.950135411736488,
    "arrivals": 776317,
    "finished_requests": 65595,
    "scheduler_time": 96.69990032944948
}
#Debug simulation 
Total elapsed time: 5.050963487941772. Arrivals time: 0.23038103012368083 Scheduler time: 4.681557423435152 Scheduler overhead time: 0.04494865145534277 Adapter cache time: 0.027699805796146393 Engine time: 0.04570649657398462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.507884773891419,
    "estimated_duration": 3600.0358849182676,
    "input_throughput": 4008.021436799535,
    "output_throughput": 3513.873306928772,
    "total_throughput": 7521.894743728307,
    "itl": 98.2479740669739,
    "ttft": 2241222.6514377124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2706,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.36377537305482,
    "arrivals": 776317,
    "finished_requests": 58608,
    "scheduler_time": 100.15697552028256
}
#Debug simulation 
Total elapsed time: 4.507984898984432. Arrivals time: 0.21763687394559383 Scheduler time: 4.105867923703045 Scheduler overhead time: 0.051954287104308605 Adapter cache time: 0.05479289451614022 Engine time: 0.05335415853187442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.081256705801934,
    "estimated_duration": 3600.0398971838194,
    "input_throughput": 4488.075538451458,
    "output_throughput": 3938.974123895922,
    "total_throughput": 8427.04966234738,
    "itl": 122.6263081208813,
    "ttft": 2179617.3944478366,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1359,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.312941984985995,
    "arrivals": 776317,
    "finished_requests": 65611,
    "scheduler_time": 96.7172169717491
}
#Debug simulation 
Total elapsed time: 5.081353900954127. Arrivals time: 0.2760641071945429 Scheduler time: 4.667331291362643 Scheduler overhead time: 0.044577647000551224 Adapter cache time: 0.027504863683134317 Engine time: 0.04514113953337073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.516463493928313,
    "estimated_duration": 3600.0620241352544,
    "input_throughput": 4008.242886722517,
    "output_throughput": 3513.9383475035165,
    "total_throughput": 7522.181234226034,
    "itl": 98.24316843231539,
    "ttft": 2241129.6163738184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2706,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.1748845079822,
    "arrivals": 776317,
    "finished_requests": 58611,
    "scheduler_time": 100.16288860589053
}
#Debug simulation 
Total elapsed time: 4.516557181719691. Arrivals time: 0.21331350086256862 Scheduler time: 4.118613130878657 Scheduler overhead time: 0.05186198838055134 Adapter cache time: 0.054720526561141014 Engine time: 0.053658283315598965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.040964500978589,
    "estimated_duration": 3600.1305931543525,
    "input_throughput": 4488.727167488936,
    "output_throughput": 3939.705972602727,
    "total_throughput": 8428.433140091664,
    "itl": 122.60774553887731,
    "ttft": 2179566.060258486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.650212874473118,
    "arrivals": 776317,
    "finished_requests": 65620,
    "scheduler_time": 96.73637698870414
}
#Debug simulation 
Total elapsed time: 5.0410599508322775. Arrivals time: 0.23125125700607896 Scheduler time: 4.671867462806404 Scheduler overhead time: 0.04469183133915067 Adapter cache time: 0.027617114130407572 Engine time: 0.04501379141584039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519922914 . Total output tokens: 458184974
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.557766058947891,
    "estimated_duration": 3600.0857488551815,
    "input_throughput": 4008.2181388564654,
    "output_throughput": 3513.994077508539,
    "total_throughput": 7522.212216365004,
    "itl": 98.23818295872783,
    "ttft": 2241042.6573361945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2706,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.983922471143426,
    "arrivals": 776317,
    "finished_requests": 58612,
    "scheduler_time": 100.16879419867524
}
#Debug simulation 
Total elapsed time: 4.557857005856931. Arrivals time: 0.21417648112401366 Scheduler time: 4.159416785929352 Scheduler overhead time: 0.051888758316636086 Adapter cache time: 0.0545966811478138 Engine time: 0.05335931247100234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.615707206074148,
    "estimated_duration": 3600.083771797827,
    "input_throughput": 4732.629316426031,
    "output_throughput": 4097.552705734208,
    "total_throughput": 8830.18202216024,
    "itl": 134.22105929998912,
    "ttft": 2156936.785864993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 994,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.572734577758382,
    "arrivals": 775108,
    "finished_requests": 68911,
    "scheduler_time": 95.6459872511454
}
#Debug simulation 
Total elapsed time: 5.615775933954865. Arrivals time: 0.6569174509495497 Scheduler time: 4.834758771117777 Scheduler overhead time: 0.04147143429145217 Adapter cache time: 0.02164614712819457 Engine time: 0.04201204935088754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.082994979806244,
    "estimated_duration": 3600.0594992248766,
    "input_throughput": 4554.6333341241725,
    "output_throughput": 3942.4990067680615,
    "total_throughput": 8497.132340892234,
    "itl": 122.66540902581161,
    "ttft": 2177928.8891510847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1368,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.029802235774623,
    "arrivals": 775108,
    "finished_requests": 66277,
    "scheduler_time": 96.68824143296385
}
#Debug simulation 
Total elapsed time: 5.083095760084689. Arrivals time: 0.3107995088212192 Scheduler time: 4.634562157094479 Scheduler overhead time: 0.044508387334644794 Adapter cache time: 0.027679672930389643 Engine time: 0.04497995460405946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.602352160960436,
    "estimated_duration": 3600.045302896689,
    "input_throughput": 4071.6379286132124,
    "output_throughput": 3529.082533982932,
    "total_throughput": 7600.720462596145,
    "itl": 98.06306959397038,
    "ttft": 2237290.555430945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2519,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.997089389325094,
    "arrivals": 775108,
    "finished_requests": 59287,
    "scheduler_time": 100.42378290365497
}
#Debug simulation 
Total elapsed time: 4.602446636650711. Arrivals time: 0.2956307427957654 Scheduler time: 4.123118527233601 Scheduler overhead time: 0.05185651779174805 Adapter cache time: 0.053713705856353045 Engine time: 0.05360193643718958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.100961436051875,
    "estimated_duration": 3600.0823646520867,
    "input_throughput": 4555.145504729249,
    "output_throughput": 3942.859513263323,
    "total_throughput": 8498.005017992573,
    "itl": 122.64458536689227,
    "ttft": 2177704.543697827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1368,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.374561936371054,
    "arrivals": 775108,
    "finished_requests": 66283,
    "scheduler_time": 96.70590082310318
}
#Debug simulation 
Total elapsed time: 5.1010560020804405. Arrivals time: 0.31223564594984055 Scheduler time: 4.651084411423653 Scheduler overhead time: 0.0445259939879179 Adapter cache time: 0.027822754345834255 Engine time: 0.0448463992215693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.60912077780813,
    "estimated_duration": 3600.030753631456,
    "input_throughput": 4064.527500283115,
    "output_throughput": 3522.132133790672,
    "total_throughput": 7586.659634073787,
    "itl": 97.60521937869626,
    "ttft": 2237814.3162596943,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2535,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.930787231079265,
    "arrivals": 775108,
    "finished_requests": 59172,
    "scheduler_time": 100.54659683878354
}
#Debug simulation 
Total elapsed time: 4.609218421857804. Arrivals time: 0.2976023559458554 Scheduler time: 4.1255767326802015 Scheduler overhead time: 0.05214336374774575 Adapter cache time: 0.05479786777868867 Engine time: 0.054565164260566235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.105835629161447,
    "estimated_duration": 3600.1246309803746,
    "input_throughput": 4556.0364379755865,
    "output_throughput": 3943.735135673251,
    "total_throughput": 8499.771573648837,
    "itl": 122.62433763074782,
    "ttft": 2177579.1667005545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1369,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.739587767641117,
    "arrivals": 775108,
    "finished_requests": 66295,
    "scheduler_time": 96.72355132875143
}
#Debug simulation 
Total elapsed time: 5.105936472304165. Arrivals time: 0.3149686800315976 Scheduler time: 4.653111530467868 Scheduler overhead time: 0.04433185840025544 Adapter cache time: 0.027808988001197577 Engine time: 0.04511608928442001 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 519078986 . Total output tokens: 457455386
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.622929130215198,
    "estimated_duration": 3600.072489146637,
    "input_throughput": 4064.697042661122,
    "output_throughput": 3522.300186518133,
    "total_throughput": 7586.997229179256,
    "itl": 97.60112377106219,
    "ttft": 2237753.3517040904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2535,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.752459342013996,
    "arrivals": 775108,
    "finished_requests": 59176,
    "scheduler_time": 100.55260892290491
}
#Debug simulation 
Total elapsed time: 4.623022906016558. Arrivals time: 0.21993636479601264 Scheduler time: 4.215950996614993 Scheduler overhead time: 0.052332311403006315 Adapter cache time: 0.05439602769911289 Engine time: 0.05591539479792118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.325773924123496,
    "estimated_duration": 3600.0823109532257,
    "input_throughput": 4723.709496380165,
    "output_throughput": 4095.07470291602,
    "total_throughput": 8818.784199296184,
    "itl": 133.8824836050895,
    "ttft": 2159793.239700844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 969,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.407424351959628,
    "arrivals": 774530,
    "finished_requests": 68707,
    "scheduler_time": 95.67223847232766
}
#Debug simulation 
Total elapsed time: 5.325871613807976. Arrivals time: 0.37311265245079994 Scheduler time: 4.828793264459819 Scheduler overhead time: 0.0412361235357821 Adapter cache time: 0.021468788851052523 Engine time: 0.04212299454957247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.128650753293186,
    "estimated_duration": 3600.1112736727514,
    "input_throughput": 4537.777518008178,
    "output_throughput": 3940.384594148377,
    "total_throughput": 8478.162112156555,
    "itl": 122.4572021227299,
    "ttft": 2180951.0253578676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.720588110997296,
    "arrivals": 774530,
    "finished_requests": 66064,
    "scheduler_time": 96.70495213938717
}
#Debug simulation 
Total elapsed time: 5.128784478176385. Arrivals time: 0.3577976315282285 Scheduler time: 4.633846865966916 Scheduler overhead time: 0.044326274655759335 Adapter cache time: 0.02716652723029256 Engine time: 0.04503658460453153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.6094168019481,
    "estimated_duration": 3600.047232874348,
    "input_throughput": 4048.0449442227205,
    "output_throughput": 3522.11490010819,
    "total_throughput": 7570.15984433091,
    "itl": 97.69484782273166,
    "ttft": 2238207.9823797373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.269967077635236,
    "arrivals": 774530,
    "finished_requests": 58913,
    "scheduler_time": 100.4686247308188
}
#Debug simulation 
Total elapsed time: 4.609535388182849. Arrivals time: 0.29584595281630754 Scheduler time: 4.1278817700222135 Scheduler overhead time: 0.05220262473449111 Adapter cache time: 0.05516419280320406 Engine time: 0.05380310071632266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.057885497808456,
    "estimated_duration": 3600.052143472263,
    "input_throughput": 4539.386194622742,
    "output_throughput": 3941.2920798182654,
    "total_throughput": 8480.678274441007,
    "itl": 122.42124882999637,
    "ttft": 2180779.6646276955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1305,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.952939823488734,
    "arrivals": 774530,
    "finished_requests": 66084,
    "scheduler_time": 96.72356044450746
}
#Debug simulation 
Total elapsed time: 5.0580095229670405. Arrivals time: 0.22723604924976826 Scheduler time: 4.69444251479581 Scheduler overhead time: 0.04441201500594616 Adapter cache time: 0.026335347443819046 Engine time: 0.045065869111567736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.596045641694218,
    "estimated_duration": 3600.0791694492805,
    "input_throughput": 4048.032366529687,
    "output_throughput": 3522.1905972528207,
    "total_throughput": 7570.222963782508,
    "itl": 97.69010279063538,
    "ttft": 2238136.9532496785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.086875493507822,
    "arrivals": 774530,
    "finished_requests": 58915,
    "scheduler_time": 100.47453589342759
}
#Debug simulation 
Total elapsed time: 4.59616982890293. Arrivals time: 0.29490345623344183 Scheduler time: 4.116812732070684 Scheduler overhead time: 0.0517286192625761 Adapter cache time: 0.05456783529371023 Engine time: 0.0536836925894022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.105616193264723,
    "estimated_duration": 3600.0906520677404,
    "input_throughput": 4540.381501396824,
    "output_throughput": 3941.8071297314045,
    "total_throughput": 8482.188631128229,
    "itl": 122.39821925711824,
    "ttft": 2180785.3226872915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1305,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.331016827444548,
    "arrivals": 774530,
    "finished_requests": 66095,
    "scheduler_time": 96.74077192131298
}
#Debug simulation 
Total elapsed time: 5.105713255237788. Arrivals time: 0.3103925874456763 Scheduler time: 4.657911178655922 Scheduler overhead time: 0.04471931234002113 Adapter cache time: 0.02680369047448039 Engine time: 0.04531809315085411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518696351 . Total output tokens: 457105304
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.575402703136206,
    "estimated_duration": 3600.0040070644022,
    "input_throughput": 4048.116883037484,
    "output_throughput": 3522.2641350168806,
    "total_throughput": 7570.381018054364,
    "itl": 97.68495649798537,
    "ttft": 2238072.489474575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.90461237808686,
    "arrivals": 774530,
    "finished_requests": 58915,
    "scheduler_time": 100.47741395821468
}
#Debug simulation 
Total elapsed time: 4.575522946193814. Arrivals time: 0.2954115029424429 Scheduler time: 4.096074928529561 Scheduler overhead time: 0.05195100558921695 Adapter cache time: 0.05442341836169362 Engine time: 0.053295341320335865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.405234512872994,
    "estimated_duration": 3600.0645690588462,
    "input_throughput": 4692.671388506936,
    "output_throughput": 4088.757775766269,
    "total_throughput": 8781.429164273204,
    "itl": 134.42246873491928,
    "ttft": 2156276.0961654144,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.553912279322214,
    "arrivals": 716829,
    "finished_requests": 68483,
    "scheduler_time": 95.39059325571608
}
#Debug simulation 
Total elapsed time: 5.405330345034599. Arrivals time: 0.3082797797396779 Scheduler time: 4.956961557734758 Scheduler overhead time: 0.04168525477871299 Adapter cache time: 0.03719352185726166 Engine time: 0.04204650269821286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.166239800862968,
    "estimated_duration": 3600.028166314127,
    "input_throughput": 4497.603977520431,
    "output_throughput": 3929.8909192938563,
    "total_throughput": 8427.494896814287,
    "itl": 122.92435140749015,
    "ttft": 2175740.7824098417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.335482337600123,
    "arrivals": 716829,
    "finished_requests": 65602,
    "scheduler_time": 96.36802713158743
}
#Debug simulation 
Total elapsed time: 5.1663386155851185. Arrivals time: 0.3058837950229645 Scheduler time: 4.704329250846058 Scheduler overhead time: 0.044588970486074686 Adapter cache time: 0.04575646435841918 Engine time: 0.04519150173291564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.9659415748901665,
    "estimated_duration": 3600.1039442906704,
    "input_throughput": 4062.524645488165,
    "output_throughput": 3552.108827379876,
    "total_throughput": 7614.633472868041,
    "itl": 97.22622244061557,
    "ttft": 2229675.827517413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3955,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.611767077018772,
    "arrivals": 716829,
    "finished_requests": 59241,
    "scheduler_time": 101.1835427174389
}
#Debug simulation 
Total elapsed time: 4.966057470999658. Arrivals time: 0.3004588899202645 Scheduler time: 4.462723388336599 Scheduler overhead time: 0.05232994817197323 Adapter cache time: 0.07206483371555805 Engine time: 0.05382236186414957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.1386732826940715,
    "estimated_duration": 3600.0246635389485,
    "input_throughput": 4499.7363946044925,
    "output_throughput": 3931.268066949143,
    "total_throughput": 8431.004461553635,
    "itl": 122.88243709640744,
    "ttft": 2175546.011389695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.10413033427247,
    "arrivals": 716829,
    "finished_requests": 65630,
    "scheduler_time": 96.40034505613643
}
#Debug simulation 
Total elapsed time: 5.138823490589857. Arrivals time: 0.3054565414786339 Scheduler time: 4.676756226457655 Scheduler overhead time: 0.04468377958983183 Adapter cache time: 0.04626850364729762 Engine time: 0.04488542769104242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.6481084218248725,
    "estimated_duration": 3600.0606989820526,
    "input_throughput": 4062.9573285072324,
    "output_throughput": 3552.5139905602905,
    "total_throughput": 7615.471319067523,
    "itl": 97.21720806806651,
    "ttft": 2229737.3747137627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3955,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.350592317307623,
    "arrivals": 716829,
    "finished_requests": 59247,
    "scheduler_time": 101.1894459877862
}
#Debug simulation 
Total elapsed time: 4.648207108024508. Arrivals time: 0.2860885914415121 Scheduler time: 4.159841884858906 Scheduler overhead time: 0.05234324745833874 Adapter cache time: 0.07134941639378667 Engine time: 0.05388374114409089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.107083707116544,
    "estimated_duration": 3600.1036584569074,
    "input_throughput": 4501.103450711644,
    "output_throughput": 3932.8998115762115,
    "total_throughput": 8434.003262287855,
    "itl": 122.838971633831,
    "ttft": 2175463.849887932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2790,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.811139424193055,
    "arrivals": 716829,
    "finished_requests": 65654,
    "scheduler_time": 96.4366122631694
}
#Debug simulation 
Total elapsed time: 5.107184101827443. Arrivals time: 0.2985922987572849 Scheduler time: 4.652897365856916 Scheduler overhead time: 0.0445890030823648 Adapter cache time: 0.04565598489716649 Engine time: 0.04495026031509042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 480182021 . Total output tokens: 422756442
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.6306715700775385,
    "estimated_duration": 3600.0031483695197,
    "input_throughput": 4063.14394658929,
    "output_throughput": 3552.814948451585,
    "total_throughput": 7615.958895040875,
    "itl": 97.20979745092521,
    "ttft": 2229760.8241089396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3955,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 29.074919355233433,
    "arrivals": 716829,
    "finished_requests": 59249,
    "scheduler_time": 101.19537339496742
}
#Debug simulation 
Total elapsed time: 4.630768940318376. Arrivals time: 0.286455764900893 Scheduler time: 4.141990036237985 Scheduler overhead time: 0.05253272317349911 Adapter cache time: 0.07132133841514587 Engine time: 0.05389260407537222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.2214046670123935,
    "estimated_duration": 3600.088492241056,
    "input_throughput": 4668.829401339769,
    "output_throughput": 4089.0264313575976,
    "total_throughput": 8757.855832697367,
    "itl": 134.64295886127036,
    "ttft": 2146514.5370544414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.790432768296892,
    "arrivals": 707393,
    "finished_requests": 68299,
    "scheduler_time": 95.34368856877676
}
#Debug simulation 
Total elapsed time: 5.221496303100139. Arrivals time: 0.2252642335370183 Scheduler time: 4.8537901150994 Scheduler overhead time: 0.04118960723280907 Adapter cache time: 0.040393517818301916 Engine time: 0.04181287344545126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.040550779085606,
    "estimated_duration": 3600.131519820373,
    "input_throughput": 4507.68976373666,
    "output_throughput": 3946.830531543736,
    "total_throughput": 8454.520295280396,
    "itl": 122.69436253591665,
    "ttft": 2166654.0704942225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2908,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.295072749684977,
    "arrivals": 707393,
    "finished_requests": 65942,
    "scheduler_time": 96.67077692032656
}
#Debug simulation 
Total elapsed time: 5.040645606815815. Arrivals time: 0.2888591573573649 Scheduler time: 4.592672718688846 Scheduler overhead time: 0.04419473558664322 Adapter cache time: 0.04927662434056401 Engine time: 0.04503242950886488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.708257032092661,
    "estimated_duration": 3600.008557687694,
    "input_throughput": 4138.646550765373,
    "output_throughput": 3637.364686824717,
    "total_throughput": 7776.01123759009,
    "itl": 95.35196669146212,
    "ttft": 2209646.6154825273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2976,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.38368738285305,
    "arrivals": 707393,
    "finished_requests": 60695,
    "scheduler_time": 103.33842012251645
}
#Debug simulation 
Total elapsed time: 4.7083717891946435. Arrivals time: 0.2081151190213859 Scheduler time: 4.30084665864706 Scheduler overhead time: 0.053408531472086906 Adapter cache time: 0.06591887399554253 Engine time: 0.05512476246803999 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.0313400980085135,
    "estimated_duration": 3600.0260098491403,
    "input_throughput": 4508.966311796236,
    "output_throughput": 3947.9567539556715,
    "total_throughput": 8456.923065751907,
    "itl": 122.64711545900661,
    "ttft": 2166390.299126439,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2909,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.9257296860735,
    "arrivals": 707393,
    "finished_requests": 65962,
    "scheduler_time": 96.70406944528884
}
#Debug simulation 
Total elapsed time: 5.031435802113265. Arrivals time: 0.2975051738321781 Scheduler time: 4.5741936303675175 Scheduler overhead time: 0.04426616011187434 Adapter cache time: 0.04978223191574216 Engine time: 0.04515885515138507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.729459831956774,
    "estimated_duration": 3600.015001621983,
    "input_throughput": 4139.03941880424,
    "output_throughput": 3637.9523402261666,
    "total_throughput": 7776.991759030407,
    "itl": 95.34660111990149,
    "ttft": 2209584.1787737035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2976,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.175120386002025,
    "arrivals": 707393,
    "finished_requests": 60700,
    "scheduler_time": 103.3444119154385
}
#Debug simulation 
Total elapsed time: 4.729553244076669. Arrivals time: 0.2788060698658228 Scheduler time: 4.251595601905137 Scheduler overhead time: 0.053514526691287756 Adapter cache time: 0.06536508444696665 Engine time: 0.05521063692867756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.05641369568184,
    "estimated_duration": 3600.0785491548513,
    "input_throughput": 4510.316032913195,
    "output_throughput": 3949.203275950094,
    "total_throughput": 8459.51930886329,
    "itl": 122.60042508729961,
    "ttft": 2166195.995387349,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2910,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.57720993706162,
    "arrivals": 707393,
    "finished_requests": 65991,
    "scheduler_time": 96.74106060749962
}
#Debug simulation 
Total elapsed time: 5.056510000023991. Arrivals time: 0.31144413771107793 Scheduler time: 4.585425118450075 Scheduler overhead time: 0.044121050741523504 Adapter cache time: 0.04990996001288295 Engine time: 0.045015066396445036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473765644 . Total output tokens: 417149554
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.707088933326304,
    "estimated_duration": 3600.021215003773,
    "input_throughput": 4139.166718767346,
    "output_throughput": 3638.0735606265794,
    "total_throughput": 7777.240279393925,
    "itl": 95.34102850270905,
    "ttft": 2209599.3557730634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2976,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.96738185785746,
    "arrivals": 707393,
    "finished_requests": 60702,
    "scheduler_time": 103.35041862818174
}
#Debug simulation 
Total elapsed time: 4.707204565405846. Arrivals time: 0.2798422542400658 Scheduler time: 4.228562514297664 Scheduler overhead time: 0.05323369475081563 Adapter cache time: 0.06533158710226417 Engine time: 0.05516562657430768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.1410409971140325,
    "estimated_duration": 3600.0476300218934,
    "input_throughput": 4710.857117158993,
    "output_throughput": 4104.9531891638135,
    "total_throughput": 8815.810306322806,
    "itl": 133.8176437140097,
    "ttft": 2148567.760074773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.797045177328842,
    "arrivals": 702770,
    "finished_requests": 68511,
    "scheduler_time": 95.80300558467295
}
#Debug simulation 
Total elapsed time: 5.141129685100168. Arrivals time: 0.26613107742741704 Scheduler time: 4.730614261701703 Scheduler overhead time: 0.04111897945404053 Adapter cache time: 0.042001294903457165 Engine time: 0.04227009415626526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.997786037623882,
    "estimated_duration": 3600.0393949103905,
    "input_throughput": 4577.688517325296,
    "output_throughput": 3994.160458446293,
    "total_throughput": 8571.848975771589,
    "itl": 121.0996088289147,
    "ttft": 2164725.1575279324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.400823573385896,
    "arrivals": 702770,
    "finished_requests": 66594,
    "scheduler_time": 97.89632981664957
}
#Debug simulation 
Total elapsed time: 4.997910193633288. Arrivals time: 0.228005591314286 Scheduler time: 4.611663224641234 Scheduler overhead time: 0.04419396072626114 Adapter cache time: 0.048055089078843594 Engine time: 0.04528596019372344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.705693373922259,
    "estimated_duration": 3600.0198415981913,
    "input_throughput": 4210.59234864402,
    "output_throughput": 3682.025817423111,
    "total_throughput": 7892.618166067131,
    "itl": 93.85298380511783,
    "ttft": 2209997.4379645325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.505518636195742,
    "arrivals": 702770,
    "finished_requests": 61325,
    "scheduler_time": 104.78872273879908
}
#Debug simulation 
Total elapsed time: 4.705784320831299. Arrivals time: 0.20906646410003304 Scheduler time: 4.3020169930532575 Scheduler overhead time: 0.054169070441275835 Adapter cache time: 0.059243579395115376 Engine time: 0.05578938126564026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.979043007362634,
    "estimated_duration": 3600.022468123518,
    "input_throughput": 4578.478925047219,
    "output_throughput": 3994.9270115240165,
    "total_throughput": 8573.405936571235,
    "itl": 121.06024686319934,
    "ttft": 2164281.5668329056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.20972997828423,
    "arrivals": 702770,
    "finished_requests": 66607,
    "scheduler_time": 97.92783904604417
}
#Debug simulation 
Total elapsed time: 4.979138643946499. Arrivals time: 0.22097304835915565 Scheduler time: 4.600200586952269 Scheduler overhead time: 0.04415842471644282 Adapter cache time: 0.04802103620022535 Engine time: 0.04521697573363781 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.6750614531338215,
    "estimated_duration": 3600.0600127843572,
    "input_throughput": 4210.545920393237,
    "output_throughput": 3682.2308386318687,
    "total_throughput": 7892.776759025106,
    "itl": 93.84880341871727,
    "ttft": 2209913.7885409044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.33982489490396,
    "arrivals": 702770,
    "finished_requests": 61327,
    "scheduler_time": 104.79455585622688
}
#Debug simulation 
Total elapsed time: 4.6751512861810625. Arrivals time: 0.209664526861161 Scheduler time: 4.2710774801671505 Scheduler overhead time: 0.05411321669816971 Adapter cache time: 0.05903194798156619 Engine time: 0.05590727040544152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.998173983767629,
    "estimated_duration": 3600.0302166577903,
    "input_throughput": 4579.952113653968,
    "output_throughput": 3996.3028458568,
    "total_throughput": 8576.254959510768,
    "itl": 121.02084835028192,
    "ttft": 2164023.2975322073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2515,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 16.055561165535924,
    "arrivals": 702770,
    "finished_requests": 66629,
    "scheduler_time": 97.95917607542886
}
#Debug simulation 
Total elapsed time: 4.998266615904868. Arrivals time: 0.22544495481997728 Scheduler time: 4.612975190859288 Scheduler overhead time: 0.04439805215224624 Adapter cache time: 0.048243154771625996 Engine time: 0.04648613044992089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 470554347 . Total output tokens: 414384673
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.741166582796723,
    "estimated_duration": 3600.0008612041056,
    "input_throughput": 4210.78954823632,
    "output_throughput": 3682.401896861213,
    "total_throughput": 7893.191445097534,
    "itl": 93.84456927559076,
    "ttft": 2209883.859874893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.178066379967856,
    "arrivals": 702770,
    "finished_requests": 61328,
    "scheduler_time": 104.79739115182996
}
#Debug simulation 
Total elapsed time: 4.741258473135531. Arrivals time: 0.2540978896431625 Scheduler time: 4.292912991717458 Scheduler overhead time: 0.053973095025867224 Adapter cache time: 0.05899467132985592 Engine time: 0.05587788764387369 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.2100509139709175,
    "estimated_duration": 3600.065549899185,
    "input_throughput": 4755.015919218297,
    "output_throughput": 4138.770751109588,
    "total_throughput": 8893.786670327885,
    "itl": 133.02659557458193,
    "ttft": 2142077.7701725666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2067,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.667849469040894,
    "arrivals": 700393,
    "finished_requests": 69171,
    "scheduler_time": 96.482651787659
}
#Debug simulation 
Total elapsed time: 5.210141024086624. Arrivals time: 0.2960215611383319 Scheduler time: 4.771308507304639 Scheduler overhead time: 0.04091493412852287 Adapter cache time: 0.040756127797067165 Engine time: 0.042064165230840445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.09575490327552,
    "estimated_duration": 3600.078677246583,
    "input_throughput": 4624.780315282984,
    "output_throughput": 4029.1180555792303,
    "total_throughput": 8653.898370862215,
    "itl": 120.17923418522788,
    "ttft": 2157983.0741457907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.576807005116407,
    "arrivals": 700393,
    "finished_requests": 67314,
    "scheduler_time": 98.69539646769194
}
#Debug simulation 
Total elapsed time: 5.095849882345647. Arrivals time: 0.2977820304222405 Scheduler time: 4.641524409409612 Scheduler overhead time: 0.04440833209082484 Adapter cache time: 0.045720980037003756 Engine time: 0.04565963987261057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.793217356316745,
    "estimated_duration": 3600.021424139987,
    "input_throughput": 4251.999417935901,
    "output_throughput": 3710.5087515391897,
    "total_throughput": 7962.508169475092,
    "itl": 93.1851528571878,
    "ttft": 2201879.8558490747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1959,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.773529419125687,
    "arrivals": 700393,
    "finished_requests": 61895,
    "scheduler_time": 105.53160658147355
}
#Debug simulation 
Total elapsed time: 4.793314932379872. Arrivals time: 0.2801594058983028 Scheduler time: 4.319508771412075 Scheduler overhead time: 0.05434338981285691 Adapter cache time: 0.05757286632433534 Engine time: 0.05614025518298149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.432756914757192,
    "estimated_duration": 3600.02326843792,
    "input_throughput": 4626.028155430848,
    "output_throughput": 4029.896730721694,
    "total_throughput": 8655.924886152541,
    "itl": 120.14432176299053,
    "ttft": 2158091.331483939,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.578118941379023,
    "arrivals": 700393,
    "finished_requests": 67329,
    "scheduler_time": 98.72043767208162
}
#Debug simulation 
Total elapsed time: 5.43287670891732. Arrivals time: 0.6205763597972691 Scheduler time: 4.654787519946694 Scheduler overhead time: 0.044344518799334764 Adapter cache time: 0.04658904951065779 Engine time: 0.04592953296378255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.7902787839993834,
    "estimated_duration": 3600.085773875779,
    "input_throughput": 4252.07757856277,
    "output_throughput": 3710.749920721459,
    "total_throughput": 7962.82749928423,
    "itl": 93.18196853529493,
    "ttft": 2201922.390660189,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1959,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.631861270321233,
    "arrivals": 700393,
    "finished_requests": 61899,
    "scheduler_time": 105.5375489755138
}
#Debug simulation 
Total elapsed time: 4.790378372184932. Arrivals time: 0.2825495474971831 Scheduler time: 4.313931053504348 Scheduler overhead time: 0.054277414456009865 Adapter cache time: 0.05785674974322319 Engine time: 0.056140124797821045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.115218008868396,
    "estimated_duration": 3600.052627480618,
    "input_throughput": 4627.017636588615,
    "output_throughput": 4031.064404232277,
    "total_throughput": 8658.082040820891,
    "itl": 120.11263920531769,
    "ttft": 2157748.2094142763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.578599840594228,
    "arrivals": 700393,
    "finished_requests": 67346,
    "scheduler_time": 98.74857185590385
}
#Debug simulation 
Total elapsed time: 5.115311816800386. Arrivals time: 0.2315204255282879 Scheduler time: 4.7259620456025 Scheduler overhead time: 0.04439128469675779 Adapter cache time: 0.046701548621058464 Engine time: 0.045885862316936255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468915456 . Total output tokens: 413005167
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.789795503020287,
    "estimated_duration": 3600.051013394668,
    "input_throughput": 4252.234188638643,
    "output_throughput": 3710.8629711896365,
    "total_throughput": 7963.0971598282795,
    "itl": 93.17836005340133,
    "ttft": 2201884.016515411,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1959,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.494749699402304,
    "arrivals": 700393,
    "finished_requests": 61900,
    "scheduler_time": 105.54044613730822
}
#Debug simulation 
Total elapsed time: 4.78994121029973. Arrivals time: 0.28907017642632127 Scheduler time: 4.307241704314947 Scheduler overhead time: 0.05410410091280937 Adapter cache time: 0.057962600607424974 Engine time: 0.05608329828828573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.09489079983905,
    "estimated_duration": 3600.1188401903946,
    "input_throughput": 4801.890928435503,
    "output_throughput": 4160.6807066424035,
    "total_throughput": 8962.571635077906,
    "itl": 132.23128491249707,
    "ttft": 2133174.6836288036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1852,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.246181527171611,
    "arrivals": 699127,
    "finished_requests": 70026,
    "scheduler_time": 96.96684917985635
}
#Debug simulation 
Total elapsed time: 5.094981160014868. Arrivals time: 0.22882355470210314 Scheduler time: 4.724826148245484 Scheduler overhead time: 0.04087517270818353 Adapter cache time: 0.039317259564995766 Engine time: 0.0420435625128448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.03686532471329,
    "estimated_duration": 3600.048713274506,
    "input_throughput": 4673.640369909366,
    "output_throughput": 4050.625189106456,
    "total_throughput": 8724.265559015823,
    "itl": 119.45737745814175,
    "ttft": 2147780.5763246114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1854,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.582171408245138,
    "arrivals": 699127,
    "finished_requests": 68161,
    "scheduler_time": 99.22260039844171
}
#Debug simulation 
Total elapsed time: 5.036963941063732. Arrivals time: 0.2242670156992972 Scheduler time: 4.656975525896996 Scheduler overhead time: 0.04469260713085532 Adapter cache time: 0.04433394270017743 Engine time: 0.0459389123134315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 3.7801203839480877,
    "estimated_duration": 3600.0658398594696,
    "input_throughput": 3308.694765556002,
    "output_throughput": 2875.6699073048394,
    "total_throughput": 6184.364672860842,
    "itl": 73.14794309151588,
    "ttft": 2254733.1344462577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.01124154842447,
    "arrivals": 699127,
    "finished_requests": 48271,
    "scheduler_time": 110.61469476244363
}
#Debug simulation 
Total elapsed time: 3.7802043110132217. Arrivals time: 0.1809042594395578 Scheduler time: 3.3932584868744016 Scheduler overhead time: 0.06400086684152484 Adapter cache time: 0.04866440827026963 Engine time: 0.0641777734272182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.051013506948948,
    "estimated_duration": 3600.051937351321,
    "input_throughput": 4674.6056148238595,
    "output_throughput": 4052.1404284886007,
    "total_throughput": 8726.74604331246,
    "itl": 119.42634706185778,
    "ttft": 2147662.5252975356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1854,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.696486427271923,
    "arrivals": 699127,
    "finished_requests": 68182,
    "scheduler_time": 99.24759890335288
}
#Debug simulation 
Total elapsed time: 5.051114059984684. Arrivals time: 0.22350785182788968 Scheduler time: 4.671136621385813 Scheduler overhead time: 0.0446453164331615 Adapter cache time: 0.04473401326686144 Engine time: 0.04619053239002824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 3.776460335124284,
    "estimated_duration": 3600.034560119981,
    "input_throughput": 3308.885456811558,
    "output_throughput": 2875.7232262945186,
    "total_throughput": 6184.608683106077,
    "itl": 73.14534574524255,
    "ttft": 2254706.9510495546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.904990436821132,
    "arrivals": 699127,
    "finished_requests": 48273,
    "scheduler_time": 110.61687605891879
}
#Debug simulation 
Total elapsed time: 3.7765513341873884. Arrivals time: 0.1783444252796471 Scheduler time: 3.391669037286192 Scheduler overhead time: 0.06502695521339774 Adapter cache time: 0.048892013262957335 Engine time: 0.06342178443446755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.050736352801323,
    "estimated_duration": 3600.084486308865,
    "input_throughput": 4675.6933244248985,
    "output_throughput": 4053.1129354024097,
    "total_throughput": 8728.806259827308,
    "itl": 119.39573106373628,
    "ttft": 2147485.4607000486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1855,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.84217334475881,
    "arrivals": 699127,
    "finished_requests": 68199,
    "scheduler_time": 99.27256511465497
}
#Debug simulation 
Total elapsed time: 5.050827573984861. Arrivals time: 0.22186841676011682 Scheduler time: 4.673199401702732 Scheduler overhead time: 0.044504080433398485 Adapter cache time: 0.044542028568685055 Engine time: 0.04593382263556123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 468099455 . Total output tokens: 412296334
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 3.775608450639993,
    "estimated_duration": 3600.003449026632,
    "input_throughput": 3309.055718605194,
    "output_throughput": 2875.7497448507065,
    "total_throughput": 6184.8054634559,
    "itl": 73.14072686612775,
    "ttft": 2254824.399369935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.798739325217793,
    "arrivals": 699127,
    "finished_requests": 48276,
    "scheduler_time": 110.61906484833844
}
#Debug simulation 
Total elapsed time: 3.7756973346695304. Arrivals time: 0.18128466000780463 Scheduler time: 3.388701956253499 Scheduler overhead time: 0.06430742843076587 Adapter cache time: 0.04880689410492778 Engine time: 0.06320049799978733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.160969526972622,
    "estimated_duration": 3600.0418895506673,
    "input_throughput": 4809.222373287701,
    "output_throughput": 4167.910668915239,
    "total_throughput": 8977.133042202939,
    "itl": 131.7240502164798,
    "ttft": 2143240.1238878947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1739,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.498979306561244,
    "arrivals": 698509,
    "finished_requests": 69947,
    "scheduler_time": 97.25464573663194
}
#Debug simulation 
Total elapsed time: 5.161088011227548. Arrivals time: 0.2274785446934402 Scheduler time: 4.791927990037948 Scheduler overhead time: 0.0412734760902822 Adapter cache time: 0.03884271392598748 Engine time: 0.042354338336735964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.060495615005493,
    "estimated_duration": 3600.049453895401,
    "input_throughput": 4681.832073657318,
    "output_throughput": 4060.206724155933,
    "total_throughput": 8742.038797813251,
    "itl": 118.90593883779286,
    "ttft": 2158114.5880111814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1728,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.641751714348626,
    "arrivals": 698509,
    "finished_requests": 68062,
    "scheduler_time": 99.58004645665604
}
#Debug simulation 
Total elapsed time: 5.060610313899815. Arrivals time: 0.22368757845833898 Scheduler time: 4.68144642887637 Scheduler overhead time: 0.04481400549411774 Adapter cache time: 0.04356847424060106 Engine time: 0.04621640546247363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.744445766787976,
    "estimated_duration": 3600.0296217765413,
    "input_throughput": 4297.711859483239,
    "output_throughput": 3735.332042452481,
    "total_throughput": 8033.04390193572,
    "itl": 92.47357682565391,
    "ttft": 2203831.904832262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.180521130394018,
    "arrivals": 698509,
    "finished_requests": 62523,
    "scheduler_time": 106.2851956487001
}
#Debug simulation 
Total elapsed time: 4.744558174628764. Arrivals time: 0.21340574556961656 Scheduler time: 4.338704604189843 Scheduler overhead time: 0.05462912330403924 Adapter cache time: 0.05527746398001909 Engine time: 0.05666754860430956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.054142827633768,
    "estimated_duration": 3600.051603826938,
    "input_throughput": 4683.165647425113,
    "output_throughput": 4061.3578939972513,
    "total_throughput": 8744.523541422364,
    "itl": 118.87884637029815,
    "ttft": 2158169.9726779633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1728,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.821313119121053,
    "arrivals": 698509,
    "finished_requests": 68078,
    "scheduler_time": 99.6016814488971
}
#Debug simulation 
Total elapsed time: 5.05425413697958. Arrivals time: 0.22349361376836896 Scheduler time: 4.675340264104307 Scheduler overhead time: 0.044858729001134634 Adapter cache time: 0.0434083454310894 Engine time: 0.04634996969252825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.804379316978157,
    "estimated_duration": 3600.0130267524482,
    "input_throughput": 4297.826670354417,
    "output_throughput": 3735.5256495089166,
    "total_throughput": 8033.352319863333,
    "itl": 92.47072103938824,
    "ttft": 2203731.5421048393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.067435151962394,
    "arrivals": 698509,
    "finished_requests": 62526,
    "scheduler_time": 106.28802079955233
}
#Debug simulation 
Total elapsed time: 4.804470941890031. Arrivals time: 0.22090055420994759 Scheduler time: 4.390915399417281 Scheduler overhead time: 0.05485110776498914 Adapter cache time: 0.05508599476888776 Engine time: 0.056828174740076065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.0683948141522706,
    "estimated_duration": 3600.0910252902436,
    "input_throughput": 4683.931567713769,
    "output_throughput": 4062.1403451377982,
    "total_throughput": 8746.071912851567,
    "itl": 118.85100807451906,
    "ttft": 2158068.075335241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1728,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.031415385306245,
    "arrivals": 698509,
    "finished_requests": 68093,
    "scheduler_time": 99.6235313481445
}
#Debug simulation 
Total elapsed time: 5.068488325923681. Arrivals time: 0.222056751139462 Scheduler time: 4.6868916396051645 Scheduler overhead time: 0.044803159311413765 Adapter cache time: 0.04744503367692232 Engine time: 0.04622555850073695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467701672 . Total output tokens: 411944134
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.741685998160392,
    "estimated_duration": 3600.0946935190896,
    "input_throughput": 4297.944170150467,
    "output_throughput": 3735.914509196696,
    "total_throughput": 8033.858679347162,
    "itl": 92.46742209979557,
    "ttft": 2203855.4935430577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1621,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.955177642237228,
    "arrivals": 698509,
    "finished_requests": 62532,
    "scheduler_time": 106.29384151816352
}
#Debug simulation 
Total elapsed time: 4.741776364855468. Arrivals time: 0.21407741541042924 Scheduler time: 4.33610559348017 Scheduler overhead time: 0.054660082794725895 Adapter cache time: 0.05458718538284302 Engine time: 0.056660143192857504 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 268384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 435376779 . Total output tokens: 383506976
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.849427775014192,
    "estimated_duration": 3600.128973730668,
    "input_throughput": 5132.1569684915,
    "output_throughput": 4514.081056146014,
    "total_throughput": 9646.238024637514,
    "itl": 122.1696630524212,
    "ttft": 2086315.313925706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.404879343938415,
    "arrivals": 650482,
    "finished_requests": 75252,
    "scheduler_time": 105.04344094328646
}
#Debug simulation 
Total elapsed time: 5.849495343863964. Arrivals time: 0.5473339022137225 Scheduler time: 5.153082894627005 Scheduler overhead time: 0.044071000535041094 Adapter cache time: 0.03931284276768565 Engine time: 0.04531562654301524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 235584,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 435376779 . Total output tokens: 383506976
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.425221289042383,
    "estimated_duration": 3600.1161670441475,
    "input_throughput": 5003.7213145792075,
    "output_throughput": 4403.987333835824,
    "total_throughput": 9407.708648415031,
    "itl": 110.11855121905468,
    "ttft": 2101597.4226136557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1835,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.36508966321575,
    "arrivals": 650482,
    "finished_requests": 73371,
    "scheduler_time": 107.64777356871399
}
#Debug simulation 
Total elapsed time: 5.425324508920312. Arrivals time: 0.23971343785524368 Scheduler time: 5.025227392092347 Scheduler overhead time: 0.04779060697183013 Adapter cache time: 0.04081033822149038 Engine time: 0.049336429219692945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 168064,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 435376779 . Total output tokens: 383506976
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.116933627054095,
    "estimated_duration": 3600.0667399435315,
    "input_throughput": 4598.432250247579,
    "output_throughput": 4038.201247410216,
    "total_throughput": 8636.633497657796,
    "itl": 85.66398956039008,
    "ttft": 2152102.591545865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.577000489490871,
    "arrivals": 650482,
    "finished_requests": 67350,
    "scheduler_time": 114.62715045319402
}
#Debug simulation 
Total elapsed time: 5.117038163822144. Arrivals time: 0.23134607216343284 Scheduler time: 4.694022348616272 Scheduler overhead time: 0.058682555332779884 Adapter cache time: 0.044662089087069035 Engine time: 0.06056258035823703 
