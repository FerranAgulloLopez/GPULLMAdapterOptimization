INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_32_slots_32_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_32_slots_32_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.597360612009652,
    "estimated_duration": 3600.0185597175077,
    "input_throughput": 5500.311921045706,
    "output_throughput": 4833.006750211112,
    "total_throughput": 10333.318671256819,
    "itl": 51.801827629807896,
    "ttft": 13883.210886597846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 80362,
    "finished_requests": 80054,
    "scheduler_time": 58.11924338412183
}
#Debug simulation 
Total elapsed time: 5.597488206927665. Arrivals time: 0.18081794865429401 Scheduler time: 5.188739010482095 Scheduler overhead time: 0.08426106418482959 Adapter cache time: 0.017775065964087844 Engine time: 0.08607954275794327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_32_slots_32_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_32_slots_32_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.569119602907449,
    "estimated_duration": 3600.013165401963,
    "input_throughput": 5500.32016279837,
    "output_throughput": 4833.013992063362,
    "total_throughput": 10333.334154861732,
    "itl": 51.80191253654949,
    "ttft": 13838.512373400268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 80362,
    "finished_requests": 80054,
    "scheduler_time": 58.119178994992296
}
#Debug simulation 
Total elapsed time: 5.569237230927683. Arrivals time: 0.17966784175951034 Scheduler time: 5.1633058487204835 Scheduler overhead time: 0.084488385473378 Adapter cache time: 0.017732607666403055 Engine time: 0.08458615315612406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_32_slots_32_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_32_slots_32_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.593176224036142,
    "estimated_duration": 3600.037821513401,
    "input_throughput": 5500.282769717088,
    "output_throughput": 4833.015613343115,
    "total_throughput": 10333.298383060202,
    "itl": 51.802050303086304,
    "ttft": 13838.461552901315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 80362,
    "finished_requests": 80055,
    "scheduler_time": 58.119676260532465
}
#Debug simulation 
Total elapsed time: 5.59327996103093. Arrivals time: 0.1804918380221352 Scheduler time: 5.1831339474301785 Scheduler overhead time: 0.0842335243942216 Adapter cache time: 0.017761722556315362 Engine time: 0.08793191541917622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_32_slots_32_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_32_slots_32_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.556084006093442,
    "estimated_duration": 3600.042681937411,
    "input_throughput": 5500.275343775565,
    "output_throughput": 4833.009088280163,
    "total_throughput": 10333.284432055727,
    "itl": 51.80175996308348,
    "ttft": 13838.451770393442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469845,
    "arrivals": 80362,
    "finished_requests": 80055,
    "scheduler_time": 58.11976996120376
}
#Debug simulation 
Total elapsed time: 5.5561858779983595. Arrivals time: 0.1788002640241757 Scheduler time: 5.151384998229332 Scheduler overhead time: 0.08396972599439323 Adapter cache time: 0.017731974716298282 Engine time: 0.08486411964986473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_32_slots_32_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_32_slots_32_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.578483457909897,
    "estimated_duration": 3600.034020954222,
    "input_throughput": 5500.288576370594,
    "output_throughput": 4833.020715562078,
    "total_throughput": 10333.309291932672,
    "itl": 51.802231325697726,
    "ttft": 13838.52047027167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 80362,
    "finished_requests": 80055,
    "scheduler_time": 58.11966020422456
}
#Debug simulation 
Total elapsed time: 5.578607133938931. Arrivals time: 0.1822795910993591 Scheduler time: 5.168839146965183 Scheduler overhead time: 0.08438085205852985 Adapter cache time: 0.0178163037635386 Engine time: 0.08549578255042434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_32_slots_32_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_32_slots_32_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.5934560829773545,
    "estimated_duration": 3600.052752724286,
    "input_throughput": 5500.259957306381,
    "output_throughput": 4832.995568421473,
    "total_throughput": 10333.255525727853,
    "itl": 51.80151789474483,
    "ttft": 13838.356147694338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 80362,
    "finished_requests": 80055,
    "scheduler_time": 58.11981866279938
}
#Debug simulation 
Total elapsed time: 5.593588000978343. Arrivals time: 0.18158736161421984 Scheduler time: 5.183864031452686 Scheduler overhead time: 0.08429026475641876 Adapter cache time: 0.017704992438666523 Engine time: 0.08627514902036637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_32_slots_32_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_32_slots_32_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 270, 270, 4320, 270, 17280, 4320, 270, 270, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 17280, 17280, 270, 17280, 17280, 17280, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 240300 . Total input tokens: 53587571 . Total output tokens: 47108315
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.581564013031311,
    "estimated_duration": 3600.0208839077586,
    "input_throughput": 5500.308647794876,
    "output_throughput": 4833.0383520202395,
    "total_throughput": 10333.346999815116,
    "itl": 51.80219411771671,
    "ttft": 13838.484916327505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 80362,
    "finished_requests": 80055,
    "scheduler_time": 58.11941726989228
}
#Debug simulation 
Total elapsed time: 5.581721011083573. Arrivals time: 0.18037133023608476 Scheduler time: 5.17427449405659 Scheduler overhead time: 0.08482449280563742 Adapter cache time: 0.017708864994347095 Engine time: 0.0848056779941544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.517696491093375,
    "estimated_duration": 3600.0346577631935,
    "input_throughput": 5478.711977804907,
    "output_throughput": 4803.489033837379,
    "total_throughput": 10282.201011642286,
    "itl": 50.29744135149414,
    "ttft": 10360.156128499273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 79870,
    "finished_requests": 79641,
    "scheduler_time": 57.27749952870249
}
#Debug simulation 
Total elapsed time: 5.51782006805297. Arrivals time: 0.17996876942925155 Scheduler time: 5.110441364813596 Scheduler overhead time: 0.08533241145778447 Adapter cache time: 0.01673220645170659 Engine time: 0.08542554371524602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.598593486938626,
    "estimated_duration": 3600.033893884518,
    "input_throughput": 5478.713140313755,
    "output_throughput": 4803.4900530730165,
    "total_throughput": 10282.203193386771,
    "itl": 50.29761429447104,
    "ttft": 10360.170018944349,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 79870,
    "finished_requests": 79641,
    "scheduler_time": 57.27757237622215
}
#Debug simulation 
Total elapsed time: 5.598726530908607. Arrivals time: 0.1817058614687994 Scheduler time: 5.189485017210245 Scheduler overhead time: 0.08450246299616992 Adapter cache time: 0.016804232727736235 Engine time: 0.08603872114326805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.5677945599891245,
    "estimated_duration": 3600.00221854481,
    "input_throughput": 5478.761345867348,
    "output_throughput": 4803.532317541196,
    "total_throughput": 10282.293663408544,
    "itl": 50.298047917390264,
    "ttft": 10225.191322740773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 79870,
    "finished_requests": 79641,
    "scheduler_time": 57.27712269146758
}
#Debug simulation 
Total elapsed time: 5.5679240740137175. Arrivals time: 0.18462766613811255 Scheduler time: 5.1536751934327185 Scheduler overhead time: 0.08532835519872606 Adapter cache time: 0.01686538860667497 Engine time: 0.08725787349976599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.633577179047279,
    "estimated_duration": 3600.005322705097,
    "input_throughput": 5478.756621720612,
    "output_throughput": 4803.528175621138,
    "total_throughput": 10282.28479734175,
    "itl": 50.29780552525506,
    "ttft": 10225.222113201271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 79870,
    "finished_requests": 79641,
    "scheduler_time": 57.27696878388737
}
#Debug simulation 
Total elapsed time: 5.633675600052811. Arrivals time: 0.18803466740064323 Scheduler time: 5.216481743962504 Scheduler overhead time: 0.08513531601056457 Adapter cache time: 0.016917278757318854 Engine time: 0.08681236661504954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.6197979329153895,
    "estimated_duration": 3600.0381895546075,
    "input_throughput": 5478.706602954169,
    "output_throughput": 4803.4843214092225,
    "total_throughput": 10282.19092436339,
    "itl": 50.297680431833165,
    "ttft": 10360.034560346086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 79870,
    "finished_requests": 79641,
    "scheduler_time": 57.277628962559945
}
#Debug simulation 
Total elapsed time: 5.619892210001126. Arrivals time: 0.18818428053054959 Scheduler time: 5.201929385773838 Scheduler overhead time: 0.08642832154873759 Adapter cache time: 0.016918788431212306 Engine time: 0.08622614771593362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.578889415948652,
    "estimated_duration": 3600.0226055251806,
    "input_throughput": 5478.730319562168,
    "output_throughput": 4803.505115067824,
    "total_throughput": 10282.235434629993,
    "itl": 50.297487034697234,
    "ttft": 10360.14932525188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 79870,
    "finished_requests": 79641,
    "scheduler_time": 57.27730925882406
}
#Debug simulation 
Total elapsed time: 5.578986700042151. Arrivals time: 0.18696865322999656 Scheduler time: 5.164087977260351 Scheduler overhead time: 0.08491428568959236 Adapter cache time: 0.016759846708737314 Engine time: 0.08611409575678408 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 135, 135, 4320, 135, 17280, 4320, 135, 135, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 17280, 17280, 135, 17280, 17280, 17280, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238950 . Total input tokens: 53284739 . Total output tokens: 46851078
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.581843937980011,
    "estimated_duration": 3600.036786429931,
    "input_throughput": 5478.708738295802,
    "output_throughput": 4803.486193581032,
    "total_throughput": 10282.194931876833,
    "itl": 50.29757221817647,
    "ttft": 10360.145368931364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 79870,
    "finished_requests": 79641,
    "scheduler_time": 57.27766548875689
}
#Debug simulation 
Total elapsed time: 5.582013077917509. Arrivals time: 0.18475096230395138 Scheduler time: 5.169131378526799 Scheduler overhead time: 0.08455497235991061 Adapter cache time: 0.0168036901159212 Engine time: 0.08652484288904816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.527243777061813,
    "estimated_duration": 3599.993442444324,
    "input_throughput": 5491.273058146892,
    "output_throughput": 4742.190582549659,
    "total_throughput": 10233.463640696551,
    "itl": 48.913149702821656,
    "ttft": 12145.092535762396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 79675,
    "finished_requests": 79408,
    "scheduler_time": 56.04681097464699
}
#Debug simulation 
Total elapsed time: 5.527337677078322. Arrivals time: 0.18695930822286755 Scheduler time: 5.112067778012715 Scheduler overhead time: 0.08509716961998492 Adapter cache time: 0.01610046171117574 Engine time: 0.08677553199231625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.524070305051282,
    "estimated_duration": 3599.9963448317926,
    "input_throughput": 5491.268630975144,
    "output_throughput": 4742.186759302855,
    "total_throughput": 10233.455390277999,
    "itl": 48.91320859593867,
    "ttft": 12145.097260939081,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 79675,
    "finished_requests": 79408,
    "scheduler_time": 56.04698518821725
}
#Debug simulation 
Total elapsed time: 5.524167479947209. Arrivals time: 0.18619142507668585 Scheduler time: 5.109967302298173 Scheduler overhead time: 0.08641797152813524 Adapter cache time: 0.016129738884046674 Engine time: 0.08522591774817556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.531003708019853,
    "estimated_duration": 3600.008225939648,
    "input_throughput": 5491.250508140202,
    "output_throughput": 4742.171108662961,
    "total_throughput": 10233.421616803163,
    "itl": 48.913371541061714,
    "ttft": 12144.962846961409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 79675,
    "finished_requests": 79408,
    "scheduler_time": 56.04715461943088
}
#Debug simulation 
Total elapsed time: 5.53110296302475. Arrivals time: 0.1860104869119823 Scheduler time: 5.117084354860708 Scheduler overhead time: 0.0852773868246004 Adapter cache time: 0.016185324639081955 Engine time: 0.08604557218495756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.551808802993037,
    "estimated_duration": 3600.0108429685474,
    "input_throughput": 5491.246516274094,
    "output_throughput": 4742.167661340333,
    "total_throughput": 10233.414177614428,
    "itl": 48.91331204029264,
    "ttft": 12144.96963921743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2195559008046984,
    "arrivals": 79675,
    "finished_requests": 79408,
    "scheduler_time": 56.04707043049813
}
#Debug simulation 
Total elapsed time: 5.551904013962485. Arrivals time: 0.18326153443194926 Scheduler time: 5.139658325235359 Scheduler overhead time: 0.08500104094855487 Adapter cache time: 0.01619227754417807 Engine time: 0.08729573851451278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.545153382001445,
    "estimated_duration": 3600.0002234674207,
    "input_throughput": 5491.2627146893565,
    "output_throughput": 4742.181650076916,
    "total_throughput": 10233.444364766274,
    "itl": 48.91332710340343,
    "ttft": 12145.012536039625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 79675,
    "finished_requests": 79408,
    "scheduler_time": 56.047001367449575
}
#Debug simulation 
Total elapsed time: 5.545253608957864. Arrivals time: 0.18407929479144514 Scheduler time: 5.132882966892794 Scheduler overhead time: 0.0854692185530439 Adapter cache time: 0.01619939540978521 Engine time: 0.08632984571158886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.552917896071449,
    "estimated_duration": 3599.9876326595463,
    "input_throughput": 5491.28192015362,
    "output_throughput": 4742.198235661133,
    "total_throughput": 10233.480155814754,
    "itl": 48.912977088025706,
    "ttft": 12144.913928605343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 79675,
    "finished_requests": 79408,
    "scheduler_time": 56.046750056676736
}
#Debug simulation 
Total elapsed time: 5.553044522064738. Arrivals time: 0.18421090464107692 Scheduler time: 5.140861981431954 Scheduler overhead time: 0.08492258645128459 Adapter cache time: 0.016219455166719854 Engine time: 0.08644621667917818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_32_slots_32_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 66, 66, 4320, 66, 17280, 4320, 66, 66, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 17280, 17280, 66, 17280, 17280, 17280, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 238260 . Total input tokens: 53134811 . Total output tokens: 46720168
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.5097487069433555,
    "estimated_duration": 3599.996716615959,
    "input_throughput": 5491.268063872757,
    "output_throughput": 4742.186269560754,
    "total_throughput": 10233.454333433512,
    "itl": 48.91326766880609,
    "ttft": 12145.035309407775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 79675,
    "finished_requests": 79408,
    "scheduler_time": 56.04698130730868
}
#Debug simulation 
Total elapsed time: 5.50990186992567. Arrivals time: 0.18352516321465373 Scheduler time: 5.096878313925117 Scheduler overhead time: 0.08513854362536222 Adapter cache time: 0.016153593314811587 Engine time: 0.08765727421268821 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.540466228034347,
    "estimated_duration": 3600.035077778792,
    "input_throughput": 5427.012953455588,
    "output_throughput": 4759.258904380261,
    "total_throughput": 10186.271857835849,
    "itl": 48.916683241656635,
    "ttft": 11033.947889658468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 79540,
    "finished_requests": 79298,
    "scheduler_time": 56.26524373988049
}
#Debug simulation 
Total elapsed time: 5.5405946749961. Arrivals time: 0.1821126762079075 Scheduler time: 5.132010285160504 Scheduler overhead time: 0.08484662079717964 Adapter cache time: 0.0156822846038267 Engine time: 0.08572799689136446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.5775681550148875,
    "estimated_duration": 3600.0329330670716,
    "input_throughput": 5427.016186586647,
    "output_throughput": 4759.26173969831,
    "total_throughput": 10186.277926284956,
    "itl": 48.91674916253929,
    "ttft": 11033.964267265075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 79540,
    "finished_requests": 79298,
    "scheduler_time": 56.265312091867784
}
#Debug simulation 
Total elapsed time: 5.577695916988887. Arrivals time: 0.18291687802411616 Scheduler time: 5.167010068660602 Scheduler overhead time: 0.08521188655868173 Adapter cache time: 0.015773328253999352 Engine time: 0.08653224480804056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.569628978031687,
    "estimated_duration": 3599.9894821932044,
    "input_throughput": 5427.033911248375,
    "output_throughput": 4759.193348965597,
    "total_throughput": 10186.227260213973,
    "itl": 48.91669159463826,
    "ttft": 11079.135859032045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 79540,
    "finished_requests": 79297,
    "scheduler_time": 56.26461632942224
}
#Debug simulation 
Total elapsed time: 5.5697405700339. Arrivals time: 0.18477210146375 Scheduler time: 5.156987116555683 Scheduler overhead time: 0.08504872652702034 Adapter cache time: 0.015745835611596704 Engine time: 0.08688842738047242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 5.575693142018281,
    "estimated_duration": 3599.9893658954356,
    "input_throughput": 5427.034086568875,
    "output_throughput": 4759.193502711486,
    "total_throughput": 10186.227589280361,
    "itl": 48.91653216227093,
    "ttft": 11079.127431626006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 79540,
    "finished_requests": 79297,
    "scheduler_time": 56.264503074797034
}
#Debug simulation 
Total elapsed time: 5.5757983409566805. Arrivals time: 0.18400961894076318 Scheduler time: 5.162830771878362 Scheduler overhead time: 0.08554886607453227 Adapter cache time: 0.015812118304893374 Engine time: 0.0871872054412961 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 5.585025522042997,
    "estimated_duration": 3599.986479774341,
    "input_throughput": 5427.038437440093,
    "output_throughput": 4759.1973181726935,
    "total_throughput": 10186.235755612786,
    "itl": 48.916748025160274,
    "ttft": 11079.100138752718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 79540,
    "finished_requests": 79297,
    "scheduler_time": 56.264591978624324
}
#Debug simulation 
Total elapsed time: 5.585124603006989. Arrivals time: 0.18625534465536475 Scheduler time: 5.171496962546371 Scheduler overhead time: 0.08503601921256632 Adapter cache time: 0.01582806999795139 Engine time: 0.08603422401938587 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 5.566634726943448,
    "estimated_duration": 3600.028055570284,
    "input_throughput": 5427.023539377683,
    "output_throughput": 4759.268187782461,
    "total_throughput": 10186.291727160144,
    "itl": 48.916843152189,
    "ttft": 11033.980706656015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 79540,
    "finished_requests": 79298,
    "scheduler_time": 56.265194423660205
}
#Debug simulation 
Total elapsed time: 5.566730948979966. Arrivals time: 0.18312739569228142 Scheduler time: 5.155049743596464 Scheduler overhead time: 0.08530504535883665 Adapter cache time: 0.015726909041404724 Engine time: 0.08715424011461437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 4320, 33, 33, 4320, 33, 17280, 4320, 33, 33, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 17280, 17280, 33, 17280, 17280, 17280, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280]
Prompts retrieved: 237930 . Total input tokens: 53065168 . Total output tokens: 46652625
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 5.5551904330495745,
    "estimated_duration": 3600.0355509047035,
    "input_throughput": 5427.0122402236175,
    "output_throughput": 4759.258278906241,
    "total_throughput": 10186.27051912986,
    "itl": 48.91680622668653,
    "ttft": 11033.899108660094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 79540,
    "finished_requests": 79298,
    "scheduler_time": 56.26530057206779
}
#Debug simulation 
Total elapsed time: 5.555362411076203. Arrivals time: 0.18737985391635448 Scheduler time: 5.13853793614544 Scheduler overhead time: 0.08617751067504287 Adapter cache time: 0.015756498789414763 Engine time: 0.08681507420260459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_32_slots_32_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_32_slots_32_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.887301984010264,
    "estimated_duration": 3600.0185699531025,
    "input_throughput": 4778.081464236469,
    "output_throughput": 4153.068854918371,
    "total_throughput": 8931.15031915484,
    "itl": 42.8700316736703,
    "ttft": 9297.486957924652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 69453,
    "finished_requests": 69274,
    "scheduler_time": 45.52837838645951
}
#Debug simulation 
Total elapsed time: 4.887403532047756. Arrivals time: 0.16000742348842323 Scheduler time: 4.480656710336916 Scheduler overhead time: 0.08883208420593292 Adapter cache time: 0.02593065006658435 Engine time: 0.08960095536895096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_32_slots_32_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_32_slots_32_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.8830576579784974,
    "estimated_duration": 3600.041067867646,
    "input_throughput": 4778.051604335864,
    "output_throughput": 4153.0429009399495,
    "total_throughput": 8931.094505275812,
    "itl": 42.87054041586283,
    "ttft": 9349.19107889442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 69453,
    "finished_requests": 69274,
    "scheduler_time": 45.5289065883407
}
#Debug simulation 
Total elapsed time: 4.883156743948348. Arrivals time: 0.16180933290161192 Scheduler time: 4.475577388191596 Scheduler overhead time: 0.08927975955884904 Adapter cache time: 0.02572433464229107 Engine time: 0.08849476801697165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_32_slots_32_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_32_slots_32_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.893510686000809,
    "estimated_duration": 3600.0409828770157,
    "input_throughput": 4778.05171713725,
    "output_throughput": 4153.042998985981,
    "total_throughput": 8931.09471612323,
    "itl": 42.87036521739684,
    "ttft": 9349.164338316003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 69453,
    "finished_requests": 69274,
    "scheduler_time": 45.52882272720726
}
#Debug simulation 
Total elapsed time: 4.893606664030813. Arrivals time: 0.16459476936142892 Scheduler time: 4.481139117036946 Scheduler overhead time: 0.08990185894072056 Adapter cache time: 0.025619960739277303 Engine time: 0.08989341091364622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_32_slots_32_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_32_slots_32_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.901167456060648,
    "estimated_duration": 3600.040966130157,
    "input_throughput": 4778.051739364041,
    "output_throughput": 4153.043018305323,
    "total_throughput": 8931.094757669365,
    "itl": 42.87019145773685,
    "ttft": 9349.241507986253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 69453,
    "finished_requests": 69274,
    "scheduler_time": 45.52869004155301
}
#Debug simulation 
Total elapsed time: 4.901264354004525. Arrivals time: 0.16363597102463245 Scheduler time: 4.489985179854557 Scheduler overhead time: 0.08920393779408187 Adapter cache time: 0.025685604545287788 Engine time: 0.08995235478505492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_32_slots_32_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_32_slots_32_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.9106883419444785,
    "estimated_duration": 3600.0408519089688,
    "input_throughput": 4778.051890960862,
    "output_throughput": 4153.043150072025,
    "total_throughput": 8931.095041032886,
    "itl": 42.87037626267375,
    "ttft": 9349.229290761026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 69453,
    "finished_requests": 69274,
    "scheduler_time": 45.52885083598902
}
#Debug simulation 
Total elapsed time: 4.910788013949059. Arrivals time: 0.16506215638946742 Scheduler time: 4.498178553418256 Scheduler overhead time: 0.0892101003555581 Adapter cache time: 0.025976414559409022 Engine time: 0.08971134212333709 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_32_slots_32_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_32_slots_32_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.889305338962004,
    "estimated_duration": 3600.000208284672,
    "input_throughput": 4778.105834664942,
    "output_throughput": 4153.090037493056,
    "total_throughput": 8931.195872158,
    "itl": 42.8698305352746,
    "ttft": 9297.462397424377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 69453,
    "finished_requests": 69274,
    "scheduler_time": 45.528110800017444
}
#Debug simulation 
Total elapsed time: 4.889407498994842. Arrivals time: 0.16343720501754433 Scheduler time: 4.479488640325144 Scheduler overhead time: 0.08874814747832716 Adapter cache time: 0.025692757568322122 Engine time: 0.08972838625777513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_32_slots_32_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_32_slots_32_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 540, 540, 1080, 540, 17280, 1080, 540, 540, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 17280, 17280, 540, 17280, 17280, 17280, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 207360 . Total input tokens: 46234556 . Total output tokens: 40662444
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.914972385042347,
    "estimated_duration": 3600.041054325835,
    "input_throughput": 4778.051622308845,
    "output_throughput": 4153.042916561916,
    "total_throughput": 8931.09453887076,
    "itl": 42.87043253987314,
    "ttft": 9349.192401511817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 69453,
    "finished_requests": 69274,
    "scheduler_time": 45.52886126146624
}
#Debug simulation 
Total elapsed time: 4.9151266609551385. Arrivals time: 0.16383493680041283 Scheduler time: 4.502985205966979 Scheduler overhead time: 0.09065381763502955 Adapter cache time: 0.025844690506346524 Engine time: 0.08913732890505344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_32_slots_32_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_32_slots_32_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.860952828079462,
    "estimated_duration": 3599.9682564634422,
    "input_throughput": 4683.597687209557,
    "output_throughput": 4132.462827495789,
    "total_throughput": 8816.060514705345,
    "itl": 41.445794042079754,
    "ttft": 11991.64008608223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 68538,
    "finished_requests": 68311,
    "scheduler_time": 44.71696734347984
}
#Debug simulation 
Total elapsed time: 4.861041697091423. Arrivals time: 0.16204615321476012 Scheduler time: 4.447553402744234 Scheduler overhead time: 0.09192234941292554 Adapter cache time: 0.0243824680801481 Engine time: 0.09159348485991359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_32_slots_32_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_32_slots_32_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.881159387063235,
    "estimated_duration": 3599.969205127295,
    "input_throughput": 4683.596452987937,
    "output_throughput": 4132.461738509221,
    "total_throughput": 8816.058191497157,
    "itl": 41.446048552613455,
    "ttft": 11991.684027650303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 68538,
    "finished_requests": 68311,
    "scheduler_time": 44.71712096423552
}
#Debug simulation 
Total elapsed time: 4.8812731630168855. Arrivals time: 0.16248173720669 Scheduler time: 4.467142017441802 Scheduler overhead time: 0.09161526605021209 Adapter cache time: 0.024269121699035168 Engine time: 0.09225988294929266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_32_slots_32_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_32_slots_32_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.8517173760337755,
    "estimated_duration": 3599.98313577369,
    "input_throughput": 4683.578329145801,
    "output_throughput": 4132.445747361194,
    "total_throughput": 8816.024076506996,
    "itl": 41.44627041848508,
    "ttft": 11991.763042770834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 68538,
    "finished_requests": 68311,
    "scheduler_time": 44.71731948762913
}
#Debug simulation 
Total elapsed time: 4.851848744088784. Arrivals time: 0.16186573740560561 Scheduler time: 4.437822295818478 Scheduler overhead time: 0.09222345694433898 Adapter cache time: 0.024493288714438677 Engine time: 0.09156848420388997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_32_slots_32_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_32_slots_32_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.892557397019118,
    "estimated_duration": 3599.983507245693,
    "input_throughput": 4683.577845860747,
    "output_throughput": 4132.445320945934,
    "total_throughput": 8816.02316680668,
    "itl": 41.445989215377146,
    "ttft": 11991.763180556643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469845,
    "arrivals": 68538,
    "finished_requests": 68311,
    "scheduler_time": 44.71718180025606
}
#Debug simulation 
Total elapsed time: 4.892682819976471. Arrivals time: 0.16282452014274895 Scheduler time: 4.4773005661554635 Scheduler overhead time: 0.09206227515824139 Adapter cache time: 0.024457688094116747 Engine time: 0.09247675852384418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_32_slots_32_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_32_slots_32_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.892583967070095,
    "estimated_duration": 3599.9830323366587,
    "input_throughput": 4683.57846371739,
    "output_throughput": 4132.445866097287,
    "total_throughput": 8816.024329814678,
    "itl": 41.44620030356243,
    "ttft": 11991.736320654907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 68538,
    "finished_requests": 68311,
    "scheduler_time": 44.717343756477405
}
#Debug simulation 
Total elapsed time: 4.892681778059341. Arrivals time: 0.16335443407297134 Scheduler time: 4.477869553375058 Scheduler overhead time: 0.09153443668037653 Adapter cache time: 0.024423735681921244 Engine time: 0.09206773492041975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_32_slots_32_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_32_slots_32_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.8840761160245165,
    "estimated_duration": 3600.000668304873,
    "input_throughput": 4683.555519432506,
    "output_throughput": 4132.425621744394,
    "total_throughput": 8815.9811411769,
    "itl": 41.44575863669972,
    "ttft": 11991.762842700911,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 68538,
    "finished_requests": 68311,
    "scheduler_time": 44.717413270250276
}
#Debug simulation 
Total elapsed time: 4.884175883024. Arrivals time: 0.16332141414750367 Scheduler time: 4.4666239893995225 Scheduler overhead time: 0.09166805807035416 Adapter cache time: 0.02434008801355958 Engine time: 0.09313411160837859 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_32_slots_32_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_32_slots_32_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 270, 270, 1080, 270, 17280, 1080, 270, 270, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 17280, 17280, 270, 17280, 17280, 17280, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 204660 . Total input tokens: 45615803 . Total output tokens: 40119636
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.889823073055595,
    "estimated_duration": 3599.9681776634234,
    "input_throughput": 4683.597789729237,
    "output_throughput": 4132.46291795163,
    "total_throughput": 8816.060707680866,
    "itl": 41.44601194923717,
    "ttft": 11991.667818643005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 68538,
    "finished_requests": 68311,
    "scheduler_time": 44.71708055713011
}
#Debug simulation 
Total elapsed time: 4.889980097068474. Arrivals time: 0.16342212876770645 Scheduler time: 4.4742444776929915 Scheduler overhead time: 0.09164829854853451 Adapter cache time: 0.024558938574045897 Engine time: 0.0922603594372049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.826430040993728,
    "estimated_duration": 3600.0273219593378,
    "input_throughput": 4707.5784388148095,
    "output_throughput": 4062.652500103775,
    "total_throughput": 8770.230938918585,
    "itl": 39.985847263811536,
    "ttft": 10056.585090668794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 68105,
    "finished_requests": 67916,
    "scheduler_time": 43.16927989226894
}
#Debug simulation 
Total elapsed time: 4.826540875947103. Arrivals time: 0.16516744869295508 Scheduler time: 4.404238910181448 Scheduler overhead time: 0.09389741031918675 Adapter cache time: 0.02352665294893086 Engine time: 0.09479705849662423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.76270535599906,
    "estimated_duration": 3600.027314361108,
    "input_throughput": 4707.578448750641,
    "output_throughput": 4062.6525086784227,
    "total_throughput": 8770.230957429063,
    "itl": 39.98607411556345,
    "ttft": 10056.566183948455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 68105,
    "finished_requests": 67916,
    "scheduler_time": 43.16939310591917
}
#Debug simulation 
Total elapsed time: 4.762840645969845. Arrivals time: 0.16118953772820532 Scheduler time: 4.344596786424518 Scheduler overhead time: 0.09397612337488681 Adapter cache time: 0.023660363629460335 Engine time: 0.09421928483061492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.799224736983888,
    "estimated_duration": 3600.026681975659,
    "input_throughput": 4707.579275690098,
    "output_throughput": 4062.6532223293366,
    "total_throughput": 8770.232498019435,
    "itl": 39.98601433337344,
    "ttft": 10056.549152096413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 68105,
    "finished_requests": 67916,
    "scheduler_time": 43.16938910208602
}
#Debug simulation 
Total elapsed time: 4.799360536038876. Arrivals time: 0.16138337610755116 Scheduler time: 4.382326395018026 Scheduler overhead time: 0.09386812977027148 Adapter cache time: 0.023612513672560453 Engine time: 0.09335050696972758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.783476551063359,
    "estimated_duration": 3600.035981932527,
    "input_throughput": 4707.567114621587,
    "output_throughput": 4062.642727295418,
    "total_throughput": 8770.209841917005,
    "itl": 39.985801362206146,
    "ttft": 10056.630655155815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2195559008046984,
    "arrivals": 68105,
    "finished_requests": 67916,
    "scheduler_time": 43.169373592940396
}
#Debug simulation 
Total elapsed time: 4.783571645035408. Arrivals time: 0.16217702231369913 Scheduler time: 4.365111555554904 Scheduler overhead time: 0.0938564173411578 Adapter cache time: 0.02371580619364977 Engine time: 0.094166872324422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.794533821986988,
    "estimated_duration": 3600.0272934932386,
    "input_throughput": 4707.578476038526,
    "output_throughput": 4062.652532227939,
    "total_throughput": 8770.231008266464,
    "itl": 39.98603700026621,
    "ttft": 10056.567337381231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 68105,
    "finished_requests": 67916,
    "scheduler_time": 43.16939715072721
}
#Debug simulation 
Total elapsed time: 4.794633382000029. Arrivals time: 0.16164251789450645 Scheduler time: 4.376636489410885 Scheduler overhead time: 0.0939105796860531 Adapter cache time: 0.023730784188956022 Engine time: 0.09408454596996307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.800975926918909,
    "estimated_duration": 3600.016095464864,
    "input_throughput": 4707.593119194543,
    "output_throughput": 4062.6651693098647,
    "total_throughput": 8770.258288504408,
    "itl": 39.9858649440862,
    "ttft": 10056.631418099785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 68105,
    "finished_requests": 67916,
    "scheduler_time": 43.16911405513931
}
#Debug simulation 
Total elapsed time: 4.80107143195346. Arrivals time: 0.162046245415695 Scheduler time: 4.381058065104298 Scheduler overhead time: 0.0954266331391409 Adapter cache time: 0.023778304108418524 Engine time: 0.09394739242270589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 135, 135, 1080, 135, 17280, 1080, 135, 135, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 17280, 17280, 135, 17280, 17280, 17280, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 203310 . Total input tokens: 45312707 . Total output tokens: 39856658
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.806063502910547,
    "estimated_duration": 3600.0272976958354,
    "input_throughput": 4707.578470542997,
    "output_throughput": 4062.6525274852834,
    "total_throughput": 8770.23099802828,
    "itl": 39.98602837947337,
    "ttft": 10056.54802895517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 68105,
    "finished_requests": 67916,
    "scheduler_time": 43.16938505727796
}
#Debug simulation 
Total elapsed time: 4.80621146899648. Arrivals time: 0.16255493310745806 Scheduler time: 4.387566192657687 Scheduler overhead time: 0.09344472223892808 Adapter cache time: 0.02376862766686827 Engine time: 0.0940210212720558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.804780949023552,
    "estimated_duration": 3599.948711314243,
    "input_throughput": 4692.864080786428,
    "output_throughput": 4092.063299041289,
    "total_throughput": 8784.927379827717,
    "itl": 39.96737155503732,
    "ttft": 9346.166401541768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 67887,
    "finished_requests": 67712,
    "scheduler_time": 43.565863646862496
}
#Debug simulation 
Total elapsed time: 4.804878861061297. Arrivals time: 0.1601452340837568 Scheduler time: 4.389651802484877 Scheduler overhead time: 0.09401974477805197 Adapter cache time: 0.022768944152630866 Engine time: 0.09362149296794087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.834543557954021,
    "estimated_duration": 3599.946220866806,
    "input_throughput": 4692.867327315849,
    "output_throughput": 4092.066129935956,
    "total_throughput": 8784.933457251806,
    "itl": 39.9674401577106,
    "ttft": 9346.194318401751,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 67887,
    "finished_requests": 67712,
    "scheduler_time": 43.56589834118799
}
#Debug simulation 
Total elapsed time: 4.834638351923786. Arrivals time: 0.16003996704239398 Scheduler time: 4.42063931375742 Scheduler overhead time: 0.09330261312425137 Adapter cache time: 0.022546364227309823 Engine time: 0.09350654715672135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.813160515972413,
    "estimated_duration": 3599.9524303034514,
    "input_throughput": 4692.859232747124,
    "output_throughput": 4092.0590716689717,
    "total_throughput": 8784.918304416096,
    "itl": 39.96734566142732,
    "ttft": 9346.140874435123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137895,
    "arrivals": 67887,
    "finished_requests": 67712,
    "scheduler_time": 43.56599612764215
}
#Debug simulation 
Total elapsed time: 4.813257811940275. Arrivals time: 0.15924932633060962 Scheduler time: 4.39898109366186 Scheduler overhead time: 0.09414337680209428 Adapter cache time: 0.02276628080289811 Engine time: 0.09347593516577035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.822373287985101,
    "estimated_duration": 3599.953977725731,
    "input_throughput": 4692.8572155449665,
    "output_throughput": 4092.0573127177695,
    "total_throughput": 8784.914528262736,
    "itl": 39.96703443778877,
    "ttft": 9346.128464256248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 67887,
    "finished_requests": 67712,
    "scheduler_time": 43.56588779278602
}
#Debug simulation 
Total elapsed time: 4.822470101993531. Arrivals time: 0.16051471955142915 Scheduler time: 4.4060093392618 Scheduler overhead time: 0.09405919839628041 Adapter cache time: 0.022844878141768277 Engine time: 0.09398741775657982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.847299358109012,
    "estimated_duration": 3599.953976860678,
    "input_throughput": 4692.85721667264,
    "output_throughput": 4092.057313701073,
    "total_throughput": 8784.914530373713,
    "itl": 39.96749855009432,
    "ttft": 9346.149000199159,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861723,
    "arrivals": 67887,
    "finished_requests": 67712,
    "scheduler_time": 43.56602043746536
}
#Debug simulation 
Total elapsed time: 4.847404120024294. Arrivals time: 0.1635409821756184 Scheduler time: 4.429034061846323 Scheduler overhead time: 0.09421053435653448 Adapter cache time: 0.022505721542984247 Engine time: 0.09330918046180159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.82360082201194,
    "estimated_duration": 3599.952442331456,
    "input_throughput": 4692.8592170675465,
    "output_throughput": 4092.059057996762,
    "total_throughput": 8784.918275064309,
    "itl": 39.966631558168835,
    "ttft": 9346.264903023979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 67887,
    "finished_requests": 67712,
    "scheduler_time": 43.565730715359095
}
#Debug simulation 
Total elapsed time: 4.823704788926989. Arrivals time: 0.1629195441491902 Scheduler time: 4.40756842517294 Scheduler overhead time: 0.09327145747374743 Adapter cache time: 0.022599881165660918 Engine time: 0.0928176975576207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_32_slots_32_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 66, 66, 1080, 66, 17280, 1080, 66, 66, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 17280, 17280, 66, 17280, 17280, 17280, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202620 . Total input tokens: 45159304 . Total output tokens: 39717742
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.816166792064905,
    "estimated_duration": 3599.9523325899754,
    "input_throughput": 4692.859360125363,
    "output_throughput": 4092.059182739697,
    "total_throughput": 8784.91854286506,
    "itl": 39.96759653649356,
    "ttft": 9346.13726600422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2367897061258555,
    "arrivals": 67887,
    "finished_requests": 67712,
    "scheduler_time": 43.56603653474743
}
#Debug simulation 
Total elapsed time: 4.8163145580329. Arrivals time: 0.15625168720725924 Scheduler time: 4.404795840731822 Scheduler overhead time: 0.0939296503784135 Adapter cache time: 0.022567647509276867 Engine time: 0.0938728601904586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.80956489907112,
    "estimated_duration": 3600.040883205813,
    "input_throughput": 4680.723232507981,
    "output_throughput": 4074.535672199867,
    "total_throughput": 8755.258904707849,
    "itl": 39.79073703225597,
    "ttft": 9414.699191505828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 67772,
    "finished_requests": 67596,
    "scheduler_time": 43.21468332404513
}
#Debug simulation 
Total elapsed time: 4.809661534032784. Arrivals time: 0.15885948785580695 Scheduler time: 4.395272367983125 Scheduler overhead time: 0.09423370542936027 Adapter cache time: 0.021983646554872394 Engine time: 0.09455283789429814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.806423312984407,
    "estimated_duration": 3600.010237447908,
    "input_throughput": 4680.661411659062,
    "output_throughput": 4074.569801889974,
    "total_throughput": 8755.231213549036,
    "itl": 39.791165573196665,
    "ttft": 9414.832351817071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 67772,
    "finished_requests": 67595,
    "scheduler_time": 43.2144924250561
}
#Debug simulation 
Total elapsed time: 4.806522039929405. Arrivals time: 0.15966708958148956 Scheduler time: 4.391360575216822 Scheduler overhead time: 0.09396920946892351 Adapter cache time: 0.021959813544526696 Engine time: 0.09483543876558542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.801476489985362,
    "estimated_duration": 3600.0220446673698,
    "input_throughput": 4680.747726242593,
    "output_throughput": 4074.556993818442,
    "total_throughput": 8755.304720061034,
    "itl": 39.790817534502764,
    "ttft": 9414.758771720555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 67772,
    "finished_requests": 67596,
    "scheduler_time": 43.214666392776756
}
#Debug simulation 
Total elapsed time: 4.80160202702973. Arrivals time: 0.16074080497492105 Scheduler time: 4.38528706098441 Scheduler overhead time: 0.09436462819576263 Adapter cache time: 0.021973784547299147 Engine time: 0.09431717672850937 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.822820479050279,
    "estimated_duration": 3600.022012543275,
    "input_throughput": 4680.747768010332,
    "output_throughput": 4074.557030176957,
    "total_throughput": 8755.30479818729,
    "itl": 39.79062630932197,
    "ttft": 9414.714621857212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469845,
    "arrivals": 67772,
    "finished_requests": 67596,
    "scheduler_time": 43.214524824495186
}
#Debug simulation 
Total elapsed time: 4.822919053956866. Arrivals time: 0.1604184927418828 Scheduler time: 4.404703703825362 Scheduler overhead time: 0.09490056452341378 Adapter cache time: 0.021875502541661263 Engine time: 0.09587818698491901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.802355343010277,
    "estimated_duration": 3600.0199541520456,
    "input_throughput": 4680.648778228502,
    "output_throughput": 4074.5588043428056,
    "total_throughput": 8755.207582571307,
    "itl": 39.79129536894844,
    "ttft": 9467.886678652732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 67772,
    "finished_requests": 67595,
    "scheduler_time": 43.21463403431227
}
#Debug simulation 
Total elapsed time: 4.802483277977444. Arrivals time: 0.1613157979445532 Scheduler time: 4.385941704269499 Scheduler overhead time: 0.09421254426706582 Adapter cache time: 0.022086360841058195 Engine time: 0.09402451559435576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.802334620966576,
    "estimated_duration": 3600.0422440109546,
    "input_throughput": 4680.721463208676,
    "output_throughput": 4074.5341320376365,
    "total_throughput": 8755.255595246312,
    "itl": 39.79073485933625,
    "ttft": 9414.710720996425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 67772,
    "finished_requests": 67596,
    "scheduler_time": 43.21473590654975
}
#Debug simulation 
Total elapsed time: 4.8024455569684505. Arrivals time: 0.16401345178019255 Scheduler time: 4.3832697733305395 Scheduler overhead time: 0.0940469247289002 Adapter cache time: 0.022151691606268287 Engine time: 0.09422056772746146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 1080, 33, 33, 1080, 33, 17280, 1080, 33, 33, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 17280, 17280, 33, 17280, 17280, 17280, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280]
Prompts retrieved: 202290 . Total input tokens: 45083836 . Total output tokens: 39656756
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.797483408008702,
    "estimated_duration": 3600.013709799631,
    "input_throughput": 4680.656896981056,
    "output_throughput": 4074.5658718106424,
    "total_throughput": 8755.222768791698,
    "itl": 39.79122094483719,
    "ttft": 9414.875528871527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 67772,
    "finished_requests": 67595,
    "scheduler_time": 43.21453279118645
}
#Debug simulation 
Total elapsed time: 4.797639622003771. Arrivals time: 0.15871736884582788 Scheduler time: 4.381528603844345 Scheduler overhead time: 0.09554608224425465 Adapter cache time: 0.02187861443962902 Engine time: 0.09466609172523022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_32_slots_32_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_32_slots_32_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.706289823981933,
    "estimated_duration": 3599.9630997086692,
    "input_throughput": 4589.44898666796,
    "output_throughput": 3991.0009080822806,
    "total_throughput": 8580.44989475024,
    "itl": 38.63768876881112,
    "ttft": 7847.045950505908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 66605,
    "finished_requests": 66461,
    "scheduler_time": 41.5004966901772
}
#Debug simulation 
Total elapsed time: 4.706386107951403. Arrivals time: 0.15672515600454062 Scheduler time: 4.287078736233525 Scheduler overhead time: 0.09577031596563756 Adapter cache time: 0.024076377390883863 Engine time: 0.0968796043889597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_32_slots_32_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_32_slots_32_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.703552881022915,
    "estimated_duration": 3599.9640650984525,
    "input_throughput": 4589.447755931463,
    "output_throughput": 3990.9998378295136,
    "total_throughput": 8580.447593760977,
    "itl": 38.637956500834264,
    "ttft": 7847.052046419855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 66605,
    "finished_requests": 66461,
    "scheduler_time": 41.50061394863545
}
#Debug simulation 
Total elapsed time: 4.70364799303934. Arrivals time: 0.15694992116186768 Scheduler time: 4.284582832129672 Scheduler overhead time: 0.09599354572128505 Adapter cache time: 0.02389526681508869 Engine time: 0.09639787452761084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_32_slots_32_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_32_slots_32_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.694804370054044,
    "estimated_duration": 3599.93782617205,
    "input_throughput": 4589.252036490705,
    "output_throughput": 3990.868368767592,
    "total_throughput": 8580.120405258296,
    "itl": 38.63792080707338,
    "ttft": 7955.156582856255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 66605,
    "finished_requests": 66459,
    "scheduler_time": 41.500366381358894
}
#Debug simulation 
Total elapsed time: 4.694902559975162. Arrivals time: 0.1566704447614029 Scheduler time: 4.275913030025549 Scheduler overhead time: 0.09788770671002567 Adapter cache time: 0.023928037146106362 Engine time: 0.09503619628958404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_32_slots_32_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_32_slots_32_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.674259942024946,
    "estimated_duration": 3599.9378439723223,
    "input_throughput": 4589.252013798664,
    "output_throughput": 3990.8683490343224,
    "total_throughput": 8580.120362832986,
    "itl": 38.637645944671114,
    "ttft": 7955.1743043228325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 66605,
    "finished_requests": 66459,
    "scheduler_time": 41.500261257324674
}
#Debug simulation 
Total elapsed time: 4.674358627060428. Arrivals time: 0.1566245430149138 Scheduler time: 4.257570803747512 Scheduler overhead time: 0.09536287153605372 Adapter cache time: 0.023838216671720147 Engine time: 0.09518588171340525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_32_slots_32_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_32_slots_32_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.701504332013428,
    "estimated_duration": 3599.9696620492073,
    "input_throughput": 4589.440620617698,
    "output_throughput": 3990.9936329356806,
    "total_throughput": 8580.43425355338,
    "itl": 38.63778689816178,
    "ttft": 7847.088038930343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861723,
    "arrivals": 66605,
    "finished_requests": 66461,
    "scheduler_time": 41.50065827762435
}
#Debug simulation 
Total elapsed time: 4.701621007057838. Arrivals time: 0.15663040813524276 Scheduler time: 4.283323483890854 Scheduler overhead time: 0.09589159826282412 Adapter cache time: 0.023797839297913015 Engine time: 0.09607838944066316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_32_slots_32_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_32_slots_32_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.711498216027394,
    "estimated_duration": 3599.9510253033877,
    "input_throughput": 4589.3021554668685,
    "output_throughput": 3991.0015161356755,
    "total_throughput": 8580.303671602544,
    "itl": 38.63759282874287,
    "ttft": 7901.200636284064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 66605,
    "finished_requests": 66460,
    "scheduler_time": 41.500290609842
}
#Debug simulation 
Total elapsed time: 4.711633490980603. Arrivals time: 0.15653032367117703 Scheduler time: 4.293546187924221 Scheduler overhead time: 0.09641333878971636 Adapter cache time: 0.023997770971618593 Engine time: 0.09532688139006495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_32_slots_32_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_32_slots_32_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 270, 270, 540, 270, 17280, 540, 270, 270, 270, 17280, 17280, 270, 270, 540, 540, 540, 17280, 17280, 270, 17280, 17280, 17280, 17280, 540, 270, 540, 540, 540, 540, 17280]
Prompts retrieved: 198720 . Total input tokens: 44288177 . Total output tokens: 38964011
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.693626796943136,
    "estimated_duration": 3599.9679430179485,
    "input_throughput": 4589.442812134959,
    "output_throughput": 3990.995538686764,
    "total_throughput": 8580.438350821723,
    "itl": 38.6378487251069,
    "ttft": 7847.061574258834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2367897061258555,
    "arrivals": 66605,
    "finished_requests": 66461,
    "scheduler_time": 41.50067453880624
}
#Debug simulation 
Total elapsed time: 4.693776835920289. Arrivals time: 0.156071393401362 Scheduler time: 4.276174295460805 Scheduler overhead time: 0.09596073720604181 Adapter cache time: 0.023783304262906313 Engine time: 0.09616550442297012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.65960560797248,
    "estimated_duration": 3600.0167115725185,
    "input_throughput": 4543.194743353217,
    "output_throughput": 3967.9324137804774,
    "total_throughput": 8511.127157133695,
    "itl": 37.85611925403166,
    "ttft": 8552.712280154845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 66148,
    "finished_requests": 65991,
    "scheduler_time": 40.806742483565145
}
#Debug simulation 
Total elapsed time: 4.6597046019742265. Arrivals time: 0.156145399203524 Scheduler time: 4.2385669451905414 Scheduler overhead time: 0.09811615024227649 Adapter cache time: 0.022755977232009172 Engine time: 0.09762123390100896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.679425631067716,
    "estimated_duration": 3600.0168115849096,
    "input_throughput": 4543.194617138315,
    "output_throughput": 3967.9323035469897,
    "total_throughput": 8511.126920685305,
    "itl": 37.85637173913156,
    "ttft": 8552.670847832887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 66148,
    "finished_requests": 65991,
    "scheduler_time": 40.8068235846005
}
#Debug simulation 
Total elapsed time: 4.679522211081348. Arrivals time: 0.15707803575787693 Scheduler time: 4.258323850808665 Scheduler overhead time: 0.09744109155144542 Adapter cache time: 0.02271462173666805 Engine time: 0.09754815057385713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.669915774022229,
    "estimated_duration": 3600.0285715928853,
    "input_throughput": 4543.17977614362,
    "output_throughput": 3967.9193417288793,
    "total_throughput": 8511.0991178725,
    "itl": 37.856182978245144,
    "ttft": 8552.627070825305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 66148,
    "finished_requests": 65991,
    "scheduler_time": 40.80698517204776
}
#Debug simulation 
Total elapsed time: 4.670013608993031. Arrivals time: 0.15769575373269618 Scheduler time: 4.248313827556558 Scheduler overhead time: 0.0974462644662708 Adapter cache time: 0.022708783042617142 Engine time: 0.0973115530796349 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.693022074061446,
    "estimated_duration": 3600.0340247039703,
    "input_throughput": 4543.172894413106,
    "output_throughput": 3967.9133313676443,
    "total_throughput": 8511.086225780751,
    "itl": 37.855940165233974,
    "ttft": 8607.004733703165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 66148,
    "finished_requests": 65991,
    "scheduler_time": 40.80694460104261
}
#Debug simulation 
Total elapsed time: 4.693120034993626. Arrivals time: 0.1563187080901116 Scheduler time: 4.272145256632939 Scheduler overhead time: 0.0976198281859979 Adapter cache time: 0.02289950381964445 Engine time: 0.09779353125486523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.667239001952112,
    "estimated_duration": 3600.0262570252007,
    "input_throughput": 4543.182697093731,
    "output_throughput": 3967.9218928263517,
    "total_throughput": 8511.104589920082,
    "itl": 37.856382055020596,
    "ttft": 8552.611809769198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 66148,
    "finished_requests": 65991,
    "scheduler_time": 40.8069689928153
}
#Debug simulation 
Total elapsed time: 4.667337471968494. Arrivals time: 0.15715900482609868 Scheduler time: 4.244451293372549 Scheduler overhead time: 0.0976387420669198 Adapter cache time: 0.022572997608222067 Engine time: 0.09882652235683054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.682772652013227,
    "estimated_duration": 3600.0048647323224,
    "input_throughput": 4543.209694028043,
    "output_throughput": 3967.9454713909477,
    "total_throughput": 8511.15516541899,
    "itl": 37.85583533722834,
    "ttft": 8444.269899510828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 66148,
    "finished_requests": 65991,
    "scheduler_time": 40.80653207159768
}
#Debug simulation 
Total elapsed time: 4.682876948034391. Arrivals time: 0.15607220365200192 Scheduler time: 4.263334687100723 Scheduler overhead time: 0.09776849241461605 Adapter cache time: 0.02268902154173702 Engine time: 0.09656921157147735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 135, 135, 540, 135, 17280, 540, 135, 135, 135, 17280, 17280, 135, 135, 540, 540, 540, 17280, 17280, 135, 17280, 17280, 17280, 17280, 540, 135, 540, 540, 540, 540, 17280]
Prompts retrieved: 197370 . Total input tokens: 43987512 . Total output tokens: 38692223
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.682545074028894,
    "estimated_duration": 3600.022364693254,
    "input_throughput": 4543.187609167424,
    "output_throughput": 3967.9261829300176,
    "total_throughput": 8511.113792097442,
    "itl": 37.856314712841986,
    "ttft": 8552.648062818356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 66148,
    "finished_requests": 65991,
    "scheduler_time": 40.80687612612997
}
#Debug simulation 
Total elapsed time: 4.682722517056391. Arrivals time: 0.15529302414506674 Scheduler time: 4.26381199772004 Scheduler overhead time: 0.09793940559029579 Adapter cache time: 0.022804361884482205 Engine time: 0.09624188800808042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.637339359964244,
    "estimated_duration": 3600.013192674318,
    "input_throughput": 4520.66176677267,
    "output_throughput": 3960.8177072838766,
    "total_throughput": 8481.479474056545,
    "itl": 37.47555052049598,
    "ttft": 9018.0479220888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 65922,
    "finished_requests": 65757,
    "scheduler_time": 40.53631833465084
}
#Debug simulation 
Total elapsed time: 4.637438143021427. Arrivals time: 0.15467919199727476 Scheduler time: 4.217219748417847 Scheduler overhead time: 0.09819495037663728 Adapter cache time: 0.021693091606721282 Engine time: 0.09892461879644543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.628747279988602,
    "estimated_duration": 3600.013171878323,
    "input_throughput": 4520.661792886924,
    "output_throughput": 3960.8177301641103,
    "total_throughput": 8481.479523051034,
    "itl": 37.47579309361858,
    "ttft": 9018.031355750654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113988,
    "arrivals": 65922,
    "finished_requests": 65757,
    "scheduler_time": 40.536434800098114
}
#Debug simulation 
Total elapsed time: 4.628845343948342. Arrivals time: 0.15502318984363228 Scheduler time: 4.2106788866221905 Scheduler overhead time: 0.09812366298865527 Adapter cache time: 0.0219019646756351 Engine time: 0.09657891176175326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.663956725969911,
    "estimated_duration": 3600.0181619415152,
    "input_throughput": 4520.655526699643,
    "output_throughput": 3960.812239988818,
    "total_throughput": 8481.46776668846,
    "itl": 37.47575408392589,
    "ttft": 9018.014887723111,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 65922,
    "finished_requests": 65757,
    "scheduler_time": 40.53648012697255
}
#Debug simulation 
Total elapsed time: 4.664059125003405. Arrivals time: 0.15579779259860516 Scheduler time: 4.243870498728938 Scheduler overhead time: 0.09786205703858286 Adapter cache time: 0.021822597831487656 Engine time: 0.09781419881619513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.668528488953598,
    "estimated_duration": 3600.0181543106437,
    "input_throughput": 4520.655536281967,
    "output_throughput": 3960.812248384456,
    "total_throughput": 8481.467784666424,
    "itl": 37.47558768952674,
    "ttft": 9018.01180217879,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 65922,
    "finished_requests": 65757,
    "scheduler_time": 40.536343437485144
}
#Debug simulation 
Total elapsed time: 4.66863353701774. Arrivals time: 0.15609347529243678 Scheduler time: 4.246073845541105 Scheduler overhead time: 0.09836027130950242 Adapter cache time: 0.021694509079679847 Engine time: 0.09966399287804961 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.639919862034731,
    "estimated_duration": 3600.013302644254,
    "input_throughput": 4520.661628679601,
    "output_throughput": 3960.817586292415,
    "total_throughput": 8481.479214972016,
    "itl": 37.475785375611494,
    "ttft": 9018.04443575282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 65922,
    "finished_requests": 65757,
    "scheduler_time": 40.536395104053774
}
#Debug simulation 
Total elapsed time: 4.640027987072244. Arrivals time: 0.15826494817156345 Scheduler time: 4.217604978010058 Scheduler overhead time: 0.09782303730025887 Adapter cache time: 0.021712584071792662 Engine time: 0.09791768505237997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.634414853993803,
    "estimated_duration": 3600.0301315289503,
    "input_throughput": 4520.640496163894,
    "output_throughput": 3960.7990708522584,
    "total_throughput": 8481.439567016152,
    "itl": 37.47543080420833,
    "ttft": 9072.59075721171,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 65922,
    "finished_requests": 65757,
    "scheduler_time": 40.536400023822665
}
#Debug simulation 
Total elapsed time: 4.63454830495175. Arrivals time: 0.15571316774003208 Scheduler time: 4.2146532979095355 Scheduler overhead time: 0.09823806083295494 Adapter cache time: 0.021961693419143558 Engine time: 0.09708624181803316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_32_slots_32_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 66, 66, 540, 66, 17280, 540, 66, 66, 66, 17280, 17280, 66, 66, 540, 540, 540, 17280, 17280, 66, 17280, 17280, 17280, 17280, 540, 66, 540, 540, 540, 540, 17280]
Prompts retrieved: 196680 . Total input tokens: 43826344 . Total output tokens: 38563266
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.662438419996761,
    "estimated_duration": 3600.0133008093253,
    "input_throughput": 4520.661630983784,
    "output_throughput": 3960.817588311246,
    "total_throughput": 8481.47921929503,
    "itl": 37.475763959685175,
    "ttft": 9018.02229815699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 65922,
    "finished_requests": 65757,
    "scheduler_time": 40.536414576057744
}
#Debug simulation 
Total elapsed time: 4.662625742028467. Arrivals time: 0.15676326921675354 Scheduler time: 4.241849131183699 Scheduler overhead time: 0.09794605465140194 Adapter cache time: 0.021782351192086935 Engine time: 0.09752352244686335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.653257034020498,
    "estimated_duration": 3599.9441356471,
    "input_throughput": 4534.402030954235,
    "output_throughput": 3944.1645383887967,
    "total_throughput": 8478.566569343031,
    "itl": 37.15448595994278,
    "ttft": 8648.547394671488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.15827306917855
}
#Debug simulation 
Total elapsed time: 4.6533532129833475. Arrivals time: 0.1540033231722191 Scheduler time: 4.233567773248069 Scheduler overhead time: 0.09863572451286018 Adapter cache time: 0.021379175130277872 Engine time: 0.09879814321175218 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.646321360953152,
    "estimated_duration": 3599.9441332892216,
    "input_throughput": 4534.4020339241615,
    "output_throughput": 3944.1645409721314,
    "total_throughput": 8478.566574896293,
    "itl": 37.15469752504784,
    "ttft": 8648.578308884873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23482633151113985,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.15841384444885
}
#Debug simulation 
Total elapsed time: 4.646419640048407. Arrivals time: 0.15571594994980842 Scheduler time: 4.22336388041731 Scheduler overhead time: 0.09850948466919363 Adapter cache time: 0.021457791212014854 Engine time: 0.10036671347916126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.627260115928948,
    "estimated_duration": 3599.962807962115,
    "input_throughput": 4534.378511882611,
    "output_throughput": 3944.1440807655763,
    "total_throughput": 8478.522592648187,
    "itl": 37.15476727547149,
    "ttft": 8648.632440358468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24134628401137892,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.158676961846
}
#Debug simulation 
Total elapsed time: 4.627357758930884. Arrivals time: 0.1542788944207132 Scheduler time: 4.208933161455207 Scheduler overhead time: 0.09798109054099768 Adapter cache time: 0.021337177604436874 Engine time: 0.09782557212747633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.635330641060136,
    "estimated_duration": 3599.9600106294592,
    "input_throughput": 4534.38203530094,
    "output_throughput": 3944.1471455449087,
    "total_throughput": 8478.529180845848,
    "itl": 37.15448703791562,
    "ttft": 8648.608248939705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469836,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.158451327556534
}
#Debug simulation 
Total elapsed time: 4.635425455053337. Arrivals time: 0.15372509823646396 Scheduler time: 4.217926224693656 Scheduler overhead time: 0.0981501683127135 Adapter cache time: 0.021575290127657354 Engine time: 0.09699590737000108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.653313862043433,
    "estimated_duration": 3599.9576544330557,
    "input_throughput": 4534.3850030843605,
    "output_throughput": 3944.14972701564,
    "total_throughput": 8478.5347301,
    "itl": 37.1547818711237,
    "ttft": 8648.694873883604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.15858793509421
}
#Debug simulation 
Total elapsed time: 4.653414295986295. Arrivals time: 0.15536154503934085 Scheduler time: 4.232272194931284 Scheduler overhead time: 0.09850688034202904 Adapter cache time: 0.021612323937006295 Engine time: 0.09878641145769507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.651601566001773,
    "estimated_duration": 3599.9775026941693,
    "input_throughput": 4534.4158367055115,
    "output_throughput": 3944.2679820564085,
    "total_throughput": 8478.68381876192,
    "itl": 37.15449499579347,
    "ttft": 8593.80889482774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 65819,
    "finished_requests": 65663,
    "scheduler_time": 40.15863718385211
}
#Debug simulation 
Total elapsed time: 4.651698853005655. Arrivals time: 0.15683417406398803 Scheduler time: 4.228626609314233 Scheduler overhead time: 0.09827499941457063 Adapter cache time: 0.021397775155492127 Engine time: 0.09928838175255805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [10 11 11]
Adapter prompts. [17280, 540, 33, 33, 540, 33, 17280, 540, 33, 33, 33, 17280, 17280, 33, 33, 540, 540, 540, 17280, 17280, 33, 17280, 17280, 17280, 17280, 540, 33, 540, 540, 540, 540, 17280]
Prompts retrieved: 196350 . Total input tokens: 43749608 . Total output tokens: 38503962
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.647238114965148,
    "estimated_duration": 3599.945808006706,
    "input_throughput": 4534.39992449175,
    "output_throughput": 3944.1627061219224,
    "total_throughput": 8478.562630613673,
    "itl": 37.1547356859936,
    "ttft": 8648.611254919091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 65819,
    "finished_requests": 65662,
    "scheduler_time": 40.15843398653924
}
#Debug simulation 
Total elapsed time: 4.647391100996174. Arrivals time: 0.1510617433814332 Scheduler time: 4.231460433918983 Scheduler overhead time: 0.09822760289534926 Adapter cache time: 0.021321078878827393 Engine time: 0.09822935191914439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.602502280031331,
    "estimated_duration": 3599.9787711686213,
    "input_throughput": 4449.734295183064,
    "output_throughput": 3920.200339242215,
    "total_throughput": 8369.934634425279,
    "itl": 36.431948067554565,
    "ttft": 9728.693188482515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.4893923472233
}
#Debug simulation 
Total elapsed time: 4.602595646982081. Arrivals time: 0.15470926731359214 Scheduler time: 4.178672138368711 Scheduler overhead time: 0.09994440118316561 Adapter cache time: 0.02100627636536956 Engine time: 0.10037779586855322 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.6130995930870995,
    "estimated_duration": 3599.9801244019186,
    "input_throughput": 4449.7326225269935,
    "output_throughput": 3920.19886563807,
    "total_throughput": 8369.931488165063,
    "itl": 36.43222684763897,
    "ttft": 9728.648835606205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2348263315111399,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.489508894620215
}
#Debug simulation 
Total elapsed time: 4.613198787090369. Arrivals time: 0.15559680678416044 Scheduler time: 4.18818801373709 Scheduler overhead time: 0.10016776865813881 Adapter cache time: 0.021318391780368984 Engine time: 0.09971568651963025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.617232271004468,
    "estimated_duration": 3599.9930727235746,
    "input_throughput": 4449.716617893619,
    "output_throughput": 3920.1847656120863,
    "total_throughput": 8369.901383505705,
    "itl": 36.4322831440842,
    "ttft": 9728.713458901295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2413462840113789,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.489732438898194
}
#Debug simulation 
Total elapsed time: 4.6173593800049275. Arrivals time: 0.15484839165583253 Scheduler time: 4.193588186171837 Scheduler overhead time: 0.09970026696100831 Adapter cache time: 0.020928097190335393 Engine time: 0.10032397147733718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.60877457796596,
    "estimated_duration": 3599.996056926222,
    "input_throughput": 4449.712929318436,
    "output_throughput": 3920.1815159902612,
    "total_throughput": 8369.894445308697,
    "itl": 36.43191793695951,
    "ttft": 9728.697837637972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.48959491542481
}
#Debug simulation 
Total elapsed time: 4.608884898014367. Arrivals time: 0.1557102935621515 Scheduler time: 4.184316289843991 Scheduler overhead time: 0.10010362812317908 Adapter cache time: 0.020967038697563112 Engine time: 0.09994767198804766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.61788960499689,
    "estimated_duration": 3599.9870000034284,
    "input_throughput": 4449.724123999544,
    "output_throughput": 3920.1913784651333,
    "total_throughput": 8369.915502464677,
    "itl": 36.43227799353526,
    "ttft": 9728.68335328766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2390679950686172,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.48964741597957
}
#Debug simulation 
Total elapsed time: 4.617984412005171. Arrivals time: 0.15649062115699053 Scheduler time: 4.192801409517415 Scheduler overhead time: 0.10032811085693538 Adapter cache time: 0.020985502866096795 Engine time: 0.09958801057655364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.603840776951984,
    "estimated_duration": 3600.0111958527523,
    "input_throughput": 4449.694217188542,
    "output_throughput": 3920.16503066932,
    "total_throughput": 8369.859247857863,
    "itl": 36.431847913639956,
    "ttft": 9728.704130057744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.4897614226409
}
#Debug simulation 
Total elapsed time: 4.603944898000918. Arrivals time: 0.15688415977638215 Scheduler time: 4.178071658476256 Scheduler overhead time: 0.10080772754736245 Adapter cache time: 0.02118780347518623 Engine time: 0.09896656905766577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_32_slots_32_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 135, 135, 270, 135, 17280, 270, 135, 135, 135, 17280, 17280, 135, 135, 270, 270, 270, 17280, 17280, 135, 17280, 17280, 17280, 17280, 270, 135, 270, 270, 270, 270, 17280]
Prompts retrieved: 194400 . Total input tokens: 43295877 . Total output tokens: 38125307
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.611570951063186,
    "estimated_duration": 3599.9820381260038,
    "input_throughput": 4449.730257081721,
    "output_throughput": 3920.1967816890647,
    "total_throughput": 8369.927038770786,
    "itl": 36.43216963667823,
    "ttft": 9728.671258996566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585548,
    "arrivals": 65160,
    "finished_requests": 64985,
    "scheduler_time": 39.48955017668675
}
#Debug simulation 
Total elapsed time: 4.611727131064981. Arrivals time: 0.1544654028257355 Scheduler time: 4.185673477360979 Scheduler overhead time: 0.10095300327520818 Adapter cache time: 0.020960476133041084 Engine time: 0.10188536462374032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 295232,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.618046157993376,
    "estimated_duration": 3600.0264099236247,
    "input_throughput": 4458.329515516111,
    "output_throughput": 3905.8082910837056,
    "total_throughput": 8364.137806599818,
    "itl": 36.11196579485597,
    "ttft": 8095.773238419296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2115970890223979,
    "arrivals": 64957,
    "finished_requests": 64812,
    "scheduler_time": 39.113797894030014
}
#Debug simulation 
Total elapsed time: 4.618143565021455. Arrivals time: 0.15564663079567254 Scheduler time: 4.190908197895624 Scheduler overhead time: 0.10108277667313814 Adapter cache time: 0.02032789820805192 Engine time: 0.10209698288235813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.585990672931075,
    "estimated_duration": 3600.027254716268,
    "input_throughput": 4458.328469311817,
    "output_throughput": 3905.8073745356137,
    "total_throughput": 8364.13584384743,
    "itl": 36.11220160218636,
    "ttft": 8095.807626824789,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2348263315111399,
    "arrivals": 64957,
    "finished_requests": 64812,
    "scheduler_time": 39.113919238271166
}
#Debug simulation 
Total elapsed time: 4.586090010940097. Arrivals time: 0.1544426567852497 Scheduler time: 4.1608446459285915 Scheduler overhead time: 0.10118094203062356 Adapter cache time: 0.02034529612865299 Engine time: 0.10106529667973518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.604647280997597,
    "estimated_duration": 3600.0002799726453,
    "input_throughput": 4458.36187549462,
    "output_throughput": 3905.8366406868286,
    "total_throughput": 8364.198516181448,
    "itl": 36.11220696118661,
    "ttft": 8040.50916727814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2413462840113789,
    "arrivals": 64957,
    "finished_requests": 64812,
    "scheduler_time": 39.11366362235376
}
#Debug simulation 
Total elapsed time: 4.604744722950272. Arrivals time: 0.1540461239637807 Scheduler time: 4.18094837537501 Scheduler overhead time: 0.10098907619249076 Adapter cache time: 0.020502562867477536 Engine time: 0.10023183387238532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 4.579161903006025,
    "estimated_duration": 3600.004614053154,
    "input_throughput": 4458.35650802947,
    "output_throughput": 3905.8319384121737,
    "total_throughput": 8364.188446441643,
    "itl": 36.11195590712695,
    "ttft": 8095.813135335364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21955590080469842,
    "arrivals": 64957,
    "finished_requests": 64812,
    "scheduler_time": 39.113615125632506
}
#Debug simulation 
Total elapsed time: 4.579260035068728. Arrivals time: 0.1563567304983735 Scheduler time: 4.1546130534261465 Scheduler overhead time: 0.1001978301210329 Adapter cache time: 0.020282182027585804 Engine time: 0.09997840982396156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 4.611111367936246,
    "estimated_duration": 3600.036112526734,
    "input_throughput": 4458.317499691695,
    "output_throughput": 3905.7977643816157,
    "total_throughput": 8364.11526407331,
    "itl": 36.11231864199338,
    "ttft": 8095.840101678588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23906799506861717,
    "arrivals": 64957,
    "finished_requests": 64812,
    "scheduler_time": 39.114028448088725
}
#Debug simulation 
Total elapsed time: 4.611207681009546. Arrivals time: 0.16020959010347724 Scheduler time: 4.179621159681119 Scheduler overhead time: 0.10228216007817537 Adapter cache time: 0.020432433346286416 Engine time: 0.10019369493238628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 284176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 4.550781945930794,
    "estimated_duration": 3600.011453313207,
    "input_throughput": 4458.348038095426,
    "output_throughput": 3905.8245181579064,
    "total_throughput": 8364.172556253332,
    "itl": 36.11182661909641,
    "ttft": 8095.8918269919,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20428547009825704,
    "arrivals": 64957,
    "finished_requests": 64812,
    "scheduler_time": 39.11357943342122
}
#Debug simulation 
Total elapsed time: 4.550873833009973. Arrivals time: 0.14884063974022865 Scheduler time: 4.134040677454323 Scheduler overhead time: 0.10009347251616418 Adapter cache time: 0.020303815486840904 Engine time: 0.09993095090612769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 32,
    "served_adapters": 32,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 258176,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_32_slots_32_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [10 11 11]
Adapter prompts. [17280, 270, 66, 66, 270, 66, 17280, 270, 66, 66, 66, 17280, 17280, 66, 66, 270, 270, 270, 17280, 17280, 66, 17280, 17280, 17280, 17280, 270, 66, 270, 270, 270, 270, 17280]
Prompts retrieved: 193710 . Total input tokens: 43134486 . Total output tokens: 38000619
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 4.599860418005846,
    "estimated_duration": 3600.0322985618336,
    "input_throughput": 4458.322222945558,
    "output_throughput": 3905.8019022821527,
    "total_throughput": 8364.12412522771,
    "itl": 36.112339733581344,
    "ttft": 8095.7825869175185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 32,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23678970612585545,
    "arrivals": 64957,
    "finished_requests": 64812,
    "scheduler_time": 39.11399608962409
}
#Debug simulation 
Total elapsed time: 4.600015367963351. Arrivals time: 0.150220749899745 Scheduler time: 4.1805030920077115 Scheduler overhead time: 0.09993630484677851 Adapter cache time: 0.02042223885655403 Engine time: 0.1006533526815474 
