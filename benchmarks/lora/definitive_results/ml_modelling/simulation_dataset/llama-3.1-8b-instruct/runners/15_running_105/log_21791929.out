INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:53 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 66, 66, 270, 66, 1080, 270, 66, 66, 66, 1080, 1080, 66, 66, 270, 270, 270, 1080, 1080, 66, 1080, 1080, 1080, 1080, 270, 66, 270, 270, 270, 270, 1080]
Prompts retrieved: 15510 . Total input tokens: 3441771 . Total output tokens: 3085118
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.837749439291656,
    "estimated_duration": 3599.6889014398625,
    "input_throughput": 359.93804894687946,
    "output_throughput": 313.5898770442284,
    "total_throughput": 673.5279259911078,
    "itl": 21.187748344989455,
    "ttft": 6938.163548287358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1138,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.524921478359204,
    "arrivals": 5216,
    "finished_requests": 5206,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8378494270145893. Arrivals time: 0.02599232131615281 Scheduler time: 0.43981668818742037 Scheduler overhead time: 0.1346558015793562 Adapter cache time: 0.0312826600857079 Engine time: 0.13908886490389705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 66, 66, 270, 66, 1080, 270, 66, 66, 66, 1080, 1080, 66, 66, 270, 270, 270, 1080, 1080, 66, 1080, 1080, 1080, 1080, 270, 66, 270, 270, 270, 270, 1080]
Prompts retrieved: 15510 . Total input tokens: 3441771 . Total output tokens: 3085118
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.8318403558805585,
    "estimated_duration": 3599.6995979079347,
    "input_throughput": 359.936979394895,
    "output_throughput": 313.5889452153309,
    "total_throughput": 673.5259246102258,
    "itl": 21.083922971427814,
    "ttft": 6938.254490596573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.6158769797906,
    "arrivals": 5216,
    "finished_requests": 5206,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8319749408401549. Arrivals time: 0.02581116510555148 Scheduler time: 0.4370971005409956 Scheduler overhead time: 0.1352979871444404 Adapter cache time: 0.03122272016480565 Engine time: 0.1349012996070087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 66, 66, 270, 66, 1080, 270, 66, 66, 66, 1080, 1080, 66, 66, 270, 270, 270, 1080, 1080, 66, 1080, 1080, 1080, 1080, 270, 66, 270, 270, 270, 270, 1080]
Prompts retrieved: 15510 . Total input tokens: 3441771 . Total output tokens: 3085118
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.8352818829007447,
    "estimated_duration": 3599.694150289187,
    "input_throughput": 359.9375241076831,
    "output_throughput": 313.5894197870433,
    "total_throughput": 673.5269438947264,
    "itl": 21.08556760418531,
    "ttft": 6938.266913796196,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.859956725062784,
    "arrivals": 5216,
    "finished_requests": 5206,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8353780591860414. Arrivals time: 0.02533564530313015 Scheduler time: 0.44095450919121504 Scheduler overhead time: 0.1341149965301156 Adapter cache time: 0.03128090174868703 Engine time: 0.13644836097955704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 66, 66, 270, 66, 1080, 270, 66, 66, 66, 1080, 1080, 66, 66, 270, 270, 270, 1080, 1080, 66, 1080, 1080, 1080, 1080, 270, 66, 270, 270, 270, 270, 1080]
Prompts retrieved: 15510 . Total input tokens: 3441771 . Total output tokens: 3085118
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.8353663450106978,
    "estimated_duration": 3599.6923347700717,
    "input_throughput": 359.9377056436019,
    "output_throughput": 313.5895779471117,
    "total_throughput": 673.5272835907136,
    "itl": 21.189760222762782,
    "ttft": 6938.329724708597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.833515445501534,
    "arrivals": 5216,
    "finished_requests": 5206,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8354513668455184. Arrivals time: 0.025612836237996817 Scheduler time: 0.4387633050791919 Scheduler overhead time: 0.13779624737799168 Adapter cache time: 0.031158162746578455 Engine time: 0.13515387289226055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 66, 66, 270, 66, 1080, 270, 66, 66, 66, 1080, 1080, 66, 66, 270, 270, 270, 1080, 1080, 66, 1080, 1080, 1080, 1080, 270, 66, 270, 270, 270, 270, 1080]
Prompts retrieved: 15510 . Total input tokens: 3441771 . Total output tokens: 3085118
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.8348079212009907,
    "estimated_duration": 3599.6956395306966,
    "input_throughput": 359.93737519678746,
    "output_throughput": 313.589290050969,
    "total_throughput": 673.5266652477565,
    "itl": 21.086776984138062,
    "ttft": 6938.318974072785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.793674739003169,
    "arrivals": 5216,
    "finished_requests": 5206,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.83557946421206. Arrivals time: 0.025882152374833822 Scheduler time: 0.4380886200815439 Scheduler overhead time: 0.13445920636877418 Adapter cache time: 0.031762846279889345 Engine time: 0.13690743269398808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 66, 66, 270, 66, 1080, 270, 66, 66, 66, 1080, 1080, 66, 66, 270, 270, 270, 1080, 1080, 66, 1080, 1080, 1080, 1080, 270, 66, 270, 270, 270, 270, 1080]
Prompts retrieved: 15510 . Total input tokens: 3441771 . Total output tokens: 3085118
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.8293667510151863,
    "estimated_duration": 3599.6940360239937,
    "input_throughput": 359.93753553319044,
    "output_throughput": 313.58942974132145,
    "total_throughput": 673.5269652745119,
    "itl": 21.186650269300486,
    "ttft": 6938.246148376936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.252134188488089,
    "arrivals": 5216,
    "finished_requests": 5206,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.829469250049442. Arrivals time: 0.025755276903510094 Scheduler time: 0.4384425994940102 Scheduler overhead time: 0.13414306286722422 Adapter cache time: 0.031058177817612886 Engine time: 0.1334032486192882 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.1-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 66, 66, 270, 66, 1080, 270, 66, 66, 66, 1080, 1080, 66, 66, 270, 270, 270, 1080, 1080, 66, 1080, 1080, 1080, 1080, 270, 66, 270, 270, 270, 270, 1080]
Prompts retrieved: 15510 . Total input tokens: 3441771 . Total output tokens: 3085118
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.833563428837806,
    "estimated_duration": 3599.695487546968,
    "input_throughput": 359.9373903938019,
    "output_throughput": 313.5893032911083,
    "total_throughput": 673.5266936849101,
    "itl": 21.086032747174613,
    "ttft": 6938.3150539561075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.703371649999157,
    "arrivals": 5216,
    "finished_requests": 5206,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8336351439356804. Arrivals time: 0.02504654135555029 Scheduler time: 0.439891021233052 Scheduler overhead time: 0.13414949597790837 Adapter cache time: 0.03119106823578477 Engine time: 0.1365417600609362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 33, 33, 270, 33, 1080, 270, 33, 33, 33, 1080, 1080, 33, 33, 270, 270, 270, 1080, 1080, 33, 1080, 1080, 1080, 1080, 270, 33, 270, 270, 270, 270, 1080]
Prompts retrieved: 15180 . Total input tokens: 3359563 . Total output tokens: 3023723
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.8403897429816425,
    "estimated_duration": 3599.4007407727313,
    "input_throughput": 343.48273199904384,
    "output_throughput": 310.096341137158,
    "total_throughput": 653.5790731362018,
    "itl": 21.04084711711566,
    "ttft": 6383.816438984748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 985,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.513222896470831,
    "arrivals": 5104,
    "finished_requests": 5095,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8404900999739766. Arrivals time: 0.02593406569212675 Scheduler time: 0.4444880080409348 Scheduler overhead time: 0.1352389957755804 Adapter cache time: 0.030764212366193533 Engine time: 0.1364859538152814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 33, 33, 270, 33, 1080, 270, 33, 33, 33, 1080, 1080, 33, 33, 270, 270, 270, 1080, 1080, 33, 1080, 1080, 1080, 1080, 270, 33, 270, 270, 270, 270, 1080]
Prompts retrieved: 15180 . Total input tokens: 3359563 . Total output tokens: 3023723
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.8328909277915955,
    "estimated_duration": 3599.390888302433,
    "input_throughput": 343.48367220073914,
    "output_throughput": 310.0971899516062,
    "total_throughput": 653.5808621523453,
    "itl": 21.047556900363645,
    "ttft": 6384.203839548784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 985,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.33765718228651,
    "arrivals": 5104,
    "finished_requests": 5095,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8329587341286242. Arrivals time: 0.02525875438004732 Scheduler time: 0.43913644226267934 Scheduler overhead time: 0.13493813574314117 Adapter cache time: 0.03087860392406583 Engine time: 0.1355261690914631 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 33, 33, 270, 33, 1080, 270, 33, 33, 33, 1080, 1080, 33, 33, 270, 270, 270, 1080, 1080, 33, 1080, 1080, 1080, 1080, 270, 33, 270, 270, 270, 270, 1080]
Prompts retrieved: 15180 . Total input tokens: 3359563 . Total output tokens: 3023723
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.8503162516281009,
    "estimated_duration": 3599.3946580147303,
    "input_throughput": 343.483312463965,
    "output_throughput": 310.0968651811096,
    "total_throughput": 653.5801776450746,
    "itl": 21.046919413760726,
    "ttft": 6384.107480051155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 984,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.555539535325991,
    "arrivals": 5104,
    "finished_requests": 5095,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8504476537927985. Arrivals time: 0.026746013201773167 Scheduler time: 0.45321294385939837 Scheduler overhead time: 0.13478189520537853 Adapter cache time: 0.031045941170305014 Engine time: 0.13759765168651938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 33, 33, 270, 33, 1080, 270, 33, 33, 33, 1080, 1080, 33, 33, 270, 270, 270, 1080, 1080, 33, 1080, 1080, 1080, 1080, 270, 33, 270, 270, 270, 270, 1080]
Prompts retrieved: 15180 . Total input tokens: 3359563 . Total output tokens: 3023723
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.844860474113375,
    "estimated_duration": 3599.380428786308,
    "input_throughput": 343.4846703372459,
    "output_throughput": 310.09809106962433,
    "total_throughput": 653.5827614068703,
    "itl": 21.041134126469352,
    "ttft": 6384.07363768803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 986,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.8012467026617305,
    "arrivals": 5104,
    "finished_requests": 5095,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8449450777843595. Arrivals time: 0.02591367345303297 Scheduler time: 0.4473798545077443 Scheduler overhead time: 0.13477933034300804 Adapter cache time: 0.03096635127440095 Engine time: 0.13863852061331272 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 33, 33, 270, 33, 1080, 270, 33, 33, 33, 1080, 1080, 33, 33, 270, 270, 270, 1080, 1080, 33, 1080, 1080, 1080, 1080, 270, 33, 270, 270, 270, 270, 1080]
Prompts retrieved: 15180 . Total input tokens: 3359563 . Total output tokens: 3023723
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.8360698618926108,
    "estimated_duration": 3599.3931155170712,
    "input_throughput": 343.48345966161423,
    "output_throughput": 310.0969980712034,
    "total_throughput": 653.5804577328176,
    "itl": 21.048732902735807,
    "ttft": 6383.924473182428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 985,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.482425675466697,
    "arrivals": 5104,
    "finished_requests": 5095,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8361922241747379. Arrivals time: 0.026007520500570536 Scheduler time: 0.4401762164197862 Scheduler overhead time: 0.13547059148550034 Adapter cache time: 0.0309918406419456 Engine time: 0.13624179316684604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 33, 33, 270, 33, 1080, 270, 33, 33, 33, 1080, 1080, 33, 33, 270, 270, 270, 1080, 1080, 33, 1080, 1080, 1080, 1080, 270, 33, 270, 270, 270, 270, 1080]
Prompts retrieved: 15180 . Total input tokens: 3359563 . Total output tokens: 3023723
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.8434527940116823,
    "estimated_duration": 3599.3977618301255,
    "input_throughput": 343.4830162730842,
    "output_throughput": 310.0965977798698,
    "total_throughput": 653.5796140529541,
    "itl": 21.03967825739277,
    "ttft": 6383.8035948504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 984,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.281778205521373,
    "arrivals": 5104,
    "finished_requests": 5095,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8438124130479991. Arrivals time: 0.026402578223496675 Scheduler time: 0.44610182056203485 Scheduler overhead time: 0.13482844550162554 Adapter cache time: 0.031152320560067892 Engine time: 0.13780209654942155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.1-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 270, 33, 33, 270, 33, 1080, 270, 33, 33, 33, 1080, 1080, 33, 33, 270, 270, 270, 1080, 1080, 33, 1080, 1080, 1080, 1080, 270, 33, 270, 270, 270, 270, 1080]
Prompts retrieved: 15180 . Total input tokens: 3359563 . Total output tokens: 3023723
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.8377283243462443,
    "estimated_duration": 3599.392287048365,
    "input_throughput": 343.48353872087614,
    "output_throughput": 310.0970694459351,
    "total_throughput": 653.5806081668113,
    "itl": 21.046957861929723,
    "ttft": 6384.03538609063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 984,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.399166066981894,
    "arrivals": 5104,
    "finished_requests": 5095,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8381783524528146. Arrivals time: 0.025958647951483727 Scheduler time: 0.44130677450448275 Scheduler overhead time: 0.13588309194892645 Adapter cache time: 0.030944579280912876 Engine time: 0.13643647963181138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 66, 66, 135, 66, 1080, 135, 66, 66, 66, 1080, 1080, 66, 66, 135, 135, 135, 1080, 1080, 66, 1080, 1080, 1080, 1080, 135, 66, 135, 135, 135, 135, 1080]
Prompts retrieved: 14025 . Total input tokens: 3094296 . Total output tokens: 2810286
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.8014806569553912,
    "estimated_duration": 3598.869285139837,
    "input_throughput": 310.6281199531154,
    "output_throughput": 284.98145354566526,
    "total_throughput": 595.6095734987807,
    "itl": 20.81273283183822,
    "ttft": 5400.502517635491,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 751,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.9659191829944955,
    "arrivals": 4697,
    "finished_requests": 4690,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8015509168617427. Arrivals time: 0.024734835140407085 Scheduler time: 0.40907217701897025 Scheduler overhead time: 0.13523404905572534 Adapter cache time: 0.030072213150560856 Engine time: 0.13486861903220415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 66, 66, 135, 66, 1080, 135, 66, 66, 66, 1080, 1080, 66, 66, 135, 135, 135, 1080, 1080, 66, 1080, 1080, 1080, 1080, 135, 66, 135, 135, 135, 135, 1080]
Prompts retrieved: 14025 . Total input tokens: 3094296 . Total output tokens: 2810286
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.8002567300572991,
    "estimated_duration": 3598.8722594536353,
    "input_throughput": 310.62786323227715,
    "output_throughput": 284.9812180206984,
    "total_throughput": 595.6090812529756,
    "itl": 20.817642361445802,
    "ttft": 5400.5800779395695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 749,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.5020434696367095,
    "arrivals": 4697,
    "finished_requests": 4690,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8003361197188497. Arrivals time: 0.024592282250523567 Scheduler time: 0.40777906496077776 Scheduler overhead time: 0.13472883542999625 Adapter cache time: 0.029896039981395006 Engine time: 0.135941322427243 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 66, 66, 135, 66, 1080, 135, 66, 66, 66, 1080, 1080, 66, 66, 135, 135, 135, 1080, 1080, 66, 1080, 1080, 1080, 1080, 135, 66, 135, 135, 135, 135, 1080]
Prompts retrieved: 14025 . Total input tokens: 3094296 . Total output tokens: 2810286
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.8103509712964296,
    "estimated_duration": 3598.8789358574336,
    "input_throughput": 310.6272869758698,
    "output_throughput": 284.9806893422626,
    "total_throughput": 595.6079763181325,
    "itl": 20.81786625882058,
    "ttft": 5400.702098716583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 751,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.670396826472165,
    "arrivals": 4697,
    "finished_requests": 4690,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8104422781616449. Arrivals time: 0.024766711983829737 Scheduler time: 0.4163719112984836 Scheduler overhead time: 0.13480674801394343 Adapter cache time: 0.03007511980831623 Engine time: 0.13668279582634568 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 66, 66, 135, 66, 1080, 135, 66, 66, 66, 1080, 1080, 66, 66, 135, 135, 135, 1080, 1080, 66, 1080, 1080, 1080, 1080, 135, 66, 135, 135, 135, 135, 1080]
Prompts retrieved: 14025 . Total input tokens: 3094296 . Total output tokens: 2810286
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.808847364038229,
    "estimated_duration": 3598.871564478145,
    "input_throughput": 310.6279232173996,
    "output_throughput": 284.9812730532158,
    "total_throughput": 595.6091962706154,
    "itl": 20.815314664674016,
    "ttft": 5400.607651223061,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 749,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.1216709229489705,
    "arrivals": 4697,
    "finished_requests": 4690,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8089160490781069. Arrivals time: 0.02467522118240595 Scheduler time: 0.41293935617432 Scheduler overhead time: 0.13548517553135753 Adapter cache time: 0.02990661608055234 Engine time: 0.13795365719124675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 66, 66, 135, 66, 1080, 135, 66, 66, 66, 1080, 1080, 66, 66, 135, 135, 135, 1080, 1080, 66, 1080, 1080, 1080, 1080, 135, 66, 135, 135, 135, 135, 1080]
Prompts retrieved: 14025 . Total input tokens: 3094296 . Total output tokens: 2810286
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.805142629891634,
    "estimated_duration": 3598.8765702308674,
    "input_throughput": 310.6274911585218,
    "output_throughput": 284.98087666680027,
    "total_throughput": 595.608367825322,
    "itl": 20.816833157630004,
    "ttft": 5400.5598335783725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 750,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.605777763882684,
    "arrivals": 4697,
    "finished_requests": 4690,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8052214779891074. Arrivals time: 0.02494095964357257 Scheduler time: 0.41069900523871183 Scheduler overhead time: 0.13589084893465042 Adapter cache time: 0.030319460667669773 Engine time: 0.13526251073926687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 66, 66, 135, 66, 1080, 135, 66, 66, 66, 1080, 1080, 66, 66, 135, 135, 135, 1080, 1080, 66, 1080, 1080, 1080, 1080, 135, 66, 135, 135, 135, 135, 1080]
Prompts retrieved: 14025 . Total input tokens: 3094296 . Total output tokens: 2810286
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.8055016878060997,
    "estimated_duration": 3598.884098214533,
    "input_throughput": 310.62684140192624,
    "output_throughput": 284.9802805566378,
    "total_throughput": 595.607121958564,
    "itl": 20.81236123394404,
    "ttft": 5400.589085292119,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 751,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.794324626368447,
    "arrivals": 4697,
    "finished_requests": 4690,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8058556281030178. Arrivals time: 0.02437627036124468 Scheduler time: 0.4084855988621712 Scheduler overhead time: 0.13871277403086424 Adapter cache time: 0.030033082235604525 Engine time: 0.13577479030936956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.1-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 66, 66, 135, 66, 1080, 135, 66, 66, 66, 1080, 1080, 66, 66, 135, 135, 135, 1080, 1080, 66, 1080, 1080, 1080, 1080, 135, 66, 135, 135, 135, 135, 1080]
Prompts retrieved: 14025 . Total input tokens: 3094296 . Total output tokens: 2810286
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.8079525688663125,
    "estimated_duration": 3598.8740896295085,
    "input_throughput": 310.6277052652,
    "output_throughput": 284.98107309599794,
    "total_throughput": 595.608778361198,
    "itl": 20.816243175567013,
    "ttft": 5400.759712413485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 750,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.5563286276534525,
    "arrivals": 4697,
    "finished_requests": 4690,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8084155889227986. Arrivals time: 0.025025841780006886 Scheduler time: 0.41435836534947157 Scheduler overhead time: 0.1352898497134447 Adapter cache time: 0.030099349562078714 Engine time: 0.13555925758555532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 33, 33, 135, 33, 1080, 135, 33, 33, 33, 1080, 1080, 33, 33, 135, 135, 135, 1080, 1080, 33, 1080, 1080, 1080, 1080, 135, 33, 135, 135, 135, 135, 1080]
Prompts retrieved: 13695 . Total input tokens: 3025817 . Total output tokens: 2745069
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.7937789149582386,
    "estimated_duration": 3598.273155659589,
    "input_throughput": 300.51692943299696,
    "output_throughput": 280.19495918874634,
    "total_throughput": 580.7118886217432,
    "itl": 20.76941595563313,
    "ttft": 7914.189575941912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7095614669239674,
    "arrivals": 4569,
    "finished_requests": 4559,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7938446342013776. Arrivals time: 0.024441625457257032 Scheduler time: 0.4012721301987767 Scheduler overhead time: 0.1348901609890163 Adapter cache time: 0.029117850121110678 Engine time: 0.13637639209628105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 33, 33, 135, 33, 1080, 135, 33, 33, 33, 1080, 1080, 33, 33, 135, 135, 135, 1080, 1080, 33, 1080, 1080, 1080, 1080, 135, 33, 135, 135, 135, 135, 1080]
Prompts retrieved: 13695 . Total input tokens: 3025817 . Total output tokens: 2745069
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7987184571102262,
    "estimated_duration": 3598.2687397144587,
    "input_throughput": 300.517298239878,
    "output_throughput": 280.19530305565985,
    "total_throughput": 580.7126012955379,
    "itl": 20.774762490512373,
    "ttft": 7914.390315142256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.150550246718347,
    "arrivals": 4569,
    "finished_requests": 4559,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7988243349827826. Arrivals time: 0.024348235689103603 Scheduler time: 0.4069933029823005 Scheduler overhead time: 0.1353176822885871 Adapter cache time: 0.02977242972701788 Engine time: 0.13443446485325694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 33, 33, 135, 33, 1080, 135, 33, 33, 33, 1080, 1080, 33, 33, 135, 135, 135, 1080, 1080, 33, 1080, 1080, 1080, 1080, 135, 33, 135, 135, 135, 135, 1080]
Prompts retrieved: 13695 . Total input tokens: 3025817 . Total output tokens: 2745069
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.8002699371427298,
    "estimated_duration": 3598.273146979297,
    "input_throughput": 300.51693015794876,
    "output_throughput": 280.19495986467444,
    "total_throughput": 580.7118900226233,
    "itl": 20.77523893680778,
    "ttft": 7914.514207536652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.27264739704322,
    "arrivals": 4569,
    "finished_requests": 4559,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8003497463651001. Arrivals time: 0.0249778525903821 Scheduler time: 0.4074904350563884 Scheduler overhead time: 0.13509763684123755 Adapter cache time: 0.02941119810566306 Engine time: 0.1361956805922091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 33, 33, 135, 33, 1080, 135, 33, 33, 33, 1080, 1080, 33, 33, 135, 135, 135, 1080, 1080, 33, 1080, 1080, 1080, 1080, 135, 33, 135, 135, 135, 135, 1080]
Prompts retrieved: 13695 . Total input tokens: 3025817 . Total output tokens: 2745069
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7967034778557718,
    "estimated_duration": 3598.2832891929743,
    "input_throughput": 300.51608311321263,
    "output_throughput": 280.1941700999656,
    "total_throughput": 580.7102532131782,
    "itl": 20.773273188786977,
    "ttft": 7914.318968205502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8423651906428806,
    "arrivals": 4569,
    "finished_requests": 4559,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7967867976985872. Arrivals time: 0.024059205781668425 Scheduler time: 0.4072686214931309 Scheduler overhead time: 0.13393085356801748 Adapter cache time: 0.02904850710183382 Engine time: 0.13498989399522543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 33, 33, 135, 33, 1080, 135, 33, 33, 33, 1080, 1080, 33, 33, 135, 135, 135, 1080, 1080, 33, 1080, 1080, 1080, 1080, 135, 33, 135, 135, 135, 135, 1080]
Prompts retrieved: 13695 . Total input tokens: 3025817 . Total output tokens: 2745069
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7968153390102088,
    "estimated_duration": 3598.2721060536146,
    "input_throughput": 300.5170170929502,
    "output_throughput": 280.1950409208373,
    "total_throughput": 580.7120580137876,
    "itl": 20.774369392093305,
    "ttft": 7914.514467344629,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.226667383834757,
    "arrivals": 4569,
    "finished_requests": 4559,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7968993480317295. Arrivals time: 0.024636399000883102 Scheduler time: 0.40342811308801174 Scheduler overhead time: 0.13539044372737408 Adapter cache time: 0.02964302385225892 Engine time: 0.13535350328311324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 33, 33, 135, 33, 1080, 135, 33, 33, 33, 1080, 1080, 33, 33, 135, 135, 135, 1080, 1080, 33, 1080, 1080, 1080, 1080, 135, 33, 135, 135, 135, 135, 1080]
Prompts retrieved: 13695 . Total input tokens: 3025817 . Total output tokens: 2745069
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.8085423251613975,
    "estimated_duration": 3598.269166849299,
    "input_throughput": 300.5172625667801,
    "output_throughput": 280.1952697948974,
    "total_throughput": 580.7125323616775,
    "itl": 20.772192871547055,
    "ttft": 7914.209282809994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.581379647660053,
    "arrivals": 4569,
    "finished_requests": 4559,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8086309181526303. Arrivals time: 0.024837526958435774 Scheduler time: 0.40692247450351715 Scheduler overhead time: 0.14237268874421716 Adapter cache time: 0.02998812310397625 Engine time: 0.135949089191854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.1-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 135, 33, 33, 135, 33, 1080, 135, 33, 33, 33, 1080, 1080, 33, 33, 135, 135, 135, 1080, 1080, 33, 1080, 1080, 1080, 1080, 135, 33, 135, 135, 135, 135, 1080]
Prompts retrieved: 13695 . Total input tokens: 3025817 . Total output tokens: 2745069
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7967605772428215,
    "estimated_duration": 3598.2701603115415,
    "input_throughput": 300.5171795956467,
    "output_throughput": 280.1951924345524,
    "total_throughput": 580.712372030199,
    "itl": 20.773254131128677,
    "ttft": 7914.499209554353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.195598310828246,
    "arrivals": 4569,
    "finished_requests": 4559,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.79699791315943. Arrivals time: 0.024859540164470673 Scheduler time: 0.40339940832927823 Scheduler overhead time: 0.1345403459854424 Adapter cache time: 0.02956161741167307 Engine time: 0.13704082369804382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 66, 33, 33, 66, 33, 1080, 66, 33, 33, 33, 1080, 1080, 33, 33, 66, 66, 66, 1080, 1080, 33, 1080, 1080, 1080, 1080, 66, 33, 66, 66, 66, 66, 1080]
Prompts retrieved: 12936 . Total input tokens: 2852361 . Total output tokens: 2591608
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.7817289978265762,
    "estimated_duration": 3599.046656129913,
    "input_throughput": 285.6967686842024,
    "output_throughput": 255.28343691658864,
    "total_throughput": 540.9802056007911,
    "itl": 20.67782411215914,
    "ttft": 5916.88420148916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.109358481192031,
    "arrivals": 4284,
    "finished_requests": 4277,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7818613541312516. Arrivals time: 0.023646387737244368 Scheduler time: 0.38431520108133554 Scheduler overhead time: 0.1394934500567615 Adapter cache time: 0.02884430857375264 Engine time: 0.13674102211371064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 66, 33, 33, 66, 33, 1080, 66, 33, 33, 33, 1080, 1080, 33, 33, 66, 66, 66, 1080, 1080, 33, 1080, 1080, 1080, 1080, 66, 33, 66, 66, 66, 66, 1080]
Prompts retrieved: 12936 . Total input tokens: 2852361 . Total output tokens: 2591608
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7687435778789222,
    "estimated_duration": 3599.0521260097253,
    "input_throughput": 285.6963344790471,
    "output_throughput": 255.28304893395625,
    "total_throughput": 540.9793834130033,
    "itl": 20.67901506826971,
    "ttft": 5916.759498234469,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.347432278064083,
    "arrivals": 4284,
    "finished_requests": 4277,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7688303519971669. Arrivals time: 0.023353153374046087 Scheduler time: 0.3790032370015979 Scheduler overhead time: 0.13462695805355906 Adapter cache time: 0.02874575834721327 Engine time: 0.13508599111810327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 66, 33, 33, 66, 33, 1080, 66, 33, 33, 33, 1080, 1080, 33, 33, 66, 66, 66, 1080, 1080, 33, 1080, 1080, 1080, 1080, 66, 33, 66, 66, 66, 66, 1080]
Prompts retrieved: 12936 . Total input tokens: 2852361 . Total output tokens: 2591608
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7706878772005439,
    "estimated_duration": 3599.0533865729785,
    "input_throughput": 285.6962344143183,
    "output_throughput": 255.28295952143688,
    "total_throughput": 540.9791939357552,
    "itl": 20.679951791533988,
    "ttft": 5916.865009342605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.413930798713131,
    "arrivals": 4284,
    "finished_requests": 4277,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7707576090469956. Arrivals time: 0.023995804134756327 Scheduler time: 0.37711619632318616 Scheduler overhead time: 0.1367896581068635 Adapter cache time: 0.0285369660705328 Engine time: 0.13626579195261002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 66, 33, 33, 66, 33, 1080, 66, 33, 33, 33, 1080, 1080, 33, 33, 66, 66, 66, 1080, 1080, 33, 1080, 1080, 1080, 1080, 66, 33, 66, 66, 66, 66, 1080]
Prompts retrieved: 12936 . Total input tokens: 2852361 . Total output tokens: 2591608
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.764580646995455,
    "estimated_duration": 3599.0461939623433,
    "input_throughput": 285.69680537163964,
    "output_throughput": 255.28346969853123,
    "total_throughput": 540.9802750701708,
    "itl": 20.676554636287086,
    "ttft": 5916.757839235289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.172516435426657,
    "arrivals": 4284,
    "finished_requests": 4277,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7646907889284194. Arrivals time: 0.023150111082941294 Scheduler time: 0.3760228268802166 Scheduler overhead time: 0.13482241472229362 Adapter cache time: 0.028111227322369814 Engine time: 0.13522631488740444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 66, 33, 33, 66, 33, 1080, 66, 33, 33, 33, 1080, 1080, 33, 33, 66, 66, 66, 1080, 1080, 33, 1080, 1080, 1080, 1080, 66, 33, 66, 66, 66, 66, 1080]
Prompts retrieved: 12936 . Total input tokens: 2852361 . Total output tokens: 2591608
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7752267220057547,
    "estimated_duration": 3599.052544578193,
    "input_throughput": 285.69630125267,
    "output_throughput": 255.28301924463292,
    "total_throughput": 540.979320497303,
    "itl": 20.67969227627056,
    "ttft": 5916.853096044149,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.387834034459678,
    "arrivals": 4284,
    "finished_requests": 4277,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7753151492215693. Arrivals time: 0.023587862960994244 Scheduler time: 0.3781715543009341 Scheduler overhead time: 0.13983723474666476 Adapter cache time: 0.02851139148697257 Engine time: 0.13683556905016303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 66, 33, 33, 66, 33, 1080, 66, 33, 33, 33, 1080, 1080, 33, 33, 66, 66, 66, 1080, 1080, 33, 1080, 1080, 1080, 1080, 66, 33, 66, 66, 66, 66, 1080]
Prompts retrieved: 12936 . Total input tokens: 2852361 . Total output tokens: 2591608
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.7641719467937946,
    "estimated_duration": 3599.044498605871,
    "input_throughput": 285.6969399512283,
    "output_throughput": 255.28358995169364,
    "total_throughput": 540.980529902922,
    "itl": 20.677273335428758,
    "ttft": 5916.902677948257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0364707800419923,
    "arrivals": 4284,
    "finished_requests": 4277,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7642965046688914. Arrivals time: 0.023491763044148684 Scheduler time: 0.37453462835401297 Scheduler overhead time: 0.13436346827074885 Adapter cache time: 0.028157192282378674 Engine time: 0.1363339638337493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.1-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [10 11 11]
Adapter prompts. [1080, 66, 33, 33, 66, 33, 1080, 66, 33, 33, 33, 1080, 1080, 33, 33, 66, 66, 66, 1080, 1080, 33, 1080, 1080, 1080, 1080, 66, 33, 66, 66, 66, 66, 1080]
Prompts retrieved: 12936 . Total input tokens: 2852361 . Total output tokens: 2591608
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7782861446030438,
    "estimated_duration": 3599.0523374610166,
    "input_throughput": 285.69631769383443,
    "output_throughput": 255.283033935583,
    "total_throughput": 540.9793516294175,
    "itl": 20.679516363052077,
    "ttft": 5916.7780519722255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3675365511514372,
    "arrivals": 4284,
    "finished_requests": 4277,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7785791796632111. Arrivals time: 0.023827743250876665 Scheduler time: 0.3815137827768922 Scheduler overhead time: 0.13892533304169774 Adapter cache time: 0.028500163927674294 Engine time: 0.1373037756420672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 135, 135, 270, 135, 540, 270, 135, 135, 135, 540, 540, 135, 135, 270, 270, 270, 540, 540, 135, 540, 540, 540, 540, 270, 135, 270, 270, 270, 270, 540]
Prompts retrieved: 10260 . Total input tokens: 2259751 . Total output tokens: 2045991
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.7171115246601403,
    "estimated_duration": 3599.4130481140824,
    "input_throughput": 220.9657489619657,
    "output_throughput": 202.59358685774473,
    "total_throughput": 423.5593358197104,
    "itl": 20.530388384445374,
    "ttft": 4390.623271039522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.589519332503178,
    "arrivals": 3307,
    "finished_requests": 3303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7171744648367167. Arrivals time: 0.021991852205246687 Scheduler time: 0.3277355250902474 Scheduler overhead time: 0.13367714453488588 Adapter cache time: 0.02970301080495119 Engine time: 0.1364581431262195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 135, 135, 270, 135, 540, 270, 135, 135, 135, 540, 540, 135, 135, 270, 270, 270, 540, 540, 135, 540, 540, 540, 540, 270, 135, 270, 270, 270, 270, 540]
Prompts retrieved: 10260 . Total input tokens: 2259751 . Total output tokens: 2045991
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7123502981849015,
    "estimated_duration": 3599.421703943435,
    "input_throughput": 220.96521758721352,
    "output_throughput": 202.59309966406192,
    "total_throughput": 423.55831725127547,
    "itl": 20.54231783610327,
    "ttft": 4391.0962139097755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.61985255228815,
    "arrivals": 3307,
    "finished_requests": 3303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7124321470037103. Arrivals time: 0.021404987201094627 Scheduler time: 0.32387996884062886 Scheduler overhead time: 0.13473046943545341 Adapter cache time: 0.029480197001248598 Engine time: 0.13505821349099278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 135, 135, 270, 135, 540, 270, 135, 135, 135, 540, 540, 135, 135, 270, 270, 270, 540, 540, 135, 540, 540, 540, 540, 270, 135, 270, 270, 270, 270, 540]
Prompts retrieved: 10260 . Total input tokens: 2259751 . Total output tokens: 2045991
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7129154577851295,
    "estimated_duration": 3599.4240687792662,
    "input_throughput": 220.96507241219274,
    "output_throughput": 202.5929665595952,
    "total_throughput": 423.55803897178794,
    "itl": 20.542873058656188,
    "ttft": 4391.128043397987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.904698719046946,
    "arrivals": 3307,
    "finished_requests": 3303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.712998999748379. Arrivals time: 0.02184245642274618 Scheduler time: 0.32418803544715047 Scheduler overhead time: 0.13435087632387877 Adapter cache time: 0.029365014284849167 Engine time: 0.13556578243151307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 135, 135, 270, 135, 540, 270, 135, 135, 135, 540, 540, 135, 135, 270, 270, 270, 540, 540, 135, 540, 540, 540, 540, 270, 135, 270, 270, 270, 270, 540]
Prompts retrieved: 10260 . Total input tokens: 2259751 . Total output tokens: 2045991
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.7201139531098306,
    "estimated_duration": 3599.4161527503015,
    "input_throughput": 220.96555837042573,
    "output_throughput": 202.59341211290808,
    "total_throughput": 423.55897048333384,
    "itl": 20.538179713209196,
    "ttft": 4390.649682439999,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.9687769158045,
    "arrivals": 3307,
    "finished_requests": 3303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7201783559285104. Arrivals time: 0.021605890709906816 Scheduler time: 0.32874326733872294 Scheduler overhead time: 0.1343843713402748 Adapter cache time: 0.029684328008443117 Engine time: 0.13762793131172657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 135, 135, 270, 135, 540, 270, 135, 135, 135, 540, 540, 135, 135, 270, 270, 270, 540, 540, 135, 540, 540, 540, 540, 270, 135, 270, 270, 270, 270, 540]
Prompts retrieved: 10260 . Total input tokens: 2259751 . Total output tokens: 2045991
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7140188021585345,
    "estimated_duration": 3599.42324031056,
    "input_throughput": 220.96512327107638,
    "output_throughput": 202.59301318982503,
    "total_throughput": 423.5581364609014,
    "itl": 20.539754147019934,
    "ttft": 4391.21870947708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.807560763214653,
    "arrivals": 3307,
    "finished_requests": 3303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7141133248806. Arrivals time: 0.021795678418129683 Scheduler time: 0.32517866091802716 Scheduler overhead time: 0.13448118418455124 Adapter cache time: 0.029770344030112028 Engine time: 0.1348957414738834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 135, 135, 270, 135, 540, 270, 135, 135, 135, 540, 540, 135, 135, 270, 270, 270, 540, 540, 135, 540, 540, 540, 540, 270, 135, 270, 270, 270, 270, 540]
Prompts retrieved: 10260 . Total input tokens: 2259751 . Total output tokens: 2045991
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.718336827121675,
    "estimated_duration": 3599.4322721086473,
    "input_throughput": 220.9645688190887,
    "output_throughput": 202.59250483765982,
    "total_throughput": 423.5570736567485,
    "itl": 20.53328502601038,
    "ttft": 4390.564341678587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.29271330180112,
    "arrivals": 3307,
    "finished_requests": 3303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7184180002659559. Arrivals time: 0.021662486717104912 Scheduler time: 0.32598934788256884 Scheduler overhead time: 0.13706780318170786 Adapter cache time: 0.02937588281929493 Engine time: 0.13660711282864213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.05-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 135, 135, 270, 135, 540, 270, 135, 135, 135, 540, 540, 135, 135, 270, 270, 270, 540, 540, 135, 540, 540, 540, 540, 270, 135, 270, 270, 270, 270, 540]
Prompts retrieved: 10260 . Total input tokens: 2259751 . Total output tokens: 2045991
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.7152752499096096,
    "estimated_duration": 3599.4224118418533,
    "input_throughput": 220.9651741299834,
    "output_throughput": 202.59305982007632,
    "total_throughput": 423.55823395005973,
    "itl": 20.540521131624974,
    "ttft": 4391.262474587805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.706694698203295,
    "arrivals": 3307,
    "finished_requests": 3303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7154019079171121. Arrivals time: 0.021822622045874596 Scheduler time: 0.3253501155413687 Scheduler overhead time: 0.13529347581788898 Adapter cache time: 0.02956270845606923 Engine time: 0.13573533156886697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 66, 66, 270, 66, 540, 270, 66, 66, 66, 540, 540, 66, 66, 270, 270, 270, 540, 540, 66, 540, 540, 540, 540, 270, 66, 270, 270, 270, 270, 540]
Prompts retrieved: 9570 . Total input tokens: 2107260 . Total output tokens: 1907621
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.6967306300066411,
    "estimated_duration": 3592.38231904635,
    "input_throughput": 211.06662171783645,
    "output_throughput": 184.63318797763026,
    "total_throughput": 395.6998096954667,
    "itl": 20.28756653319521,
    "ttft": 5829.062668198909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.93641707451564,
    "arrivals": 3107,
    "finished_requests": 3102,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.696789728011936. Arrivals time: 0.020936846267431974 Scheduler time: 0.31076950347051024 Scheduler overhead time: 0.13374945288524032 Adapter cache time: 0.028377877548336983 Engine time: 0.13558858912438154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 66, 66, 270, 66, 540, 270, 66, 66, 66, 540, 540, 66, 66, 270, 270, 270, 540, 540, 66, 540, 540, 540, 540, 270, 66, 270, 270, 270, 270, 540]
Prompts retrieved: 9570 . Total input tokens: 2107260 . Total output tokens: 1907621
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6923785950057209,
    "estimated_duration": 3592.3725340356987,
    "input_throughput": 211.06719662734878,
    "output_throughput": 184.63369088697326,
    "total_throughput": 395.70088751432206,
    "itl": 20.293888423034478,
    "ttft": 5829.516243176699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.783710088762471,
    "arrivals": 3107,
    "finished_requests": 3102,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6924658101052046. Arrivals time: 0.020833666902035475 Scheduler time: 0.3057407783344388 Scheduler overhead time: 0.1343006701208651 Adapter cache time: 0.028649742249399424 Engine time: 0.1355322771705687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 66, 66, 270, 66, 540, 270, 66, 66, 66, 540, 540, 66, 66, 270, 270, 270, 540, 540, 66, 540, 540, 540, 540, 270, 66, 270, 270, 270, 270, 540]
Prompts retrieved: 9570 . Total input tokens: 2107260 . Total output tokens: 1907621
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.69598139077425,
    "estimated_duration": 3592.375844819212,
    "input_throughput": 211.06700210488648,
    "output_throughput": 184.6335207259973,
    "total_throughput": 395.70052283088376,
    "itl": 20.29784338744949,
    "ttft": 5829.518140058424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.017258468014225,
    "arrivals": 3107,
    "finished_requests": 3102,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6960457898676395. Arrivals time: 0.021144438069313765 Scheduler time: 0.30686421412974596 Scheduler overhead time: 0.13495812192559242 Adapter cache time: 0.028758171945810318 Engine time: 0.1362913209013641 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 66, 66, 270, 66, 540, 270, 66, 66, 66, 540, 540, 66, 66, 270, 270, 270, 540, 540, 66, 540, 540, 540, 540, 270, 66, 270, 270, 270, 270, 540]
Prompts retrieved: 9570 . Total input tokens: 2107260 . Total output tokens: 1907621
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.6908445567823946,
    "estimated_duration": 3592.3874686412078,
    "input_throughput": 211.06631915927355,
    "output_throughput": 184.6329233107134,
    "total_throughput": 395.699242469987,
    "itl": 20.290491681279914,
    "ttft": 5829.010152544875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.247856793063685,
    "arrivals": 3107,
    "finished_requests": 3102,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6909725219011307. Arrivals time: 0.020609700120985508 Scheduler time: 0.3055348973721266 Scheduler overhead time: 0.13317783176898956 Adapter cache time: 0.028710285667330027 Engine time: 0.1351294289343059 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 66, 66, 270, 66, 540, 270, 66, 66, 66, 540, 540, 66, 66, 270, 270, 270, 540, 540, 66, 540, 540, 540, 540, 270, 66, 270, 270, 270, 270, 540]
Prompts retrieved: 9570 . Total input tokens: 2107260 . Total output tokens: 1907621
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6939407279714942,
    "estimated_duration": 3592.374809233329,
    "input_throughput": 211.06706294987606,
    "output_throughput": 184.6335739509189,
    "total_throughput": 395.70063690079496,
    "itl": 20.29732724797315,
    "ttft": 5829.390758265325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.93731123784095,
    "arrivals": 3107,
    "finished_requests": 3102,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6940267290920019. Arrivals time: 0.020802507642656565 Scheduler time: 0.3056178456172347 Scheduler overhead time: 0.1345458747819066 Adapter cache time: 0.028786977753043175 Engine time: 0.13585282675921917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 66, 66, 270, 66, 540, 270, 66, 66, 66, 540, 540, 66, 66, 270, 270, 270, 540, 540, 66, 540, 540, 540, 540, 270, 66, 270, 270, 270, 270, 540]
Prompts retrieved: 9570 . Total input tokens: 2107260 . Total output tokens: 1907621
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.6958247511647642,
    "estimated_duration": 3592.379340247367,
    "input_throughput": 211.06679673416366,
    "output_throughput": 184.63334107536866,
    "total_throughput": 395.70013780953235,
    "itl": 20.28614473687912,
    "ttft": 5829.0329720997415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.696733066658456,
    "arrivals": 3107,
    "finished_requests": 3102,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6958799851126969. Arrivals time: 0.020463863853365183 Scheduler time: 0.31086222268640995 Scheduler overhead time: 0.1336487173102796 Adapter cache time: 0.0283484966494143 Engine time: 0.1355563118122518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.05-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 66, 66, 270, 66, 540, 270, 66, 66, 66, 540, 540, 66, 66, 270, 270, 270, 540, 540, 66, 540, 540, 540, 540, 270, 66, 270, 270, 270, 270, 540]
Prompts retrieved: 9570 . Total input tokens: 2107260 . Total output tokens: 1907621
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.692279149312526,
    "estimated_duration": 3592.3735665302693,
    "input_throughput": 211.06713596390983,
    "output_throughput": 184.63363782086532,
    "total_throughput": 395.70077378477515,
    "itl": 20.295626681398623,
    "ttft": 5829.4525403550015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.855085718724914,
    "arrivals": 3107,
    "finished_requests": 3102,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6923896721564233. Arrivals time: 0.020839934702962637 Scheduler time: 0.30673440964892507 Scheduler overhead time: 0.1337802936322987 Adapter cache time: 0.028485930059105158 Engine time: 0.13519587460905313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 33, 33, 270, 33, 540, 270, 33, 33, 33, 540, 540, 33, 33, 270, 270, 270, 540, 540, 33, 540, 540, 540, 540, 270, 33, 270, 270, 270, 270, 540]
Prompts retrieved: 9240 . Total input tokens: 2034863 . Total output tokens: 1835558
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.6916796592995524,
    "estimated_duration": 3597.3311113595396,
    "input_throughput": 203.09445457854588,
    "output_throughput": 182.88113594062958,
    "total_throughput": 385.97559051917546,
    "itl": 20.35174444490747,
    "ttft": 7257.1129423127595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 883,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.838757175211915,
    "arrivals": 2991,
    "finished_requests": 2985,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6917561064474285. Arrivals time: 0.02030129451304674 Scheduler time: 0.3043800722807646 Scheduler overhead time: 0.13441546447575092 Adapter cache time: 0.028354655019938946 Engine time: 0.136167723685503 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 33, 33, 270, 33, 540, 270, 33, 33, 33, 540, 540, 33, 33, 270, 270, 270, 540, 540, 33, 540, 540, 540, 540, 270, 33, 270, 270, 270, 270, 540]
Prompts retrieved: 9240 . Total input tokens: 2034863 . Total output tokens: 1835558
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6919172219932079,
    "estimated_duration": 3597.335521802977,
    "input_throughput": 203.09420557852937,
    "output_throughput": 182.8809117227603,
    "total_throughput": 385.97511730128963,
    "itl": 20.353649492010884,
    "ttft": 7257.622459569897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 882,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.577384973382593,
    "arrivals": 2991,
    "finished_requests": 2985,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.691993486136198. Arrivals time: 0.021116916090250015 Scheduler time: 0.3038174510002136 Scheduler overhead time: 0.13423172244802117 Adapter cache time: 0.028602666687220335 Engine time: 0.13613472552970052 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 33, 33, 270, 33, 540, 270, 33, 33, 33, 540, 540, 33, 33, 270, 270, 270, 540, 540, 33, 540, 540, 540, 540, 270, 33, 270, 270, 270, 270, 540]
Prompts retrieved: 9240 . Total input tokens: 2034863 . Total output tokens: 1835558
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6906052539125085,
    "estimated_duration": 3597.337017865794,
    "input_throughput": 203.0941211155814,
    "output_throughput": 182.88083566612985,
    "total_throughput": 385.9749567817113,
    "itl": 20.355256348883803,
    "ttft": 7257.316517554397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 882,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.781335436655247,
    "arrivals": 2991,
    "finished_requests": 2985,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6906691347248852. Arrivals time: 0.020611626096069813 Scheduler time: 0.3029990578070283 Scheduler overhead time: 0.13438268983736634 Adapter cache time: 0.028312054928392172 Engine time: 0.1360660744830966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 33, 33, 270, 33, 540, 270, 33, 33, 33, 540, 540, 33, 33, 270, 270, 270, 540, 540, 33, 540, 540, 540, 540, 270, 33, 270, 270, 270, 270, 540]
Prompts retrieved: 9240 . Total input tokens: 2034863 . Total output tokens: 1835558
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.6964742676354945,
    "estimated_duration": 3597.3327453610304,
    "input_throughput": 203.0943623277965,
    "output_throughput": 182.88105287129184,
    "total_throughput": 385.97541519908833,
    "itl": 20.34984129798263,
    "ttft": 7257.2370199145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 883,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.106220879503519,
    "arrivals": 2991,
    "finished_requests": 2985,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6965820686891675. Arrivals time: 0.021357552148401737 Scheduler time: 0.3056093277409673 Scheduler overhead time: 0.1348186293616891 Adapter cache time: 0.02865201188251376 Engine time: 0.1372229759581387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 33, 33, 270, 33, 540, 270, 33, 33, 33, 540, 540, 33, 33, 270, 270, 270, 540, 540, 33, 540, 540, 540, 540, 270, 33, 270, 270, 270, 270, 540]
Prompts retrieved: 9240 . Total input tokens: 2034863 . Total output tokens: 1835558
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6900388449430466,
    "estimated_duration": 3597.336603631441,
    "input_throughput": 203.09414450192835,
    "output_throughput": 182.88085672491113,
    "total_throughput": 385.9750012268395,
    "itl": 20.35340814601866,
    "ttft": 7257.418415539143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 882,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.709880010723176,
    "arrivals": 2991,
    "finished_requests": 2985,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6901043429970741. Arrivals time: 0.02029148954898119 Scheduler time: 0.30321340821683407 Scheduler overhead time: 0.13425864651799202 Adapter cache time: 0.028218384832143784 Engine time: 0.13664829544723034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 33, 33, 270, 33, 540, 270, 33, 33, 33, 540, 540, 33, 33, 270, 270, 270, 540, 540, 33, 540, 540, 540, 540, 270, 33, 270, 270, 270, 270, 540]
Prompts retrieved: 9240 . Total input tokens: 2034863 . Total output tokens: 1835558
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.6990884603001177,
    "estimated_duration": 3597.329968919084,
    "input_throughput": 203.0945190773056,
    "output_throughput": 182.88119402004125,
    "total_throughput": 385.97571309734684,
    "itl": 20.348182590848968,
    "ttft": 7256.921682888521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 882,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.630618269583183,
    "arrivals": 2991,
    "finished_requests": 2985,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6992051480337977. Arrivals time: 0.02069301577284932 Scheduler time: 0.3079887358471751 Scheduler overhead time: 0.13556084549054503 Adapter cache time: 0.02856311807408929 Engine time: 0.13829692639410496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.05-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 270, 33, 33, 270, 33, 540, 270, 33, 33, 33, 540, 540, 33, 33, 270, 270, 270, 540, 540, 33, 540, 540, 540, 540, 270, 33, 270, 270, 270, 270, 540]
Prompts retrieved: 9240 . Total input tokens: 2034863 . Total output tokens: 1835558
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6997552327811718,
    "estimated_duration": 3597.3358924061477,
    "input_throughput": 203.0941846554466,
    "output_throughput": 182.8808928820827,
    "total_throughput": 385.9750775375293,
    "itl": 20.35439587160511,
    "ttft": 7257.404049951315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 882,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.640081522204022,
    "arrivals": 2991,
    "finished_requests": 2985,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6998574286699295. Arrivals time: 0.02117306599393487 Scheduler time: 0.31017650151625276 Scheduler overhead time: 0.1344385459087789 Adapter cache time: 0.02828654833137989 Engine time: 0.13785034697502851 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 66, 66, 135, 66, 540, 135, 66, 66, 66, 540, 540, 66, 66, 135, 135, 135, 540, 540, 66, 540, 540, 540, 540, 135, 66, 135, 135, 135, 135, 540]
Prompts retrieved: 8085 . Total input tokens: 1779099 . Total output tokens: 1607481
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.6638476080261171,
    "estimated_duration": 3599.3822924277492,
    "input_throughput": 179.05266727455756,
    "output_throughput": 162.85183189158732,
    "total_throughput": 341.9044991661449,
    "itl": 20.213356051976607,
    "ttft": 6928.562145150867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 732,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.840283411387443,
    "arrivals": 2611,
    "finished_requests": 2606,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6639158870093524. Arrivals time: 0.020118897780776024 Scheduler time: 0.28233090601861477 Scheduler overhead time: 0.13294356362894177 Adapter cache time: 0.027144877705723047 Engine time: 0.13425463903695345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 66, 66, 135, 66, 540, 135, 66, 66, 66, 540, 540, 66, 66, 135, 135, 135, 540, 540, 66, 540, 540, 540, 540, 135, 66, 135, 135, 135, 135, 540]
Prompts retrieved: 8085 . Total input tokens: 1779099 . Total output tokens: 1607481
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6736742798238993,
    "estimated_duration": 3599.3843834054223,
    "input_throughput": 179.05256325812314,
    "output_throughput": 162.85173728664708,
    "total_throughput": 341.90430054477025,
    "itl": 20.215449459402137,
    "ttft": 6928.955736212184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 730,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.387690076632436,
    "arrivals": 2611,
    "finished_requests": 2606,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6737397001124918. Arrivals time: 0.019564186688512564 Scheduler time: 0.28830399084836245 Scheduler overhead time: 0.1369439447298646 Adapter cache time: 0.026936732232570648 Engine time: 0.13413496129214764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 66, 66, 135, 66, 540, 135, 66, 66, 66, 540, 540, 66, 66, 135, 135, 135, 540, 540, 66, 540, 540, 540, 540, 135, 66, 135, 135, 135, 135, 540]
Prompts retrieved: 8085 . Total input tokens: 1779099 . Total output tokens: 1607481
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6668324167840183,
    "estimated_duration": 3599.38497461438,
    "input_throughput": 179.0525338482434,
    "output_throughput": 162.85171053779789,
    "total_throughput": 341.9042443860413,
    "itl": 20.214858479518714,
    "ttft": 6929.062776319575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 730,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.543519445490125,
    "arrivals": 2611,
    "finished_requests": 2606,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6669073356315494. Arrivals time: 0.019960730336606503 Scheduler time: 0.28216547705233097 Scheduler overhead time: 0.1343335947021842 Adapter cache time: 0.027521178126335144 Engine time: 0.13506273459643126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 66, 66, 135, 66, 540, 135, 66, 66, 66, 540, 540, 66, 66, 135, 135, 135, 540, 540, 66, 540, 540, 540, 540, 135, 66, 135, 135, 135, 135, 540]
Prompts retrieved: 8085 . Total input tokens: 1779099 . Total output tokens: 1607481
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.6709748180583119,
    "estimated_duration": 3599.381606963476,
    "input_throughput": 179.05270137325002,
    "output_throughput": 162.8518629050015,
    "total_throughput": 341.90456427825154,
    "itl": 20.2121990591508,
    "ttft": 6928.628120124335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 731,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.042854091324838,
    "arrivals": 2611,
    "finished_requests": 2606,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6710719433613122. Arrivals time: 0.019932654686272144 Scheduler time: 0.2861090428195894 Scheduler overhead time: 0.13346430007368326 Adapter cache time: 0.027187970001250505 Engine time: 0.13690832909196615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 66, 66, 135, 66, 540, 135, 66, 66, 66, 540, 540, 66, 66, 135, 135, 135, 540, 540, 66, 540, 540, 540, 540, 135, 66, 135, 135, 135, 135, 540]
Prompts retrieved: 8085 . Total input tokens: 1779099 . Total output tokens: 1607481
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6627827216871083,
    "estimated_duration": 3599.384560380027,
    "input_throughput": 179.05255445446352,
    "output_throughput": 162.85172927954994,
    "total_throughput": 341.90428373401346,
    "itl": 20.21462015791712,
    "ttft": 6929.086092012484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 731,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.498987756003659,
    "arrivals": 2611,
    "finished_requests": 2606,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6628541769459844. Arrivals time: 0.019863185472786427 Scheduler time: 0.2821030202321708 Scheduler overhead time: 0.13213845808058977 Adapter cache time: 0.02697771042585373 Engine time: 0.13487878581508994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 66, 66, 135, 66, 540, 135, 66, 66, 66, 540, 540, 66, 66, 135, 135, 135, 540, 540, 66, 540, 540, 540, 540, 135, 66, 135, 135, 135, 135, 540]
Prompts retrieved: 8085 . Total input tokens: 1779099 . Total output tokens: 1607481
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.6642408617772162,
    "estimated_duration": 3599.381606963476,
    "input_throughput": 179.05270137325002,
    "output_throughput": 162.8518629050015,
    "total_throughput": 341.90456427825154,
    "itl": 20.208413087497433,
    "ttft": 6928.9957472967,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 731,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.666646207557037,
    "arrivals": 2611,
    "finished_requests": 2606,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6643425198271871. Arrivals time: 0.019604768604040146 Scheduler time: 0.2829318321309984 Scheduler overhead time: 0.13268631463870406 Adapter cache time: 0.02727742539718747 Engine time: 0.1344030788168311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.05-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 66, 66, 135, 66, 540, 135, 66, 66, 66, 540, 540, 66, 66, 135, 135, 135, 540, 540, 66, 540, 540, 540, 540, 135, 66, 135, 135, 135, 135, 540]
Prompts retrieved: 8085 . Total input tokens: 1779099 . Total output tokens: 1607481
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.663514198269695,
    "estimated_duration": 3599.384560380027,
    "input_throughput": 179.05255445446352,
    "output_throughput": 162.85172927954994,
    "total_throughput": 341.90428373401346,
    "itl": 20.217005149511643,
    "ttft": 6928.940076022745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 731,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.442859001141075,
    "arrivals": 2611,
    "finished_requests": 2606,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6636752062477171. Arrivals time: 0.01975133828818798 Scheduler time: 0.2828739834949374 Scheduler overhead time: 0.13216320564970374 Adapter cache time: 0.026964854914695024 Engine time: 0.13507123105227947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 33, 33, 135, 33, 540, 135, 33, 33, 33, 540, 540, 33, 33, 135, 135, 135, 540, 540, 33, 540, 540, 540, 540, 135, 33, 135, 135, 135, 135, 540]
Prompts retrieved: 7755 . Total input tokens: 1705563 . Total output tokens: 1541512
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.6534565519541502,
    "estimated_duration": 3599.266004546826,
    "input_throughput": 171.32020784822151,
    "output_throughput": 152.73808585015013,
    "total_throughput": 324.05829369837164,
    "itl": 20.195244139535045,
    "ttft": 4293.1973355358405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8550344656268707,
    "arrivals": 2536,
    "finished_requests": 2533,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6535273119807243. Arrivals time: 0.019081130158156157 Scheduler time: 0.2762337690219283 Scheduler overhead time: 0.13178651593625546 Adapter cache time: 0.02653220435604453 Engine time: 0.13332970393821597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 33, 33, 135, 33, 540, 135, 33, 33, 33, 540, 540, 33, 33, 135, 135, 135, 540, 540, 33, 540, 540, 540, 540, 135, 33, 135, 135, 135, 135, 540]
Prompts retrieved: 7755 . Total input tokens: 1705563 . Total output tokens: 1541512
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6484303441829979,
    "estimated_duration": 3599.265776058735,
    "input_throughput": 171.3202187239472,
    "output_throughput": 152.73809554624813,
    "total_throughput": 324.05831427019535,
    "itl": 20.196709062336748,
    "ttft": 4293.3291481897295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 582,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.317929889829838,
    "arrivals": 2536,
    "finished_requests": 2533,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6484956690110266. Arrivals time: 0.019705445040017366 Scheduler time: 0.27203894034028053 Scheduler overhead time: 0.13130173413082957 Adapter cache time: 0.026307142805308104 Engine time: 0.1332091921940446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 33, 33, 135, 33, 540, 135, 33, 33, 33, 540, 540, 33, 33, 135, 135, 135, 540, 540, 33, 540, 540, 540, 540, 135, 33, 135, 135, 135, 135, 540]
Prompts retrieved: 7755 . Total input tokens: 1705563 . Total output tokens: 1541512
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6461357497610152,
    "estimated_duration": 3599.2657594047732,
    "input_throughput": 171.32021951665342,
    "output_throughput": 152.73809625297406,
    "total_throughput": 324.0583157696275,
    "itl": 20.197940113471635,
    "ttft": 4293.299278958085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.455447430447711,
    "arrivals": 2536,
    "finished_requests": 2533,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6462021395564079. Arrivals time: 0.01943503087386489 Scheduler time: 0.2696918975561857 Scheduler overhead time: 0.13108434109017253 Adapter cache time: 0.026612511835992336 Engine time: 0.13300035893917084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 33, 33, 135, 33, 540, 135, 33, 33, 33, 540, 540, 33, 33, 135, 135, 135, 540, 540, 33, 540, 540, 540, 540, 135, 33, 135, 135, 135, 135, 540]
Prompts retrieved: 7755 . Total input tokens: 1705563 . Total output tokens: 1541512
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.6437438214197755,
    "estimated_duration": 3599.265776058735,
    "input_throughput": 171.3202187239472,
    "output_throughput": 152.73809554624813,
    "total_throughput": 324.05831427019535,
    "itl": 20.194823335135858,
    "ttft": 4293.372770641209,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.016128754694941,
    "arrivals": 2536,
    "finished_requests": 2533,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6438860930502415. Arrivals time: 0.01923746056854725 Scheduler time: 0.2688052519224584 Scheduler overhead time: 0.13063460122793913 Adapter cache time: 0.026225424837321043 Engine time: 0.1330636036582291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 33, 33, 135, 33, 540, 135, 33, 33, 33, 540, 540, 33, 33, 135, 135, 135, 540, 540, 33, 540, 540, 540, 540, 135, 33, 135, 135, 135, 135, 540]
Prompts retrieved: 7755 . Total input tokens: 1705563 . Total output tokens: 1541512
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6492261700332165,
    "estimated_duration": 3599.2657594047732,
    "input_throughput": 171.32021951665342,
    "output_throughput": 152.73809625297406,
    "total_throughput": 324.0583157696275,
    "itl": 20.19726944163985,
    "ttft": 4293.264518648292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.409260300062633,
    "arrivals": 2536,
    "finished_requests": 2533,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6492980499751866. Arrivals time: 0.01971489144489169 Scheduler time: 0.26969960471615195 Scheduler overhead time: 0.13173323962837458 Adapter cache time: 0.026768592186272144 Engine time: 0.1345199104398489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 33, 33, 135, 33, 540, 135, 33, 33, 33, 540, 540, 33, 33, 135, 135, 135, 540, 540, 33, 540, 540, 540, 540, 135, 33, 135, 135, 135, 135, 540]
Prompts retrieved: 7755 . Total input tokens: 1705563 . Total output tokens: 1541512
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.6456172242760658,
    "estimated_duration": 3599.265776058735,
    "input_throughput": 171.3202187239472,
    "output_throughput": 152.73809554624813,
    "total_throughput": 324.05831427019535,
    "itl": 20.19429397982616,
    "ttft": 4293.194227382365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7282098292931742,
    "arrivals": 2536,
    "finished_requests": 2533,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.645685279276222. Arrivals time: 0.019174024928361177 Scheduler time: 0.2701855804771185 Scheduler overhead time: 0.13037849590182304 Adapter cache time: 0.02633102424442768 Engine time: 0.1334279696457088 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.05-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 135, 33, 33, 135, 33, 540, 135, 33, 33, 33, 540, 540, 33, 33, 135, 135, 135, 540, 540, 33, 540, 540, 540, 540, 135, 33, 135, 135, 135, 135, 540]
Prompts retrieved: 7755 . Total input tokens: 1705563 . Total output tokens: 1541512
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6435825391672552,
    "estimated_duration": 3599.2657594047732,
    "input_throughput": 171.32021951665342,
    "output_throughput": 152.73809625297406,
    "total_throughput": 324.0583157696275,
    "itl": 20.1970068652206,
    "ttft": 4293.270326779939,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 582,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.357482502423262,
    "arrivals": 2536,
    "finished_requests": 2533,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6436355011537671. Arrivals time: 0.018736429046839476 Scheduler time: 0.2703629396855831 Scheduler overhead time: 0.1297701634466648 Adapter cache time: 0.02601032890379429 Engine time: 0.13311708020046353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 66, 33, 33, 66, 33, 540, 66, 33, 33, 33, 540, 540, 33, 33, 66, 66, 66, 540, 540, 33, 540, 540, 540, 540, 66, 33, 66, 66, 66, 66, 540]
Prompts retrieved: 6996 . Total input tokens: 1542781 . Total output tokens: 1380480
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.6136647081002593,
    "estimated_duration": 3598.903421255237,
    "input_throughput": 150.93764305865633,
    "output_throughput": 134.4278918802609,
    "total_throughput": 285.3655349389172,
    "itl": 20.10023337087013,
    "ttft": 4812.400037515073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3870796605339373,
    "arrivals": 2260,
    "finished_requests": 2257,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6137468172237277. Arrivals time: 0.018239823635667562 Scheduler time: 0.24757381156086922 Scheduler overhead time: 0.12865690048784018 Adapter cache time: 0.02503626374527812 Engine time: 0.1297455681487918 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 66, 33, 33, 66, 33, 540, 66, 33, 33, 33, 540, 540, 33, 33, 66, 66, 66, 540, 540, 33, 540, 540, 540, 540, 66, 33, 66, 66, 66, 66, 540]
Prompts retrieved: 6996 . Total input tokens: 1542781 . Total output tokens: 1380480
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6248692600056529,
    "estimated_duration": 3598.9045843696385,
    "input_throughput": 150.93759427777252,
    "output_throughput": 134.42784843509213,
    "total_throughput": 285.36544271286465,
    "itl": 20.10208110469656,
    "ttft": 4812.538924409876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.673862238447186,
    "arrivals": 2260,
    "finished_requests": 2257,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6249866811558604. Arrivals time: 0.01821512309834361 Scheduler time: 0.25761291617527604 Scheduler overhead time: 0.12869591126218438 Adapter cache time: 0.025028239469975233 Engine time: 0.13061528792604804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 66, 33, 33, 66, 33, 540, 66, 33, 33, 33, 540, 540, 33, 33, 66, 66, 66, 540, 540, 33, 540, 540, 540, 540, 66, 33, 66, 66, 66, 66, 540]
Prompts retrieved: 6996 . Total input tokens: 1542781 . Total output tokens: 1380480
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6245737397111952,
    "estimated_duration": 3598.9048883010983,
    "input_throughput": 150.93758153092736,
    "output_throughput": 134.4278370825131,
    "total_throughput": 285.36541861344045,
    "itl": 20.102672792410477,
    "ttft": 4812.611798057031,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.761201953780855,
    "arrivals": 2260,
    "finished_requests": 2257,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6246436447836459. Arrivals time: 0.018089042976498604 Scheduler time: 0.25367722706869245 Scheduler overhead time: 0.1316779418848455 Adapter cache time: 0.025008438155055046 Engine time: 0.1306510237045586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 66, 33, 33, 66, 33, 540, 66, 33, 33, 33, 540, 540, 33, 33, 66, 66, 66, 540, 540, 33, 540, 540, 540, 540, 66, 33, 66, 66, 66, 66, 540]
Prompts retrieved: 6996 . Total input tokens: 1542781 . Total output tokens: 1380480
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.6171048651449382,
    "estimated_duration": 3598.903192767146,
    "input_throughput": 150.9376526414242,
    "output_throughput": 134.42790041485344,
    "total_throughput": 285.36555305627763,
    "itl": 20.100809154120032,
    "ttft": 4812.3487609229305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.472570197316815,
    "arrivals": 2260,
    "finished_requests": 2257,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6171979657374322. Arrivals time: 0.018824034370481968 Scheduler time: 0.2467109588906169 Scheduler overhead time: 0.12882769154384732 Adapter cache time: 0.024798625614494085 Engine time: 0.12982231937348843 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 66, 33, 33, 66, 33, 540, 66, 33, 33, 33, 540, 540, 33, 33, 66, 66, 66, 540, 540, 33, 540, 540, 540, 540, 66, 33, 66, 66, 66, 66, 540]
Prompts retrieved: 6996 . Total input tokens: 1542781 . Total output tokens: 1380480
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6129693030379713,
    "estimated_duration": 3598.9046811839216,
    "input_throughput": 150.93759021739407,
    "output_throughput": 134.42784481884303,
    "total_throughput": 285.3654350362371,
    "itl": 20.102428309663264,
    "ttft": 4812.644724005319,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7311699631717232,
    "arrivals": 2260,
    "finished_requests": 2257,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6130317901261151. Arrivals time: 0.017724584322422743 Scheduler time: 0.24782414734363556 Scheduler overhead time: 0.1277049076743424 Adapter cache time: 0.02518364740535617 Engine time: 0.1300582461990416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 66, 33, 33, 66, 33, 540, 66, 33, 33, 33, 540, 540, 33, 33, 66, 66, 66, 540, 540, 33, 540, 540, 540, 540, 66, 33, 66, 66, 66, 66, 540]
Prompts retrieved: 6996 . Total input tokens: 1542781 . Total output tokens: 1380480
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.6166306668892503,
    "estimated_duration": 3598.903192767146,
    "input_throughput": 150.9376526414242,
    "output_throughput": 134.42790041485344,
    "total_throughput": 285.36555305627763,
    "itl": 20.099797343679175,
    "ttft": 4812.369993447771,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3045954595459532,
    "arrivals": 2260,
    "finished_requests": 2257,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6166992192156613. Arrivals time: 0.018278611823916435 Scheduler time: 0.24950073892250657 Scheduler overhead time: 0.12810753053054214 Adapter cache time: 0.025152453687041998 Engine time: 0.13054776983335614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.05-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [10 11 11]
Adapter prompts. [540, 66, 33, 33, 66, 33, 540, 66, 33, 33, 33, 540, 540, 33, 33, 66, 66, 66, 540, 540, 33, 540, 540, 540, 540, 66, 33, 66, 66, 66, 66, 540]
Prompts retrieved: 6996 . Total input tokens: 1542781 . Total output tokens: 1380480
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6145126321353018,
    "estimated_duration": 3598.9046811839216,
    "input_throughput": 150.93759021739407,
    "output_throughput": 134.42784481884303,
    "total_throughput": 285.3654350362371,
    "itl": 20.102260547393143,
    "ttft": 4812.583119169479,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.705901667624729,
    "arrivals": 2260,
    "finished_requests": 2257,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6145657040178776. Arrivals time: 0.018293859902769327 Scheduler time: 0.24901222018525004 Scheduler overhead time: 0.12765560997650027 Adapter cache time: 0.024952750653028488 Engine time: 0.12958739278838038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 66, 66, 135, 66, 270, 135, 66, 66, 66, 270, 270, 66, 66, 135, 135, 135, 270, 270, 66, 270, 270, 270, 270, 135, 66, 135, 135, 135, 135, 270]
Prompts retrieved: 5115 . Total input tokens: 1131339 . Total output tokens: 1006643
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.544340408872813,
    "estimated_duration": 3595.98461535196,
    "input_throughput": 115.21350737465528,
    "output_throughput": 95.776549913379,
    "total_throughput": 210.99005728803428,
    "itl": 19.847779332466775,
    "ttft": 10963.290537139184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.311290688831431,
    "arrivals": 1647,
    "finished_requests": 1642,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5444010868668556. Arrivals time: 0.01649115700274706 Scheduler time: 0.20200640568509698 Scheduler overhead time: 0.12032941775396466 Adapter cache time: 0.023789997212588787 Engine time: 0.12089983792975545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 66, 66, 135, 66, 270, 135, 66, 66, 66, 270, 270, 66, 66, 135, 135, 135, 270, 270, 66, 270, 270, 270, 270, 135, 66, 135, 135, 135, 135, 270]
Prompts retrieved: 5115 . Total input tokens: 1131339 . Total output tokens: 1006643
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.5535257402807474,
    "estimated_duration": 3595.985546596751,
    "input_throughput": 115.21347753805632,
    "output_throughput": 95.77652511032791,
    "total_throughput": 210.99000264838423,
    "itl": 19.850646494336523,
    "ttft": 10963.803622937283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.809227426815786,
    "arrivals": 1647,
    "finished_requests": 1642,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5536033590324223. Arrivals time: 0.015894180163741112 Scheduler time: 0.21041031926870346 Scheduler overhead time: 0.12152299331501126 Adapter cache time: 0.02323736110702157 Engine time: 0.12165853660553694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 66, 66, 135, 66, 270, 135, 66, 66, 66, 270, 270, 66, 66, 135, 135, 135, 270, 270, 66, 270, 270, 270, 270, 135, 66, 135, 135, 135, 135, 270]
Prompts retrieved: 5115 . Total input tokens: 1131339 . Total output tokens: 1006643
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.543297978118062,
    "estimated_duration": 3595.9858338742492,
    "input_throughput": 115.21346833383777,
    "output_throughput": 95.77651745889608,
    "total_throughput": 210.98998579273385,
    "itl": 19.851287168353213,
    "ttft": 10963.582450395084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.947761850236018,
    "arrivals": 1647,
    "finished_requests": 1642,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5434020468965173. Arrivals time: 0.016461086459457874 Scheduler time: 0.20214806124567986 Scheduler overhead time: 0.11924898531287909 Adapter cache time: 0.023515476379543543 Engine time: 0.12096602516248822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 66, 66, 135, 66, 270, 135, 66, 66, 66, 270, 270, 66, 66, 135, 135, 135, 270, 270, 66, 270, 270, 270, 270, 135, 66, 135, 135, 135, 135, 270]
Prompts retrieved: 5115 . Total input tokens: 1131339 . Total output tokens: 1006643
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.539820509031415,
    "estimated_duration": 3595.985546596751,
    "input_throughput": 115.21347753805632,
    "output_throughput": 95.77652511032791,
    "total_throughput": 210.99000264838423,
    "itl": 19.848898534001954,
    "ttft": 10963.480850986052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4968777078203805,
    "arrivals": 1647,
    "finished_requests": 1642,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5398738961666822. Arrivals time: 0.015576838981360197 Scheduler time: 0.20088608842343092 Scheduler overhead time: 0.11983643984422088 Adapter cache time: 0.022865071892738342 Engine time: 0.12072819890454412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 66, 66, 135, 66, 270, 135, 66, 66, 66, 270, 270, 66, 66, 135, 135, 135, 270, 270, 66, 270, 270, 270, 270, 135, 66, 135, 135, 135, 135, 270]
Prompts retrieved: 5115 . Total input tokens: 1131339 . Total output tokens: 1006643
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.5418181270360947,
    "estimated_duration": 3595.9858338742492,
    "input_throughput": 115.21346833383777,
    "output_throughput": 95.77651745889608,
    "total_throughput": 210.98998579273385,
    "itl": 19.851007230278505,
    "ttft": 10963.579911916016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.901160485497711,
    "arrivals": 1647,
    "finished_requests": 1642,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5418804427608848. Arrivals time: 0.01598552241921425 Scheduler time: 0.2016033655963838 Scheduler overhead time: 0.11940811248496175 Adapter cache time: 0.023398395627737045 Engine time: 0.12129016360267997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 66, 66, 135, 66, 270, 135, 66, 66, 66, 270, 270, 66, 66, 135, 135, 135, 270, 270, 66, 270, 270, 270, 270, 135, 66, 135, 135, 135, 135, 270]
Prompts retrieved: 5115 . Total input tokens: 1131339 . Total output tokens: 1006643
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.5413763988763094,
    "estimated_duration": 3595.984158375778,
    "input_throughput": 115.21352201594023,
    "output_throughput": 95.77656208462342,
    "total_throughput": 210.99008410056365,
    "itl": 19.847017415036756,
    "ttft": 10963.173837770231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.162316453251968,
    "arrivals": 1647,
    "finished_requests": 1642,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5414842488244176. Arrivals time: 0.01631901739165187 Scheduler time: 0.20150112686678767 Scheduler overhead time: 0.11884048162028193 Adapter cache time: 0.02328121941536665 Engine time: 0.1210489347577095 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.025-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 66, 66, 135, 66, 270, 135, 66, 66, 66, 270, 270, 66, 66, 135, 135, 135, 270, 270, 66, 270, 270, 270, 270, 135, 66, 135, 135, 135, 135, 270]
Prompts retrieved: 5115 . Total input tokens: 1131339 . Total output tokens: 1006643
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.5417161001823843,
    "estimated_duration": 3595.9856267570726,
    "input_throughput": 115.21347496976203,
    "output_throughput": 95.77652297531465,
    "total_throughput": 210.98999794507668,
    "itl": 19.850749883712005,
    "ttft": 10963.745882972049,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.851245245933568,
    "arrivals": 1647,
    "finished_requests": 1642,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5418071239255369. Arrivals time: 0.016288003884255886 Scheduler time: 0.20174520974978805 Scheduler overhead time: 0.11918427841737866 Adapter cache time: 0.02303838264197111 Engine time: 0.12150939833372831 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 33, 33, 135, 33, 270, 135, 33, 33, 33, 270, 270, 33, 33, 135, 135, 135, 270, 270, 33, 270, 270, 270, 270, 135, 33, 135, 135, 135, 135, 270]
Prompts retrieved: 4785 . Total input tokens: 1057278 . Total output tokens: 940766
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.5328201549127698,
    "estimated_duration": 3590.007596046365,
    "input_throughput": 103.72791422784239,
    "output_throughput": 89.06137144448277,
    "total_throughput": 192.78928567232518,
    "itl": 19.75589992516275,
    "ttft": 9426.72602762211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5773132862849644,
    "arrivals": 1533,
    "finished_requests": 1529,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5328990477137268. Arrivals time: 0.015849895775318146 Scheduler time: 0.19431175105273724 Scheduler overhead time: 0.1181513499468565 Adapter cache time: 0.0225376826710999 Engine time: 0.12198678217828274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 33, 33, 135, 33, 270, 135, 33, 33, 33, 270, 270, 33, 33, 135, 135, 135, 270, 270, 33, 270, 270, 270, 270, 135, 33, 135, 135, 135, 135, 270]
Prompts retrieved: 4785 . Total input tokens: 1057278 . Total output tokens: 940766
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.5315033779479563,
    "estimated_duration": 3589.9946655396575,
    "input_throughput": 103.72828783689077,
    "output_throughput": 89.06169222731621,
    "total_throughput": 192.789980064207,
    "itl": 19.757737789346034,
    "ttft": 9426.920819421714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.017318944013685,
    "arrivals": 1533,
    "finished_requests": 1529,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5315853180363774. Arrivals time: 0.016293791588395834 Scheduler time: 0.19423539843410254 Scheduler overhead time: 0.11911103688180447 Adapter cache time: 0.022688500583171844 Engine time: 0.11930914921686053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 33, 33, 135, 33, 270, 135, 33, 33, 33, 270, 270, 33, 33, 135, 135, 135, 270, 270, 33, 270, 270, 270, 270, 135, 33, 135, 135, 135, 135, 270]
Prompts retrieved: 4785 . Total input tokens: 1057278 . Total output tokens: 940766
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.5305644068866968,
    "estimated_duration": 3589.997063683412,
    "input_throughput": 103.72821854565147,
    "output_throughput": 89.06163273346785,
    "total_throughput": 192.78985127911932,
    "itl": 19.75836734986309,
    "ttft": 9427.052846377032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.138466831883451,
    "arrivals": 1533,
    "finished_requests": 1529,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.530640859156847. Arrivals time: 0.0161309028044343 Scheduler time: 0.1929207444190979 Scheduler overhead time: 0.11879795230925083 Adapter cache time: 0.022608575876802206 Engine time: 0.12026847619563341 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 33, 33, 135, 33, 270, 135, 33, 33, 33, 270, 270, 33, 33, 135, 135, 135, 270, 270, 33, 270, 270, 270, 270, 135, 33, 135, 135, 135, 135, 270]
Prompts retrieved: 4785 . Total input tokens: 1057278 . Total output tokens: 940766
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.531632708851248,
    "estimated_duration": 3589.9905008767378,
    "input_throughput": 103.72840816961978,
    "output_throughput": 89.06179554567517,
    "total_throughput": 192.79020371529495,
    "itl": 19.756266525261815,
    "ttft": 9426.87365173919,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7327336444845387,
    "arrivals": 1533,
    "finished_requests": 1529,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5317069161683321. Arrivals time: 0.015652924310415983 Scheduler time: 0.1958194593898952 Scheduler overhead time: 0.11832470865920186 Adapter cache time: 0.02263027662411332 Engine time: 0.1190738514997065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 33, 33, 135, 33, 270, 135, 33, 33, 33, 270, 270, 33, 33, 135, 135, 135, 270, 270, 33, 270, 270, 270, 270, 135, 33, 135, 135, 135, 135, 270]
Prompts retrieved: 4785 . Total input tokens: 1057278 . Total output tokens: 940766
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.5316893979907036,
    "estimated_duration": 3589.996442331882,
    "input_throughput": 103.72823649878549,
    "output_throughput": 89.06164814812985,
    "total_throughput": 192.78988464691534,
    "itl": 19.758139082380406,
    "ttft": 9427.054549530909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.096007810677438,
    "arrivals": 1533,
    "finished_requests": 1529,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5318139968439937. Arrivals time: 0.01576005993410945 Scheduler time: 0.19481975212693214 Scheduler overhead time: 0.11842520488426089 Adapter cache time: 0.022577933967113495 Engine time: 0.12029363727197051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 33, 33, 135, 33, 270, 135, 33, 33, 33, 270, 270, 33, 33, 135, 135, 135, 270, 270, 33, 270, 270, 270, 270, 135, 33, 135, 135, 135, 135, 270]
Prompts retrieved: 4785 . Total input tokens: 1057278 . Total output tokens: 940766
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.5302870450541377,
    "estimated_duration": 3590.005311165454,
    "input_throughput": 103.72798024610995,
    "output_throughput": 89.06142812813918,
    "total_throughput": 192.78940837424912,
    "itl": 19.755286191376886,
    "ttft": 9426.691153138105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.453701228848643,
    "arrivals": 1533,
    "finished_requests": 1529,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5303642707876861. Arrivals time: 0.015526364091783762 Scheduler time: 0.19425041088834405 Scheduler overhead time: 0.1186000993475318 Adapter cache time: 0.02276824275031686 Engine time: 0.118871312122792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 135, 33, 33, 135, 33, 270, 135, 33, 33, 33, 270, 270, 33, 33, 135, 135, 135, 270, 270, 33, 270, 270, 270, 270, 135, 33, 135, 135, 135, 135, 270]
Prompts retrieved: 4785 . Total input tokens: 1057278 . Total output tokens: 940766
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.5283108800649643,
    "estimated_duration": 3589.995406745999,
    "input_throughput": 103.72826642068935,
    "output_throughput": 89.06167383924505,
    "total_throughput": 192.78994025993438,
    "itl": 19.75792478142293,
    "ttft": 9427.021197120044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.054377258177883,
    "arrivals": 1533,
    "finished_requests": 1529,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5284140617586672. Arrivals time: 0.015793525613844395 Scheduler time: 0.1928952462039888 Scheduler overhead time: 0.11808160040527582 Adapter cache time: 0.022675089072436094 Engine time: 0.11941240029409528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 66, 33, 33, 66, 33, 270, 66, 33, 33, 33, 270, 270, 33, 33, 66, 66, 66, 270, 270, 33, 270, 270, 270, 270, 66, 33, 66, 66, 66, 66, 270]
Prompts retrieved: 4026 . Total input tokens: 883288 . Total output tokens: 783629
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.46650484995916486,
    "estimated_duration": 3599.8564195433387,
    "input_throughput": 90.21665370787161,
    "output_throughput": 73.26958891139869,
    "total_throughput": 163.4862426192703,
    "itl": 19.69643446134436,
    "ttft": 5591.9677137137105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3474052063422364,
    "arrivals": 1295,
    "finished_requests": 1293,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.466567886993289. Arrivals time: 0.013921841513365507 Scheduler time: 0.16657566372305155 Scheduler overhead time: 0.1051504798233509 Adapter cache time: 0.0199061818420887 Engine time: 0.10748433414846659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 66, 33, 33, 66, 33, 270, 66, 33, 33, 33, 270, 270, 33, 33, 66, 66, 66, 270, 270, 33, 270, 270, 270, 270, 66, 33, 66, 66, 66, 66, 270]
Prompts retrieved: 4026 . Total input tokens: 883288 . Total output tokens: 783629
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.47315576719120145,
    "estimated_duration": 3599.857579276221,
    "input_throughput": 90.21662464360519,
    "output_throughput": 73.26956530681166,
    "total_throughput": 163.48618995041684,
    "itl": 19.69788798788638,
    "ttft": 5592.292879063908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.62445294501726,
    "arrivals": 1295,
    "finished_requests": 1293,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4732549572363496. Arrivals time: 0.014062924776226282 Scheduler time: 0.1707607195712626 Scheduler overhead time: 0.10663212463259697 Adapter cache time: 0.020184314344078302 Engine time: 0.10754181398078799 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 66, 33, 33, 66, 33, 270, 66, 33, 33, 33, 270, 270, 33, 33, 66, 66, 66, 270, 270, 33, 270, 270, 270, 270, 66, 33, 66, 66, 66, 66, 270]
Prompts retrieved: 4026 . Total input tokens: 883288 . Total output tokens: 783629
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.47063426999375224,
    "estimated_duration": 3599.8578832076805,
    "input_throughput": 90.21661702672938,
    "output_throughput": 73.26955912075469,
    "total_throughput": 163.48617614748406,
    "itl": 19.698321745320264,
    "ttft": 5592.346343982277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7012518273759802,
    "arrivals": 1295,
    "finished_requests": 1293,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.47069218615069985. Arrivals time: 0.013866733759641647 Scheduler time: 0.1690492588095367 Scheduler overhead time: 0.10703885741531849 Adapter cache time: 0.02003840170800686 Engine time: 0.10695308819413185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 66, 33, 33, 66, 33, 270, 66, 33, 33, 33, 270, 270, 33, 33, 66, 66, 66, 270, 270, 33, 270, 270, 270, 270, 66, 33, 66, 66, 66, 66, 270]
Prompts retrieved: 4026 . Total input tokens: 883288 . Total output tokens: 783629
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.47313580475747585,
    "estimated_duration": 3599.857579276221,
    "input_throughput": 90.21662464360519,
    "output_throughput": 73.26956530681166,
    "total_throughput": 163.48618995041684,
    "itl": 19.696931943225568,
    "ttft": 5592.120162379754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.452313544326459,
    "arrivals": 1295,
    "finished_requests": 1293,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4732097927480936. Arrivals time: 0.013914691284298897 Scheduler time: 0.16760482266545296 Scheduler overhead time: 0.10713995760306716 Adapter cache time: 0.020419815089553595 Engine time: 0.11025668820366263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 66, 33, 33, 66, 33, 270, 66, 33, 33, 33, 270, 270, 33, 33, 66, 66, 66, 270, 270, 33, 270, 270, 270, 270, 66, 33, 66, 66, 66, 66, 270]
Prompts retrieved: 4026 . Total input tokens: 883288 . Total output tokens: 783629
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.46942619187757373,
    "estimated_duration": 3599.8578832076805,
    "input_throughput": 90.21661702672938,
    "output_throughput": 73.26955912075469,
    "total_throughput": 163.48617614748406,
    "itl": 19.698174063597502,
    "ttft": 5592.363240494352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6755692974757572,
    "arrivals": 1295,
    "finished_requests": 1293,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.46950922627002. Arrivals time: 0.013807089067995548 Scheduler time: 0.16805710783228278 Scheduler overhead time: 0.1062023239210248 Adapter cache time: 0.020096550229936838 Engine time: 0.10761122405529022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 66, 33, 33, 66, 33, 270, 66, 33, 33, 33, 270, 270, 33, 33, 66, 66, 66, 270, 270, 33, 270, 270, 270, 270, 66, 33, 66, 66, 66, 66, 270]
Prompts retrieved: 4026 . Total input tokens: 883288 . Total output tokens: 783629
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.4690395765937865,
    "estimated_duration": 3599.8561910552476,
    "input_throughput": 90.21665943405341,
    "output_throughput": 73.26959356192572,
    "total_throughput": 163.48625299597913,
    "itl": 19.69600672838794,
    "ttft": 5591.766680408691,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2662919339025303,
    "arrivals": 1295,
    "finished_requests": 1293,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4691391456872225. Arrivals time: 0.013611316215246916 Scheduler time: 0.1672315038740635 Scheduler overhead time: 0.10654667159542441 Adapter cache time: 0.019978707190603018 Engine time: 0.1078800167888403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [10 11 11]
Adapter prompts. [270, 66, 33, 33, 66, 33, 270, 66, 33, 33, 33, 270, 270, 33, 33, 66, 66, 66, 270, 270, 33, 270, 270, 270, 270, 66, 33, 66, 66, 66, 66, 270]
Prompts retrieved: 4026 . Total input tokens: 883288 . Total output tokens: 783629
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.4694741158746183,
    "estimated_duration": 3599.857676090504,
    "input_throughput": 90.21662221732653,
    "output_throughput": 73.26956333630586,
    "total_throughput": 163.4861855536324,
    "itl": 19.69802262447155,
    "ttft": 5592.3257925641055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6478155958093863,
    "arrivals": 1295,
    "finished_requests": 1293,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4695421550422907. Arrivals time: 0.013789191376417875 Scheduler time: 0.16770184179767966 Scheduler overhead time: 0.10658374382182956 Adapter cache time: 0.02015572739765048 Engine time: 0.10707936994731426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [10 11 11]
Adapter prompts. [135, 66, 33, 33, 66, 33, 135, 66, 33, 33, 33, 135, 135, 33, 33, 66, 66, 66, 135, 135, 33, 135, 135, 135, 135, 66, 33, 66, 66, 66, 66, 135]
Prompts retrieved: 2541 . Total input tokens: 557882 . Total output tokens: 512159
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.4053470939397812,
    "estimated_duration": 3598.01031664571,
    "input_throughput": 54.394785666555805,
    "output_throughput": 54.871715927722974,
    "total_throughput": 109.26650159427878,
    "itl": 19.548275722416665,
    "ttft": 4228.2176923399775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3606300244061367,
    "arrivals": 858,
    "finished_requests": 857,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4053974710404873. Arrivals time: 0.011433321982622147 Scheduler time: 0.13735032407566905 Scheduler overhead time: 0.09407771797850728 Adapter cache time: 0.01734469272196293 Engine time: 0.09752102568745613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [10 11 11]
Adapter prompts. [135, 66, 33, 33, 66, 33, 135, 66, 33, 33, 33, 135, 135, 33, 33, 66, 66, 66, 135, 135, 33, 135, 135, 135, 135, 66, 33, 66, 66, 66, 66, 135]
Prompts retrieved: 2541 . Total input tokens: 557882 . Total output tokens: 512159
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.40470823599025607,
    "estimated_duration": 3598.01031664571,
    "input_throughput": 54.394785666555805,
    "output_throughput": 54.871715927722974,
    "total_throughput": 109.26650159427878,
    "itl": 19.54943874281277,
    "ttft": 4228.754972769895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6233385771652715,
    "arrivals": 858,
    "finished_requests": 857,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.40477211121469736. Arrivals time: 0.011627222411334515 Scheduler time: 0.1399190961383283 Scheduler overhead time: 0.09383210632950068 Adapter cache time: 0.017050084192305803 Engine time: 0.09502106998115778 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [10 11 11]
Adapter prompts. [135, 66, 33, 33, 66, 33, 135, 66, 33, 33, 33, 135, 135, 33, 33, 66, 66, 66, 135, 135, 33, 135, 135, 135, 135, 66, 33, 66, 66, 66, 66, 135]
Prompts retrieved: 2541 . Total input tokens: 557882 . Total output tokens: 512159
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.40733654890209436,
    "estimated_duration": 3598.01031664571,
    "input_throughput": 54.394785666555805,
    "output_throughput": 54.871715927722974,
    "total_throughput": 109.26650159427878,
    "itl": 19.549791087245843,
    "ttft": 4228.691901298797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.696898297388122,
    "arrivals": 858,
    "finished_requests": 857,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4073996520601213. Arrivals time: 0.011679065879434347 Scheduler time: 0.1394494352862239 Scheduler overhead time: 0.09548773942515254 Adapter cache time: 0.01718049682676792 Engine time: 0.09604504704475403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [10 11 11]
Adapter prompts. [135, 66, 33, 33, 66, 33, 135, 66, 33, 33, 33, 135, 135, 33, 33, 66, 66, 66, 135, 135, 33, 135, 135, 135, 135, 66, 33, 66, 66, 66, 66, 135]
Prompts retrieved: 2541 . Total input tokens: 557882 . Total output tokens: 512159
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 0.4061643620952964,
    "estimated_duration": 3598.01031664571,
    "input_throughput": 54.394785666555805,
    "output_throughput": 54.871715927722974,
    "total_throughput": 109.26650159427878,
    "itl": 19.548630501585038,
    "ttft": 4228.360878532227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.45536383939441,
    "arrivals": 858,
    "finished_requests": 857,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.40622095623984933. Arrivals time: 0.011671289335936308 Scheduler time: 0.14053860679268837 Scheduler overhead time: 0.09369550039991736 Adapter cache time: 0.017323367297649384 Engine time: 0.09538771025836468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [10 11 11]
Adapter prompts. [135, 66, 33, 33, 66, 33, 135, 66, 33, 33, 33, 135, 135, 33, 33, 66, 66, 66, 135, 135, 33, 135, 135, 135, 135, 66, 33, 66, 66, 66, 66, 135]
Prompts retrieved: 2541 . Total input tokens: 557882 . Total output tokens: 512159
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 0.40525735914707184,
    "estimated_duration": 3598.01031664571,
    "input_throughput": 54.394785666555805,
    "output_throughput": 54.871715927722974,
    "total_throughput": 109.26650159427878,
    "itl": 19.549671104271965,
    "ttft": 4228.720957358055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.671837119017743,
    "arrivals": 858,
    "finished_requests": 857,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4053951087407768. Arrivals time: 0.011872298084199429 Scheduler time: 0.13856209395453334 Scheduler overhead time: 0.09473644755780697 Adapter cache time: 0.01744521176442504 Engine time: 0.09499467629939318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [10 11 11]
Adapter prompts. [135, 66, 33, 33, 66, 33, 135, 66, 33, 33, 33, 135, 135, 33, 33, 66, 66, 66, 135, 135, 33, 135, 135, 135, 135, 66, 33, 66, 66, 66, 66, 135]
Prompts retrieved: 2541 . Total input tokens: 557882 . Total output tokens: 512159
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 0.40271043684333563,
    "estimated_duration": 3598.01031664571,
    "input_throughput": 54.394785666555805,
    "output_throughput": 54.871715927722974,
    "total_throughput": 109.26650159427878,
    "itl": 19.54774635145771,
    "ttft": 4228.0479094953935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2790597757836712,
    "arrivals": 858,
    "finished_requests": 857,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4027844602242112. Arrivals time: 0.011536139529198408 Scheduler time: 0.1372398054227233 Scheduler overhead time: 0.0932878702878952 Adapter cache time: 0.017480332870036364 Engine time: 0.09565169969573617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [10 11 11]
Adapter prompts. [135, 66, 33, 33, 66, 33, 135, 66, 33, 33, 33, 135, 135, 33, 33, 66, 66, 66, 135, 135, 33, 135, 135, 135, 135, 66, 33, 66, 66, 66, 66, 135]
Prompts retrieved: 2541 . Total input tokens: 557882 . Total output tokens: 512159
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 0.402110043913126,
    "estimated_duration": 3598.01031664571,
    "input_throughput": 54.394785666555805,
    "output_throughput": 54.871715927722974,
    "total_throughput": 109.26650159427878,
    "itl": 19.549537566141776,
    "ttft": 4228.757918437447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6455332375876757,
    "arrivals": 858,
    "finished_requests": 857,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4022013358771801. Arrivals time: 0.011817643884569407 Scheduler time: 0.13585217157378793 Scheduler overhead time: 0.09340672520920634 Adapter cache time: 0.01760219130665064 Engine time: 0.09592544287443161 
