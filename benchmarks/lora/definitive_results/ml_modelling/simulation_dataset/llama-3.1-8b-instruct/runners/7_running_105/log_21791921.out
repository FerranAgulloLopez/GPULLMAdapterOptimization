INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 1080, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8160 . Total input tokens: 1793497 . Total output tokens: 1657466
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6672545117326081,
    "estimated_duration": 3599.1991371641634,
    "input_throughput": 173.1257361022142,
    "output_throughput": 158.77782201577986,
    "total_throughput": 331.90355811799407,
    "itl": 20.170487474747887,
    "ttft": 9600.326200629848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 2634,
    "finished_requests": 2627,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6673231096938252. Arrivals time: 0.019293255172669888 Scheduler time: 0.2813669960014522 Scheduler overhead time: 0.13435643259435892 Adapter cache time: 0.02604772849008441 Engine time: 0.13885982986539602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 1080, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8160 . Total input tokens: 1793497 . Total output tokens: 1657466
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6554492018185556,
    "estimated_duration": 3599.1991371641634,
    "input_throughput": 173.1257361022142,
    "output_throughput": 158.77782201577986,
    "total_throughput": 331.90355811799407,
    "itl": 20.170527189458955,
    "ttft": 9600.33635790364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 2634,
    "finished_requests": 2627,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6555389896966517. Arrivals time: 0.019240080378949642 Scheduler time: 0.27837648103013635 Scheduler overhead time: 0.1322701433673501 Adapter cache time: 0.02486208640038967 Engine time: 0.13395933480933309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 1080, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8160 . Total input tokens: 1793497 . Total output tokens: 1657466
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6598747582174838,
    "estimated_duration": 3599.1991371641634,
    "input_throughput": 173.1257361022142,
    "output_throughput": 158.77782201577986,
    "total_throughput": 331.90355811799407,
    "itl": 20.170529137332686,
    "ttft": 9600.363701954575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 2634,
    "finished_requests": 2627,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6599765289574862. Arrivals time: 0.019801408052444458 Scheduler time: 0.27967529790475965 Scheduler overhead time: 0.1327470443211496 Adapter cache time: 0.02492671413347125 Engine time: 0.1353153856471181 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 1080, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8160 . Total input tokens: 1793497 . Total output tokens: 1657466
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6556515279226005,
    "estimated_duration": 3599.1991371641634,
    "input_throughput": 173.1257361022142,
    "output_throughput": 158.77782201577986,
    "total_throughput": 331.90355811799407,
    "itl": 20.17051043555743,
    "ttft": 9600.285264270278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 2634,
    "finished_requests": 2627,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6557113309390843. Arrivals time: 0.018924258183687925 Scheduler time: 0.2771826065145433 Scheduler overhead time: 0.1323109408840537 Adapter cache time: 0.025174745358526707 Engine time: 0.13450649613514543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 1080, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8160 . Total input tokens: 1793497 . Total output tokens: 1657466
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.6567874127067626,
    "estimated_duration": 3599.1991371641634,
    "input_throughput": 173.1257361022142,
    "output_throughput": 158.77782201577986,
    "total_throughput": 331.90355811799407,
    "itl": 20.170526463751756,
    "ttft": 9600.355043771182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 2634,
    "finished_requests": 2627,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6568635497242212. Arrivals time: 0.01900826906785369 Scheduler time: 0.2769624455831945 Scheduler overhead time: 0.13283138209953904 Adapter cache time: 0.02519680093973875 Engine time: 0.1356688761152327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 1080, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8160 . Total input tokens: 1793497 . Total output tokens: 1657466
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6597290108911693,
    "estimated_duration": 3599.1991371641634,
    "input_throughput": 173.1257361022142,
    "output_throughput": 158.77782201577986,
    "total_throughput": 331.90355811799407,
    "itl": 20.170477317088555,
    "ttft": 9600.373199219595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 2634,
    "finished_requests": 2627,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6597884260118008. Arrivals time: 0.018969879020005465 Scheduler time: 0.2812886922620237 Scheduler overhead time: 0.13203031849116087 Adapter cache time: 0.02479195687919855 Engine time: 0.13526611542329192 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.1-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 1080, 66, 1080, 66, 270, 270, 66, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 8160 . Total input tokens: 1793497 . Total output tokens: 1657466
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.66624452220276,
    "estimated_duration": 3599.1991371641634,
    "input_throughput": 173.1257361022142,
    "output_throughput": 158.77782201577986,
    "total_throughput": 331.90355811799407,
    "itl": 20.170522653626406,
    "ttft": 9600.339268313439,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 2634,
    "finished_requests": 2627,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6663168189115822. Arrivals time: 0.01890531275421381 Scheduler time: 0.2771261394955218 Scheduler overhead time: 0.1313183936290443 Adapter cache time: 0.03761158976703882 Engine time: 0.13442434463649988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 1080, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 7995 . Total input tokens: 1756080 . Total output tokens: 1627923
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6521659381687641,
    "estimated_duration": 3599.8901068311434,
    "input_throughput": 169.07766124443805,
    "output_throughput": 156.1497666090747,
    "total_throughput": 325.2274278535128,
    "itl": 20.092081749566006,
    "ttft": 5612.1040567897435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 2581,
    "finished_requests": 2577,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6522161210887134. Arrivals time: 0.01845704484730959 Scheduler time: 0.27416255604475737 Scheduler overhead time: 0.13065293477848172 Adapter cache time: 0.02454483602195978 Engine time: 0.13751933770254254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 1080, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 7995 . Total input tokens: 1756080 . Total output tokens: 1627923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6438226322643459,
    "estimated_duration": 3599.8901068311434,
    "input_throughput": 169.07766124443805,
    "output_throughput": 156.1497666090747,
    "total_throughput": 325.2274278535128,
    "itl": 20.092142790884193,
    "ttft": 5612.121215810417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 2581,
    "finished_requests": 2577,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.643875147216022. Arrivals time: 0.01858623046427965 Scheduler time: 0.2724009524099529 Scheduler overhead time: 0.13117259554564953 Adapter cache time: 0.02454346651211381 Engine time: 0.1311797145754099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 1080, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 7995 . Total input tokens: 1756080 . Total output tokens: 1627923
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.645054052118212,
    "estimated_duration": 3599.8901068311434,
    "input_throughput": 169.07766124443805,
    "output_throughput": 156.1497666090747,
    "total_throughput": 325.2274278535128,
    "itl": 20.09214925545098,
    "ttft": 5612.102044241165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 2581,
    "finished_requests": 2577,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6451053838245571. Arrivals time: 0.018991127610206604 Scheduler time: 0.2720996793359518 Scheduler overhead time: 0.13080853037536144 Adapter cache time: 0.02446391899138689 Engine time: 0.13241646764799953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 1080, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 7995 . Total input tokens: 1756080 . Total output tokens: 1627923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6487385942600667,
    "estimated_duration": 3599.8901068311434,
    "input_throughput": 169.07766124443805,
    "output_throughput": 156.1497666090747,
    "total_throughput": 325.2274278535128,
    "itl": 20.09210771818415,
    "ttft": 5612.120451906324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 2581,
    "finished_requests": 2577,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6487955651246011. Arrivals time: 0.018294999841600657 Scheduler time: 0.2764862566255033 Scheduler overhead time: 0.13050153758376837 Adapter cache time: 0.024576355703175068 Engine time: 0.13224771060049534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 1080, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 7995 . Total input tokens: 1756080 . Total output tokens: 1627923
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.6478513670153916,
    "estimated_duration": 3599.8901068311434,
    "input_throughput": 169.07766124443805,
    "output_throughput": 156.1497666090747,
    "total_throughput": 325.2274278535128,
    "itl": 20.092145051607904,
    "ttft": 5612.108616487479,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 2581,
    "finished_requests": 2577,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.647904303856194. Arrivals time: 0.018738824408501387 Scheduler time: 0.2734091831371188 Scheduler overhead time: 0.13193419901654124 Adapter cache time: 0.02481297543272376 Engine time: 0.13199804024770856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 1080, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 7995 . Total input tokens: 1756080 . Total output tokens: 1627923
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6622044621035457,
    "estimated_duration": 3599.8901068311434,
    "input_throughput": 169.07766124443805,
    "output_throughput": 156.1497666090747,
    "total_throughput": 325.2274278535128,
    "itl": 20.09206255701515,
    "ttft": 5612.087703612003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 2581,
    "finished_requests": 2577,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.662264899816364. Arrivals time: 0.018703587353229523 Scheduler time: 0.2823548619635403 Scheduler overhead time: 0.1365799941122532 Adapter cache time: 0.02448889845982194 Engine time: 0.13221310125663877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 1080, 33, 1080, 33, 270, 270, 33, 270, 1080, 1080, 1080, 1080]
Prompts retrieved: 7995 . Total input tokens: 1756080 . Total output tokens: 1627923
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6490375949069858,
    "estimated_duration": 3599.8901068311434,
    "input_throughput": 169.07766124443805,
    "output_throughput": 156.1497666090747,
    "total_throughput": 325.2274278535128,
    "itl": 20.09214669454034,
    "ttft": 5612.1242339667215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 2581,
    "finished_requests": 2577,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6490945061668754. Arrivals time: 0.018592421896755695 Scheduler time: 0.2756713479757309 Scheduler overhead time: 0.13080214755609632 Adapter cache time: 0.024498482700437307 Engine time: 0.13314024917781353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 1080, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7485 . Total input tokens: 1645904 . Total output tokens: 1523231
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6377831152640283,
    "estimated_duration": 3598.072456576852,
    "input_throughput": 161.78440179434628,
    "output_throughput": 148.30357266003062,
    "total_throughput": 310.0879744543769,
    "itl": 20.180174039295395,
    "ttft": 4488.057884745443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 2424,
    "finished_requests": 2421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6378554771654308. Arrivals time: 0.01837133290246129 Scheduler time: 0.26663950737565756 Scheduler overhead time: 0.1299608750268817 Adapter cache time: 0.024089877028018236 Engine time: 0.1325996364466846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 1080, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7485 . Total input tokens: 1645904 . Total output tokens: 1523231
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6343258619308472,
    "estimated_duration": 3598.072456576852,
    "input_throughput": 161.78440179434628,
    "output_throughput": 148.30357266003062,
    "total_throughput": 310.0879744543769,
    "itl": 20.180277333757207,
    "ttft": 4488.076070336704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 2424,
    "finished_requests": 2421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6343807550147176. Arrivals time: 0.018434995785355568 Scheduler time: 0.26479568984359503 Scheduler overhead time: 0.12931918213143945 Adapter cache time: 0.024124604649841785 Engine time: 0.13172920886427164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 1080, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7485 . Total input tokens: 1645904 . Total output tokens: 1523231
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6355729792267084,
    "estimated_duration": 3598.072456576852,
    "input_throughput": 161.78440179434628,
    "output_throughput": 148.30357266003062,
    "total_throughput": 310.0879744543769,
    "itl": 20.180288192908332,
    "ttft": 4488.082861013804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 2424,
    "finished_requests": 2421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6356231714598835. Arrivals time: 0.01828262023627758 Scheduler time: 0.26461618719622493 Scheduler overhead time: 0.13017141819000244 Adapter cache time: 0.024223394226282835 Engine time: 0.1320071336813271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 1080, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7485 . Total input tokens: 1645904 . Total output tokens: 1523231
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6354790539480746,
    "estimated_duration": 3598.072456576852,
    "input_throughput": 161.78440179434628,
    "output_throughput": 148.30357266003062,
    "total_throughput": 310.0879744543769,
    "itl": 20.180248718323465,
    "ttft": 4488.059872182972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 2424,
    "finished_requests": 2421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6355372299440205. Arrivals time: 0.01821053447201848 Scheduler time: 0.26446550618857145 Scheduler overhead time: 0.13041401747614145 Adapter cache time: 0.023828318808227777 Engine time: 0.13246027566492558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 1080, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7485 . Total input tokens: 1645904 . Total output tokens: 1523231
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.6365421209484339,
    "estimated_duration": 3598.072456576852,
    "input_throughput": 161.78440179434628,
    "output_throughput": 148.30357266003062,
    "total_throughput": 310.0879744543769,
    "itl": 20.180284098137573,
    "ttft": 4488.079016016714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 2424,
    "finished_requests": 2421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6366076078265905. Arrivals time: 0.018190080299973488 Scheduler time: 0.26762038422748446 Scheduler overhead time: 0.1296656560152769 Adapter cache time: 0.02432102570310235 Engine time: 0.13053694274276495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 1080, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7485 . Total input tokens: 1645904 . Total output tokens: 1523231
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6439424371346831,
    "estimated_duration": 3598.072456576852,
    "input_throughput": 161.78440179434628,
    "output_throughput": 148.30357266003062,
    "total_throughput": 310.0879744543769,
    "itl": 20.18015819888756,
    "ttft": 4488.06476428369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 2424,
    "finished_requests": 2421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6440009763464332. Arrivals time: 0.01868167705833912 Scheduler time: 0.26823857612907887 Scheduler overhead time: 0.1310693509876728 Adapter cache time: 0.024541806429624557 Engine time: 0.13463820703327656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.1-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 1080, 66, 1080, 66, 135, 135, 66, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7485 . Total input tokens: 1645904 . Total output tokens: 1523231
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6465173908509314,
    "estimated_duration": 3598.072456576852,
    "input_throughput": 161.78440179434628,
    "output_throughput": 148.30357266003062,
    "total_throughput": 310.0879744543769,
    "itl": 20.180280387175415,
    "ttft": 4488.072009577571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 2424,
    "finished_requests": 2421,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.646567968185991. Arrivals time: 0.018205945380032063 Scheduler time: 0.2715794751420617 Scheduler overhead time: 0.1306152045726776 Adapter cache time: 0.024395569693297148 Engine time: 0.13503755955025554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 1080, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7320 . Total input tokens: 1608485 . Total output tokens: 1493032
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.642041411716491,
    "estimated_duration": 3598.068720741151,
    "input_throughput": 155.66461995885086,
    "output_throughput": 150.75003900655668,
    "total_throughput": 306.41465896540757,
    "itl": 20.253337039942778,
    "ttft": 9177.37189205693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 2362,
    "finished_requests": 2356,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6420907317660749. Arrivals time: 0.018238406162708998 Scheduler time: 0.26987498067319393 Scheduler overhead time: 0.13090282445773482 Adapter cache time: 0.02410123823210597 Engine time: 0.13211154146119952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 1080, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7320 . Total input tokens: 1608485 . Total output tokens: 1493032
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6385352648794651,
    "estimated_duration": 3598.063040064438,
    "input_throughput": 155.66486572452305,
    "output_throughput": 150.75027701301363,
    "total_throughput": 306.4151427375367,
    "itl": 20.357764084064176,
    "ttft": 9177.635509943699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 2362,
    "finished_requests": 2356,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6385856317356229. Arrivals time: 0.018226778134703636 Scheduler time: 0.2677355734631419 Scheduler overhead time: 0.12998139625415206 Adapter cache time: 0.02370064379647374 Engine time: 0.1326224533841014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 1080, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7320 . Total input tokens: 1608485 . Total output tokens: 1493032
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6381330182775855,
    "estimated_duration": 3598.063040064438,
    "input_throughput": 155.66486572452305,
    "output_throughput": 150.75027701301363,
    "total_throughput": 306.4151427375367,
    "itl": 20.357778003254943,
    "ttft": 9177.62807328031,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 2362,
    "finished_requests": 2356,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6381898112595081. Arrivals time: 0.018132109194993973 Scheduler time: 0.26750793075188994 Scheduler overhead time: 0.12971431808546185 Adapter cache time: 0.02374029066413641 Engine time: 0.1328030852600932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 1080, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7320 . Total input tokens: 1608485 . Total output tokens: 1493032
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6342666721902788,
    "estimated_duration": 3598.068720741151,
    "input_throughput": 155.66461995885086,
    "output_throughput": 150.75003900655668,
    "total_throughput": 306.41465896540757,
    "itl": 20.25336779693511,
    "ttft": 9177.379405530906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 2362,
    "finished_requests": 2356,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6343186991289258. Arrivals time: 0.01777117419987917 Scheduler time: 0.26459057722240686 Scheduler overhead time: 0.12965945154428482 Adapter cache time: 0.02386691188439727 Engine time: 0.13226431282237172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 1080, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7320 . Total input tokens: 1608485 . Total output tokens: 1493032
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.6371478522196412,
    "estimated_duration": 3598.063040064438,
    "input_throughput": 155.66486572452305,
    "output_throughput": 150.75027701301363,
    "total_throughput": 306.4151427375367,
    "itl": 20.357776599307623,
    "ttft": 9177.61851537504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 2362,
    "finished_requests": 2356,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6371942074038088. Arrivals time: 0.018425604328513145 Scheduler time: 0.2680745078250766 Scheduler overhead time: 0.12986002210527658 Adapter cache time: 0.023649179842323065 Engine time: 0.13171823602169752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 1080, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7320 . Total input tokens: 1608485 . Total output tokens: 1493032
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6346752028912306,
    "estimated_duration": 3598.068720741151,
    "input_throughput": 155.66461995885086,
    "output_throughput": 150.75003900655668,
    "total_throughput": 306.41465896540757,
    "itl": 20.253320924377526,
    "ttft": 9177.380542043195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 2362,
    "finished_requests": 2356,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6347359260544181. Arrivals time: 0.018237635027617216 Scheduler time: 0.2659132997505367 Scheduler overhead time: 0.1303688520565629 Adapter cache time: 0.024036158807575703 Engine time: 0.13003714475780725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 1080, 33, 1080, 33, 135, 135, 33, 135, 1080, 1080, 1080, 1080]
Prompts retrieved: 7320 . Total input tokens: 1608485 . Total output tokens: 1493032
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6332197641022503,
    "estimated_duration": 3598.063040064438,
    "input_throughput": 155.66486572452305,
    "output_throughput": 150.75027701301363,
    "total_throughput": 306.4151427375367,
    "itl": 20.357768626511664,
    "ttft": 9177.625761236568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 2362,
    "finished_requests": 2356,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6332849352620542. Arrivals time: 0.0179776418954134 Scheduler time: 0.26514505641534925 Scheduler overhead time: 0.12993653863668442 Adapter cache time: 0.023367210756987333 Engine time: 0.13078929344192147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 1080, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 1080]
Prompts retrieved: 6975 . Total input tokens: 1525396 . Total output tokens: 1427989
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6200986769981682,
    "estimated_duration": 3599.8159131588486,
    "input_throughput": 154.1653832828954,
    "output_throughput": 140.02716032156084,
    "total_throughput": 294.1925436044562,
    "itl": 20.045536689580395,
    "ttft": 9602.667961782265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 2257,
    "finished_requests": 2251,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6201760140247643. Arrivals time: 0.017725453712046146 Scheduler time: 0.2545334752649069 Scheduler overhead time: 0.12796522211283445 Adapter cache time: 0.023326055146753788 Engine time: 0.13117613876238465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 1080, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 1080]
Prompts retrieved: 6975 . Total input tokens: 1525396 . Total output tokens: 1427989
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6217297553084791,
    "estimated_duration": 3599.8159131588486,
    "input_throughput": 154.1653832828954,
    "output_throughput": 140.02716032156084,
    "total_throughput": 294.1925436044562,
    "itl": 20.04558598280199,
    "ttft": 9602.675746376746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 2257,
    "finished_requests": 2251,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6217840411700308. Arrivals time: 0.01783005055040121 Scheduler time: 0.2545010466128588 Scheduler overhead time: 0.1288916300982237 Adapter cache time: 0.02355670416727662 Engine time: 0.13086205394938588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 1080, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 1080]
Prompts retrieved: 6975 . Total input tokens: 1525396 . Total output tokens: 1427989
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6410297541879117,
    "estimated_duration": 3599.8159131588486,
    "input_throughput": 154.1653832828954,
    "output_throughput": 140.02716032156084,
    "total_throughput": 294.1925436044562,
    "itl": 20.045600143121273,
    "ttft": 9602.682664200918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 2257,
    "finished_requests": 2251,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6410837341099977. Arrivals time: 0.018069959245622158 Scheduler time: 0.2571234768256545 Scheduler overhead time: 0.14070568652823567 Adapter cache time: 0.02455960726365447 Engine time: 0.1339458324946463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 1080, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 1080]
Prompts retrieved: 6975 . Total input tokens: 1525396 . Total output tokens: 1427989
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.6213043043389916,
    "estimated_duration": 3599.8159131588486,
    "input_throughput": 154.1653832828954,
    "output_throughput": 140.02716032156084,
    "total_throughput": 294.1925436044562,
    "itl": 20.045551619547783,
    "ttft": 9602.674522902618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 2257,
    "finished_requests": 2251,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6213601492345333. Arrivals time: 0.017773656640201807 Scheduler time: 0.25424164813011885 Scheduler overhead time: 0.12818339234218 Adapter cache time: 0.02352345222607255 Engine time: 0.1319226836785674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 1080, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 1080]
Prompts retrieved: 6975 . Total input tokens: 1525396 . Total output tokens: 1427989
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.6295577199198306,
    "estimated_duration": 3599.8159131588486,
    "input_throughput": 154.1653832828954,
    "output_throughput": 140.02716032156084,
    "total_throughput": 294.1925436044562,
    "itl": 20.04559497249672,
    "ttft": 9602.679497255202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 2257,
    "finished_requests": 2251,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6296276259236038. Arrivals time: 0.018305940553545952 Scheduler time: 0.25793494237586856 Scheduler overhead time: 0.13067022617906332 Adapter cache time: 0.02404239820316434 Engine time: 0.13243966875597835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 1080, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 1080]
Prompts retrieved: 6975 . Total input tokens: 1525396 . Total output tokens: 1427989
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.6277963081374764,
    "estimated_duration": 3599.8159131588486,
    "input_throughput": 154.1653832828954,
    "output_throughput": 140.02716032156084,
    "total_throughput": 294.1925436044562,
    "itl": 20.045522785616356,
    "ttft": 9602.659653096987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 2257,
    "finished_requests": 2251,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6278626569546759. Arrivals time: 0.017777911387383938 Scheduler time: 0.25850049452856183 Scheduler overhead time: 0.12913892744109035 Adapter cache time: 0.02349087316542864 Engine time: 0.1331903557293117 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.1-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 1080, 33, 1080, 33, 66, 66, 33, 66, 1080, 1080, 1080, 1080]
Prompts retrieved: 6975 . Total input tokens: 1525396 . Total output tokens: 1427989
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.6239163912832737,
    "estimated_duration": 3599.8159131588486,
    "input_throughput": 154.1653832828954,
    "output_throughput": 140.02716032156084,
    "total_throughput": 294.1925436044562,
    "itl": 20.04559089301498,
    "ttft": 9602.667659366065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 2257,
    "finished_requests": 2251,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.6239725523628294. Arrivals time: 0.017875520512461662 Scheduler time: 0.2553584296256304 Scheduler overhead time: 0.12895785132423043 Adapter cache time: 0.023974248208105564 Engine time: 0.13184506678953767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-8/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 540, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 540]
Prompts retrieved: 5265 . Total input tokens: 1167892 . Total output tokens: 1078998
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.5504056899808347,
    "estimated_duration": 3595.082696769514,
    "input_throughput": 109.74100828181695,
    "output_throughput": 103.3119489389626,
    "total_throughput": 213.05295722077955,
    "itl": 19.98438737671717,
    "ttft": 8603.014022363936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 1680,
    "finished_requests": 1676,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.55046212580055. Arrivals time: 0.015867816284298897 Scheduler time: 0.2092047268524766 Scheduler overhead time: 0.12063770089298487 Adapter cache time: 0.021919645834714174 Engine time: 0.12189453328028321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-16/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 540, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 540]
Prompts retrieved: 5265 . Total input tokens: 1167892 . Total output tokens: 1078998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.5573621387593448,
    "estimated_duration": 3595.082696769514,
    "input_throughput": 109.74100828181695,
    "output_throughput": 103.3119489389626,
    "total_throughput": 213.05295722077955,
    "itl": 19.984451816635293,
    "ttft": 8603.023819925125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 1680,
    "finished_requests": 1676,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5574251818470657. Arrivals time: 0.016289979685097933 Scheduler time: 0.21194999804720283 Scheduler overhead time: 0.12179639469832182 Adapter cache time: 0.022324466612190008 Engine time: 0.12328739557415247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-32/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 540, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 540]
Prompts retrieved: 5265 . Total input tokens: 1167892 . Total output tokens: 1078998
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.5506744859740138,
    "estimated_duration": 3595.082696769514,
    "input_throughput": 109.74100828181695,
    "output_throughput": 103.3119489389626,
    "total_throughput": 213.05295722077955,
    "itl": 19.98447045388029,
    "ttft": 8603.036788556965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568942,
    "arrivals": 1680,
    "finished_requests": 1676,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5507346512749791. Arrivals time: 0.0159428040497005 Scheduler time: 0.20907827373594046 Scheduler overhead time: 0.12024649791419506 Adapter cache time: 0.02207340719178319 Engine time: 0.12210029549896717 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-16/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 540, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 540]
Prompts retrieved: 5265 . Total input tokens: 1167892 . Total output tokens: 1078998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.5495608150959015,
    "estimated_duration": 3595.082696769514,
    "input_throughput": 109.74100828181695,
    "output_throughput": 103.3119489389626,
    "total_throughput": 213.05295722077955,
    "itl": 19.984392650391946,
    "ttft": 8603.004708995328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1104720608890057,
    "arrivals": 1680,
    "finished_requests": 1676,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5496353260241449. Arrivals time: 0.0160136790946126 Scheduler time: 0.20945882750675082 Scheduler overhead time: 0.11959604173898697 Adapter cache time: 0.02207786636427045 Engine time: 0.12152572348713875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-32/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 540, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 540]
Prompts retrieved: 5265 . Total input tokens: 1167892 . Total output tokens: 1078998
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.5527595398016274,
    "estimated_duration": 3595.082696769514,
    "input_throughput": 109.74100828181695,
    "output_throughput": 103.3119489389626,
    "total_throughput": 213.05295722077955,
    "itl": 19.984460773027518,
    "ttft": 8603.030007935111,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261591,
    "arrivals": 1680,
    "finished_requests": 1676,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5528118791989982. Arrivals time: 0.015863425098359585 Scheduler time: 0.21020091604441404 Scheduler overhead time: 0.1203580372966826 Adapter cache time: 0.02217880403622985 Engine time: 0.1232523899525404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-16/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 540, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 540]
Prompts retrieved: 5265 . Total input tokens: 1167892 . Total output tokens: 1078998
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.5536407348699868,
    "estimated_duration": 3595.082696769514,
    "input_throughput": 109.74100828181695,
    "output_throughput": 103.3119489389626,
    "total_throughput": 213.05295722077955,
    "itl": 19.984370987796453,
    "ttft": 8603.014759637195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 1680,
    "finished_requests": 1676,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5536893559619784. Arrivals time: 0.015698085073381662 Scheduler time: 0.21161095658317208 Scheduler overhead time: 0.1207904601469636 Adapter cache time: 0.022198896389454603 Engine time: 0.12193228537216783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_16_slots_16_rate_0.05-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [5 5 6]
Adapter prompts. [270, 135, 135, 270, 540, 135, 540, 135, 270, 270, 135, 270, 540, 540, 540, 540]
Prompts retrieved: 5265 . Total input tokens: 1167892 . Total output tokens: 1078998
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.5575316278263927,
    "estimated_duration": 3595.082696769514,
    "input_throughput": 109.74100828181695,
    "output_throughput": 103.3119489389626,
    "total_throughput": 213.05295722077955,
    "itl": 19.984457661812463,
    "ttft": 8603.027788822505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1183948530629277,
    "arrivals": 1680,
    "finished_requests": 1676,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5575809259898961. Arrivals time: 0.016009889543056488 Scheduler time: 0.2115980125963688 Scheduler overhead time: 0.12272909097373486 Adapter cache time: 0.022236004006117582 Engine time: 0.12288877507671714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 540, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 540]
Prompts retrieved: 4920 . Total input tokens: 1095402 . Total output tokens: 1002999
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.546897697262466,
    "estimated_duration": 3598.552675062103,
    "input_throughput": 100.44288152424008,
    "output_throughput": 99.7159225948547,
    "total_throughput": 200.15880411909478,
    "itl": 19.987376987862806,
    "ttft": 2318.556168477602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 1574,
    "finished_requests": 1573,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5469620539806783. Arrivals time: 0.015673416201025248 Scheduler time: 0.20535732246935368 Scheduler overhead time: 0.12027432164177299 Adapter cache time: 0.02204842073842883 Engine time: 0.12250490300357342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 540, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 540]
Prompts retrieved: 4920 . Total input tokens: 1095402 . Total output tokens: 1002999
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.551979960873723,
    "estimated_duration": 3598.552675062103,
    "input_throughput": 100.44288152424008,
    "output_throughput": 99.7159225948547,
    "total_throughput": 200.15880411909478,
    "itl": 19.987375157322283,
    "ttft": 2318.5427190608257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 1574,
    "finished_requests": 1573,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.55204435903579. Arrivals time: 0.015714308246970177 Scheduler time: 0.20813325559720397 Scheduler overhead time: 0.12161627737805247 Adapter cache time: 0.022189747542142868 Engine time: 0.12229199055582285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 540, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 540]
Prompts retrieved: 4920 . Total input tokens: 1095402 . Total output tokens: 1002999
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.555777037050575,
    "estimated_duration": 3598.552675062103,
    "input_throughput": 100.44288152424008,
    "output_throughput": 99.7159225948547,
    "total_throughput": 200.15880411909478,
    "itl": 19.987381945991945,
    "ttft": 2318.557978848392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 1574,
    "finished_requests": 1573,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5558400871232152. Arrivals time: 0.015739806927740574 Scheduler time: 0.21210823440924287 Scheduler overhead time: 0.12174240779131651 Adapter cache time: 0.022018921095877886 Engine time: 0.12223044969141483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 540, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 540]
Prompts retrieved: 4920 . Total input tokens: 1095402 . Total output tokens: 1002999
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.5520884487777948,
    "estimated_duration": 3598.552675062103,
    "input_throughput": 100.44288152424008,
    "output_throughput": 99.7159225948547,
    "total_throughput": 200.15880411909478,
    "itl": 19.987388258884426,
    "ttft": 2318.579754918568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 1574,
    "finished_requests": 1573,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5521463598124683. Arrivals time: 0.01564635429531336 Scheduler time: 0.2067223433405161 Scheduler overhead time: 0.12125975452363491 Adapter cache time: 0.022009686566889286 Engine time: 0.12464180495589972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 540, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 540]
Prompts retrieved: 4920 . Total input tokens: 1095402 . Total output tokens: 1002999
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.5519685903564095,
    "estimated_duration": 3598.552675062103,
    "input_throughput": 100.44288152424008,
    "output_throughput": 99.7159225948547,
    "total_throughput": 200.15880411909478,
    "itl": 19.987380183491762,
    "ttft": 2318.5533733203224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 1574,
    "finished_requests": 1573,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.552024023141712. Arrivals time: 0.01562777068465948 Scheduler time: 0.20756243728101254 Scheduler overhead time: 0.1219920925796032 Adapter cache time: 0.022375145461410284 Engine time: 0.12267276085913181 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 540, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 540]
Prompts retrieved: 4920 . Total input tokens: 1095402 . Total output tokens: 1002999
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.5523315072059631,
    "estimated_duration": 3598.552675062103,
    "input_throughput": 100.44288152424008,
    "output_throughput": 99.7159225948547,
    "total_throughput": 200.15880411909478,
    "itl": 19.987370316388745,
    "ttft": 2318.565420621747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 1574,
    "finished_requests": 1573,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5523805730044842. Arrivals time: 0.015834926161915064 Scheduler time: 0.20641699573025107 Scheduler overhead time: 0.12158704688772559 Adapter cache time: 0.02207812573760748 Engine time: 0.12453103857114911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.05-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [5 5 6]
Adapter prompts. [270, 66, 66, 270, 540, 66, 540, 66, 270, 270, 66, 270, 540, 540, 540, 540]
Prompts retrieved: 4920 . Total input tokens: 1095402 . Total output tokens: 1002999
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.5511171529069543,
    "estimated_duration": 3598.552675062103,
    "input_throughput": 100.44288152424008,
    "output_throughput": 99.7159225948547,
    "total_throughput": 200.15880411909478,
    "itl": 19.987377125771378,
    "ttft": 2318.547320340573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 1574,
    "finished_requests": 1573,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5511647551320493. Arrivals time: 0.01594486739486456 Scheduler time: 0.20679082116112113 Scheduler overhead time: 0.1223600641824305 Adapter cache time: 0.0223124329932034 Engine time: 0.12192563619464636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 540, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 540]
Prompts retrieved: 4755 . Total input tokens: 1055661 . Total output tokens: 965252
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.5341186532750726,
    "estimated_duration": 3599.5878690181185,
    "input_throughput": 105.42456909199286,
    "output_throughput": 91.04264485961642,
    "total_throughput": 196.46721395160927,
    "itl": 19.766014396306833,
    "ttft": 4765.162999131316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 1521,
    "finished_requests": 1519,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5341844763606787. Arrivals time: 0.015075483359396458 Scheduler time: 0.19639522675424814 Scheduler overhead time: 0.11848470568656921 Adapter cache time: 0.02134679676964879 Engine time: 0.12272948399186134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 540, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 540]
Prompts retrieved: 4755 . Total input tokens: 1055661 . Total output tokens: 965252
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.530346245970577,
    "estimated_duration": 3599.5878690181185,
    "input_throughput": 105.42456909199286,
    "output_throughput": 91.04264485961642,
    "total_throughput": 196.46721395160927,
    "itl": 19.76614719901232,
    "ttft": 4765.144656432503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 1521,
    "finished_requests": 1519,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5304184798151255. Arrivals time: 0.01521656708791852 Scheduler time: 0.19522309303283691 Scheduler overhead time: 0.11830133153125644 Adapter cache time: 0.021520149428397417 Engine time: 0.11988837597891688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 540, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 540]
Prompts retrieved: 4755 . Total input tokens: 1055661 . Total output tokens: 965252
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.5293198726139963,
    "estimated_duration": 3599.5878690181185,
    "input_throughput": 105.42456909199286,
    "output_throughput": 91.04264485961642,
    "total_throughput": 196.46721395160927,
    "itl": 19.766155280948063,
    "ttft": 4765.151380884869,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 1521,
    "finished_requests": 1519,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.529375143814832. Arrivals time: 0.015178598463535309 Scheduler time: 0.19507923675701022 Scheduler overhead time: 0.11737243039533496 Adapter cache time: 0.021358924452215433 Engine time: 0.12033258704468608 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 540, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 540]
Prompts retrieved: 4755 . Total input tokens: 1055661 . Total output tokens: 965252
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.52906195493415,
    "estimated_duration": 3599.5878690181185,
    "input_throughput": 105.42456909199286,
    "output_throughput": 91.04264485961642,
    "total_throughput": 196.46721395160927,
    "itl": 19.766124059090796,
    "ttft": 4765.130903934464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 1521,
    "finished_requests": 1519,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5291148270480335. Arrivals time: 0.015150896273553371 Scheduler time: 0.1948077087290585 Scheduler overhead time: 0.11816999269649386 Adapter cache time: 0.021333999466151 Engine time: 0.11955673946067691 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 540, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 540]
Prompts retrieved: 4755 . Total input tokens: 1055661 . Total output tokens: 965252
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.5361005249433219,
    "estimated_duration": 3599.5878690181185,
    "input_throughput": 105.42456909199286,
    "output_throughput": 91.04264485961642,
    "total_throughput": 196.46721395160927,
    "itl": 19.766151871995074,
    "ttft": 4765.145253157751,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 1521,
    "finished_requests": 1519,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5361493099480867. Arrivals time: 0.014989069662988186 Scheduler time: 0.1950225131586194 Scheduler overhead time: 0.11839407263323665 Adapter cache time: 0.02566311275586486 Engine time: 0.12106671137735248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 540, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 540]
Prompts retrieved: 4755 . Total input tokens: 1055661 . Total output tokens: 965252
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.5311270141974092,
    "estimated_duration": 3599.5878690181185,
    "input_throughput": 105.42456909199286,
    "output_throughput": 91.04264485961642,
    "total_throughput": 196.46721395160927,
    "itl": 19.766002211291195,
    "ttft": 4765.142118234076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 1521,
    "finished_requests": 1519,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5311739300377667. Arrivals time: 0.015199482906609774 Scheduler time: 0.19556710869073868 Scheduler overhead time: 0.11873536324128509 Adapter cache time: 0.021596776321530342 Engine time: 0.11944056116044521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.05-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [5 5 6]
Adapter prompts. [270, 33, 33, 270, 540, 33, 540, 33, 270, 270, 33, 270, 540, 540, 540, 540]
Prompts retrieved: 4755 . Total input tokens: 1055661 . Total output tokens: 965252
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.5344028319232166,
    "estimated_duration": 3599.5878690181185,
    "input_throughput": 105.42456909199286,
    "output_throughput": 91.04264485961642,
    "total_throughput": 196.46721395160927,
    "itl": 19.766149952108897,
    "ttft": 4765.15111678854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 1521,
    "finished_requests": 1519,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5344606079161167. Arrivals time: 0.0150973298586905 Scheduler time: 0.19746532384306192 Scheduler overhead time: 0.12001708010211587 Adapter cache time: 0.021291165612637997 Engine time: 0.12041682656854391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 540, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 540]
Prompts retrieved: 4245 . Total input tokens: 945875 . Total output tokens: 857143
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.5100566130131483,
    "estimated_duration": 3596.100010038733,
    "input_throughput": 87.70334504590237,
    "output_throughput": 84.53185371691757,
    "total_throughput": 172.23519876281995,
    "itl": 19.93432198881652,
    "ttft": 7925.676762559135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 1368,
    "finished_requests": 1365,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5101192849688232. Arrivals time: 0.01489659771323204 Scheduler time: 0.18471928080543876 Scheduler overhead time: 0.11430756654590368 Adapter cache time: 0.020581409335136414 Engine time: 0.1174252713099122 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 540, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 540]
Prompts retrieved: 4245 . Total input tokens: 945875 . Total output tokens: 857143
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.5087544401176274,
    "estimated_duration": 3596.100010038733,
    "input_throughput": 87.70334504590237,
    "output_throughput": 84.53185371691757,
    "total_throughput": 172.23519876281995,
    "itl": 19.93439163642251,
    "ttft": 7925.654339687701,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 1368,
    "finished_requests": 1365,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5088121010921896. Arrivals time: 0.01461716927587986 Scheduler time: 0.18594175344333053 Scheduler overhead time: 0.1139354701153934 Adapter cache time: 0.020455494988709688 Engine time: 0.1160065671429038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 540, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 540]
Prompts retrieved: 4245 . Total input tokens: 945875 . Total output tokens: 857143
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.5075248838402331,
    "estimated_duration": 3596.100010038733,
    "input_throughput": 87.70334504590237,
    "output_throughput": 84.53185371691757,
    "total_throughput": 172.23519876281995,
    "itl": 19.934413199310484,
    "ttft": 7925.658481730806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 1368,
    "finished_requests": 1365,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5075736050494015. Arrivals time: 0.014746739994734526 Scheduler time: 0.18409481411799788 Scheduler overhead time: 0.1149615291506052 Adapter cache time: 0.020583966746926308 Engine time: 0.11539728054776788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 540, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 540]
Prompts retrieved: 4245 . Total input tokens: 945875 . Total output tokens: 857143
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.5090734250843525,
    "estimated_duration": 3596.100010038733,
    "input_throughput": 87.70334504590237,
    "output_throughput": 84.53185371691757,
    "total_throughput": 172.23519876281995,
    "itl": 19.934358288283782,
    "ttft": 7925.663297425399,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 1368,
    "finished_requests": 1365,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5091215800493956. Arrivals time: 0.014651465695351362 Scheduler time: 0.1842103898525238 Scheduler overhead time: 0.11427124589681625 Adapter cache time: 0.02074452582746744 Engine time: 0.1170734791085124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 540, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 540]
Prompts retrieved: 4245 . Total input tokens: 945875 . Total output tokens: 857143
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.5136495679616928,
    "estimated_duration": 3596.100010038733,
    "input_throughput": 87.70334504590237,
    "output_throughput": 84.53185371691757,
    "total_throughput": 172.23519876281995,
    "itl": 19.934408628245748,
    "ttft": 7925.6527284758995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 1368,
    "finished_requests": 1365,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5136968409642577. Arrivals time: 0.01440554903820157 Scheduler time: 0.18473638221621513 Scheduler overhead time: 0.11459329584613442 Adapter cache time: 0.02133649494498968 Engine time: 0.1172947259619832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 540, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 540]
Prompts retrieved: 4245 . Total input tokens: 945875 . Total output tokens: 857143
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.5134500092826784,
    "estimated_duration": 3596.100010038733,
    "input_throughput": 87.70334504590237,
    "output_throughput": 84.53185371691757,
    "total_throughput": 172.23519876281995,
    "itl": 19.93429981621481,
    "ttft": 7925.719819423979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 1368,
    "finished_requests": 1365,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5135208880528808. Arrivals time: 0.014416101388633251 Scheduler time: 0.18831196194514632 Scheduler overhead time: 0.11613690527155995 Adapter cache time: 0.020357401110231876 Engine time: 0.11550066992640495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.05-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 540, 66, 540, 66, 135, 135, 66, 135, 540, 540, 540, 540]
Prompts retrieved: 4245 . Total input tokens: 945875 . Total output tokens: 857143
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.5088862148113549,
    "estimated_duration": 3596.100010038733,
    "input_throughput": 87.70334504590237,
    "output_throughput": 84.53185371691757,
    "total_throughput": 172.23519876281995,
    "itl": 19.934397557276736,
    "ttft": 7925.6603293830185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 1368,
    "finished_requests": 1365,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.5089371050707996. Arrivals time: 0.014254878275096416 Scheduler time: 0.18733423529192805 Scheduler overhead time: 0.11397634912282228 Adapter cache time: 0.02034351835027337 Engine time: 0.11546007683500648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 540, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 540]
Prompts retrieved: 4080 . Total input tokens: 909993 . Total output tokens: 826445
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.4894259958527982,
    "estimated_duration": 3598.2809698954775,
    "input_throughput": 83.7744474436515,
    "output_throughput": 79.46211048891648,
    "total_throughput": 163.236557932568,
    "itl": 19.93083108886918,
    "ttft": 5444.228018455756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 1330,
    "finished_requests": 1328,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.48947687167674303. Arrivals time: 0.014250837732106447 Scheduler time: 0.17694829776883125 Scheduler overhead time: 0.11032517207786441 Adapter cache time: 0.01973864808678627 Engine time: 0.11219848738983274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 540, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 540]
Prompts retrieved: 4080 . Total input tokens: 909993 . Total output tokens: 826445
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.49175399262458086,
    "estimated_duration": 3598.2809698954775,
    "input_throughput": 83.7744474436515,
    "output_throughput": 79.46211048891648,
    "total_throughput": 163.236557932568,
    "itl": 19.930882296438504,
    "ttft": 5444.256250795911,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 1330,
    "finished_requests": 1328,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.49182072887197137. Arrivals time: 0.013799601700156927 Scheduler time: 0.17804862698540092 Scheduler overhead time: 0.11152333999052644 Adapter cache time: 0.019715975038707256 Engine time: 0.11239958787336946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 540, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 540]
Prompts retrieved: 4080 . Total input tokens: 909993 . Total output tokens: 826445
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.4924517306499183,
    "estimated_duration": 3598.2809698954775,
    "input_throughput": 83.7744474436515,
    "output_throughput": 79.46211048891648,
    "total_throughput": 163.236557932568,
    "itl": 19.93089971134073,
    "ttft": 5444.257132200539,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 1330,
    "finished_requests": 1328,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.49251065077260137. Arrivals time: 0.01408015750348568 Scheduler time: 0.1798375165089965 Scheduler overhead time: 0.11044322000816464 Adapter cache time: 0.019718031864613295 Engine time: 0.11174892587587237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 540, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 540]
Prompts retrieved: 4080 . Total input tokens: 909993 . Total output tokens: 826445
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.48827893240377307,
    "estimated_duration": 3598.2809698954775,
    "input_throughput": 83.7744474436515,
    "output_throughput": 79.46211048891648,
    "total_throughput": 163.236557932568,
    "itl": 19.930817888209198,
    "ttft": 5444.24791242121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 1330,
    "finished_requests": 1328,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4883341742679477. Arrivals time: 0.01413592416793108 Scheduler time: 0.17717302963137627 Scheduler overhead time: 0.10976253775879741 Adapter cache time: 0.019743459299206734 Engine time: 0.11149701057001948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 540, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 540]
Prompts retrieved: 4080 . Total input tokens: 909993 . Total output tokens: 826445
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.4955302090384066,
    "estimated_duration": 3598.2809698954775,
    "input_throughput": 83.7744474436515,
    "output_throughput": 79.46211048891648,
    "total_throughput": 163.236557932568,
    "itl": 19.930889768415984,
    "ttft": 5444.251058839721,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 1330,
    "finished_requests": 1328,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4955822662450373. Arrivals time: 0.014013698790222406 Scheduler time: 0.17915453854948282 Scheduler overhead time: 0.11372459586709738 Adapter cache time: 0.02027182513847947 Engine time: 0.11147509841248393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 540, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 540]
Prompts retrieved: 4080 . Total input tokens: 909993 . Total output tokens: 826445
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.4956213920377195,
    "estimated_duration": 3598.2809698954775,
    "input_throughput": 83.7744474436515,
    "output_throughput": 79.46211048891648,
    "total_throughput": 163.236557932568,
    "itl": 19.930810326903682,
    "ttft": 5444.241438517528,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 1330,
    "finished_requests": 1328,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4956733901053667. Arrivals time: 0.013947880361229181 Scheduler time: 0.17904781736433506 Scheduler overhead time: 0.11277090152725577 Adapter cache time: 0.019888821989297867 Engine time: 0.11359278485178947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.05-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 540, 33, 540, 33, 135, 135, 33, 135, 540, 540, 540, 540]
Prompts retrieved: 4080 . Total input tokens: 909993 . Total output tokens: 826445
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.4880723599344492,
    "estimated_duration": 3598.2809698954775,
    "input_throughput": 83.7744474436515,
    "output_throughput": 79.46211048891648,
    "total_throughput": 163.236557932568,
    "itl": 19.93088864029858,
    "ttft": 5444.2452969333035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 1330,
    "finished_requests": 1328,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4881235589273274. Arrivals time: 0.01409945311024785 Scheduler time: 0.17605480691418052 Scheduler overhead time: 0.10998611524701118 Adapter cache time: 0.0196562553755939 Engine time: 0.11251712124794722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 540, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 540]
Prompts retrieved: 3735 . Total input tokens: 829444 . Total output tokens: 757711
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.46829220512881875,
    "estimated_duration": 3599.4049987786125,
    "input_throughput": 80.08490294863032,
    "output_throughput": 72.91760724038019,
    "total_throughput": 153.0025101890105,
    "itl": 19.9064382752508,
    "ttft": 2961.993786823204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 1228,
    "finished_requests": 1227,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.46834258222952485. Arrivals time: 0.013278247322887182 Scheduler time: 0.16731141693890095 Scheduler overhead time: 0.10750097362324595 Adapter cache time: 0.018789025023579597 Engine time: 0.1071262382902205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 540, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 540]
Prompts retrieved: 3735 . Total input tokens: 829444 . Total output tokens: 757711
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.4773839679546654,
    "estimated_duration": 3599.4049987786125,
    "input_throughput": 80.08490294863032,
    "output_throughput": 72.91760724038019,
    "total_throughput": 153.0025101890105,
    "itl": 19.906497806741143,
    "ttft": 2962.0336301926245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556994,
    "arrivals": 1228,
    "finished_requests": 1227,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4774362719617784. Arrivals time: 0.013834876008331776 Scheduler time: 0.16625146567821503 Scheduler overhead time: 0.10880224779248238 Adapter cache time: 0.019369439221918583 Engine time: 0.11060736561194062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 540, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 540]
Prompts retrieved: 3735 . Total input tokens: 829444 . Total output tokens: 757711
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.4655372849665582,
    "estimated_duration": 3599.4049987786125,
    "input_throughput": 80.08490294863032,
    "output_throughput": 72.91760724038019,
    "total_throughput": 153.0025101890105,
    "itl": 19.906513158349224,
    "ttft": 2962.0418877873285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 1228,
    "finished_requests": 1227,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.46559551591053605. Arrivals time: 0.013520736247301102 Scheduler time: 0.16644397005438805 Scheduler overhead time: 0.1058847620151937 Adapter cache time: 0.01903369976207614 Engine time: 0.10674523329362273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 540, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 540]
Prompts retrieved: 3735 . Total input tokens: 829444 . Total output tokens: 757711
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.46733837807551026,
    "estimated_duration": 3599.4049987786125,
    "input_throughput": 80.08490294863032,
    "output_throughput": 72.91760724038019,
    "total_throughput": 153.0025101890105,
    "itl": 19.906463109495725,
    "ttft": 2961.9974612186343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 1228,
    "finished_requests": 1227,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.4674048381857574. Arrivals time: 0.013074703048914671 Scheduler time: 0.16718811029568315 Scheduler overhead time: 0.10718161519616842 Adapter cache time: 0.018790301866829395 Engine time: 0.10699084307998419 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 540, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 540]
Prompts retrieved: 3735 . Total input tokens: 829444 . Total output tokens: 757711
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.4620840670540929,
    "estimated_duration": 3599.4049987786125,
    "input_throughput": 80.08490294863032,
    "output_throughput": 72.91760724038019,
    "total_throughput": 153.0025101890105,
    "itl": 19.9065081825322,
    "ttft": 2962.0312620689842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261594,
    "arrivals": 1228,
    "finished_requests": 1227,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.46214022068306804. Arrivals time: 0.012955704238265753 Scheduler time: 0.16407370613887906 Scheduler overhead time: 0.10537859657779336 Adapter cache time: 0.019047630950808525 Engine time: 0.1067440272308886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 540, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 540]
Prompts retrieved: 3735 . Total input tokens: 829444 . Total output tokens: 757711
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.47260444005951285,
    "estimated_duration": 3599.4049987786125,
    "input_throughput": 80.08490294863032,
    "output_throughput": 72.91760724038019,
    "total_throughput": 153.0025101890105,
    "itl": 19.906424956150914,
    "ttft": 2961.9831747820103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 1228,
    "finished_requests": 1227,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.47267661476507783. Arrivals time: 0.013304508291184902 Scheduler time: 0.1712682177312672 Scheduler overhead time: 0.10820058640092611 Adapter cache time: 0.018492431845515966 Engine time: 0.10741482535377145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.05-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 540, 33, 540, 33, 66, 66, 33, 66, 540, 540, 540, 540]
Prompts retrieved: 3735 . Total input tokens: 829444 . Total output tokens: 757711
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.46718839602544904,
    "estimated_duration": 3599.4049987786125,
    "input_throughput": 80.08490294863032,
    "output_throughput": 72.91760724038019,
    "total_throughput": 153.0025101890105,
    "itl": 19.906502418877146,
    "ttft": 2962.0414175278315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292773,
    "arrivals": 1228,
    "finished_requests": 1227,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.46723394002765417. Arrivals time: 0.013275384437292814 Scheduler time: 0.16817323816940188 Scheduler overhead time: 0.10714774485677481 Adapter cache time: 0.018685367424041033 Engine time: 0.10595976281911135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 270, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 270]
Prompts retrieved: 2625 . Total input tokens: 589389 . Total output tokens: 535905
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.39695791294798255,
    "estimated_duration": 3597.8722247042188,
    "input_throughput": 62.30071164309549,
    "output_throughput": 54.60699761680718,
    "total_throughput": 116.90770925990266,
    "itl": 19.810697572832268,
    "ttft": 8192.575809313264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 882,
    "finished_requests": 880,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.39700574800372124. Arrivals time: 0.011328151915222406 Scheduler time: 0.13562448043376207 Scheduler overhead time: 0.09304782003164291 Adapter cache time: 0.01637520221993327 Engine time: 0.09339480940252542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 270, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 270]
Prompts retrieved: 2625 . Total input tokens: 589389 . Total output tokens: 535905
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.4028705698437989,
    "estimated_duration": 3597.8722247042188,
    "input_throughput": 62.30071164309549,
    "output_throughput": 54.60699761680718,
    "total_throughput": 116.90770925990266,
    "itl": 19.810735030492996,
    "ttft": 8192.614448807682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 882,
    "finished_requests": 880,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.40292078582569957. Arrivals time: 0.011329781729727983 Scheduler time: 0.1371970921754837 Scheduler overhead time: 0.09562086313962936 Adapter cache time: 0.01643831515684724 Engine time: 0.09488484030589461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 270, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 270]
Prompts retrieved: 2625 . Total input tokens: 589389 . Total output tokens: 535905
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.39681917568668723,
    "estimated_duration": 3597.8722247042188,
    "input_throughput": 62.30071164309549,
    "output_throughput": 54.60699761680718,
    "total_throughput": 116.90770925990266,
    "itl": 19.81074518217819,
    "ttft": 8192.62626883007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 882,
    "finished_requests": 880,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.3968657087534666. Arrivals time: 0.011545246932655573 Scheduler time: 0.13537439657375216 Scheduler overhead time: 0.09315742691978812 Adapter cache time: 0.016326118260622025 Engine time: 0.09343943605199456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 270, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 270]
Prompts retrieved: 2625 . Total input tokens: 589389 . Total output tokens: 535905
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.3985909470357001,
    "estimated_duration": 3597.8722247042188,
    "input_throughput": 62.30071164309549,
    "output_throughput": 54.60699761680718,
    "total_throughput": 116.90770925990266,
    "itl": 19.810716949495824,
    "ttft": 8192.60185723423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900571,
    "arrivals": 882,
    "finished_requests": 880,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.39863664703443646. Arrivals time: 0.011199582368135452 Scheduler time: 0.13619497139006853 Scheduler overhead time: 0.0930427759885788 Adapter cache time: 0.016513498965650797 Engine time: 0.09448721678927541 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 270, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 270]
Prompts retrieved: 2625 . Total input tokens: 589389 . Total output tokens: 535905
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.3998906332999468,
    "estimated_duration": 3597.8722247042188,
    "input_throughput": 62.30071164309549,
    "output_throughput": 54.60699761680718,
    "total_throughput": 116.90770925990266,
    "itl": 19.810742484563644,
    "ttft": 8192.624390216222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 882,
    "finished_requests": 880,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.39994935411959887. Arrivals time: 0.011180944740772247 Scheduler time: 0.13694817665964365 Scheduler overhead time: 0.0951129449531436 Adapter cache time: 0.016631754115223885 Engine time: 0.09298442956060171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 270, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 270]
Prompts retrieved: 2625 . Total input tokens: 589389 . Total output tokens: 535905
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.39736465085297823,
    "estimated_duration": 3597.8722247042188,
    "input_throughput": 62.30071164309549,
    "output_throughput": 54.60699761680718,
    "total_throughput": 116.90770925990266,
    "itl": 19.810684734833192,
    "ttft": 8192.582356513396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 882,
    "finished_requests": 880,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.39745487412437797. Arrivals time: 0.011377427261322737 Scheduler time: 0.13486037123948336 Scheduler overhead time: 0.09402113454416394 Adapter cache time: 0.0162354432977736 Engine time: 0.09396439278498292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_16_slots_16_rate_0.025-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [5 5 6]
Adapter prompts. [135, 66, 66, 135, 270, 66, 270, 66, 135, 135, 66, 135, 270, 270, 270, 270]
Prompts retrieved: 2625 . Total input tokens: 589389 . Total output tokens: 535905
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.39667207188904285,
    "estimated_duration": 3597.8722247042188,
    "input_throughput": 62.30071164309549,
    "output_throughput": 54.60699761680718,
    "total_throughput": 116.90770925990266,
    "itl": 19.810738019049978,
    "ttft": 8192.617815067759,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 882,
    "finished_requests": 880,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.39671885687857866. Arrivals time: 0.011093918699771166 Scheduler time: 0.13576122745871544 Scheduler overhead time: 0.09238394908607006 Adapter cache time: 0.016351045574992895 Engine time: 0.09396350430324674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 270, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 270]
Prompts retrieved: 2460 . Total input tokens: 550514 . Total output tokens: 504464
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.378343746997416,
    "estimated_duration": 3599.083694432966,
    "input_throughput": 55.50134894305957,
    "output_throughput": 46.23287873449012,
    "total_throughput": 101.73422767754968,
    "itl": 19.57286625894734,
    "ttft": 8884.573717252671,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 813,
    "finished_requests": 811,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.37839048635214567. Arrivals time: 0.010717099532485008 Scheduler time: 0.126088195014745 Scheduler overhead time: 0.0896438742056489 Adapter cache time: 0.015678569208830595 Engine time: 0.09067209344357252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 270, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 270]
Prompts retrieved: 2460 . Total input tokens: 550514 . Total output tokens: 504464
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.3725693551823497,
    "estimated_duration": 3599.083694432966,
    "input_throughput": 55.50134894305957,
    "output_throughput": 46.23287873449012,
    "total_throughput": 101.73422767754968,
    "itl": 19.57292226453568,
    "ttft": 8884.571506484363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556991,
    "arrivals": 813,
    "finished_requests": 811,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.3726163823157549. Arrivals time: 0.010620645713061094 Scheduler time: 0.12221467355266213 Scheduler overhead time: 0.08910458255559206 Adapter cache time: 0.01566695375367999 Engine time: 0.08997304923832417 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 270, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 270]
Prompts retrieved: 2460 . Total input tokens: 550514 . Total output tokens: 504464
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.37886514188721776,
    "estimated_duration": 3599.083694432966,
    "input_throughput": 55.50134894305957,
    "output_throughput": 46.23287873449012,
    "total_throughput": 101.73422767754968,
    "itl": 19.572938371789526,
    "ttft": 8884.584841796519,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 813,
    "finished_requests": 811,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.37891394598409534. Arrivals time: 0.010719127487391233 Scheduler time: 0.12582880863919854 Scheduler overhead time: 0.09068498108536005 Adapter cache time: 0.015758864115923643 Engine time: 0.09000077983364463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 270, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 270]
Prompts retrieved: 2460 . Total input tokens: 550514 . Total output tokens: 504464
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.37515816697850823,
    "estimated_duration": 3599.083694432966,
    "input_throughput": 55.50134894305957,
    "output_throughput": 46.23287873449012,
    "total_throughput": 101.73422767754968,
    "itl": 19.572879093054308,
    "ttft": 8884.577281199685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 813,
    "finished_requests": 811,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.3752073459327221. Arrivals time: 0.011008462868630886 Scheduler time: 0.12304918514564633 Scheduler overhead time: 0.08893439406529069 Adapter cache time: 0.015669668558984995 Engine time: 0.09067344013601542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 270, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 270]
Prompts retrieved: 2460 . Total input tokens: 550514 . Total output tokens: 504464
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.3776627220213413,
    "estimated_duration": 3599.083694432966,
    "input_throughput": 55.50134894305957,
    "output_throughput": 46.23287873449012,
    "total_throughput": 101.73422767754968,
    "itl": 19.572931505163748,
    "ttft": 8884.577963366299,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 813,
    "finished_requests": 811,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.37771125929430127. Arrivals time: 0.010801556520164013 Scheduler time: 0.12336707161739469 Scheduler overhead time: 0.09022078989073634 Adapter cache time: 0.015772963874042034 Engine time: 0.09183115279302001 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 270, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 270]
Prompts retrieved: 2460 . Total input tokens: 550514 . Total output tokens: 504464
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.37486252980306745,
    "estimated_duration": 3599.083694432966,
    "input_throughput": 55.50134894305957,
    "output_throughput": 46.23287873449012,
    "total_throughput": 101.73422767754968,
    "itl": 19.57285034597707,
    "ttft": 8884.561913439831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 813,
    "finished_requests": 811,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.3749110708013177. Arrivals time: 0.010666388552635908 Scheduler time: 0.12390936026349664 Scheduler overhead time: 0.08883022423833609 Adapter cache time: 0.015814941842108965 Engine time: 0.09030471462756395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [5 5 6]
Adapter prompts. [135, 33, 33, 135, 270, 33, 270, 33, 135, 135, 33, 135, 270, 270, 270, 270]
Prompts retrieved: 2460 . Total input tokens: 550514 . Total output tokens: 504464
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.3734303661622107,
    "estimated_duration": 3599.083694432966,
    "input_throughput": 55.50134894305957,
    "output_throughput": 46.23287873449012,
    "total_throughput": 101.73422767754968,
    "itl": 19.572927216211497,
    "ttft": 8884.575670556225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 813,
    "finished_requests": 811,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.3734785090200603. Arrivals time: 0.010796705726534128 Scheduler time: 0.12327797571197152 Scheduler overhead time: 0.08905635215342045 Adapter cache time: 0.015634501352906227 Engine time: 0.08962379954755306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 270, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 270]
Prompts retrieved: 2115 . Total input tokens: 467274 . Total output tokens: 435974
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.33763451129198074,
    "estimated_duration": 3598.405708152638,
    "input_throughput": 48.73741712966425,
    "output_throughput": 41.65289079561514,
    "total_throughput": 90.39030792527939,
    "itl": 19.694104071648855,
    "ttft": 10154.771915290004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 711,
    "finished_requests": 709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.33769247215241194. Arrivals time: 0.009835652075707912 Scheduler time: 0.11161962524056435 Scheduler overhead time: 0.0801627691835165 Adapter cache time: 0.014016002416610718 Engine time: 0.0815244042314589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 270, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 270]
Prompts retrieved: 2115 . Total input tokens: 467274 . Total output tokens: 435974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.33932070527225733,
    "estimated_duration": 3598.405708152638,
    "input_throughput": 48.73741712966425,
    "output_throughput": 41.65289079561514,
    "total_throughput": 90.39030792527939,
    "itl": 19.69416804208666,
    "ttft": 10154.811135355321,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 711,
    "finished_requests": 709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.3393724840134382. Arrivals time: 0.009561190847307444 Scheduler time: 0.11249443190172315 Scheduler overhead time: 0.08120271749794483 Adapter cache time: 0.014001296367496252 Engine time: 0.08124786475673318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 270, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 270]
Prompts retrieved: 2115 . Total input tokens: 467274 . Total output tokens: 435974
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.3354660184122622,
    "estimated_duration": 3598.405708152638,
    "input_throughput": 48.73741712966425,
    "output_throughput": 41.65289079561514,
    "total_throughput": 90.39030792527939,
    "itl": 19.694185749139383,
    "ttft": 10154.82232570693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 711,
    "finished_requests": 709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.3355128630064428. Arrivals time: 0.009897876996546984 Scheduler time: 0.11041076853871346 Scheduler overhead time: 0.07975786458700895 Adapter cache time: 0.01389545388519764 Engine time: 0.08107574982568622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 270, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 270]
Prompts retrieved: 2115 . Total input tokens: 467274 . Total output tokens: 435974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.3392629576846957,
    "estimated_duration": 3598.405708152638,
    "input_throughput": 48.73741712966425,
    "output_throughput": 41.65289079561514,
    "total_throughput": 90.39030792527939,
    "itl": 19.69411988352448,
    "ttft": 10154.777943011377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900569,
    "arrivals": 711,
    "finished_requests": 709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.3393104518763721. Arrivals time: 0.009641797747462988 Scheduler time: 0.11108978698030114 Scheduler overhead time: 0.08054781891405582 Adapter cache time: 0.014085876289755106 Engine time: 0.08302923385053873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 270, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 270]
Prompts retrieved: 2115 . Total input tokens: 467274 . Total output tokens: 435974
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.33795920806005597,
    "estimated_duration": 3598.405708152638,
    "input_throughput": 48.73741712966425,
    "output_throughput": 41.65289079561514,
    "total_throughput": 90.39030792527939,
    "itl": 19.694178564068977,
    "ttft": 10154.81737353815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 711,
    "finished_requests": 709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.338012145832181. Arrivals time: 0.009937995579093695 Scheduler time: 0.11119344737380743 Scheduler overhead time: 0.08030914096161723 Adapter cache time: 0.014363182708621025 Engine time: 0.08145437994971871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 270, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 270]
Prompts retrieved: 2115 . Total input tokens: 467274 . Total output tokens: 435974
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.33776165498420596,
    "estimated_duration": 3598.405708152638,
    "input_throughput": 48.73741712966425,
    "output_throughput": 41.65289079561514,
    "total_throughput": 90.39030792527939,
    "itl": 19.694085506065235,
    "ttft": 10154.758418103173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 711,
    "finished_requests": 709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.3378102802671492. Arrivals time: 0.00987935345619917 Scheduler time: 0.11066748620942235 Scheduler overhead time: 0.08074613008648157 Adapter cache time: 0.014107827562838793 Engine time: 0.08159663993865252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 270, 33, 270, 33, 66, 66, 33, 66, 270, 270, 270, 270]
Prompts retrieved: 2115 . Total input tokens: 467274 . Total output tokens: 435974
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.338250950910151,
    "estimated_duration": 3598.405708152638,
    "input_throughput": 48.73741712966425,
    "output_throughput": 41.65289079561514,
    "total_throughput": 90.39030792527939,
    "itl": 19.6941734350856,
    "ttft": 10154.81446049769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 711,
    "finished_requests": 709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.3383003771305084. Arrivals time: 0.00963526638224721 Scheduler time: 0.11295278370380402 Scheduler overhead time: 0.08022543136030436 Adapter cache time: 0.01394514087587595 Engine time: 0.08088774513453245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 135, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 135]
Prompts retrieved: 1305 . Total input tokens: 290228 . Total output tokens: 265434
Prompts distributed
Adapter sizes. Values: [8]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.22721250308677554,
    "estimated_duration": 3597.865136518115,
    "input_throughput": 32.264967027736596,
    "output_throughput": 24.07121910184027,
    "total_throughput": 56.336186129576866,
    "itl": 19.505530397410343,
    "ttft": 8189.353788448702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10579854451119902,
    "arrivals": 441,
    "finished_requests": 440,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.22725612483918667. Arrivals time: 0.006493246648460627 Scheduler time: 0.07169026508927345 Scheduler overhead time: 0.05547569179907441 Adapter cache time: 0.009621775709092617 Engine time: 0.055903452914208174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 135, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 135]
Prompts retrieved: 1305 . Total input tokens: 290228 . Total output tokens: 265434
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.22982969228178263,
    "estimated_duration": 3597.865136518115,
    "input_throughput": 32.264967027736596,
    "output_throughput": 24.07121910184027,
    "total_throughput": 56.336186129576866,
    "itl": 19.50557973462604,
    "ttft": 8189.411682815657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11741316575556993,
    "arrivals": 441,
    "finished_requests": 440,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.22987472033128142. Arrivals time: 0.006651633884757757 Scheduler time: 0.07180793583393097 Scheduler overhead time: 0.05605330318212509 Adapter cache time: 0.009695430751889944 Engine time: 0.056965102441608906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 135, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 135]
Prompts retrieved: 1305 . Total input tokens: 290228 . Total output tokens: 265434
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.23035107320174575,
    "estimated_duration": 3597.865136518115,
    "input_throughput": 32.264967027736596,
    "output_throughput": 24.07121910184027,
    "total_throughput": 56.336186129576866,
    "itl": 19.505593318683477,
    "ttft": 8189.427345295856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12067314200568943,
    "arrivals": 441,
    "finished_requests": 440,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.23039552615955472. Arrivals time: 0.006673489231616259 Scheduler time: 0.07218384929001331 Scheduler overhead time: 0.05637657130137086 Adapter cache time: 0.009834758937358856 Engine time: 0.0565298218280077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 135, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 135]
Prompts retrieved: 1305 . Total input tokens: 290228 . Total output tokens: 265434
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 6 10]
---Simulation End---
#Simulation results
{
    "duration": 0.22776673128828406,
    "estimated_duration": 3597.865136518115,
    "input_throughput": 32.264967027736596,
    "output_throughput": 24.07121910184027,
    "total_throughput": 56.336186129576866,
    "itl": 19.505528533880184,
    "ttft": 8189.380203882022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11047206088900567,
    "arrivals": 441,
    "finished_requests": 440,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.22781182220205665. Arrivals time: 0.006688299588859081 Scheduler time: 0.07157114008441567 Scheduler overhead time: 0.055799515917897224 Adapter cache time: 0.009809685871005058 Engine time: 0.05578023614361882 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 135, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 135]
Prompts retrieved: 1305 . Total input tokens: 290228 . Total output tokens: 265434
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [6 5 5]
---Simulation End---
#Simulation results
{
    "duration": 0.22853440092876554,
    "estimated_duration": 3597.865136518115,
    "input_throughput": 32.264967027736596,
    "output_throughput": 24.07121910184027,
    "total_throughput": 56.336186129576866,
    "itl": 19.50558567973109,
    "ttft": 8189.422648761239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11963755612261592,
    "arrivals": 441,
    "finished_requests": 440,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.22858115285634995. Arrivals time: 0.006473461631685495 Scheduler time: 0.07194893900305033 Scheduler overhead time: 0.05560736171901226 Adapter cache time: 0.00973181752488017 Engine time: 0.056384886149317026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 135, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 135]
Prompts retrieved: 1305 . Total input tokens: 290228 . Total output tokens: 265434
Prompts distributed
Adapter sizes. Values: [16]. Counts: [16]
---Simulation End---
#Simulation results
{
    "duration": 0.2282670079730451,
    "estimated_duration": 3597.865136518115,
    "input_throughput": 32.264967027736596,
    "output_throughput": 24.07121910184027,
    "total_throughput": 56.336186129576866,
    "itl": 19.505516537501226,
    "ttft": 8189.339281268296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.10214273504912857,
    "arrivals": 441,
    "finished_requests": 440,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.2283126669935882. Arrivals time: 0.006647116504609585 Scheduler time: 0.07162445923313498 Scheduler overhead time: 0.055733137764036655 Adapter cache time: 0.00968300923705101 Engine time: 0.05636008596047759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 16,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_16_slots_16_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [5 5 6]
Adapter prompts. [66, 33, 33, 66, 135, 33, 135, 33, 66, 66, 33, 66, 135, 135, 135, 135]
Prompts retrieved: 1305 . Total input tokens: 290228 . Total output tokens: 265434
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [11  5]
---Simulation End---
#Simulation results
{
    "duration": 0.22890819003805518,
    "estimated_duration": 3597.865136518115,
    "input_throughput": 32.264967027736596,
    "output_throughput": 24.07121910184027,
    "total_throughput": 56.336186129576866,
    "itl": 19.505583889917947,
    "ttft": 8189.416543266237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 16,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.11839485306292771,
    "arrivals": 441,
    "finished_requests": 440,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.22895339224487543. Arrivals time: 0.006604606751352549 Scheduler time: 0.07190524600446224 Scheduler overhead time: 0.05590139515697956 Adapter cache time: 0.009868832305073738 Engine time: 0.05615892028436065 
