INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_32_slots_16_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_32_slots_16_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.713746069930494,
    "estimated_duration": 3599.9521707046106,
    "input_throughput": 2375.049610263715,
    "output_throughput": 2091.400008385772,
    "total_throughput": 4466.449618649487,
    "itl": 28.076180868435372,
    "ttft": 5608.241577554375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1934,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.788399067791524,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.589771106601638
}
#Debug simulation 
Total elapsed time: 2.713857412803918. Arrivals time: 0.09696101257577538 Scheduler time: 2.294311171863228 Scheduler overhead time: 0.11833577370271087 Adapter cache time: 0.027729593217372894 Engine time: 0.11948687117546797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_32_slots_16_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_32_slots_16_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6958993640728295,
    "estimated_duration": 3599.9673419802457,
    "input_throughput": 2375.03960113617,
    "output_throughput": 2091.3911946374856,
    "total_throughput": 4466.4307957736555,
    "itl": 28.090527127343485,
    "ttft": 5608.794009496514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1933,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.13370067564283,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.601137307903491
}
#Debug simulation 
Total elapsed time: 2.696012327913195. Arrivals time: 0.0960270850919187 Scheduler time: 2.284245150629431 Scheduler overhead time: 0.11810289928689599 Adapter cache time: 0.027270489372313023 Engine time: 0.11364729888737202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_32_slots_16_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_32_slots_16_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.7060238388366997,
    "estimated_duration": 3599.9512844776536,
    "input_throughput": 2375.050194947457,
    "output_throughput": 2091.4005232413683,
    "total_throughput": 4466.450718188825,
    "itl": 28.094328378084,
    "ttft": 5608.95555243758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1935,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.531857079108912,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.6040744545132615
}
#Debug simulation 
Total elapsed time: 2.706136815715581. Arrivals time: 0.09716466860845685 Scheduler time: 2.2875168048776686 Scheduler overhead time: 0.1177736222743988 Adapter cache time: 0.02733414340764284 Engine time: 0.11979156313464046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_32_slots_16_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_32_slots_16_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.7023479589261115,
    "estimated_duration": 3599.957497486296,
    "input_throughput": 2375.046095952567,
    "output_throughput": 2091.396913784997,
    "total_throughput": 4466.443009737564,
    "itl": 28.078372480323722,
    "ttft": 5608.2829942262615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1935,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.04838572763401,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.5918373447906164
}
#Debug simulation 
Total elapsed time: 2.7024924270808697. Arrivals time: 0.09739176463335752 Scheduler time: 2.285258201882243 Scheduler overhead time: 0.11805799137800932 Adapter cache time: 0.027382850646972656 Engine time: 0.11734646698459983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_32_slots_16_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_32_slots_16_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.7039444958791137,
    "estimated_duration": 3599.9572372961006,
    "input_throughput": 2375.0462676111915,
    "output_throughput": 2091.397064942618,
    "total_throughput": 4466.443332553809,
    "itl": 28.09257696858518,
    "ttft": 5609.418355271052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1931,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.33722478813,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.602730335096922
}
#Debug simulation 
Total elapsed time: 2.704073254019022. Arrivals time: 0.09689891105517745 Scheduler time: 2.2899288977496326 Scheduler overhead time: 0.11870199395343661 Adapter cache time: 0.027114173863083124 Engine time: 0.11464754864573479 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_32_slots_16_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_32_slots_16_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.698571525979787,
    "estimated_duration": 3599.963873320244,
    "input_throughput": 2375.0418895493754,
    "output_throughput": 2091.393209747981,
    "total_throughput": 4466.435099297356,
    "itl": 28.07327607936272,
    "ttft": 5608.082013904011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1933,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.340119178123379,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.58641977694371
}
#Debug simulation 
Total elapsed time: 2.6986827203072608. Arrivals time: 0.09687852393835783 Scheduler time: 2.2826563827693462 Scheduler overhead time: 0.11801084456965327 Adapter cache time: 0.02719449484720826 Engine time: 0.11700844718143344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_32_slots_16_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_32_slots_16_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 270, 270, 540, 270, 8640, 540, 270, 270, 270, 8640, 8640, 270, 270, 540, 540, 540, 8640, 8640, 270, 8640, 8640, 8640, 8640, 540, 270, 540, 540, 540, 540, 8640]
Prompts retrieved: 103680 . Total input tokens: 23092774 . Total output tokens: 20358034
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.707993562798947,
    "estimated_duration": 3599.9629863193854,
    "input_throughput": 2375.0424747398906,
    "output_throughput": 2091.3937250498275,
    "total_throughput": 4466.436199789718,
    "itl": 28.09203736951197,
    "ttft": 5608.750040434328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1935,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.263847452569497,
    "arrivals": 34764,
    "finished_requests": 34712,
    "scheduler_time": 7.601890273127447
}
#Debug simulation 
Total elapsed time: 2.708173784893006. Arrivals time: 0.09660887625068426 Scheduler time: 2.292090591508895 Scheduler overhead time: 0.11717712599784136 Adapter cache time: 0.02739210519939661 Engine time: 0.11827202653512359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.984719618689269,
    "estimated_duration": 3599.6454847624445,
    "input_throughput": 2370.8429166499454,
    "output_throughput": 2041.1951763313427,
    "total_throughput": 4412.038092981288,
    "itl": 27.70195011950851,
    "ttft": 7176.627804215785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1640,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.84435081239818,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.5977492195456025
}
#Debug simulation 
Total elapsed time: 2.984818662982434. Arrivals time: 0.09800778422504663 Scheduler time: 2.5594522026367486 Scheduler overhead time: 0.12163149006664753 Adapter cache time: 0.026828857138752937 Engine time: 0.12074166722595692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.626576314214617,
    "estimated_duration": 3599.6712511449728,
    "input_throughput": 2370.8259461986895,
    "output_throughput": 2041.1805654927248,
    "total_throughput": 4412.006511691414,
    "itl": 27.713496051737433,
    "ttft": 7177.155135216837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1639,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.088853181344273,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.607740410046133
}
#Debug simulation 
Total elapsed time: 2.6266736262477934. Arrivals time: 0.09428273607045412 Scheduler time: 2.216971040237695 Scheduler overhead time: 0.11753638507798314 Adapter cache time: 0.026564709842205048 Engine time: 0.11450158106163144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.626320025883615,
    "estimated_duration": 3599.6495183056923,
    "input_throughput": 2370.8402600309078,
    "output_throughput": 2041.1928890950496,
    "total_throughput": 4412.033149125958,
    "itl": 27.71749447326871,
    "ttft": 7176.960592089877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1640,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.445038940142641,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.610581693616334
}
#Debug simulation 
Total elapsed time: 2.6264377110637724. Arrivals time: 0.09293026803061366 Scheduler time: 2.2155457879416645 Scheduler overhead time: 0.11763528408482671 Adapter cache time: 0.02673855796456337 Engine time: 0.11649108678102493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.6339428098872304,
    "estimated_duration": 3599.644691636618,
    "input_throughput": 2370.8434390283765,
    "output_throughput": 2041.195626077012,
    "total_throughput": 4412.0390651053885,
    "itl": 27.701995549954876,
    "ttft": 7175.971869901642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1644,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.10320681260915,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.599546112897541
}
#Debug simulation 
Total elapsed time: 2.6340439398773015. Arrivals time: 0.0939854378812015 Scheduler time: 2.2197134899906814 Scheduler overhead time: 0.11849362263455987 Adapter cache time: 0.02673571463674307 Engine time: 0.11778070032596588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6375774769112468,
    "estimated_duration": 3599.661124703276,
    "input_throughput": 2370.8326157239267,
    "output_throughput": 2041.1863076710224,
    "total_throughput": 4412.0189233949495,
    "itl": 27.715183749765195,
    "ttft": 7176.827651776753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1640,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.292807815330843,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.609377737554966
}
#Debug simulation 
Total elapsed time: 2.637674947734922. Arrivals time: 0.0946861975826323 Scheduler time: 2.222860937472433 Scheduler overhead time: 0.1184066729620099 Adapter cache time: 0.02691318793222308 Engine time: 0.11748468643054366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.7136406637728214,
    "estimated_duration": 3599.642850533876,
    "input_throughput": 2370.8446516393324,
    "output_throughput": 2041.1966700836042,
    "total_throughput": 4412.041321722936,
    "itl": 27.697546169693187,
    "ttft": 7175.850949541623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1645,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.50154994723882,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.594692470730659
}
#Debug simulation 
Total elapsed time: 2.7137416186742485. Arrivals time: 0.09565849602222443 Scheduler time: 2.2986761745996773 Scheduler overhead time: 0.11817898321896791 Adapter cache time: 0.026877971831709146 Engine time: 0.11645779805257916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 135, 135, 540, 135, 8640, 540, 135, 135, 135, 8640, 8640, 135, 135, 540, 540, 540, 8640, 8640, 135, 8640, 8640, 8640, 8640, 540, 135, 540, 540, 540, 540, 8640]
Prompts retrieved: 102330 . Total input tokens: 22790423 . Total output tokens: 20104095
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.703718761447817,
    "estimated_duration": 3599.6439465119665,
    "input_throughput": 2370.843929791885,
    "output_throughput": 2041.1960486035737,
    "total_throughput": 4412.039978395459,
    "itl": 27.714070705073595,
    "ttft": 7176.380537563676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1640,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.200795919932236,
    "arrivals": 34293,
    "finished_requests": 34226,
    "scheduler_time": 6.6083509632125566
}
#Debug simulation 
Total elapsed time: 2.703880722168833. Arrivals time: 0.09608543198555708 Scheduler time: 2.2851421744562685 Scheduler overhead time: 0.11853413097560406 Adapter cache time: 0.026729451958090067 Engine time: 0.11974712228402495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.6709369719028473,
    "estimated_duration": 3599.787235771644,
    "input_throughput": 2343.3493280309854,
    "output_throughput": 2036.4106320379838,
    "total_throughput": 4379.759960068969,
    "itl": 27.582333572815415,
    "ttft": 8223.26259150798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1423,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.409458052464997,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.384084959562292
}
#Debug simulation 
Total elapsed time: 2.6710354732349515. Arrivals time: 0.09526086505502462 Scheduler time: 2.2542067035101354 Scheduler overhead time: 0.11909786658361554 Adapter cache time: 0.025926013011485338 Engine time: 0.1187721942551434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.631516871973872,
    "estimated_duration": 3599.8075852296174,
    "input_throughput": 2343.336081242778,
    "output_throughput": 2036.3991203525416,
    "total_throughput": 4379.73520159532,
    "itl": 27.594821170516227,
    "ttft": 8223.26503196572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1424,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.6135818270965,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.393302978159481
}
#Debug simulation 
Total elapsed time: 2.631613806821406. Arrivals time: 0.09173761354759336 Scheduler time: 2.223961461801082 Scheduler overhead time: 0.11800992581993341 Adapter cache time: 0.025995639618486166 Engine time: 0.11468404298648238 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6814887379296124,
    "estimated_duration": 3599.805530612713,
    "input_throughput": 2343.337418720007,
    "output_throughput": 2036.4002826431213,
    "total_throughput": 4379.7377013631285,
    "itl": 27.59695217110028,
    "ttft": 8223.458722155267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.949624866442614,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.396385051825791
}
#Debug simulation 
Total elapsed time: 2.6815852923318744. Arrivals time: 0.09506478114053607 Scheduler time: 2.2650897460989654 Scheduler overhead time: 0.11904237326234579 Adapter cache time: 0.026013105176389217 Engine time: 0.11847640434280038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.6834019999951124,
    "estimated_duration": 3599.8007622849236,
    "input_throughput": 2343.340522725387,
    "output_throughput": 2036.402980076868,
    "total_throughput": 4379.7435028022555,
    "itl": 27.584800388133534,
    "ttft": 8223.187955401414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.662093276451413,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.3857109857528105
}
#Debug simulation 
Total elapsed time: 2.6834995192475617. Arrivals time: 0.09722808003425598 Scheduler time: 2.2630503377877176 Scheduler overhead time: 0.11879503726959229 Adapter cache time: 0.025789204519242048 Engine time: 0.1211389135569334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6477027158252895,
    "estimated_duration": 3599.808073025709,
    "input_throughput": 2343.3357637063546,
    "output_throughput": 2036.3988444079603,
    "total_throughput": 4379.734608114315,
    "itl": 27.595287030962407,
    "ttft": 8223.424712820155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1423,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.790354750654528,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.395113678258104
}
#Debug simulation 
Total elapsed time: 2.6477831909433007. Arrivals time: 0.08900652267038822 Scheduler time: 2.238698341883719 Scheduler overhead time: 0.11837620381265879 Adapter cache time: 0.025922010652720928 Engine time: 0.11842749640345573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.689529378898442,
    "estimated_duration": 3599.784035063966,
    "input_throughput": 2343.3514115938083,
    "output_throughput": 2036.4124426897013,
    "total_throughput": 4379.76385428351,
    "itl": 27.578705824323357,
    "ttft": 8222.964829177798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.097087340313115,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.381199534441355
}
#Debug simulation 
Total elapsed time: 2.689625362865627. Arrivals time: 0.09618603764101863 Scheduler time: 2.272160756867379 Scheduler overhead time: 0.11933685466647148 Adapter cache time: 0.02630335185676813 Engine time: 0.1176176336593926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 66, 66, 540, 66, 8640, 540, 66, 66, 66, 8640, 8640, 66, 66, 540, 540, 540, 8640, 8640, 66, 8640, 8640, 8640, 8640, 540, 66, 540, 540, 540, 540, 8640]
Prompts retrieved: 101640 . Total input tokens: 22630512 . Total output tokens: 19966803
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.685746228788048,
    "estimated_duration": 3599.799377156414,
    "input_throughput": 2343.34142439446,
    "output_throughput": 2036.4037636427088,
    "total_throughput": 4379.745188037168,
    "itl": 27.59626361754527,
    "ttft": 8223.457474980441,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1424,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.714341250322628,
    "arrivals": 34087,
    "finished_requests": 34010,
    "scheduler_time": 6.394444687978188
}
#Debug simulation 
Total elapsed time: 2.6859013019129634. Arrivals time: 0.09638174995779991 Scheduler time: 2.270061277318746 Scheduler overhead time: 0.11887202318757772 Adapter cache time: 0.02590666152536869 Engine time: 0.11704666679725051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.6969040920957923,
    "estimated_duration": 3599.876660262089,
    "input_throughput": 2325.191052348608,
    "output_throughput": 2028.6575594699157,
    "total_throughput": 4353.848611818524,
    "itl": 27.518001270826524,
    "ttft": 6962.843066861049,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.444046333800275,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.2138548945363405
}
#Debug simulation 
Total elapsed time: 2.6970070363022387. Arrivals time: 0.09495039284229279 Scheduler time: 2.2812893665395677 Scheduler overhead time: 0.11867926549166441 Adapter cache time: 0.025361512787640095 Engine time: 0.11886138981208205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6584322680719197,
    "estimated_duration": 3599.8828384946232,
    "input_throughput": 2325.1870617823447,
    "output_throughput": 2028.6540778237907,
    "total_throughput": 4353.841139606136,
    "itl": 27.53018529878863,
    "ttft": 6962.993873812089,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.555758445127788,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.222495622161638
}
#Debug simulation 
Total elapsed time: 2.658554187975824. Arrivals time: 0.09667357988655567 Scheduler time: 2.2406484382227063 Scheduler overhead time: 0.1184372822754085 Adapter cache time: 0.025604010093957186 Engine time: 0.11936462670564651 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6849368973635137,
    "estimated_duration": 3599.9014431573587,
    "input_throughput": 2325.1750449752835,
    "output_throughput": 2028.6435935298396,
    "total_throughput": 4353.818638505123,
    "itl": 27.53266602543846,
    "ttft": 6962.972273053332,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.866679270584061,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.2252289616867555
}
#Debug simulation 
Total elapsed time: 2.685035048983991. Arrivals time: 0.09312932519242167 Scheduler time: 2.272352326195687 Scheduler overhead time: 0.11907474743202329 Adapter cache time: 0.025428385008126497 Engine time: 0.11717011732980609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.6913823056966066,
    "estimated_duration": 3599.895859203308,
    "input_throughput": 2325.1786516547872,
    "output_throughput": 2028.6467402465933,
    "total_throughput": 4353.8253919013805,
    "itl": 27.52089319406827,
    "ttft": 6962.989734513142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.665908801234346,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.215497439733099
}
#Debug simulation 
Total elapsed time: 2.6914846878498793. Arrivals time: 0.09399759955704212 Scheduler time: 2.279751468449831 Scheduler overhead time: 0.11803441774100065 Adapter cache time: 0.025102453771978617 Engine time: 0.11708244401961565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.662902574054897,
    "estimated_duration": 3599.8983188027746,
    "input_throughput": 2325.177062996535,
    "output_throughput": 2028.645354191211,
    "total_throughput": 4353.822417187746,
    "itl": 27.530614965548693,
    "ttft": 6962.984910979108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.725841087000381,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.223972106417325
}
#Debug simulation 
Total elapsed time: 2.6630074670538306. Arrivals time: 0.0944320042617619 Scheduler time: 2.2531182751990855 Scheduler overhead time: 0.11833072546869516 Adapter cache time: 0.025000702124089003 Engine time: 0.11450392799451947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.6693438687361777,
    "estimated_duration": 3599.8860461366553,
    "input_throughput": 2325.1849899479434,
    "output_throughput": 2028.6522702121038,
    "total_throughput": 4353.837260160047,
    "itl": 27.514398394312252,
    "ttft": 6962.8684773602245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.15226704110855,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.211421454548294
}
#Debug simulation 
Total elapsed time: 2.6694480408914387. Arrivals time: 0.09316474292427301 Scheduler time: 2.259229080285877 Scheduler overhead time: 0.11865031160414219 Adapter cache time: 0.024898091796785593 Engine time: 0.11632522055879235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 540, 33, 33, 540, 33, 8640, 540, 33, 33, 33, 8640, 8640, 33, 33, 540, 540, 540, 8640, 8640, 33, 8640, 8640, 8640, 8640, 540, 33, 540, 540, 540, 540, 8640]
Prompts retrieved: 101310 . Total input tokens: 22561559 . Total output tokens: 19899845
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6503291372209787,
    "estimated_duration": 3599.8842601693636,
    "input_throughput": 2325.1861435140136,
    "output_throughput": 2028.6532766629614,
    "total_throughput": 4353.839420176975,
    "itl": 27.530345764557808,
    "ttft": 6962.979130380989,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.649207731652949,
    "arrivals": 33973,
    "finished_requests": 33908,
    "scheduler_time": 6.223397594960304
}
#Debug simulation 
Total elapsed time: 2.6504839193075895. Arrivals time: 0.09343987563624978 Scheduler time: 2.2407882013358176 Scheduler overhead time: 0.11747147748246789 Adapter cache time: 0.024989443365484476 Engine time: 0.11641978239640594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.654155156109482,
    "estimated_duration": 3599.943231487229,
    "input_throughput": 2279.999008931349,
    "output_throughput": 1992.5166978360023,
    "total_throughput": 4272.515706767352,
    "itl": 27.116532302704407,
    "ttft": 8919.095026410858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.445572569975802,
    "arrivals": 33289,
    "finished_requests": 33207,
    "scheduler_time": 5.492422445493275
}
#Debug simulation 
Total elapsed time: 2.654253722168505. Arrivals time: 0.09167404100298882 Scheduler time: 2.2456887303851545 Scheduler overhead time: 0.11885457253083587 Adapter cache time: 0.0242244815453887 Engine time: 0.11615304974839091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6493263342417777,
    "estimated_duration": 3599.9418640700133,
    "input_throughput": 2279.9998749758615,
    "output_throughput": 1992.517454682012,
    "total_throughput": 4272.517329657873,
    "itl": 27.12358039707952,
    "ttft": 8919.087679141483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.248895802693447,
    "arrivals": 33289,
    "finished_requests": 33207,
    "scheduler_time": 5.498545859597198
}
#Debug simulation 
Total elapsed time: 2.6494348072446883. Arrivals time: 0.09216421470046043 Scheduler time: 2.2412268803454936 Scheduler overhead time: 0.11810931796208024 Adapter cache time: 0.024434447288513184 Engine time: 0.1163171618245542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.565022579859942,
    "estimated_duration": 3599.9679490406575,
    "input_throughput": 2279.9833543482755,
    "output_throughput": 1992.503017120331,
    "total_throughput": 4272.486371468606,
    "itl": 27.12633598279305,
    "ttft": 8919.255257303668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.475070703905079,
    "arrivals": 33289,
    "finished_requests": 33207,
    "scheduler_time": 5.500360310435712
}
#Debug simulation 
Total elapsed time: 2.5651192399673164. Arrivals time: 0.08858865266665816 Scheduler time: 2.165145589504391 Scheduler overhead time: 0.11740680132061243 Adapter cache time: 0.02398693561553955 Engine time: 0.11266924673691392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.609406183939427,
    "estimated_duration": 3599.9554311129955,
    "input_throughput": 2279.991282409399,
    "output_throughput": 1992.5099455418383,
    "total_throughput": 4272.5012279512375,
    "itl": 27.118227649381335,
    "ttft": 8919.160972780059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.592267282316444,
    "arrivals": 33289,
    "finished_requests": 33207,
    "scheduler_time": 5.493596736629872
}
#Debug simulation 
Total elapsed time: 2.609510513022542. Arrivals time: 0.0912524233572185 Scheduler time: 2.201005343813449 Scheduler overhead time: 0.12005658214911819 Adapter cache time: 0.024548284243792295 Engine time: 0.11503267474472523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6241666357964277,
    "estimated_duration": 3599.9576152568306,
    "input_throughput": 2279.9898991073064,
    "output_throughput": 1992.5087366586295,
    "total_throughput": 4272.498635765935,
    "itl": 27.12575571906832,
    "ttft": 8919.13818131536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.37710427936633,
    "arrivals": 33289,
    "finished_requests": 33207,
    "scheduler_time": 5.499452264926334
}
#Debug simulation 
Total elapsed time: 2.624296228867024. Arrivals time: 0.09205265389755368 Scheduler time: 2.2130882497876883 Scheduler overhead time: 0.1183583764359355 Adapter cache time: 0.02466767467558384 Engine time: 0.11818298837170005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.619364080019295,
    "estimated_duration": 3599.938260321451,
    "input_throughput": 2280.002157388969,
    "output_throughput": 1992.5194493084175,
    "total_throughput": 4272.521606697386,
    "itl": 27.114443588419523,
    "ttft": 8919.035355960808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.188294979082384,
    "arrivals": 33289,
    "finished_requests": 33207,
    "scheduler_time": 5.490617183575499
}
#Debug simulation 
Total elapsed time: 2.619471757672727. Arrivals time: 0.09059963561594486 Scheduler time: 2.2087532291188836 Scheduler overhead time: 0.11809120327234268 Adapter cache time: 0.02464935090392828 Engine time: 0.11981307063251734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 135, 135, 270, 135, 8640, 270, 135, 135, 135, 8640, 8640, 135, 135, 270, 270, 270, 8640, 8640, 135, 8640, 8640, 8640, 8640, 270, 135, 270, 270, 270, 270, 8640]
Prompts retrieved: 99360 . Total input tokens: 22143755 . Total output tokens: 19527907
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6066472339443862,
    "estimated_duration": 3599.9693467819684,
    "input_throughput": 2279.982469111037,
    "output_throughput": 1992.5022435015828,
    "total_throughput": 4272.48471261262,
    "itl": 27.124894683949044,
    "ttft": 8919.087598129205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1126,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.316833180971457,
    "arrivals": 33289,
    "finished_requests": 33207,
    "scheduler_time": 5.499103234073181
}
#Debug simulation 
Total elapsed time: 2.606793122831732. Arrivals time: 0.0908987820148468 Scheduler time: 2.2000964023172855 Scheduler overhead time: 0.11851169029250741 Adapter cache time: 0.024350174237042665 Engine time: 0.11528142681345344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.6433252142742276,
    "estimated_duration": 3599.948158376246,
    "input_throughput": 2280.229502999991,
    "output_throughput": 1984.6019124960137,
    "total_throughput": 4264.831415496004,
    "itl": 26.91853771291026,
    "ttft": 6255.129531407178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.792470311988264,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.193446304687159
}
#Debug simulation 
Total elapsed time: 2.6434266259893775. Arrivals time: 0.0907645826227963 Scheduler time: 2.2331917309202254 Scheduler overhead time: 0.11999297514557838 Adapter cache time: 0.02357475133612752 Engine time: 0.1179022379219532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.620764289982617,
    "estimated_duration": 3599.9544044666673,
    "input_throughput": 2280.2255466944225,
    "output_throughput": 1984.5984691182364,
    "total_throughput": 4264.824015812659,
    "itl": 26.923991324945092,
    "ttft": 6255.177415057246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 875,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.46189825715964,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.198412563155172
}
#Debug simulation 
Total elapsed time: 2.620868341997266. Arrivals time: 0.09046257147565484 Scheduler time: 2.212235035840422 Scheduler overhead time: 0.11911262478679419 Adapter cache time: 0.023193989880383015 Engine time: 0.11812232062220573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.6047955760732293,
    "estimated_duration": 3599.9560791958056,
    "input_throughput": 2280.224485914768,
    "output_throughput": 1984.597545866727,
    "total_throughput": 4264.822031781495,
    "itl": 26.92585340436499,
    "ttft": 6255.221171914413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 875,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.6496154416771915,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.199838285792074
}
#Debug simulation 
Total elapsed time: 2.6048929099924862. Arrivals time: 0.09089945536106825 Scheduler time: 2.1953525063581765 Scheduler overhead time: 0.11906131124123931 Adapter cache time: 0.023479767609387636 Engine time: 0.11783659551292658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.583554900251329,
    "estimated_duration": 3599.9620956823037,
    "input_throughput": 2280.2206750580235,
    "output_throughput": 1984.5942290805992,
    "total_throughput": 4264.814904138623,
    "itl": 26.918745846782773,
    "ttft": 6255.1502908251205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.919934893641607,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.194371981986556
}
#Debug simulation 
Total elapsed time: 2.5836771838366985. Arrivals time: 0.08925252920016646 Scheduler time: 2.179700370877981 Scheduler overhead time: 0.11895927134901285 Adapter cache time: 0.023317778017371893 Engine time: 0.1146649238653481 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5874380241148174,
    "estimated_duration": 3599.949600274295,
    "input_throughput": 2280.184144626513,
    "output_throughput": 1984.4977828177546,
    "total_throughput": 4264.681927444268,
    "itl": 26.924870963681453,
    "ttft": 6364.026896071805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.5756731131114385,
    "arrivals": 33058,
    "finished_requests": 33000,
    "scheduler_time": 5.199408390768444
}
#Debug simulation 
Total elapsed time: 2.587535995990038. Arrivals time: 0.08969996264204383 Scheduler time: 2.1823826571926475 Scheduler overhead time: 0.11913735093548894 Adapter cache time: 0.023456959053874016 Engine time: 0.11472561908885837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.605746162123978,
    "estimated_duration": 3599.9492470063715,
    "input_throughput": 2280.2288134551222,
    "output_throughput": 1984.601312349378,
    "total_throughput": 4264.8301258045,
    "itl": 26.916036145531958,
    "ttft": 6255.140460748972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 875,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.585930822999189,
    "arrivals": 33058,
    "finished_requests": 33001,
    "scheduler_time": 5.1918117436185
}
#Debug simulation 
Total elapsed time: 2.6058419072069228. Arrivals time: 0.08958523673936725 Scheduler time: 2.200426819268614 Scheduler overhead time: 0.11944368062540889 Adapter cache time: 0.02347045438364148 Engine time: 0.1144188423641026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 66, 66, 270, 66, 8640, 270, 66, 66, 66, 8640, 8640, 66, 66, 270, 270, 270, 8640, 8640, 66, 8640, 8640, 8640, 8640, 270, 66, 270, 270, 270, 270, 8640]
Prompts retrieved: 98670 . Total input tokens: 22004526 . Total output tokens: 19379321
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5879403850995004,
    "estimated_duration": 3599.947135220766,
    "input_throughput": 2280.1857059760996,
    "output_throughput": 1984.4991416969488,
    "total_throughput": 4264.684847673048,
    "itl": 26.925841693299063,
    "ttft": 6364.102827444158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.526793459430369,
    "arrivals": 33058,
    "finished_requests": 33000,
    "scheduler_time": 5.198921470953503
}
#Debug simulation 
Total elapsed time: 2.588104447349906. Arrivals time: 0.08982294471934438 Scheduler time: 2.183219013735652 Scheduler overhead time: 0.11815957864746451 Adapter cache time: 0.023226995952427387 Engine time: 0.11582627613097429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.5407112948596478,
    "estimated_duration": 3600.010282741187,
    "input_throughput": 2268.222965680636,
    "output_throughput": 1970.8110374056039,
    "total_throughput": 4239.03400308624,
    "itl": 26.767620413304694,
    "ttft": 6488.195271026425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 691,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5691746410774865,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.872776350112186
}
#Debug simulation 
Total elapsed time: 2.540803215932101. Arrivals time: 0.08816906670108438 Scheduler time: 2.1363177774474025 Scheduler overhead time: 0.11887534102424979 Adapter cache time: 0.022437815088778734 Engine time: 0.11671959143131971 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5725027262233198,
    "estimated_duration": 3600.0037768060947,
    "input_throughput": 2268.227064818388,
    "output_throughput": 1970.8145990598364,
    "total_throughput": 4239.041663878224,
    "itl": 26.770587244828675,
    "ttft": 6379.145546293258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.124003913169737,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.8751557814704345
}
#Debug simulation 
Total elapsed time: 2.572598470374942. Arrivals time: 0.08752404665574431 Scheduler time: 2.1715159411542118 Scheduler overhead time: 0.11963972263038158 Adapter cache time: 0.022393780294805765 Engine time: 0.11341345449909568 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5461528901942074,
    "estimated_duration": 3600.020240846103,
    "input_throughput": 2268.216691493061,
    "output_throughput": 1970.8055858965101,
    "total_throughput": 4239.022277389571,
    "itl": 26.774001802900585,
    "ttft": 6488.021130035543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 691,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.286652001333458,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.877614784393144
}
#Debug simulation 
Total elapsed time: 2.5462472732178867. Arrivals time: 0.08846742007881403 Scheduler time: 2.142653262708336 Scheduler overhead time: 0.11950871720910072 Adapter cache time: 0.022399233654141426 Engine time: 0.11512223118916154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.56916338019073,
    "estimated_duration": 3600.0107083635135,
    "input_throughput": 2268.222697513007,
    "output_throughput": 1970.8108044004139,
    "total_throughput": 4239.033501913421,
    "itl": 26.767347921429828,
    "ttft": 6488.160145977474,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.672832096843039,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.871812236079962
}
#Debug simulation 
Total elapsed time: 2.5692621381022036. Arrivals time: 0.08850783761590719 Scheduler time: 2.1664237286895514 Scheduler overhead time: 0.11826474126428366 Adapter cache time: 0.02247632760554552 Engine time: 0.1154390568844974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5640319529920816,
    "estimated_duration": 3600.027580372198,
    "input_throughput": 2268.2120671852676,
    "output_throughput": 1970.801567933119,
    "total_throughput": 4239.013635118386,
    "itl": 26.771887927428068,
    "ttft": 6488.177666425341,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.211262845560012,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.875910867968147
}
#Debug simulation 
Total elapsed time: 2.5641368981450796. Arrivals time: 0.08798640314489603 Scheduler time: 2.163207002915442 Scheduler overhead time: 0.11919083911925554 Adapter cache time: 0.022264423314481974 Engine time: 0.11355022434145212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.5056293830275536,
    "estimated_duration": 3600.002251718108,
    "input_throughput": 2268.2280257194116,
    "output_throughput": 1970.8154339664445,
    "total_throughput": 4239.043459685856,
    "itl": 26.7663194563697,
    "ttft": 6379.035059359198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.404905448993647,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.871372249970236
}
#Debug simulation 
Total elapsed time: 2.5057247867807746. Arrivals time: 0.0870088585652411 Scheduler time: 2.1099227238446474 Scheduler overhead time: 0.11774572916328907 Adapter cache time: 0.022261062636971474 Engine time: 0.11135187139734626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 270, 33, 33, 270, 33, 8640, 270, 33, 33, 33, 8640, 8640, 33, 33, 270, 270, 270, 8640, 8640, 33, 8640, 8640, 8640, 8640, 270, 33, 270, 270, 270, 270, 8640]
Prompts retrieved: 98340 . Total input tokens: 21922176 . Total output tokens: 19317572
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5277305990457535,
    "estimated_duration": 3600.0048301308498,
    "input_throughput": 2268.226401158246,
    "output_throughput": 1970.814022419553,
    "total_throughput": 4239.040423577799,
    "itl": 26.771989831777013,
    "ttft": 6379.010442366962,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.171289230473374,
    "arrivals": 32964,
    "finished_requests": 32905,
    "scheduler_time": 4.875580056845671
}
#Debug simulation 
Total elapsed time: 2.5278641879558563. Arrivals time: 0.08811000175774097 Scheduler time: 2.1278590532019734 Scheduler overhead time: 0.11798855243250728 Adapter cache time: 0.02260564360767603 Engine time: 0.11373642319813371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.52841589320451,
    "estimated_duration": 3599.9792673482575,
    "input_throughput": 2227.958664303079,
    "output_throughput": 1957.8896089565035,
    "total_throughput": 4185.848273259582,
    "itl": 26.57772050510161,
    "ttft": 6455.190538176264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 523,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4582899237098617,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.6119966897401135
}
#Debug simulation 
Total elapsed time: 2.5285221789963543. Arrivals time: 0.08792457729578018 Scheduler time: 2.125862584915012 Scheduler overhead time: 0.1190574043430388 Adapter cache time: 0.021343843545764685 Engine time: 0.11604308849200606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.5238870582543314,
    "estimated_duration": 3600.000556866184,
    "input_throughput": 2227.9454887034717,
    "output_throughput": 1957.8780304788702,
    "total_throughput": 4185.823519182341,
    "itl": 26.581504779850405,
    "ttft": 6455.181642584209,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8416004973789732,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.614724465200275
}
#Debug simulation 
Total elapsed time: 2.5239914511330426. Arrivals time: 0.08707939134910703 Scheduler time: 2.122936053201556 Scheduler overhead time: 0.11894574202597141 Adapter cache time: 0.021246117539703846 Engine time: 0.11581890610978007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.4973308062180877,
    "estimated_duration": 3600.0038489163885,
    "input_throughput": 2227.9434513422048,
    "output_throughput": 1957.8762400827925,
    "total_throughput": 4185.8196914249975,
    "itl": 26.58281352273941,
    "ttft": 6455.2996883801015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9379477479309184,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.615445743515264
}
#Debug simulation 
Total elapsed time: 2.497423617169261. Arrivals time: 0.08594343112781644 Scheduler time: 2.1007409850135446 Scheduler overhead time: 0.11880983784794807 Adapter cache time: 0.021106433123350143 Engine time: 0.11263879667967558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.488291510846466,
    "estimated_duration": 3600.0012420757084,
    "input_throughput": 2227.945064645432,
    "output_throughput": 1957.877657824367,
    "total_throughput": 4185.822722469799,
    "itl": 26.579992822308167,
    "ttft": 6455.191726825531,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 523,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5095418316358624,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.613693748013776
}
#Debug simulation 
Total elapsed time: 2.488394503016025. Arrivals time: 0.08647653600201011 Scheduler time: 2.0895257983356714 Scheduler overhead time: 0.11905920784920454 Adapter cache time: 0.02140781795606017 Engine time: 0.1142456540837884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.545317537151277,
    "estimated_duration": 3599.981275561718,
    "input_throughput": 2227.957421458676,
    "output_throughput": 1957.8885167674152,
    "total_throughput": 4185.845938226091,
    "itl": 26.58372514615622,
    "ttft": 6455.213134330796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 523,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8826489582890984,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.616282675404152
}
#Debug simulation 
Total elapsed time: 2.5454141912050545. Arrivals time: 0.08704350097104907 Scheduler time: 2.144358293619007 Scheduler overhead time: 0.11890371236950159 Adapter cache time: 0.021370524540543556 Engine time: 0.11567408265545964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.5123567669652402,
    "estimated_duration": 3599.9976241677255,
    "input_throughput": 2227.9473036747527,
    "output_throughput": 1957.8796254426677,
    "total_throughput": 4185.82692911742,
    "itl": 26.578848727134172,
    "ttft": 6455.1737605641365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3451745728589444,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.61263456993621
}
#Debug simulation 
Total elapsed time: 2.5124545828439295. Arrivals time: 0.08609018800780177 Scheduler time: 2.1130180237814784 Scheduler overhead time: 0.11986637767404318 Adapter cache time: 0.02142204623669386 Engine time: 0.11364582134410739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 66, 66, 135, 66, 8640, 135, 66, 66, 66, 8640, 8640, 66, 66, 135, 135, 135, 8640, 8640, 66, 8640, 8640, 8640, 8640, 135, 66, 135, 135, 135, 135, 8640]
Prompts retrieved: 97185 . Total input tokens: 21673826 . Total output tokens: 19081313
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.481717213988304,
    "estimated_duration": 3600.0027432079855,
    "input_throughput": 2227.94413563496,
    "output_throughput": 1957.8768414268372,
    "total_throughput": 4185.820977061797,
    "itl": 26.583215511467085,
    "ttft": 6455.174698805015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 523,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8571735455654896,
    "arrivals": 32570,
    "finished_requests": 32512,
    "scheduler_time": 4.616037161343298
}
#Debug simulation 
Total elapsed time: 2.481834384147078. Arrivals time: 0.08412419352680445 Scheduler time: 2.089508547447622 Scheduler overhead time: 0.11830136738717556 Adapter cache time: 0.021114989183843136 Engine time: 0.11102788941934705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.469500300940126,
    "estimated_duration": 3599.956213623347,
    "input_throughput": 2222.995649145775,
    "output_throughput": 1938.0205163600806,
    "total_throughput": 4161.016165505856,
    "itl": 26.4418679063104,
    "ttft": 6368.926961159164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.823498656642647,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.251063457727935
}
#Debug simulation 
Total elapsed time: 2.4695906350389123. Arrivals time: 0.0867804130539298 Scheduler time: 2.070044143125415 Scheduler overhead time: 0.11877066362649202 Adapter cache time: 0.021290565375238657 Engine time: 0.114600645378232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.4863420482724905,
    "estimated_duration": 3599.960278157186,
    "input_throughput": 2222.9931392733483,
    "output_throughput": 1938.0183282387236,
    "total_throughput": 4161.011467512072,
    "itl": 26.444228884511254,
    "ttft": 6369.045282840076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1604474062705474,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.2524077517256815
}
#Debug simulation 
Total elapsed time: 2.486433075275272. Arrivals time: 0.0862419456243515 Scheduler time: 2.084875052794814 Scheduler overhead time: 0.119587158318609 Adapter cache time: 0.021303778048604727 Engine time: 0.11614563735201955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.508253653999418,
    "estimated_duration": 3599.95451005278,
    "input_throughput": 2222.9967011118347,
    "output_throughput": 1938.0214334702,
    "total_throughput": 4161.018134582035,
    "itl": 26.447445795973564,
    "ttft": 6368.9437181739295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.253679401571888,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.2545287326033385
}
#Debug simulation 
Total elapsed time: 2.5083473441191018. Arrivals time: 0.0854782834649086 Scheduler time: 2.108211612328887 Scheduler overhead time: 0.12042724573984742 Adapter cache time: 0.021369175519794226 Engine time: 0.11400189343839884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.4722617268562317,
    "estimated_duration": 3599.9694859801966,
    "input_throughput": 2222.9874534119936,
    "output_throughput": 1938.013371271775,
    "total_throughput": 4161.000824683769,
    "itl": 26.442020296585998,
    "ttft": 6368.9827995176065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.892520758421155,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.250717981072445
}
#Debug simulation 
Total elapsed time: 2.472354993224144. Arrivals time: 0.08622472500428557 Scheduler time: 2.072983537334949 Scheduler overhead time: 0.1192031092941761 Adapter cache time: 0.02144310437142849 Engine time: 0.1144134015776217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.461153774987906,
    "estimated_duration": 3599.9611905368206,
    "input_throughput": 2222.9925758745894,
    "output_throughput": 1938.0178370644135,
    "total_throughput": 4161.010412939003,
    "itl": 26.446496720938203,
    "ttft": 6368.95450731345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2137057864852503,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.254279121988769
}
#Debug simulation 
Total elapsed time: 2.4612485081888735. Arrivals time: 0.08493242366239429 Scheduler time: 2.0654310029931366 Scheduler overhead time: 0.11920597357675433 Adapter cache time: 0.02113622287288308 Engine time: 0.11256687436252832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.468287424184382,
    "estimated_duration": 3599.9491442280164,
    "input_throughput": 2223.0000145505164,
    "output_throughput": 1938.024322145285,
    "total_throughput": 4161.024336695801,
    "itl": 26.441122859900407,
    "ttft": 6368.958081607241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.725934241623606,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.25037750833298
}
#Debug simulation 
Total elapsed time: 2.4683843082748353. Arrivals time: 0.08709882479161024 Scheduler time: 2.0687519777566195 Scheduler overhead time: 0.11902738828212023 Adapter cache time: 0.021261780057102442 Engine time: 0.11411503050476313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 135, 33, 33, 135, 33, 8640, 135, 33, 33, 33, 8640, 8640, 33, 33, 135, 135, 135, 8640, 8640, 33, 8640, 8640, 8640, 8640, 135, 33, 135, 135, 135, 135, 8640]
Prompts retrieved: 96855 . Total input tokens: 21596190 . Total output tokens: 19011756
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.4865481117740273,
    "estimated_duration": 3599.968744623798,
    "input_throughput": 2222.9879112009603,
    "output_throughput": 1938.0137703748549,
    "total_throughput": 4161.001681575815,
    "itl": 26.44417599178326,
    "ttft": 6368.954072862237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.188851725291486,
    "arrivals": 32442,
    "finished_requests": 32385,
    "scheduler_time": 4.252524133918857
}
#Debug simulation 
Total elapsed time: 2.4866632358171046. Arrivals time: 0.08623825898393989 Scheduler time: 2.084870317019522 Scheduler overhead time: 0.11934355879202485 Adapter cache time: 0.0213558254763484 Engine time: 0.11610109079629183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.866667710710317,
    "estimated_duration": 3599.941522998827,
    "input_throughput": 2203.9405221756947,
    "output_throughput": 1928.191043008301,
    "total_throughput": 4132.131565183995,
    "itl": 26.277289315413363,
    "ttft": 8319.734948071275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.586978167667983,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 4.000186714974257
}
#Debug simulation 
Total elapsed time: 2.866750747896731. Arrivals time: 0.08763978909701109 Scheduler time: 2.4591546175070107 Scheduler overhead time: 0.12292369781062007 Adapter cache time: 0.020626941695809364 Engine time: 0.11716618807986379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.4614852159284055,
    "estimated_duration": 3599.9264816587,
    "input_throughput": 2203.9497307579204,
    "output_throughput": 1928.199099444302,
    "total_throughput": 4132.148830202223,
    "itl": 26.279102925977142,
    "ttft": 8319.810078941739,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7578638604609313,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 4.001330061175898
}
#Debug simulation 
Total elapsed time: 2.4615775919519365. Arrivals time: 0.08378776488825679 Scheduler time: 2.066041369922459 Scheduler overhead time: 0.11950617330148816 Adapter cache time: 0.02027925243601203 Engine time: 0.1136342859826982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.4636072758585215,
    "estimated_duration": 3599.925889913975,
    "input_throughput": 2203.950093036386,
    "output_throughput": 1928.1994163957286,
    "total_throughput": 4132.149509432114,
    "itl": 26.279303183372047,
    "ttft": 8319.842662979781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8045027523022181,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 4.00161628343883
}
#Debug simulation 
Total elapsed time: 2.463700438849628. Arrivals time: 0.08595438906922936 Scheduler time: 2.0642232331447303 Scheduler overhead time: 0.1200908268801868 Adapter cache time: 0.020310243126004934 Engine time: 0.11465954640880227 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 2.4788850634358823,
    "estimated_duration": 3599.9429614274636,
    "input_throughput": 2203.9396415475308,
    "output_throughput": 1928.1902725613127,
    "total_throughput": 4132.129914108843,
    "itl": 26.277552347281734,
    "ttft": 8319.744473999068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.609881400242445,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 4.000216904978551
}
#Debug simulation 
Total elapsed time: 2.4789850120432675. Arrivals time: 0.08629687083885074 Scheduler time: 2.077444135211408 Scheduler overhead time: 0.11992989294230938 Adapter cache time: 0.020212853327393532 Engine time: 0.1163678951561451 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 2.4843554100953043,
    "estimated_duration": 3599.9504721588287,
    "input_throughput": 2203.93504337355,
    "output_throughput": 1928.1862496950898,
    "total_throughput": 4132.12129306864,
    "itl": 26.278891280616335,
    "ttft": 8319.772106846494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7755078440904652,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 4.001330436541932
}
#Debug simulation 
Total elapsed time: 2.4844549861736596. Arrivals time: 0.08722314704209566 Scheduler time: 2.0802570758387446 Scheduler overhead time: 0.12072443403303623 Adapter cache time: 0.020471318159252405 Engine time: 0.1170428516343236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.477109266910702,
    "estimated_duration": 3599.9400539912613,
    "input_throughput": 2203.941421525476,
    "output_throughput": 1928.1918298345227,
    "total_throughput": 4132.133251359999,
    "itl": 26.27708657765676,
    "ttft": 8319.799032718807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5385249466774935,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 3.9996617239148997
}
#Debug simulation 
Total elapsed time: 2.4772060248069465. Arrivals time: 0.08570653572678566 Scheduler time: 2.0773964202962816 Scheduler overhead time: 0.12086738646030426 Adapter cache time: 0.020313821732997894 Engine time: 0.11418548133224249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [10 11 11]
Adapter prompts. [8640, 66, 33, 33, 66, 33, 8640, 66, 33, 33, 33, 8640, 8640, 33, 33, 66, 66, 66, 8640, 8640, 33, 8640, 8640, 8640, 8640, 66, 33, 66, 66, 66, 66, 8640]
Prompts retrieved: 96096 . Total input tokens: 21426780 . Total output tokens: 18868516
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 2.472599414177239,
    "estimated_duration": 3599.94325319019,
    "input_throughput": 2203.9394629259823,
    "output_throughput": 1928.1901162882798,
    "total_throughput": 4132.129579214262,
    "itl": 26.279048285366144,
    "ttft": 8319.83509071193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7717782383970953,
    "arrivals": 32188,
    "finished_requests": 32114,
    "scheduler_time": 4.001312130717739
}
#Debug simulation 
Total elapsed time: 2.472721709869802. Arrivals time: 0.08392405556514859 Scheduler time: 2.075488585047424 Scheduler overhead time: 0.12004833109676838 Adapter cache time: 0.02022390253841877 Engine time: 0.11453730193898082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_32_slots_16_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_32_slots_16_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.9347791080363095,
    "estimated_duration": 3599.90235101822,
    "input_throughput": 1474.3336019930662,
    "output_throughput": 1285.843767037383,
    "total_throughput": 2760.1773690304494,
    "itl": 25.572936665681834,
    "ttft": 7636.626170343916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3725,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 24.631223644011925,
    "arrivals": 21639,
    "finished_requests": 21601,
    "scheduler_time": 0.05199201452124342
}
#Debug simulation 
Total elapsed time: 1.9348722058348358. Arrivals time: 0.0665137111209333 Scheduler time: 1.5315639949403703 Scheduler overhead time: 0.12161243380978703 Adapter cache time: 0.03517942177131772 Engine time: 0.11983868200331926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_32_slots_16_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_32_slots_16_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.9297245512716472,
    "estimated_duration": 3599.908386333743,
    "input_throughput": 1474.3311302444774,
    "output_throughput": 1285.8416112956213,
    "total_throughput": 2760.1727415400987,
    "itl": 25.591945267070674,
    "ttft": 7644.487243641869,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3718,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.332298598893978,
    "arrivals": 21639,
    "finished_requests": 21601,
    "scheduler_time": 0.05245660445762213
}
#Debug simulation 
Total elapsed time: 1.929841314908117. Arrivals time: 0.06712355045601726 Scheduler time: 1.5223209862597287 Scheduler overhead time: 0.12356312572956085 Adapter cache time: 0.035264415200799704 Engine time: 0.12145934347063303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_32_slots_16_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_32_slots_16_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.9180109947919846,
    "estimated_duration": 3599.9222414876203,
    "input_throughput": 1474.3254559317268,
    "output_throughput": 1285.8366624294538,
    "total_throughput": 2760.1621183611805,
    "itl": 25.60130728077858,
    "ttft": 7642.250542341955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3720,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 28.115459336937484,
    "arrivals": 21639,
    "finished_requests": 21601,
    "scheduler_time": 0.052410165554552156
}
#Debug simulation 
Total elapsed time: 1.9181029661558568. Arrivals time: 0.06490657245740294 Scheduler time: 1.520042959600687 Scheduler overhead time: 0.12177913496270776 Adapter cache time: 0.0350432563573122 Engine time: 0.11668172990903258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_32_slots_16_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_32_slots_16_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 1.9210954490117729,
    "estimated_duration": 3599.9195107389783,
    "input_throughput": 1474.3265742934639,
    "output_throughput": 1285.8376378114615,
    "total_throughput": 2760.1642121049254,
    "itl": 25.577230409007168,
    "ttft": 7641.101263868071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3719,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.42571401861104,
    "arrivals": 21639,
    "finished_requests": 21601,
    "scheduler_time": 0.05200792995579389
}
#Debug simulation 
Total elapsed time: 1.9212007788009942. Arrivals time: 0.06534054083749652 Scheduler time: 1.5186716965399683 Scheduler overhead time: 0.12252772552892566 Adapter cache time: 0.0354035459458828 Engine time: 0.11939320573583245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_32_slots_16_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_32_slots_16_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 1.9355505178682506,
    "estimated_duration": 3599.9034147739335,
    "input_throughput": 1474.3331663339354,
    "output_throughput": 1285.8433870761742,
    "total_throughput": 2760.1765534101096,
    "itl": 25.59966995942669,
    "ttft": 7642.697538416647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3715,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.7902928522239,
    "arrivals": 21639,
    "finished_requests": 21601,
    "scheduler_time": 0.052054185222629445
}
#Debug simulation 
Total elapsed time: 1.9356421739794314. Arrivals time: 0.06589588848873973 Scheduler time: 1.532943588681519 Scheduler overhead time: 0.12088576657697558 Adapter cache time: 0.03533919062465429 Engine time: 0.12068578973412514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_32_slots_16_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_32_slots_16_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.938407801091671,
    "estimated_duration": 3599.9121821960875,
    "input_throughput": 1474.3295756626605,
    "output_throughput": 1285.8402554631714,
    "total_throughput": 2760.169831125832,
    "itl": 25.56501722676266,
    "ttft": 7637.506728284927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3720,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.748185898924444,
    "arrivals": 21639,
    "finished_requests": 21601,
    "scheduler_time": 0.05161299964534554
}
#Debug simulation 
Total elapsed time: 1.9384995810687542. Arrivals time: 0.06713655358180404 Scheduler time: 1.5329296151176095 Scheduler overhead time: 0.12196319177746773 Adapter cache time: 0.03552618250250816 Engine time: 0.12043158803135157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_32_slots_16_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_32_slots_16_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 540, 540, 1080, 540, 4320, 1080, 540, 540, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 4320, 4320, 540, 4320, 4320, 4320, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 64800 . Total input tokens: 14417344 . Total output tokens: 12740463
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.9252010839991271,
    "estimated_duration": 3599.9104617465428,
    "input_throughput": 1474.3302802661983,
    "output_throughput": 1285.8408699849228,
    "total_throughput": 2760.171150251121,
    "itl": 25.594436135540903,
    "ttft": 7643.359630871115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 27.532899567187794,
    "arrivals": 21639,
    "finished_requests": 21601,
    "scheduler_time": 0.05245732923586561
}
#Debug simulation 
Total elapsed time: 1.9253349527716637. Arrivals time: 0.06698433076962829 Scheduler time: 1.5225693583488464 Scheduler overhead time: 0.12207068270072341 Adapter cache time: 0.035259879659861326 Engine time: 0.11845473945140839 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_32_slots_16_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_32_slots_16_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.8501412072218955,
    "estimated_duration": 3600.017607392889,
    "input_throughput": 1411.843094756648,
    "output_throughput": 1258.4694004541186,
    "total_throughput": 2670.3124952107664,
    "itl": 25.431093635882032,
    "ttft": 8798.055759669087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.594601662171826,
    "arrivals": 20840,
    "finished_requests": 20793,
    "scheduler_time": 0.011875792146265787
}
#Debug simulation 
Total elapsed time: 1.8502361383289099. Arrivals time: 0.0644508395344019 Scheduler time: 1.4535563332028687 Scheduler overhead time: 0.1214611534960568 Adapter cache time: 0.03384981630370021 Engine time: 0.1170131266117096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_32_slots_16_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_32_slots_16_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.8435598029755056,
    "estimated_duration": 3600.0112363363414,
    "input_throughput": 1411.8455933411253,
    "output_throughput": 1258.471627608199,
    "total_throughput": 2670.317220949324,
    "itl": 25.451613095770586,
    "ttft": 8629.827914387613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.21751130505376,
    "arrivals": 20840,
    "finished_requests": 20793,
    "scheduler_time": 0.012127121116222586
}
#Debug simulation 
Total elapsed time: 1.8436585939489305. Arrivals time: 0.06466376362368464 Scheduler time: 1.448132164310664 Scheduler overhead time: 0.12114074127748609 Adapter cache time: 0.03406238416209817 Engine time: 0.11572459572926164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_32_slots_16_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_32_slots_16_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.8578692069277167,
    "estimated_duration": 3600.021347539177,
    "input_throughput": 1411.841627959871,
    "output_throughput": 1258.468093000856,
    "total_throughput": 2670.309720960727,
    "itl": 25.45683873247113,
    "ttft": 8802.776299193707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.973207629556608,
    "arrivals": 20840,
    "finished_requests": 20793,
    "scheduler_time": 0.012239677874674797
}
#Debug simulation 
Total elapsed time: 1.8579697129316628. Arrivals time: 0.0647476464509964 Scheduler time: 1.4601034512743354 Scheduler overhead time: 0.12021769536659122 Adapter cache time: 0.033971389289945364 Engine time: 0.11872163973748684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_32_slots_16_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_32_slots_16_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 1.8610402019694448,
    "estimated_duration": 3600.0128539248435,
    "input_throughput": 1411.844958958613,
    "output_throughput": 1258.471062140986,
    "total_throughput": 2670.316021099599,
    "itl": 25.437663877015023,
    "ttft": 8625.769642431907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.4291898829403,
    "arrivals": 20840,
    "finished_requests": 20793,
    "scheduler_time": 0.012026571582466268
}
#Debug simulation 
Total elapsed time: 1.8611427191644907. Arrivals time: 0.06491561466827989 Scheduler time: 1.460941576398909 Scheduler overhead time: 0.1217449870891869 Adapter cache time: 0.034035082906484604 Engine time: 0.11960964277386665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_32_slots_16_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_32_slots_16_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 1.8454068559221923,
    "estimated_duration": 3600.0063330651014,
    "input_throughput": 1411.8475162993793,
    "output_throughput": 1258.4733416684442,
    "total_throughput": 2670.3208579678235,
    "itl": 25.45658049286947,
    "ttft": 8630.226784828406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.712599359364997,
    "arrivals": 20840,
    "finished_requests": 20793,
    "scheduler_time": 0.012191796537609802
}
#Debug simulation 
Total elapsed time: 1.8455103961750865. Arrivals time: 0.064051846973598 Scheduler time: 1.4513080073520541 Scheduler overhead time: 0.12087137158960104 Adapter cache time: 0.03382223192602396 Engine time: 0.11579433036968112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_32_slots_16_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_32_slots_16_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.8425900270231068,
    "estimated_duration": 3600.005924289122,
    "input_throughput": 1411.8476766128242,
    "output_throughput": 1258.4734845664514,
    "total_throughput": 2670.321161179276,
    "itl": 25.424960536048516,
    "ttft": 8628.812287517318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.80109001205017,
    "arrivals": 20840,
    "finished_requests": 20793,
    "scheduler_time": 0.011881975902728227
}
#Debug simulation 
Total elapsed time: 1.8426825730130076. Arrivals time: 0.06276294495910406 Scheduler time: 1.4479325646534562 Scheduler overhead time: 0.12160836765542626 Adapter cache time: 0.03398476028814912 Engine time: 0.11638539610430598 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_32_slots_16_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_32_slots_16_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 270, 270, 1080, 270, 4320, 1080, 270, 270, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 4320, 4320, 270, 4320, 4320, 4320, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 62100 . Total input tokens: 13807988 . Total output tokens: 12216518
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.8520864271558821,
    "estimated_duration": 3600.00981385349,
    "input_throughput": 1411.8461512079782,
    "output_throughput": 1258.4721248719293,
    "total_throughput": 2670.3182760799077,
    "itl": 25.45329611251734,
    "ttft": 8628.380556968861,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 25.455310950056443,
    "arrivals": 20840,
    "finished_requests": 20793,
    "scheduler_time": 0.012195654603520806
}
#Debug simulation 
Total elapsed time: 1.8522180700674653. Arrivals time: 0.06435081781819463 Scheduler time: 1.4556815619580448 Scheduler overhead time: 0.12070962507277727 Adapter cache time: 0.03453967161476612 Engine time: 0.11708600353449583 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 2.0539544820785522,
    "estimated_duration": 3600.007961519454,
    "input_throughput": 1386.464711565063,
    "output_throughput": 1229.7892247247676,
    "total_throughput": 2616.2539362898306,
    "itl": 25.230704831851423,
    "ttft": 5668.073399389632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.564592089363675,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.010830196703880657
}
#Debug simulation 
Total elapsed time: 2.054052389226854. Arrivals time: 0.06519043259322643 Scheduler time: 1.6507420921698213 Scheduler overhead time: 0.12462915666401386 Adapter cache time: 0.033830609172582626 Engine time: 0.11809412110596895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7992606540210545,
    "estimated_duration": 3600.01284021039,
    "input_throughput": 1386.4628326459808,
    "output_throughput": 1229.7875581302828,
    "total_throughput": 2616.250390776264,
    "itl": 25.249746286723116,
    "ttft": 5669.7446265055205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.198766156126762,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.011034084324324768
}
#Debug simulation 
Total elapsed time: 1.7993551068939269. Arrivals time: 0.06332309124991298 Scheduler time: 1.401171704288572 Scheduler overhead time: 0.12166830198839307 Adapter cache time: 0.03356849029660225 Engine time: 0.11974076135084033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.8019821271300316,
    "estimated_duration": 3600.005147966329,
    "input_throughput": 1386.4657951446584,
    "output_throughput": 1229.790185855981,
    "total_throughput": 2616.2559810006396,
    "itl": 25.254877213511037,
    "ttft": 5669.856364235992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.897273315559612,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.010977448853956847
}
#Debug simulation 
Total elapsed time: 1.802072225138545. Arrivals time: 0.06139488285407424 Scheduler time: 1.4093113034032285 Scheduler overhead time: 0.12175102531909943 Adapter cache time: 0.033325388096272945 Engine time: 0.1161371748894453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 1.799561485182494,
    "estimated_duration": 3600.0329286433985,
    "input_throughput": 1386.4550960873758,
    "output_throughput": 1229.780695830558,
    "total_throughput": 2616.235791917934,
    "itl": 25.236716886546535,
    "ttft": 5845.054835191353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3114,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.327444284102484,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.010958407493670816
}
#Debug simulation 
Total elapsed time: 1.7996596759185195. Arrivals time: 0.06190556939691305 Scheduler time: 1.4062459752894938 Scheduler overhead time: 0.12145524332299829 Adapter cache time: 0.03335585258901119 Engine time: 0.1168216341175139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7919533983804286,
    "estimated_duration": 3600.0354032518044,
    "input_throughput": 1386.4541430596828,
    "output_throughput": 1229.7798504984137,
    "total_throughput": 2616.2339935580962,
    "itl": 25.254355518465186,
    "ttft": 5846.556107578717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.604257372660076,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.011006289309616203
}
#Debug simulation 
Total elapsed time: 1.792057272978127. Arrivals time: 0.06210714299231768 Scheduler time: 1.3974811742082238 Scheduler overhead time: 0.12072328059002757 Adapter cache time: 0.03348471410572529 Engine time: 0.11871969606727362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.8197392127476633,
    "estimated_duration": 3600.0300578374154,
    "input_throughput": 1386.4562017013627,
    "output_throughput": 1229.7816765061975,
    "total_throughput": 2616.2378782075602,
    "itl": 25.222799689616963,
    "ttft": 5844.241106856302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3115,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.885913729878755,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.010851165797086311
}
#Debug simulation 
Total elapsed time: 1.8198394956998527. Arrivals time: 0.06240105628967285 Scheduler time: 1.4197730468586087 Scheduler overhead time: 0.12108268588781357 Adapter cache time: 0.033540308475494385 Engine time: 0.12311375560238957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_32_slots_16_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 135, 135, 1080, 135, 4320, 1080, 135, 135, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 4320, 4320, 135, 4320, 4320, 4320, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60750 . Total input tokens: 13511888 . Total output tokens: 11958887
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.813531968742609,
    "estimated_duration": 3600.0311092705347,
    "input_throughput": 1386.4557967698706,
    "output_throughput": 1229.7813173334168,
    "total_throughput": 2616.2371141032872,
    "itl": 25.253945605583226,
    "ttft": 5846.373723564508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.39610311364797,
    "arrivals": 20382,
    "finished_requests": 20351,
    "scheduler_time": 0.010953941613729834
}
#Debug simulation 
Total elapsed time: 1.8136545326560736. Arrivals time: 0.06157405860722065 Scheduler time: 1.4144387291744351 Scheduler overhead time: 0.1208618120290339 Adapter cache time: 0.03348146332427859 Engine time: 0.12290993239730597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.7799159968271852,
    "estimated_duration": 3599.9320108813413,
    "input_throughput": 1375.4365318659925,
    "output_throughput": 1211.5231584421604,
    "total_throughput": 2586.959690308153,
    "itl": 25.114974006170616,
    "ttft": 6010.369312343552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2761,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.256861337213692,
    "arrivals": 20177,
    "finished_requests": 20145,
    "scheduler_time": 0.010375306661367345
}
#Debug simulation 
Total elapsed time: 1.7800080268643796. Arrivals time: 0.06083138380199671 Scheduler time: 1.3870037798769772 Scheduler overhead time: 0.12209499953314662 Adapter cache time: 0.03217062400653958 Engine time: 0.117600467056036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7789287869818509,
    "estimated_duration": 3599.9171445478128,
    "input_throughput": 1375.2980419280989,
    "output_throughput": 1211.4373261631822,
    "total_throughput": 2586.735368091281,
    "itl": 25.119314150705325,
    "ttft": 6172.92655119542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2843,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.268819761075147,
    "arrivals": 20177,
    "finished_requests": 20144,
    "scheduler_time": 0.010409605977110685
}
#Debug simulation 
Total elapsed time: 1.7790263607166708. Arrivals time: 0.061142418533563614 Scheduler time: 1.3848100970499218 Scheduler overhead time: 0.12102320929989219 Adapter cache time: 0.0329825384542346 Engine time: 0.11897759046405554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7881264290772378,
    "estimated_duration": 3599.9184928786894,
    "input_throughput": 1375.297526817321,
    "output_throughput": 1211.4368724255892,
    "total_throughput": 2586.7343992429105,
    "itl": 25.124029936260452,
    "ttft": 6173.422543348711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2840,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.91930857643044,
    "arrivals": 20177,
    "finished_requests": 20144,
    "scheduler_time": 0.010411428221999638
}
#Debug simulation 
Total elapsed time: 1.7882203161716461. Arrivals time: 0.06173088354989886 Scheduler time: 1.3906859406270087 Scheduler overhead time: 0.1213743556290865 Adapter cache time: 0.0330851748585701 Engine time: 0.12074652500450611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 1.771608195733279,
    "estimated_duration": 3599.9357497967367,
    "input_throughput": 1375.2909340894614,
    "output_throughput": 1211.4310651922717,
    "total_throughput": 2586.721999281733,
    "itl": 25.103787251900354,
    "ttft": 6171.984209944506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2841,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 19.506893492821074,
    "arrivals": 20177,
    "finished_requests": 20144,
    "scheduler_time": 0.010359935265758685
}
#Debug simulation 
Total elapsed time: 1.7717161178588867. Arrivals time: 0.060996927320957184 Scheduler time: 1.3794485721737146 Scheduler overhead time: 0.12181824212893844 Adapter cache time: 0.03315666876733303 Engine time: 0.11602687183767557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 1.789899795781821,
    "estimated_duration": 3599.9140082736085,
    "input_throughput": 1375.299240098878,
    "output_throughput": 1211.4383815771803,
    "total_throughput": 2586.737621676058,
    "itl": 25.120213533245295,
    "ttft": 6172.864395697125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2839,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.64917740565746,
    "arrivals": 20177,
    "finished_requests": 20144,
    "scheduler_time": 0.010416643353555348
}
#Debug simulation 
Total elapsed time: 1.7899980540387332. Arrivals time: 0.06169159151613712 Scheduler time: 1.3943300759419799 Scheduler overhead time: 0.12109976029023528 Adapter cache time: 0.03298610961064696 Engine time: 0.11962659563869238 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.7770966161042452,
    "estimated_duration": 3599.9144338122533,
    "input_throughput": 1375.443247620878,
    "output_throughput": 1211.529073867832,
    "total_throughput": 2586.97232148871,
    "itl": 25.109542110376363,
    "ttft": 6010.04963009888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2762,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.632389637857056,
    "arrivals": 20177,
    "finished_requests": 20145,
    "scheduler_time": 0.009267035857909068
}
#Debug simulation 
Total elapsed time: 1.777194387279451. Arrivals time: 0.06091584265232086 Scheduler time: 1.3848862792365253 Scheduler overhead time: 0.1208114237524569 Adapter cache time: 0.03224417148157954 Engine time: 0.11809532344341278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_32_slots_16_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 66, 66, 1080, 66, 4320, 1080, 66, 66, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 4320, 4320, 66, 4320, 4320, 4320, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 60060 . Total input tokens: 13361522 . Total output tokens: 11813950
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7792688980698586,
    "estimated_duration": 3599.9272305622703,
    "input_throughput": 1375.2941887180073,
    "output_throughput": 1211.4339320461338,
    "total_throughput": 2586.728120764141,
    "itl": 25.118601705959584,
    "ttft": 6173.192303068042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2841,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 21.462199471127438,
    "arrivals": 20177,
    "finished_requests": 20144,
    "scheduler_time": 0.010408318689382776
}
#Debug simulation 
Total elapsed time: 1.7793935728259385. Arrivals time: 0.06112703634425998 Scheduler time: 1.3848869414068758 Scheduler overhead time: 0.12158747529610991 Adapter cache time: 0.032617251854389906 Engine time: 0.11875522881746292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [8]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.7711617089807987,
    "estimated_duration": 3599.166413496805,
    "input_throughput": 1386.0920632322488,
    "output_throughput": 1196.478157789917,
    "total_throughput": 2582.5702210221657,
    "itl": 25.014528503114356,
    "ttft": 6480.35127262572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.005589793999654,
    "arrivals": 20063,
    "finished_requests": 20028,
    "scheduler_time": 0.009262660509521209
}
#Debug simulation 
Total elapsed time: 1.7712581348605454. Arrivals time: 0.06096183881163597 Scheduler time: 1.3761455556377769 Scheduler overhead time: 0.12225286895409226 Adapter cache time: 0.032742585986852646 Engine time: 0.11871490115299821 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7516221376135945,
    "estimated_duration": 3599.147541674327,
    "input_throughput": 1385.8695544555535,
    "output_throughput": 1196.483875733723,
    "total_throughput": 2582.3534301892764,
    "itl": 25.036031188798884,
    "ttft": 6660.805169423606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.404185559101503,
    "arrivals": 20063,
    "finished_requests": 20027,
    "scheduler_time": 0.00937258622876734
}
#Debug simulation 
Total elapsed time: 1.751718265004456. Arrivals time: 0.060541167855262756 Scheduler time: 1.3600642988458276 Scheduler overhead time: 0.12154112011194229 Adapter cache time: 0.03258018009364605 Engine time: 0.1169925108551979 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7683987179771066,
    "estimated_duration": 3599.1548225447386,
    "input_throughput": 1385.8667509260774,
    "output_throughput": 1196.4814553199096,
    "total_throughput": 2582.348206245987,
    "itl": 25.04065077930419,
    "ttft": 6661.70973987262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2716,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.99989818503626,
    "arrivals": 20063,
    "finished_requests": 20027,
    "scheduler_time": 0.009380898297093966
}
#Debug simulation 
Total elapsed time: 1.7685054307803512. Arrivals time: 0.060559708159416914 Scheduler time: 1.3676562141627073 Scheduler overhead time: 0.12862724158912897 Adapter cache time: 0.03277141461148858 Engine time: 0.11837405245751143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [11 21]
---Simulation End---
#Simulation results
{
    "duration": 1.749143648892641,
    "estimated_duration": 3599.165817897108,
    "input_throughput": 1386.0922926065136,
    "output_throughput": 1196.4783557863595,
    "total_throughput": 2582.5706483928734,
    "itl": 25.019948868121556,
    "ttft": 6480.72166876687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.754979042807015,
    "arrivals": 20063,
    "finished_requests": 20028,
    "scheduler_time": 0.009303602039821973
}
#Debug simulation 
Total elapsed time: 1.7492449251003563. Arrivals time: 0.06070678262040019 Scheduler time: 1.3571828291751444 Scheduler overhead time: 0.12152163218706846 Adapter cache time: 0.0325607149861753 Engine time: 0.11693439306691289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [11 11 10]
---Simulation End---
#Simulation results
{
    "duration": 1.773406037595123,
    "estimated_duration": 3599.160688283429,
    "input_throughput": 1386.0942680998578,
    "output_throughput": 1196.4800610371867,
    "total_throughput": 2582.5743291370445,
    "itl": 25.038024176479748,
    "ttft": 6481.3125846516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2721,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.796346386834138,
    "arrivals": 20063,
    "finished_requests": 20028,
    "scheduler_time": 0.009390101294060267
}
#Debug simulation 
Total elapsed time: 1.77350290492177. Arrivals time: 0.061607171315699816 Scheduler time: 1.3706049658358097 Scheduler overhead time: 0.1282384847290814 Adapter cache time: 0.03282738337293267 Engine time: 0.11968608805909753 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [16]. Counts: [32]
---Simulation End---
#Simulation results
{
    "duration": 1.754020415712148,
    "estimated_duration": 3599.1655077730193,
    "input_throughput": 1386.0924120399234,
    "output_throughput": 1196.4784588815796,
    "total_throughput": 2582.5708709215032,
    "itl": 25.010052548418113,
    "ttft": 6480.138366214159,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 17.383416721174772,
    "arrivals": 20063,
    "finished_requests": 20028,
    "scheduler_time": 0.009243820847842914
}
#Debug simulation 
Total elapsed time: 1.7541286805644631. Arrivals time: 0.06046650232747197 Scheduler time: 1.3625834374688566 Scheduler overhead time: 0.12215334642678499 Adapter cache time: 0.03249658178538084 Engine time: 0.11594223091378808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 32,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_32_slots_16_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [10 11 11]
Adapter prompts. [4320, 1080, 33, 33, 1080, 33, 4320, 1080, 33, 33, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 4320, 4320, 33, 4320, 4320, 4320, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320]
Prompts retrieved: 59730 . Total input tokens: 13291934 . Total output tokens: 11750283
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [22 10]
---Simulation End---
#Simulation results
{
    "duration": 1.7508034673519433,
    "estimated_duration": 3599.1646662477146,
    "input_throughput": 1386.0927361239671,
    "output_throughput": 1196.4787386317419,
    "total_throughput": 2582.571474755709,
    "itl": 25.03500528402324,
    "ttft": 6481.295400823936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2718,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.567900630571007,
    "arrivals": 20063,
    "finished_requests": 20028,
    "scheduler_time": 0.009385838232390215
}
#Debug simulation 
Total elapsed time: 1.7509309290908277. Arrivals time: 0.060281693469733 Scheduler time: 1.3592893118038774 Scheduler overhead time: 0.12071452802047133 Adapter cache time: 0.03257587552070618 Engine time: 0.11776385782286525 
