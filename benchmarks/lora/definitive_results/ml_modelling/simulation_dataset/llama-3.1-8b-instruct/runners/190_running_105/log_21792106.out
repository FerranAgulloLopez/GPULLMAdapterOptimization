INFO 05-31 19:31:05 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:31:05 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_256_slots_16_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_256_slots_16_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 995791421 . Total output tokens: 876527782
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.55669038230553,
    "estimated_duration": 3600.0169277419873,
    "input_throughput": 7006.637887067125,
    "output_throughput": 6135.295873137346,
    "total_throughput": 13141.93376020447,
    "itl": 84.23753917479381,
    "ttft": 2063618.669837499,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 459,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4403438809560893,
    "arrivals": 1486681,
    "finished_requests": 102454,
    "scheduler_time": 308.11281349566195
}
#Debug simulation 
Total elapsed time: 82.55690467497334. Arrivals time: 0.5328415939584374 Scheduler time: 81.80037977267057 Scheduler overhead time: 0.08548636827617884 Adapter cache time: 0.01895507937297225 Engine time: 0.08493589097633958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_16_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_16_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 995791421 . Total output tokens: 876527782
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 82.6236771051772,
    "estimated_duration": 3600.0118375590264,
    "input_throughput": 7148.652049280391,
    "output_throughput": 6254.52804490419,
    "total_throughput": 13403.180094184581,
    "itl": 86.86043177364729,
    "ttft": 2052489.156081983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 483,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3097138329455587,
    "arrivals": 1486681,
    "finished_requests": 104536,
    "scheduler_time": 301.76403855153734
}
#Debug simulation 
Total elapsed time: 82.62387662334368. Arrivals time: 0.5231083272956312 Scheduler time: 81.87807615520433 Scheduler overhead time: 0.08671695692464709 Adapter cache time: 0.01895220810547471 Engine time: 0.08344949502497911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_256_slots_16_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_256_slots_16_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 995791421 . Total output tokens: 876527782
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 81.90202377503738,
    "estimated_duration": 3600.032488786204,
    "input_throughput": 6998.3587866160005,
    "output_throughput": 6129.467183625313,
    "total_throughput": 13127.825970241314,
    "itl": 83.70568039462901,
    "ttft": 2067915.5327777595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 457,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.399959383294945,
    "arrivals": 1486681,
    "finished_requests": 102329,
    "scheduler_time": 308.5302907167081
}
#Debug simulation 
Total elapsed time: 81.90218978980556. Arrivals time: 0.5355700762011111 Scheduler time: 81.14355398761109 Scheduler overhead time: 0.08555388357490301 Adapter cache time: 0.018940793350338936 Engine time: 0.08427244145423174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_16_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_16_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 995791421 . Total output tokens: 876527782
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 83.79674914991483,
    "estimated_duration": 3600.040709649532,
    "input_throughput": 7134.392655937599,
    "output_throughput": 6238.717506665771,
    "total_throughput": 13373.110162603369,
    "itl": 86.77816867411639,
    "ttft": 2052801.1740653825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 491,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.134505181820118,
    "arrivals": 1486681,
    "finished_requests": 104289,
    "scheduler_time": 302.5496965334244
}
#Debug simulation 
Total elapsed time: 83.79691719124094. Arrivals time: 0.5273035462014377 Scheduler time: 83.04734280705452 Scheduler overhead time: 0.08580008521676064 Adapter cache time: 0.018705934286117554 Engine time: 0.08388291951268911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_256_slots_16_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_256_slots_16_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 995791421 . Total output tokens: 876527782
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.1720632626675,
    "estimated_duration": 3600.0702126717642,
    "input_throughput": 6998.466005278755,
    "output_throughput": 6129.437676612309,
    "total_throughput": 13127.903681891064,
    "itl": 83.70581329571777,
    "ttft": 2067944.5362845603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 457,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.367856220919666,
    "arrivals": 1486681,
    "finished_requests": 102330,
    "scheduler_time": 308.5325621085085
}
#Debug simulation 
Total elapsed time: 82.1722324769944. Arrivals time: 0.5348196709528565 Scheduler time: 81.4129181150347 Scheduler overhead time: 0.08674815902486444 Adapter cache time: 0.01876585464924574 Engine time: 0.0846068155951798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.68287044391036,
    "estimated_duration": 3600.062054169158,
    "input_throughput": 7087.042283188703,
    "output_throughput": 6221.924695453461,
    "total_throughput": 13308.966978642164,
    "itl": 86.88204787718121,
    "ttft": 2054668.7593596175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 428,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8301110656745974,
    "arrivals": 1482883,
    "finished_requests": 103742,
    "scheduler_time": 303.62362277315043
}
#Debug simulation 
Total elapsed time: 86.6830382142216. Arrivals time: 0.5156241510994732 Scheduler time: 85.94809406995773 Scheduler overhead time: 0.08430616511031985 Adapter cache time: 0.018571109045296907 Engine time: 0.08266112534329295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.12074350006878,
    "estimated_duration": 3600.0601015777215,
    "input_throughput": 7026.725745193471,
    "output_throughput": 6161.565744493756,
    "total_throughput": 13188.291489687228,
    "itl": 84.88513579293047,
    "ttft": 2056755.3986244134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1257418819377243,
    "arrivals": 1482883,
    "finished_requests": 102719,
    "scheduler_time": 307.0704515137294
}
#Debug simulation 
Total elapsed time: 86.12101770425215. Arrivals time: 0.6918248310685158 Scheduler time: 85.20767438551411 Scheduler overhead time: 0.08574007824063301 Adapter cache time: 0.018338159192353487 Engine time: 0.08312420174479485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.59284016815946,
    "estimated_duration": 3600.092627669297,
    "input_throughput": 6934.729070058065,
    "output_throughput": 6088.794169218688,
    "total_throughput": 13023.523239276752,
    "itl": 83.0824828135901,
    "ttft": 2065841.9471401803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1948070949595664,
    "arrivals": 1482883,
    "finished_requests": 101480,
    "scheduler_time": 310.56852729669265
}
#Debug simulation 
Total elapsed time: 83.59301295131445. Arrivals time: 0.6668529589660466 Scheduler time: 82.70239557418972 Scheduler overhead time: 0.08647003397345543 Adapter cache time: 0.018352170009166002 Engine time: 0.08432209910824895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 86.33223256701604,
    "estimated_duration": 3600.058056933159,
    "input_throughput": 7026.843067511586,
    "output_throughput": 6161.63146515941,
    "total_throughput": 13188.474532670996,
    "itl": 84.88275241563925,
    "ttft": 2056751.4705268377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.930002724700604,
    "arrivals": 1482883,
    "finished_requests": 102721,
    "scheduler_time": 307.07971478875334
}
#Debug simulation 
Total elapsed time: 86.33240398997441. Arrivals time: 0.6804329557344317 Scheduler time: 85.43210966233164 Scheduler overhead time: 0.08507084054872394 Adapter cache time: 0.0183603479526937 Engine time: 0.0825122781097889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 83.90921000298113,
    "estimated_duration": 3600.0666127709997,
    "input_throughput": 6934.779181984005,
    "output_throughput": 6088.838168227068,
    "total_throughput": 13023.617350211072,
    "itl": 83.0821137931688,
    "ttft": 2065832.0749085916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1656035730568934,
    "arrivals": 1482883,
    "finished_requests": 101480,
    "scheduler_time": 310.5684797212012
}
#Debug simulation 
Total elapsed time: 83.9093809579499. Arrivals time: 0.5386454630643129 Scheduler time: 83.146957334131 Scheduler overhead time: 0.08674350706860423 Adapter cache time: 0.0184879950247705 Engine time: 0.08404537476599216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.87218246189877,
    "estimated_duration": 3600.030684378503,
    "input_throughput": 7027.092049457717,
    "output_throughput": 6161.775258265479,
    "total_throughput": 13188.867307723196,
    "itl": 84.87788956245195,
    "ttft": 2056774.8640229902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.725934241623606,
    "arrivals": 1482883,
    "finished_requests": 102723,
    "scheduler_time": 307.09732650666365
}
#Debug simulation 
Total elapsed time: 86.87235770560801. Arrivals time: 0.5365820522420108 Scheduler time: 86.11473497422412 Scheduler overhead time: 0.08523292141035199 Adapter cache time: 0.018218835350126028 Engine time: 0.08323644381016493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 993228341 . Total output tokens: 874275578
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 80.80678669689223,
    "estimated_duration": 3600.0206764899713,
    "input_throughput": 7077.69046061227,
    "output_throughput": 6211.291270082874,
    "total_throughput": 13288.981730695145,
    "itl": 84.66730167823751,
    "ttft": 2056679.045859421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.403837113361833,
    "arrivals": 1482883,
    "finished_requests": 103632,
    "scheduler_time": 304.18074787856415
}
#Debug simulation 
Total elapsed time: 80.80695328488946. Arrivals time: 0.5330211305990815 Scheduler time: 80.05048894463107 Scheduler overhead time: 0.0850989818572998 Adapter cache time: 0.018810609355568886 Engine time: 0.08563323086127639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.06219180393964,
    "estimated_duration": 3600.082727418943,
    "input_throughput": 7169.690241675468,
    "output_throughput": 6240.1107699283575,
    "total_throughput": 13409.801011603826,
    "itl": 86.17104318413841,
    "ttft": 2065762.0521129912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8367234747065475,
    "arrivals": 1481051,
    "finished_requests": 104145,
    "scheduler_time": 302.6925413260043
}
#Debug simulation 
Total elapsed time: 84.06236249906942. Arrivals time: 0.539779347833246 Scheduler time: 83.30370632978156 Scheduler overhead time: 0.08480527903884649 Adapter cache time: 0.01823612442240119 Engine time: 0.08232655841857195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.11385616380721,
    "estimated_duration": 3600.0560859174643,
    "input_throughput": 7204.493035943949,
    "output_throughput": 6272.794495712678,
    "total_throughput": 13477.287531656626,
    "itl": 86.6732818429963,
    "ttft": 2049671.000476544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 424,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1052018981427,
    "arrivals": 1481051,
    "finished_requests": 104621,
    "scheduler_time": 300.66886094370415
}
#Debug simulation 
Total elapsed time: 84.11403688183054. Arrivals time: 0.5393195319920778 Scheduler time: 83.35407491354272 Scheduler overhead time: 0.08472585072740912 Adapter cache time: 0.018321656621992588 Engine time: 0.08368269493803382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.66174014518037,
    "estimated_duration": 3600.071530728644,
    "input_throughput": 7114.617523951247,
    "output_throughput": 6203.864231408644,
    "total_throughput": 13318.48175535989,
    "itl": 84.75935896891554,
    "ttft": 2055296.837905472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1387425399432187,
    "arrivals": 1481051,
    "finished_requests": 103495,
    "scheduler_time": 304.5556586959863
}
#Debug simulation 
Total elapsed time: 84.66191745130345. Arrivals time: 0.5302885300479829 Scheduler time: 83.90742759406567 Scheduler overhead time: 0.08624924393370748 Adapter cache time: 0.018250994384288788 Engine time: 0.0852767932228744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 83.9201212739572,
    "estimated_duration": 3600.063359855082,
    "input_throughput": 7160.728971458358,
    "output_throughput": 6242.072084227037,
    "total_throughput": 13402.801055685393,
    "itl": 86.83568783308517,
    "ttft": 2049823.6232458553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9186231037927755,
    "arrivals": 1481051,
    "finished_requests": 104072,
    "scheduler_time": 302.2827944769188
}
#Debug simulation 
Total elapsed time: 83.92037706775591. Arrivals time: 0.5497776921838522 Scheduler time: 83.1491803759709 Scheduler overhead time: 0.08505357196554542 Adapter cache time: 0.018883340060710907 Engine time: 0.08378217183053493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 83.47284502815455,
    "estimated_duration": 3600.0311581822043,
    "input_throughput": 6972.355765019078,
    "output_throughput": 6074.649923597459,
    "total_throughput": 13047.005688616537,
    "itl": 82.68461828488655,
    "ttft": 2067493.497619599,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.964585786932169,
    "arrivals": 1481051,
    "finished_requests": 101331,
    "scheduler_time": 311.1099992355241
}
#Debug simulation 
Total elapsed time: 83.47302044695243. Arrivals time: 0.6270159250125289 Scheduler time: 82.62290197610855 Scheduler overhead time: 0.08598662400618196 Adapter cache time: 0.018064986914396286 Engine time: 0.08416650723665953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 81.98731651203707,
    "estimated_duration": 3600.059113184272,
    "input_throughput": 7215.755959357864,
    "output_throughput": 6283.306826034932,
    "total_throughput": 13499.062785392796,
    "itl": 86.69023480391967,
    "ttft": 2052203.8628079281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7706216882075996,
    "arrivals": 1481051,
    "finished_requests": 104837,
    "scheduler_time": 300.16389029792396
}
#Debug simulation 
Total elapsed time: 81.98748554615304. Arrivals time: 0.5299449237063527 Scheduler time: 81.23779026931152 Scheduler overhead time: 0.08524205628782511 Adapter cache time: 0.018332159146666527 Engine time: 0.08272650139406323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991920521 . Total output tokens: 873150813
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.78331041801721,
    "estimated_duration": 3600.057487328686,
    "input_throughput": 7036.828186540315,
    "output_throughput": 6126.338281438215,
    "total_throughput": 13163.16646797853,
    "itl": 83.56746104884385,
    "ttft": 2061898.4547558057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0246851754747546,
    "arrivals": 1481051,
    "finished_requests": 102207,
    "scheduler_time": 308.406524835212
}
#Debug simulation 
Total elapsed time: 83.78348075412214. Arrivals time: 0.5458658100105822 Scheduler time: 83.01154251117259 Scheduler overhead time: 0.08632809668779373 Adapter cache time: 0.019033539574593306 Engine time: 0.08607160160318017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 83.16115118609741,
    "estimated_duration": 3600.084564431627,
    "input_throughput": 7272.751384420226,
    "output_throughput": 6358.72674385751,
    "total_throughput": 13631.478128277735,
    "itl": 87.97740614653026,
    "ttft": 2037336.973670657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7507621572911956,
    "arrivals": 1480152,
    "finished_requests": 105916,
    "scheduler_time": 296.4533105178685
}
#Debug simulation 
Total elapsed time: 83.16132716508582. Arrivals time: 0.5517962439917028 Scheduler time: 82.39045149600133 Scheduler overhead time: 0.08444020198658109 Adapter cache time: 0.017791612539440393 Engine time: 0.08371247537434101 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.26639333367348,
    "estimated_duration": 3600.063271662392,
    "input_throughput": 7127.041683395765,
    "output_throughput": 6227.72610039353,
    "total_throughput": 13354.767783789295,
    "itl": 85.83265404878621,
    "ttft": 2051768.727958312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.822354494174946,
    "arrivals": 1480152,
    "finished_requests": 103798,
    "scheduler_time": 303.3088042707416
}
#Debug simulation 
Total elapsed time: 83.26655692979693. Arrivals time: 0.5302274189889431 Scheduler time: 82.51330515462905 Scheduler overhead time: 0.08666583895683289 Adapter cache time: 0.017831796780228615 Engine time: 0.08462593937292695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.17463464895263,
    "estimated_duration": 3600.023197323486,
    "input_throughput": 7079.443271073428,
    "output_throughput": 6191.376493510183,
    "total_throughput": 13270.81976458361,
    "itl": 84.3622026676243,
    "ttft": 2049100.185944058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 384,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.884193763374369,
    "arrivals": 1480152,
    "finished_requests": 103060,
    "scheduler_time": 305.4217865254468
}
#Debug simulation 
Total elapsed time: 83.1748023959808. Arrivals time: 0.5433860337361693 Scheduler time: 82.40900930063799 Scheduler overhead time: 0.08525006286799908 Adapter cache time: 0.018236054573208094 Engine time: 0.08434266783297062 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 83.62899427628145,
    "estimated_duration": 3600.005646110011,
    "input_throughput": 7244.685582141168,
    "output_throughput": 6336.292562387536,
    "total_throughput": 13580.978144528704,
    "itl": 87.01050863821904,
    "ttft": 2041609.3428950273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 404,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.762349228467789,
    "arrivals": 1480152,
    "finished_requests": 105601,
    "scheduler_time": 297.6677939211595
}
#Debug simulation 
Total elapsed time: 83.62926740432158. Arrivals time: 0.5484960600733757 Scheduler time: 82.86351115629077 Scheduler overhead time: 0.08260385366156697 Adapter cache time: 0.017977074719965458 Engine time: 0.0831401152536273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 83.17966172890738,
    "estimated_duration": 3600.0207777659643,
    "input_throughput": 7092.194344457149,
    "output_throughput": 6198.4064474893,
    "total_throughput": 13290.60079194645,
    "itl": 84.31123097438211,
    "ttft": 2054686.146852915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.874922366379778,
    "arrivals": 1480152,
    "finished_requests": 103291,
    "scheduler_time": 305.1012992815551
}
#Debug simulation 
Total elapsed time: 83.17983238911256. Arrivals time: 0.5263447389006615 Scheduler time: 82.42885842220858 Scheduler overhead time: 0.08630976499989629 Adapter cache time: 0.018295112065970898 Engine time: 0.08565598586574197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 82.253412283957,
    "estimated_duration": 3600.020642803709,
    "input_throughput": 7213.29697147959,
    "output_throughput": 6303.802186624678,
    "total_throughput": 13517.099158104267,
    "itl": 87.24294276985351,
    "ttft": 2041360.088033571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 406,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5918719018716256,
    "arrivals": 1480152,
    "finished_requests": 105097,
    "scheduler_time": 299.35351169009095
}
#Debug simulation 
Total elapsed time: 82.25358178373426. Arrivals time: 0.5508681265637279 Scheduler time: 81.48466986138374 Scheduler overhead time: 0.08398083737120032 Adapter cache time: 0.01782194059342146 Engine time: 0.08305918704718351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 991307680 . Total output tokens: 872597574
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 80.6228829016909,
    "estimated_duration": 3600.000095286683,
    "input_throughput": 7142.228144289103,
    "output_throughput": 6242.087890336708,
    "total_throughput": 13384.316034625812,
    "itl": 85.2619974365763,
    "ttft": 2045239.789218347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0205406179279284,
    "arrivals": 1480152,
    "finished_requests": 104053,
    "scheduler_time": 302.6392172314187
}
#Debug simulation 
Total elapsed time: 80.62305335979909. Arrivals time: 0.5253922664560378 Scheduler time: 79.87521654600278 Scheduler overhead time: 0.085643638856709 Adapter cache time: 0.018208255991339684 Engine time: 0.08445615880191326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.61210275627673,
    "estimated_duration": 3600.0562328119186,
    "input_throughput": 7017.571495061665,
    "output_throughput": 6159.497398372579,
    "total_throughput": 13177.068893434243,
    "itl": 83.36826687605455,
    "ttft": 2033377.1184368369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3987782424223103,
    "arrivals": 1357053,
    "finished_requests": 102836,
    "scheduler_time": 306.3159640682647
}
#Debug simulation 
Total elapsed time: 84.61227292008698. Arrivals time: 0.9608748015016317 Scheduler time: 83.43272170051932 Scheduler overhead time: 0.083823021966964 Adapter cache time: 0.019213763996958733 Engine time: 0.08202850352972746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.29143532505259,
    "estimated_duration": 3600.0522114962273,
    "input_throughput": 7068.201932944846,
    "output_throughput": 6209.673828788421,
    "total_throughput": 13277.875761733267,
    "itl": 84.42961089666662,
    "ttft": 2032414.927905388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 543,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.978722609882248,
    "arrivals": 1357053,
    "finished_requests": 103611,
    "scheduler_time": 304.25223303278347
}
#Debug simulation 
Total elapsed time: 82.29160510282964. Arrivals time: 0.5600759303197265 Scheduler time: 81.51201747916639 Scheduler overhead time: 0.08415071526542306 Adapter cache time: 0.019189572893083096 Engine time: 0.08267796272411942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 80.73729858500883,
    "estimated_duration": 3600.0053050389693,
    "input_throughput": 6907.367598929364,
    "output_throughput": 6066.931337414675,
    "total_throughput": 12974.29893634404,
    "itl": 80.79079955019917,
    "ttft": 2047384.215401261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 515,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.863554281173307,
    "arrivals": 1357053,
    "finished_requests": 101140,
    "scheduler_time": 311.9560016036588
}
#Debug simulation 
Total elapsed time: 80.73746660491452. Arrivals time: 0.5306535298004746 Scheduler time: 79.98585929302499 Scheduler overhead time: 0.08437241381034255 Adapter cache time: 0.019077181816101074 Engine time: 0.08317827479913831 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 82.94934980385005,
    "estimated_duration": 3600.072060465099,
    "input_throughput": 7058.993701564093,
    "output_throughput": 6204.720801370345,
    "total_throughput": 13263.714502934437,
    "itl": 84.4082622484216,
    "ttft": 2026295.8850533657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 527,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6114296689303544,
    "arrivals": 1357053,
    "finished_requests": 103527,
    "scheduler_time": 304.3391246676335
}
#Debug simulation 
Total elapsed time: 82.94961002189666. Arrivals time: 0.5503514558076859 Scheduler time: 82.1790456995368 Scheduler overhead time: 0.08447077637538314 Adapter cache time: 0.019151054322719574 Engine time: 0.0828706119209528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 82.76344056800008,
    "estimated_duration": 3600.047795805322,
    "input_throughput": 6882.270293430245,
    "output_throughput": 6054.927666626669,
    "total_throughput": 12937.197960056914,
    "itl": 80.52792433151319,
    "ttft": 2044119.781243008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 491,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.656266979975656,
    "arrivals": 1357053,
    "finished_requests": 100850,
    "scheduler_time": 312.7253480541408
}
#Debug simulation 
Total elapsed time: 82.76362857222557. Arrivals time: 0.9185327631421387 Scheduler time: 81.62383099412546 Scheduler overhead time: 0.08503834996372461 Adapter cache time: 0.018731357529759407 Engine time: 0.08320986246690154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 81.75093530397862,
    "estimated_duration": 3600.0118842643865,
    "input_throughput": 7069.039997127698,
    "output_throughput": 6210.1167214808365,
    "total_throughput": 13279.156718608536,
    "itl": 84.41835881400121,
    "ttft": 2032472.2698673387,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 544,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4728529916703543,
    "arrivals": 1357053,
    "finished_requests": 103620,
    "scheduler_time": 304.2861056564641
}
#Debug simulation 
Total elapsed time: 81.75111092207953. Arrivals time: 0.5603489186614752 Scheduler time: 80.97139292117208 Scheduler overhead time: 0.08372961357235909 Adapter cache time: 0.019166118931025267 Engine time: 0.0827308869920671 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 908767126 . Total output tokens: 799919075
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.96743381116539,
    "estimated_duration": 3600.0089382070514,
    "input_throughput": 6882.344578938654,
    "output_throughput": 6054.993022005187,
    "total_throughput": 12937.337600943842,
    "itl": 80.52739817061985,
    "ttft": 2044103.109636761,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 491,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6218855286576153,
    "arrivals": 1357053,
    "finished_requests": 100850,
    "scheduler_time": 312.72491715605787
}
#Debug simulation 
Total elapsed time: 81.96760156005621. Arrivals time: 0.5228537246584892 Scheduler time: 81.22338215913624 Scheduler overhead time: 0.08519497979432344 Adapter cache time: 0.018801943864673376 Engine time: 0.08299407130107284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.04775276919827,
    "estimated_duration": 3600.0965825669,
    "input_throughput": 6938.118582971611,
    "output_throughput": 6040.199339456447,
    "total_throughput": 12978.317922428057,
    "itl": 84.08322369549212,
    "ttft": 2037521.3435234085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.896235155994099,
    "arrivals": 1265213,
    "finished_requests": 101126,
    "scheduler_time": 312.5068546135779
}
#Debug simulation 
Total elapsed time: 85.04793574800715. Arrivals time: 0.5446854238398373 Scheduler time: 84.27986838063225 Scheduler overhead time: 0.08640230633318424 Adapter cache time: 0.018839477095752954 Engine time: 0.08387160673737526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.65102727105841,
    "estimated_duration": 3600.010848468128,
    "input_throughput": 7035.531576461091,
    "output_throughput": 6127.012925359877,
    "total_throughput": 13162.544501820968,
    "itl": 84.14454904520697,
    "ttft": 2008763.6862825851,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.751942273406317,
    "arrivals": 1265213,
    "finished_requests": 102298,
    "scheduler_time": 309.21178578159265
}
#Debug simulation 
Total elapsed time: 82.65119458083063. Arrivals time: 0.5281149749644101 Scheduler time: 81.90003407048061 Scheduler overhead time: 0.0856308713555336 Adapter cache time: 0.01893562125042081 Engine time: 0.08435573522001505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 80.3794786031358,
    "estimated_duration": 3600.052196476708,
    "input_throughput": 7013.268870020772,
    "output_throughput": 6084.054009393502,
    "total_throughput": 13097.322879414274,
    "itl": 82.71641196882884,
    "ttft": 2023855.935867429,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 520,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9073522608308457,
    "arrivals": 1265213,
    "finished_requests": 101944,
    "scheduler_time": 309.8302396584251
}
#Debug simulation 
Total elapsed time: 80.37964671384543. Arrivals time: 0.5369797036983073 Scheduler time: 79.61790656205267 Scheduler overhead time: 0.0865117497742176 Adapter cache time: 0.01946315262466669 Engine time: 0.08412938192486763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 84.03058877913281,
    "estimated_duration": 3600.0284036848343,
    "input_throughput": 6998.000064169919,
    "output_throughput": 6070.172940200255,
    "total_throughput": 13068.173004370174,
    "itl": 83.98942754090577,
    "ttft": 2022729.5541222729,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.317485974859442,
    "arrivals": 1265213,
    "finished_requests": 101807,
    "scheduler_time": 310.67308236785493
}
#Debug simulation 
Total elapsed time: 84.03086979221553. Arrivals time: 0.5449466784484684 Scheduler time: 83.26129108155146 Scheduler overhead time: 0.08779814885929227 Adapter cache time: 0.019086295273154974 Engine time: 0.083362627774477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 80.21232061600313,
    "estimated_duration": 3600.09473281255,
    "input_throughput": 7000.294400672985,
    "output_throughput": 6071.027465136989,
    "total_throughput": 13071.321865809974,
    "itl": 83.02661837122658,
    "ttft": 2021846.172199764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 527,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9205969257839364,
    "arrivals": 1265213,
    "finished_requests": 101793,
    "scheduler_time": 310.6095090532301
}
#Debug simulation 
Total elapsed time: 80.2124961190857. Arrivals time: 0.5175857972353697 Scheduler time: 79.47059567505494 Scheduler overhead time: 0.08625579718500376 Adapter cache time: 0.019054226577281952 Engine time: 0.08476738492026925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.3894637892954,
    "estimated_duration": 3600.019400175874,
    "input_throughput": 6999.143948715674,
    "output_throughput": 6070.810895889144,
    "total_throughput": 13069.95484460482,
    "itl": 83.98519048161822,
    "ttft": 2022798.1233928083,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0898177352361245,
    "arrivals": 1265213,
    "finished_requests": 101820,
    "scheduler_time": 310.692412661746
}
#Debug simulation 
Total elapsed time: 84.38963666930795. Arrivals time: 0.553384336642921 Scheduler time: 83.61137030879036 Scheduler overhead time: 0.08627634169533849 Adapter cache time: 0.01934495847672224 Engine time: 0.0847201538272202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 847323847 . Total output tokens: 746037437
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 79.73404809972271,
    "estimated_duration": 3600.057616658765,
    "input_throughput": 7000.366572852206,
    "output_throughput": 6071.090056687743,
    "total_throughput": 13071.456629539949,
    "itl": 83.02588067692517,
    "ttft": 2021832.9587302145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 527,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.884144302699749,
    "arrivals": 1265213,
    "finished_requests": 101793,
    "scheduler_time": 310.609351178638
}
#Debug simulation 
Total elapsed time: 79.73420918080956. Arrivals time: 0.5336089180782437 Scheduler time: 78.97292320150882 Scheduler overhead time: 0.08895121980458498 Adapter cache time: 0.019425416365265846 Engine time: 0.08507897472009063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 87.2685845149681,
    "estimated_duration": 3600.0040432228097,
    "input_throughput": 7024.260999818806,
    "output_throughput": 6118.709239082011,
    "total_throughput": 13142.970238900818,
    "itl": 85.08879924298576,
    "ttft": 2035844.9548145614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8367234747065475,
    "arrivals": 1250156,
    "finished_requests": 102534,
    "scheduler_time": 308.77139694980286
}
#Debug simulation 
Total elapsed time: 87.26874298090115. Arrivals time: 0.5016575381159782 Scheduler time: 86.54412521794438 Scheduler overhead time: 0.08572360221296549 Adapter cache time: 0.018609661608934402 Engine time: 0.0843454897403717 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.12461487296969,
    "estimated_duration": 3600.0818410700604,
    "input_throughput": 7054.232131692746,
    "output_throughput": 6150.908778623254,
    "total_throughput": 13205.140910316,
    "itl": 84.62379002306353,
    "ttft": 2033175.1895994532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3769435343239507,
    "arrivals": 1250156,
    "finished_requests": 103004,
    "scheduler_time": 306.78954638767163
}
#Debug simulation 
Total elapsed time: 83.12477565696463. Arrivals time: 0.499978213571012 Scheduler time: 82.39987625088543 Scheduler overhead time: 0.08696805266663432 Adapter cache time: 0.018761232495307922 Engine time: 0.08483078423887491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.40411915583536,
    "estimated_duration": 3600.051806295255,
    "input_throughput": 7008.984136249555,
    "output_throughput": 6105.844633002823,
    "total_throughput": 13114.828769252377,
    "itl": 83.21433114331846,
    "ttft": 2032699.2468386893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 474,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5580673687719084,
    "arrivals": 1250156,
    "finished_requests": 102332,
    "scheduler_time": 309.1620486638107
}
#Debug simulation 
Total elapsed time: 83.40428185882047. Arrivals time: 0.5050458926707506 Scheduler time: 82.67413684120402 Scheduler overhead time: 0.08666285080835223 Adapter cache time: 0.018968135584145784 Engine time: 0.08489773655310273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 84.43230943474919,
    "estimated_duration": 3600.04061381708,
    "input_throughput": 7046.428282680738,
    "output_throughput": 6158.390801178471,
    "total_throughput": 13204.819083859209,
    "itl": 84.87049416286112,
    "ttft": 2026338.523556039,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 458,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1376218206714794,
    "arrivals": 1250156,
    "finished_requests": 102847,
    "scheduler_time": 307.6187754404547
}
#Debug simulation 
Total elapsed time: 84.43255468085408. Arrivals time: 0.5102346981875598 Scheduler time: 83.69830864947289 Scheduler overhead time: 0.08655797457322478 Adapter cache time: 0.018763423897325993 Engine time: 0.08423140412196517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 83.53160191792995,
    "estimated_duration": 3600.009908652028,
    "input_throughput": 7009.065708224126,
    "output_throughput": 6105.915694057243,
    "total_throughput": 13114.98140228137,
    "itl": 83.21377440129268,
    "ttft": 2032678.9796094096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 474,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.525757089220015,
    "arrivals": 1250156,
    "finished_requests": 102332,
    "scheduler_time": 309.16136084765054
}
#Debug simulation 
Total elapsed time: 83.53176151681691. Arrivals time: 0.5170158031396568 Scheduler time: 82.78915866138414 Scheduler overhead time: 0.08689342578873038 Adapter cache time: 0.01895905379205942 Engine time: 0.08514123689383268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.338600280229,
    "estimated_duration": 3600.047985278138,
    "input_throughput": 7042.740292263395,
    "output_throughput": 6141.920077293553,
    "total_throughput": 13184.660369556947,
    "itl": 84.55554787495782,
    "ttft": 2023227.3570336709,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.879148344197298,
    "arrivals": 1250156,
    "finished_requests": 102720,
    "scheduler_time": 307.9295170750382
}
#Debug simulation 
Total elapsed time: 84.33876294037327. Arrivals time: 0.4931328487582505 Scheduler time: 83.62323628086597 Scheduler overhead time: 0.08575213653966784 Adapter cache time: 0.01832503406330943 Engine time: 0.08400944527238607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 837054003 . Total output tokens: 737056092
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.37321797339246,
    "estimated_duration": 3600.08045690972,
    "input_throughput": 7009.21723890039,
    "output_throughput": 6106.022146757608,
    "total_throughput": 13115.239385657998,
    "itl": 83.21327831052119,
    "ttft": 2032741.7180461467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 474,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4926183409616627,
    "arrivals": 1250156,
    "finished_requests": 102338,
    "scheduler_time": 309.16910643500023
}
#Debug simulation 
Total elapsed time: 83.37338394019753. Arrivals time: 0.5135930674150586 Scheduler time: 82.63201099308208 Scheduler overhead time: 0.08739440562203526 Adapter cache time: 0.019028004724532366 Engine time: 0.08590071042999625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.73292612703517,
    "estimated_duration": 3600.008282021838,
    "input_throughput": 7134.519975486044,
    "output_throughput": 6185.860769046118,
    "total_throughput": 13320.380744532162,
    "itl": 85.84235418464395,
    "ttft": 2031742.1925360924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 424,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8036614295467968,
    "arrivals": 1242527,
    "finished_requests": 103575,
    "scheduler_time": 305.9562827169536
}
#Debug simulation 
Total elapsed time: 86.73308662511408. Arrivals time: 0.5311377230100334 Scheduler time: 85.97994525311515 Scheduler overhead time: 0.08576357318088412 Adapter cache time: 0.018648428842425346 Engine time: 0.0837346469052136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.88613804988563,
    "estimated_duration": 3600.007252761691,
    "input_throughput": 7153.54253251688,
    "output_throughput": 6219.974968889952,
    "total_throughput": 13373.517501406832,
    "itl": 85.793347756064,
    "ttft": 2017816.0130585025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4313485277211333,
    "arrivals": 1242527,
    "finished_requests": 103897,
    "scheduler_time": 304.6781700415567
}
#Debug simulation 
Total elapsed time: 84.88629628391936. Arrivals time: 0.519901504740119 Scheduler time: 84.14479306619614 Scheduler overhead time: 0.08551088301464915 Adapter cache time: 0.0184175088070333 Engine time: 0.08383699506521225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.85463644284755,
    "estimated_duration": 3600.072963608544,
    "input_throughput": 7116.157438742862,
    "output_throughput": 6173.964034806944,
    "total_throughput": 13290.121473549807,
    "itl": 84.08693945449419,
    "ttft": 2026766.4651598579,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 430,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2386050746171033,
    "arrivals": 1242527,
    "finished_requests": 103359,
    "scheduler_time": 306.7627265017816
}
#Debug simulation 
Total elapsed time: 83.85479514114559. Arrivals time: 0.5202196128666401 Scheduler time: 83.11175023810938 Scheduler overhead time: 0.08503853436559439 Adapter cache time: 0.01865240465849638 Engine time: 0.08477995079010725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 84.14062786987051,
    "estimated_duration": 3600.031483354964,
    "input_throughput": 7174.610866438712,
    "output_throughput": 6222.253639605552,
    "total_throughput": 13396.864506044263,
    "itl": 85.79366077487185,
    "ttft": 2016643.7768656814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.119858278823081,
    "arrivals": 1242527,
    "finished_requests": 104115,
    "scheduler_time": 304.4980122192559
}
#Debug simulation 
Total elapsed time: 84.14086743909866. Arrivals time: 0.5213063568808138 Scheduler time: 83.39760615350679 Scheduler overhead time: 0.08529416099190712 Adapter cache time: 0.018774911295622587 Engine time: 0.08416164666414261 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 81.50805697077885,
    "estimated_duration": 3600.0294563193047,
    "input_throughput": 7126.980018166919,
    "output_throughput": 6188.21214390199,
    "total_throughput": 13315.192162068908,
    "itl": 84.48835740318991,
    "ttft": 2015738.0018876044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.524102007071515,
    "arrivals": 1242527,
    "finished_requests": 103426,
    "scheduler_time": 306.5373845827473
}
#Debug simulation 
Total elapsed time: 81.50821857200935. Arrivals time: 0.5052265981212258 Scheduler time: 80.7768957382068 Scheduler overhead time: 0.08750909054651856 Adapter cache time: 0.018508784007281065 Engine time: 0.08508895104750991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.43306770687923,
    "estimated_duration": 3600.073615137561,
    "input_throughput": 7112.064567885693,
    "output_throughput": 6157.521587000372,
    "total_throughput": 13269.586154886065,
    "itl": 85.42987752885462,
    "ttft": 2031430.5240111041,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.764237767267029,
    "arrivals": 1242527,
    "finished_requests": 103190,
    "scheduler_time": 306.8414746575412
}
#Debug simulation 
Total elapsed time: 86.43322981102392. Arrivals time: 0.5179963703267276 Scheduler time: 85.69381042057648 Scheduler overhead time: 0.08511469326913357 Adapter cache time: 0.018317898269742727 Engine time: 0.08376840222626925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831949930 . Total output tokens: 732524542
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.90551653923467,
    "estimated_duration": 3600.0162256465887,
    "input_throughput": 7057.5709684299145,
    "output_throughput": 6124.598784562396,
    "total_throughput": 13182.16975299231,
    "itl": 83.50039163109552,
    "ttft": 2025071.5633672343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.149447295516751,
    "arrivals": 1242527,
    "finished_requests": 102461,
    "scheduler_time": 309.2267806155488
}
#Debug simulation 
Total elapsed time: 84.90567496605217. Arrivals time: 0.6015405347570777 Scheduler time: 84.07892599329352 Scheduler overhead time: 0.08715904457494617 Adapter cache time: 0.01880928874015808 Engine time: 0.08437596401199698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.07916245795786,
    "estimated_duration": 3600.0262093063216,
    "input_throughput": 7163.8311780428385,
    "output_throughput": 6266.696876172763,
    "total_throughput": 13430.528054215602,
    "itl": 86.62485678456487,
    "ttft": 2012235.7505841937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.94252201921775,
    "arrivals": 1238735,
    "finished_requests": 104226,
    "scheduler_time": 302.0776225922911
}
#Debug simulation 
Total elapsed time: 84.0793202938512. Arrivals time: 0.5291148643009365 Scheduler time: 83.33209476061165 Scheduler overhead time: 0.08407181547954679 Adapter cache time: 0.01825510896742344 Engine time: 0.08226673677563667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.85888269264251,
    "estimated_duration": 3600.0068498211654,
    "input_throughput": 7122.798947250282,
    "output_throughput": 6214.347898007844,
    "total_throughput": 13337.146845258127,
    "itl": 85.87017613246977,
    "ttft": 2014627.3540203352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 440,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2170621800050183,
    "arrivals": 1238735,
    "finished_requests": 103560,
    "scheduler_time": 303.96891541415465
}
#Debug simulation 
Total elapsed time: 84.8590436168015. Arrivals time: 0.5244722571223974 Scheduler time: 84.11180627765134 Scheduler overhead time: 0.08627249440178275 Adapter cache time: 0.018190903589129448 Engine time: 0.08454762818291783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.90474660834298,
    "estimated_duration": 3600.0282779761337,
    "input_throughput": 7066.22171709748,
    "output_throughput": 6164.465189277917,
    "total_throughput": 13230.686906375397,
    "itl": 84.1000103682644,
    "ttft": 2030310.4825799554,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.204591974727832,
    "arrivals": 1238735,
    "finished_requests": 102707,
    "scheduler_time": 306.81027831429924
}
#Debug simulation 
Total elapsed time: 83.9049061969854. Arrivals time: 0.5091559500433505 Scheduler time: 83.17123767640442 Scheduler overhead time: 0.08678714046254754 Adapter cache time: 0.018321217503398657 Engine time: 0.084570805542171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 83.60261473758146,
    "estimated_duration": 3600.0548840190336,
    "input_throughput": 7144.014141053307,
    "output_throughput": 6251.249140645328,
    "total_throughput": 13395.263281698635,
    "itl": 85.89596692111512,
    "ttft": 2019576.7863054513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.998837634073567,
    "arrivals": 1238735,
    "finished_requests": 103949,
    "scheduler_time": 302.88347986720356
}
#Debug simulation 
Total elapsed time: 83.60285509563982. Arrivals time: 0.49720680620521307 Scheduler time: 82.88434384483844 Scheduler overhead time: 0.0853118528611958 Adapter cache time: 0.018218887969851494 Engine time: 0.08380709495395422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 83.8283030865714,
    "estimated_duration": 3600.057913758812,
    "input_throughput": 7096.924164012683,
    "output_throughput": 6179.7016972906595,
    "total_throughput": 13276.625861303342,
    "itl": 83.8897701445188,
    "ttft": 2032832.2746364211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.233574193324917,
    "arrivals": 1238735,
    "finished_requests": 103180,
    "scheduler_time": 305.5430197797003
}
#Debug simulation 
Total elapsed time: 83.82846340490505. Arrivals time: 0.5174514898099005 Scheduler time: 83.087639357429 Scheduler overhead time: 0.08580853184685111 Adapter cache time: 0.01862761750817299 Engine time: 0.08475608751177788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.0773392682895,
    "estimated_duration": 3600.017078294076,
    "input_throughput": 7144.089441983224,
    "output_throughput": 6251.358954848165,
    "total_throughput": 13395.448396831389,
    "itl": 85.89146107903656,
    "ttft": 2019489.2659703635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7961573719698816,
    "arrivals": 1238735,
    "finished_requests": 103950,
    "scheduler_time": 302.9008161349708
}
#Debug simulation 
Total elapsed time: 84.0774990702048. Arrivals time: 0.5194531599991024 Scheduler time: 83.33670261735097 Scheduler overhead time: 0.08535875845700502 Adapter cache time: 0.018462452571839094 Engine time: 0.08327790442854166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 829407116 . Total output tokens: 730232075
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.65845577511936,
    "estimated_duration": 3600.0559257857294,
    "input_throughput": 7078.484202835884,
    "output_throughput": 6172.709940651803,
    "total_throughput": 13251.194143487686,
    "itl": 84.17323074076329,
    "ttft": 2027856.6947456829,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0647509766742793,
    "arrivals": 1238735,
    "finished_requests": 102885,
    "scheduler_time": 306.71502081030434
}
#Debug simulation 
Total elapsed time: 84.65861629415303. Arrivals time: 0.5027868179604411 Scheduler time: 83.93260988220572 Scheduler overhead time: 0.08626793418079615 Adapter cache time: 0.01809996599331498 Engine time: 0.0845945836044848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.2860337710008,
    "estimated_duration": 3600.0573088047117,
    "input_throughput": 7195.635451870309,
    "output_throughput": 6275.007051901749,
    "total_throughput": 13470.642503772058,
    "itl": 87.39935458797851,
    "ttft": 2011881.2450853225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.863173110834348,
    "arrivals": 1236839,
    "finished_requests": 104689,
    "scheduler_time": 301.1305695336471
}
#Debug simulation 
Total elapsed time: 85.28620112594217. Arrivals time: 0.6404999839141965 Scheduler time: 84.42765779606998 Scheduler overhead time: 0.08380523277446628 Adapter cache time: 0.017736637964844704 Engine time: 0.08310530055314302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.3753475532867,
    "estimated_duration": 3600.0033441333608,
    "input_throughput": 7220.76079244804,
    "output_throughput": 6284.705828640438,
    "total_throughput": 13505.466621088477,
    "itl": 86.73895632815993,
    "ttft": 2009446.523091273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2809013894107237,
    "arrivals": 1236839,
    "finished_requests": 104998,
    "scheduler_time": 300.07777386204316
}
#Debug simulation 
Total elapsed time: 82.37551160017028. Arrivals time: 0.6747093382291496 Scheduler time: 81.48033915413544 Scheduler overhead time: 0.08453021477907896 Adapter cache time: 0.01844755234196782 Engine time: 0.08361266134306788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.43207945907488,
    "estimated_duration": 3600.0291990787878,
    "input_throughput": 7145.811485802062,
    "output_throughput": 6235.540257769091,
    "total_throughput": 13381.351743571153,
    "itl": 84.09641050461339,
    "ttft": 2019596.8139197968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 422,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1705788748385606,
    "arrivals": 1236839,
    "finished_requests": 104006,
    "scheduler_time": 303.10053476327886
}
#Debug simulation 
Total elapsed time: 83.43224146822467. Arrivals time: 0.6493740105070174 Scheduler time: 82.56101576099172 Scheduler overhead time: 0.08539090072736144 Adapter cache time: 0.01826114859431982 Engine time: 0.08472119411453605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 84.26922717690468,
    "estimated_duration": 3600.017472920563,
    "input_throughput": 7195.818130011508,
    "output_throughput": 6269.882624121937,
    "total_throughput": 13465.700754133444,
    "itl": 86.28901450170218,
    "ttft": 2019139.809944118,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0832168272742964,
    "arrivals": 1236839,
    "finished_requests": 104800,
    "scheduler_time": 300.8764377834463
}
#Debug simulation 
Total elapsed time: 84.26938718697056. Arrivals time: 0.6549485237337649 Scheduler time: 83.39278995385394 Scheduler overhead time: 0.08530233521014452 Adapter cache time: 0.01881919987499714 Engine time: 0.08381912391632795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 83.91384735982865,
    "estimated_duration": 3600.06287252789,
    "input_throughput": 7145.8271454954165,
    "output_throughput": 6235.583598082311,
    "total_throughput": 13381.410743577728,
    "itl": 84.0970065480077,
    "ttft": 2019572.745168605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 422,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1411682357592725,
    "arrivals": 1236839,
    "finished_requests": 104007,
    "scheduler_time": 303.10648283770246
}
#Debug simulation 
Total elapsed time: 83.91400975408033. Arrivals time: 0.677364953327924 Scheduler time: 83.01159322680905 Scheduler overhead time: 0.08853898430243134 Adapter cache time: 0.018122391775250435 Engine time: 0.08392848679795861 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 83.46207219921052,
    "estimated_duration": 3600.0629317548605,
    "input_throughput": 7243.574485873928,
    "output_throughput": 6302.206775296705,
    "total_throughput": 13545.781261170632,
    "itl": 86.5136626189441,
    "ttft": 2013651.8110163305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9366036326624325,
    "arrivals": 1236839,
    "finished_requests": 105390,
    "scheduler_time": 299.26203197244524
}
#Debug simulation 
Total elapsed time: 83.4622297459282. Arrivals time: 0.6625951970927417 Scheduler time: 82.58073051646352 Scheduler overhead time: 0.08392127463594079 Adapter cache time: 0.018400943372398615 Engine time: 0.08274371642619371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 828086356 . Total output tokens: 729057897
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.88304479373619,
    "estimated_duration": 3600.0303934521658,
    "input_throughput": 7145.891614356955,
    "output_throughput": 6235.639854827319,
    "total_throughput": 13381.531469184274,
    "itl": 84.09728971603911,
    "ttft": 2019558.95952761,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 422,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1119647138565987,
    "arrivals": 1236839,
    "finished_requests": 104007,
    "scheduler_time": 303.10622936174263
}
#Debug simulation 
Total elapsed time: 83.88321716198698. Arrivals time: 0.49983174446970224 Scheduler time: 83.16068448778242 Scheduler overhead time: 0.08594603231176734 Adapter cache time: 0.0183098204433918 Engine time: 0.0845087980851531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.91869036806747,
    "estimated_duration": 3600.099585074574,
    "input_throughput": 7193.026022768642,
    "output_throughput": 6263.838115337268,
    "total_throughput": 13456.86413810591,
    "itl": 85.53437106233733,
    "ttft": 2012992.0400151266,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7838242024509463,
    "arrivals": 1235881,
    "finished_requests": 105046,
    "scheduler_time": 301.0810757209572
}
#Debug simulation 
Total elapsed time: 85.9188461783342. Arrivals time: 0.5266228881664574 Scheduler time: 85.17388233589008 Scheduler overhead time: 0.08467484684661031 Adapter cache time: 0.017870353534817696 Engine time: 0.08231631852686405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 84.87366845132783,
    "estimated_duration": 3600.0598218470786,
    "input_throughput": 7102.498643170081,
    "output_throughput": 6174.38878796059,
    "total_throughput": 13276.88743113067,
    "itl": 84.5847792362924,
    "ttft": 2010357.7079259383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0005565742682703,
    "arrivals": 1235881,
    "finished_requests": 103555,
    "scheduler_time": 305.5428976274875
}
#Debug simulation 
Total elapsed time: 84.87381913000718. Arrivals time: 0.5177664547227323 Scheduler time: 84.13491585385054 Scheduler overhead time: 0.08518111566081643 Adapter cache time: 0.0181235927157104 Engine time: 0.08379326248541474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.90471773222089,
    "estimated_duration": 3600.00586988897,
    "input_throughput": 7180.957457938061,
    "output_throughput": 6254.840634661096,
    "total_throughput": 13435.798092599158,
    "itl": 84.63436040722036,
    "ttft": 2022571.5597846827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 430,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.230061042644109,
    "arrivals": 1235881,
    "finished_requests": 104838,
    "scheduler_time": 301.3433360799468
}
#Debug simulation 
Total elapsed time: 81.9048818810843. Arrivals time: 0.516782695427537 Scheduler time: 81.16762975556776 Scheduler overhead time: 0.08485141210258007 Adapter cache time: 0.018156360369175673 Engine time: 0.08365639112889767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 85.74904051190242,
    "estimated_duration": 3600.007660722211,
    "input_throughput": 7191.11886412111,
    "output_throughput": 6254.2136356129695,
    "total_throughput": 13445.33249973408,
    "itl": 85.62064766186798,
    "ttft": 2013989.729819233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8231381428055426,
    "arrivals": 1235881,
    "finished_requests": 104930,
    "scheduler_time": 301.52230612715215
}
#Debug simulation 
Total elapsed time: 85.74920564563945. Arrivals time: 0.5332721415907145 Scheduler time: 84.9933452247642 Scheduler overhead time: 0.08586687175557017 Adapter cache time: 0.018220926634967327 Engine time: 0.08432433241978288 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 82.12492750864476,
    "estimated_duration": 3600.0284707435317,
    "input_throughput": 7229.82421153586,
    "output_throughput": 6303.9262562589765,
    "total_throughput": 13533.750467794836,
    "itl": 85.6980002871532,
    "ttft": 2011135.4809552329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1230482964590425,
    "arrivals": 1235881,
    "finished_requests": 105563,
    "scheduler_time": 299.0021118529652
}
#Debug simulation 
Total elapsed time: 82.12508762069046. Arrivals time: 0.529864342417568 Scheduler time: 81.37528681801632 Scheduler overhead time: 0.08495434978976846 Adapter cache time: 0.017339061945676804 Engine time: 0.08399049378931522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 86.51049740193412,
    "estimated_duration": 3600.030851689585,
    "input_throughput": 7188.667004854733,
    "output_throughput": 6262.788272888964,
    "total_throughput": 13451.455277743697,
    "itl": 85.52053835630895,
    "ttft": 2014358.467770544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 408,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6046397437527666,
    "arrivals": 1235881,
    "finished_requests": 104993,
    "scheduler_time": 301.21189258196506
}
#Debug simulation 
Total elapsed time: 86.51066805794835. Arrivals time: 0.5082290987484157 Scheduler time: 85.78545084409416 Scheduler overhead time: 0.08418908109888434 Adapter cache time: 0.017257195431739092 Engine time: 0.08218512404710054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 827474156 . Total output tokens: 728492685
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.75832322100177,
    "estimated_duration": 3600.0766016455923,
    "input_throughput": 7175.178991522731,
    "output_throughput": 6258.442386948421,
    "total_throughput": 13433.62137847115,
    "itl": 84.99732407524843,
    "ttft": 2016090.7294917013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0774855106324193,
    "arrivals": 1235881,
    "finished_requests": 104755,
    "scheduler_time": 301.7370773715564
}
#Debug simulation 
Total elapsed time: 81.75847810320556. Arrivals time: 0.5279386923648417 Scheduler time: 81.0103551321663 Scheduler overhead time: 0.08580082561820745 Adapter cache time: 0.017118846531957388 Engine time: 0.083778306376189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.15927862888202,
    "estimated_duration": 3600.0359286914413,
    "input_throughput": 7001.192904527892,
    "output_throughput": 6146.183659906595,
    "total_throughput": 13147.376564434488,
    "itl": 85.80136822216478,
    "ttft": 2013884.4967469748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.777211793418996,
    "arrivals": 1142437,
    "finished_requests": 102148,
    "scheduler_time": 308.3225695358325
}
#Debug simulation 
Total elapsed time: 85.1594420587644. Arrivals time: 0.5799874700605869 Scheduler time: 84.36427164729685 Scheduler overhead time: 0.08373087039217353 Adapter cache time: 0.01695865485817194 Engine time: 0.0814382741227746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 80.24157385993749,
    "estimated_duration": 3600.055604301252,
    "input_throughput": 6984.786837724581,
    "output_throughput": 6109.5948000695025,
    "total_throughput": 13094.381637794084,
    "itl": 84.37576352879394,
    "ttft": 2000079.1930292563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 531,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8799040230223967,
    "arrivals": 1142437,
    "finished_requests": 101840,
    "scheduler_time": 309.28881396871407
}
#Debug simulation 
Total elapsed time: 80.2417307710275. Arrivals time: 0.5425591571256518 Scheduler time: 79.48823365662247 Scheduler overhead time: 0.08153336634859443 Adapter cache time: 0.016975687351077795 Engine time: 0.07956638606265187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.31814730726182,
    "estimated_duration": 3600.005722584743,
    "input_throughput": 6841.360791592531,
    "output_throughput": 5981.452991841213,
    "total_throughput": 12822.813783433745,
    "itl": 81.56229513443014,
    "ttft": 2036076.3550392184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0758427593484727,
    "arrivals": 1142437,
    "finished_requests": 99644,
    "scheduler_time": 316.16922253950344
}
#Debug simulation 
Total elapsed time: 82.31830671103671. Arrivals time: 0.4176347805187106 Scheduler time: 81.68772119330242 Scheduler overhead time: 0.08225713251158595 Adapter cache time: 0.016314951237291098 Engine time: 0.08072981052100658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 83.83533934317529,
    "estimated_duration": 3600.0160937364494,
    "input_throughput": 6997.170385939977,
    "output_throughput": 6103.642435996459,
    "total_throughput": 13100.812821936437,
    "itl": 84.40352770007394,
    "ttft": 2029312.539500748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8578341894550214,
    "arrivals": 1142437,
    "finished_requests": 101907,
    "scheduler_time": 309.17857723167924
}
#Debug simulation 
Total elapsed time: 83.83555573504418. Arrivals time: 0.5484470212832093 Scheduler time: 83.07652323506773 Scheduler overhead time: 0.08171981247141957 Adapter cache time: 0.016484253108501434 Engine time: 0.07948072673752904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 81.93819616129622,
    "estimated_duration": 3600.024534669504,
    "input_throughput": 6841.325319540088,
    "output_throughput": 5981.468679622999,
    "total_throughput": 12822.793999163086,
    "itl": 81.56179165528891,
    "ttft": 2036033.9059591815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0474677061522586,
    "arrivals": 1142437,
    "finished_requests": 99645,
    "scheduler_time": 316.1703949715603
}
#Debug simulation 
Total elapsed time: 81.93835158506408. Arrivals time: 0.5326300808228552 Scheduler time: 81.19278232939541 Scheduler overhead time: 0.08296239841729403 Adapter cache time: 0.01626481208950281 Engine time: 0.08051133994013071 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.07169324578717,
    "estimated_duration": 3600.0184621256312,
    "input_throughput": 6997.666057836143,
    "output_throughput": 6104.142306821628,
    "total_throughput": 13101.80836465777,
    "itl": 84.40059752076145,
    "ttft": 2029345.5069481703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.662095032217901,
    "arrivals": 1142437,
    "finished_requests": 101916,
    "scheduler_time": 309.1938026196239
}
#Debug simulation 
Total elapsed time: 84.07184710074216. Arrivals time: 0.4220020095817745 Scheduler time: 83.44121625926346 Scheduler overhead time: 0.080498983617872 Adapter cache time: 0.016108736395835876 Engine time: 0.07928214827552438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 765344756 . Total output tokens: 673735285
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.01740654092282,
    "estimated_duration": 3600.068767705936,
    "input_throughput": 6841.241262092403,
    "output_throughput": 5981.395186992971,
    "total_throughput": 12822.636449085374,
    "itl": 81.56183428532616,
    "ttft": 2036014.3096684364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0186784186028146,
    "arrivals": 1142437,
    "finished_requests": 99645,
    "scheduler_time": 316.17333335886144
}
#Debug simulation 
Total elapsed time: 82.01755777187645. Arrivals time: 0.5390317016281188 Scheduler time: 81.26565519953147 Scheduler overhead time: 0.08405957231298089 Adapter cache time: 0.015991322696208954 Engine time: 0.07956405123695731 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 82.07530222041532,
    "estimated_duration": 3600.008028062984,
    "input_throughput": 7096.910284877012,
    "output_throughput": 6230.877493923064,
    "total_throughput": 13327.787778800075,
    "itl": 86.36202318659076,
    "ttft": 2005468.1550432323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9226847921218995,
    "arrivals": 1127186,
    "finished_requests": 103667,
    "scheduler_time": 303.6059484581356
}
#Debug simulation 
Total elapsed time: 82.07546649919823. Arrivals time: 0.42576661380007863 Scheduler time: 81.44209291925654 Scheduler overhead time: 0.08124536229297519 Adapter cache time: 0.016211872920393944 Engine time: 0.07805268233641982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.83051480911672,
    "estimated_duration": 3600.011548692242,
    "input_throughput": 7067.845937672903,
    "output_throughput": 6200.520386693976,
    "total_throughput": 13268.36632436688,
    "itl": 85.06787604951066,
    "ttft": 2004235.825876847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 457,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3464121505944107,
    "arrivals": 1127186,
    "finished_requests": 103191,
    "scheduler_time": 305.0917797586839
}
#Debug simulation 
Total elapsed time: 81.83066468499601. Arrivals time: 0.42537238961085677 Scheduler time: 81.19760804576799 Scheduler overhead time: 0.08068064646795392 Adapter cache time: 0.016267291270196438 Engine time: 0.07810805598273873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.82575746579096,
    "estimated_duration": 3600.0555437695207,
    "input_throughput": 7022.79720204745,
    "output_throughput": 6162.34825554133,
    "total_throughput": 13185.14545758878,
    "itl": 83.40859574685844,
    "ttft": 2008409.110701822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3703039441863334,
    "arrivals": 1127186,
    "finished_requests": 102489,
    "scheduler_time": 307.1899993837857
}
#Debug simulation 
Total elapsed time: 81.8259184518829. Arrivals time: 0.7452163430862129 Scheduler time: 80.87114267982543 Scheduler overhead time: 0.08165653934702277 Adapter cache time: 0.01630418375134468 Engine time: 0.07876677438616753 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 82.78920927317813,
    "estimated_duration": 3600.019940941492,
    "input_throughput": 7056.577023669565,
    "output_throughput": 6178.6499421952785,
    "total_throughput": 13235.226965864844,
    "itl": 85.05563398692121,
    "ttft": 2007905.3151462988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1653767624543936,
    "arrivals": 1127186,
    "finished_requests": 103101,
    "scheduler_time": 304.9069346306444
}
#Debug simulation 
Total elapsed time: 82.7894068332389. Arrivals time: 0.43174863047897816 Scheduler time: 82.14865047251806 Scheduler overhead time: 0.08090975508093834 Adapter cache time: 0.01663835719227791 Engine time: 0.07884397869929671 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 82.1089239101857,
    "estimated_duration": 3600.0280928388593,
    "input_throughput": 7022.8507522737455,
    "output_throughput": 6162.395244673168,
    "total_throughput": 13185.245996946913,
    "itl": 83.40809805355983,
    "ttft": 2008400.1452850702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3384078989876684,
    "arrivals": 1127186,
    "finished_requests": 102489,
    "scheduler_time": 307.19009585578664
}
#Debug simulation 
Total elapsed time: 82.10906941024587. Arrivals time: 0.4152942467480898 Scheduler time: 81.48345436062664 Scheduler overhead time: 0.08147135935723782 Adapter cache time: 0.016573015600442886 Engine time: 0.0792931392788887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 83.16756059415638,
    "estimated_duration": 3600.0498401708637,
    "input_throughput": 7057.142575224511,
    "output_throughput": 6179.228618386071,
    "total_throughput": 13236.371193610581,
    "itl": 85.05190499691098,
    "ttft": 2007985.3523298863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 464,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9621393164247145,
    "arrivals": 1127186,
    "finished_requests": 103108,
    "scheduler_time": 304.92673148724487
}
#Debug simulation 
Total elapsed time: 83.16770863113925. Arrivals time: 0.7566946460865438 Scheduler time: 82.2021979172714 Scheduler overhead time: 0.08063727477565408 Adapter cache time: 0.01685945549979806 Engine time: 0.07863681158050895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 755082386 . Total output tokens: 664728858
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.35778484027833,
    "estimated_duration": 3600.043479125622,
    "input_throughput": 7022.962957697589,
    "output_throughput": 6162.401406715307,
    "total_throughput": 13185.364364412897,
    "itl": 83.4049421481797,
    "ttft": 2008468.4825889408,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3075474396720774,
    "arrivals": 1127186,
    "finished_requests": 102493,
    "scheduler_time": 307.1956188303449
}
#Debug simulation 
Total elapsed time: 81.35793333407491. Arrivals time: 0.42486608028411865 Scheduler time: 80.72112545417622 Scheduler overhead time: 0.08245561271905899 Adapter cache time: 0.016483379993587732 Engine time: 0.07995786052197218 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 82.39344296883792,
    "estimated_duration": 3600.0043952114906,
    "input_throughput": 7032.8572469736155,
    "output_throughput": 6191.438829810253,
    "total_throughput": 13224.296076783869,
    "itl": 86.32460895352914,
    "ttft": 1995974.9020746297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8830103379301986,
    "arrivals": 1119422,
    "finished_requests": 103019,
    "scheduler_time": 305.7192649607714
}
#Debug simulation 
Total elapsed time: 82.39358524093404. Arrivals time: 0.4293035422451794 Scheduler time: 81.75697426078841 Scheduler overhead time: 0.08033078908920288 Adapter cache time: 0.016093938145786524 Engine time: 0.0780535452067852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 82.82312291208655,
    "estimated_duration": 3600.0362667630366,
    "input_throughput": 6926.668275600838,
    "output_throughput": 6090.624475768164,
    "total_throughput": 13017.292751369003,
    "itl": 84.56135042770174,
    "ttft": 2019760.905998692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.954754759832289,
    "arrivals": 1119422,
    "finished_requests": 101348,
    "scheduler_time": 310.7860156192565
}
#Debug simulation 
Total elapsed time: 82.82326571922749. Arrivals time: 0.415160589851439 Scheduler time: 82.199168195948 Scheduler overhead time: 0.08132638316601515 Adapter cache time: 0.01617724308744073 Engine time: 0.07865741616114974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 81.86144912801683,
    "estimated_duration": 3600.013587410471,
    "input_throughput": 7030.791241598185,
    "output_throughput": 6183.131663125583,
    "total_throughput": 13213.922904723768,
    "itl": 84.07667957156829,
    "ttft": 2003117.4046992543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2254025820596377,
    "arrivals": 1119422,
    "finished_requests": 103021,
    "scheduler_time": 305.9363707998632
}
#Debug simulation 
Total elapsed time: 81.86159320315346. Arrivals time: 0.43092191126197577 Scheduler time: 81.2196094924584 Scheduler overhead time: 0.08216116949915886 Adapter cache time: 0.01624539354816079 Engine time: 0.07966392673552036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 84.35788097279146,
    "estimated_duration": 3600.0032320083037,
    "input_throughput": 6919.638232131048,
    "output_throughput": 6086.226202573993,
    "total_throughput": 13005.864434705041,
    "itl": 84.34960129539814,
    "ttft": 2024226.4675999796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.66742145140655,
    "arrivals": 1119422,
    "finished_requests": 101295,
    "scheduler_time": 311.14308191685717
}
#Debug simulation 
Total elapsed time: 84.3580727907829. Arrivals time: 0.4279428110457957 Scheduler time: 83.71702954452485 Scheduler overhead time: 0.08382971491664648 Adapter cache time: 0.016260862816125154 Engine time: 0.07986965123564005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 80.35547134280205,
    "estimated_duration": 3600.0463898435405,
    "input_throughput": 6981.643089630394,
    "output_throughput": 6137.213415452745,
    "total_throughput": 13118.856505083138,
    "itl": 83.68063190980953,
    "ttft": 2004041.8916050429,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.334938417216788,
    "arrivals": 1119422,
    "finished_requests": 102255,
    "scheduler_time": 308.47359862161943
}
#Debug simulation 
Total elapsed time: 80.35561120789498. Arrivals time: 0.5059761558659375 Scheduler time: 79.63772353576496 Scheduler overhead time: 0.0824174634180963 Adapter cache time: 0.016538230702280998 Engine time: 0.07993496907874942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 82.90054448368028,
    "estimated_duration": 3600.0014737616357,
    "input_throughput": 7077.437935984303,
    "output_throughput": 6222.890508039641,
    "total_throughput": 13300.328444023944,
    "itl": 85.89763487640157,
    "ttft": 2004658.0272693902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 432,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7578538463264586,
    "arrivals": 1119422,
    "finished_requests": 103607,
    "scheduler_time": 304.2021291617998
}
#Debug simulation 
Total elapsed time: 82.90068630687892. Arrivals time: 0.42791288206353784 Scheduler time: 82.26398273045197 Scheduler overhead time: 0.08155790157616138 Adapter cache time: 0.01612114394083619 Engine time: 0.07808756083250046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749939701 . Total output tokens: 660206521
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 80.09905065922067,
    "estimated_duration": 3600.0008497807453,
    "input_throughput": 6981.616407543557,
    "output_throughput": 6139.17188418054,
    "total_throughput": 13120.788291724097,
    "itl": 83.73370144613871,
    "ttft": 1999291.3235912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 468,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4574181175232206,
    "arrivals": 1119422,
    "finished_requests": 102249,
    "scheduler_time": 308.4936852670789
}
#Debug simulation 
Total elapsed time: 80.09919039113447. Arrivals time: 0.4369359062984586 Scheduler time: 79.44907314144075 Scheduler overhead time: 0.08316470263525844 Adapter cache time: 0.016762772109359503 Engine time: 0.07995398622006178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 87.18807418039069,
    "estimated_duration": 3600.1038430646,
    "input_throughput": 7027.748949169966,
    "output_throughput": 6151.649498295486,
    "total_throughput": 13179.398447465454,
    "itl": 85.81008457318495,
    "ttft": 2012570.0491733043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5656147043965913,
    "arrivals": 1115519,
    "finished_requests": 103027,
    "scheduler_time": 306.92623296462443
}
#Debug simulation 
Total elapsed time: 87.18821551138535. Arrivals time: 0.4327886337414384 Scheduler time: 86.54627205943689 Scheduler overhead time: 0.0816107471473515 Adapter cache time: 0.01607026718556881 Engine time: 0.07867273082956672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 86.35926427179947,
    "estimated_duration": 3600.054333354308,
    "input_throughput": 7016.208273852766,
    "output_throughput": 6162.610045756923,
    "total_throughput": 13178.818319609689,
    "itl": 84.5904039593372,
    "ttft": 2008198.0019636857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 408,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.989176953360442,
    "arrivals": 1115519,
    "finished_requests": 103034,
    "scheduler_time": 306.4963968369664
}
#Debug simulation 
Total elapsed time: 86.35940452758223. Arrivals time: 0.44395238999277353 Scheduler time: 85.70512553909793 Scheduler overhead time: 0.08189080143347383 Adapter cache time: 0.016522353049367666 Engine time: 0.07903081411495805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.19583412306383,
    "estimated_duration": 3600.0351819677107,
    "input_throughput": 7036.195681330759,
    "output_throughput": 6170.3888648828315,
    "total_throughput": 13206.584546213591,
    "itl": 83.88068435778567,
    "ttft": 1995087.811427453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2780495242867858,
    "arrivals": 1115519,
    "finished_requests": 103193,
    "scheduler_time": 306.0588778914327
}
#Debug simulation 
Total elapsed time: 83.19598025595769. Arrivals time: 0.4343110783956945 Scheduler time: 82.5497358739376 Scheduler overhead time: 0.08233730494976044 Adapter cache time: 0.01681956835091114 Engine time: 0.07979694660753012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 85.3842025171034,
    "estimated_duration": 3600.0348353362224,
    "input_throughput": 6951.091904548998,
    "output_throughput": 6106.615909439337,
    "total_throughput": 13057.707813988334,
    "itl": 84.32583902357335,
    "ttft": 1993556.4358574422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8117585218977146,
    "arrivals": 1115519,
    "finished_requests": 102046,
    "scheduler_time": 309.64099989229453
}
#Debug simulation 
Total elapsed time: 85.38439137581736. Arrivals time: 0.4408896374516189 Scheduler time: 84.73116482468322 Scheduler overhead time: 0.08314635884016752 Adapter cache time: 0.01650982117280364 Engine time: 0.07971752109006047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 84.24391802493483,
    "estimated_duration": 3600.044717787571,
    "input_throughput": 7005.405481601619,
    "output_throughput": 6140.647056624826,
    "total_throughput": 13146.052538226444,
    "itl": 83.75132525781723,
    "ttft": 1998120.989685647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9694513646932506,
    "arrivals": 1115519,
    "finished_requests": 102772,
    "scheduler_time": 307.2578098646215
}
#Debug simulation 
Total elapsed time: 84.24406875297427. Arrivals time: 0.4390903110615909 Scheduler time: 83.59118546172976 Scheduler overhead time: 0.08346025226637721 Adapter cache time: 0.01662601763382554 Engine time: 0.08053151238709688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 85.89979990618303,
    "estimated_duration": 3600.006940109449,
    "input_throughput": 6953.993538478816,
    "output_throughput": 6110.072387619136,
    "total_throughput": 13064.065926097952,
    "itl": 84.06121735648254,
    "ttft": 2001250.4560589576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5408005343470617,
    "arrivals": 1115519,
    "finished_requests": 101971,
    "scheduler_time": 310.0360759556124
}
#Debug simulation 
Total elapsed time: 85.89994585700333. Arrivals time: 0.449510729406029 Scheduler time: 85.23854927625507 Scheduler overhead time: 0.08230749005451798 Adapter cache time: 0.016698217950761318 Engine time: 0.07988722017034888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_256_slots_16_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 747396121 . Total output tokens: 657951537
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 85.1688303668052,
    "estimated_duration": 3600.0461857684854,
    "input_throughput": 7060.5385843332115,
    "output_throughput": 6199.571296676495,
    "total_throughput": 13260.109881009706,
    "itl": 84.04177769324228,
    "ttft": 1999390.6598376988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.01417335094886,
    "arrivals": 1115519,
    "finished_requests": 103536,
    "scheduler_time": 304.9175599448852
}
#Debug simulation 
Total elapsed time: 85.16896921116859. Arrivals time: 0.4447955023497343 Scheduler time: 84.51041097892448 Scheduler overhead time: 0.08316380018368363 Adapter cache time: 0.01656156685203314 Engine time: 0.08084777323529124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 746088587 . Total output tokens: 656766672
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 84.86273358343169,
    "estimated_duration": 3600.007366743502,
    "input_throughput": 7199.687211597505,
    "output_throughput": 6305.462652576104,
    "total_throughput": 13505.149864173609,
    "itl": 86.72670166092936,
    "ttft": 1988076.8059402658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.863173110834348,
    "arrivals": 1113643,
    "finished_requests": 105255,
    "scheduler_time": 299.83897823628
}
#Debug simulation 
Total elapsed time: 84.86288731405511. Arrivals time: 0.46651726588606834 Scheduler time: 84.18739616684616 Scheduler overhead time: 0.08163950685411692 Adapter cache time: 0.016299578826874495 Engine time: 0.07867468846961856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_256_slots_16_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 746088587 . Total output tokens: 656766672
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 83.97596577135846,
    "estimated_duration": 3600.0720256662207,
    "input_throughput": 7191.498618755794,
    "output_throughput": 6282.93929641982,
    "total_throughput": 13474.437915175613,
    "itl": 86.22111120688977,
    "ttft": 1998245.1304873796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.019151153163988,
    "arrivals": 1113643,
    "finished_requests": 105156,
    "scheduler_time": 299.8882604723181
}
#Debug simulation 
Total elapsed time: 83.97611715644598. Arrivals time: 0.4526066817343235 Scheduler time: 83.31654039956629 Scheduler overhead time: 0.08084334060549736 Adapter cache time: 0.01601629238575697 Engine time: 0.07788778003305197 
