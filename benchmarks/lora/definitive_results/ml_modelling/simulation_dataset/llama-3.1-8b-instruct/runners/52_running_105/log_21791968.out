INFO 05-31 19:30:52 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 05-31 19:30:52 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 104.38618585304357,
    "estimated_duration": 3600.0358671623694,
    "input_throughput": 7263.384023062748,
    "output_throughput": 6335.343547001204,
    "total_throughput": 13598.727570063951,
    "itl": 82.84998365875211,
    "ttft": 1369750.8647921006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 147,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9720241276966397,
    "arrivals": 186986,
    "finished_requests": 105746,
    "scheduler_time": 244.13828429236935
}
#Debug simulation 
Total elapsed time: 104.38641685503535. Arrivals time: 0.4823551083682105 Scheduler time: 103.66172165062744 Scheduler overhead time: 0.09682599524967372 Adapter cache time: 0.016360957291908562 Engine time: 0.09206154965795577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 109.50757311598863,
    "estimated_duration": 3600.023235018453,
    "input_throughput": 7188.382493833364,
    "output_throughput": 6272.722848102465,
    "total_throughput": 13461.105341935829,
    "itl": 81.75431143258325,
    "ttft": 1362208.340781242,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.610750348023141,
    "arrivals": 186986,
    "finished_requests": 104703,
    "scheduler_time": 246.878072064787
}
#Debug simulation 
Total elapsed time: 109.50777282007039. Arrivals time: 0.49846099968999624 Scheduler time: 108.75887824804522 Scheduler overhead time: 0.09839062567334622 Adapter cache time: 0.018053746316581964 Engine time: 0.09609033272136003 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 110.37147117301356,
    "estimated_duration": 3600.050238469654,
    "input_throughput": 7126.603047324742,
    "output_throughput": 6216.754077719145,
    "total_throughput": 13343.357125043887,
    "itl": 79.81584347725212,
    "ttft": 1364756.9267310554,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 203,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5215352536272289,
    "arrivals": 186986,
    "finished_requests": 103745,
    "scheduler_time": 249.641152972873
}
#Debug simulation 
Total elapsed time: 110.3716417059768. Arrivals time: 0.4952372573316097 Scheduler time: 109.62662269547582 Scheduler overhead time: 0.09970683511346579 Adapter cache time: 0.017612768337130547 Engine time: 0.0949323340319097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 109.45550029503647,
    "estimated_duration": 3600.039580479924,
    "input_throughput": 7187.873750140927,
    "output_throughput": 6272.210206363847,
    "total_throughput": 13460.083956504774,
    "itl": 81.7486671304117,
    "ttft": 1362043.782883057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.479709870289078,
    "arrivals": 186986,
    "finished_requests": 104699,
    "scheduler_time": 246.8649792103972
}
#Debug simulation 
Total elapsed time: 109.45567061600741. Arrivals time: 0.5149093540385365 Scheduler time: 108.6924326781882 Scheduler overhead time: 0.09886359493248165 Adapter cache time: 0.017919783713296056 Engine time: 0.093822548747994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 107.98961790197063,
    "estimated_duration": 3600.0352314748293,
    "input_throughput": 7126.632755060409,
    "output_throughput": 6216.779992686714,
    "total_throughput": 13343.412747747123,
    "itl": 79.81556872885155,
    "ttft": 1364751.5177365402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 203,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5070370512641995,
    "arrivals": 186986,
    "finished_requests": 103745,
    "scheduler_time": 249.64094757407872
}
#Debug simulation 
Total elapsed time: 107.9897853989387. Arrivals time: 0.4990289786364883 Scheduler time: 107.23849286371842 Scheduler overhead time: 0.10017254564445466 Adapter cache time: 0.017707256134599447 Engine time: 0.09608274232596159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 104.15034815995023,
    "estimated_duration": 3600.007254543592,
    "input_throughput": 7236.949027566057,
    "output_throughput": 6309.678951710832,
    "total_throughput": 13546.62797927689,
    "itl": 81.93083192381329,
    "ttft": 1373838.2783217002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9320524573232961,
    "arrivals": 186986,
    "finished_requests": 105351,
    "scheduler_time": 245.2095645206165
}
#Debug simulation 
Total elapsed time: 104.15052882803138. Arrivals time: 0.49803650146350265 Scheduler time: 103.40940269478597 Scheduler overhead time: 0.09693786851130426 Adapter cache time: 0.016668879077769816 Engine time: 0.09275625320151448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_96_slots_16_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 270, 17280, 270, 270, 270, 270, 17280, 270, 17280, 270, 33, 17280, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 33, 270, 270, 270, 33, 33, 17280, 270, 270, 33, 17280, 17280, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 17280, 33, 33, 270, 33, 17280, 33, 33, 17280, 33, 270, 270, 33, 270, 17280, 17280, 17280, 270, 17280, 17280, 33, 33, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 17280, 33, 33, 17280, 33, 33, 270]
Prompts retrieved: 562656 . Total input tokens: 125477304 . Total output tokens: 110439748
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 108.30778434802778,
    "estimated_duration": 3600.0222389083633,
    "input_throughput": 7126.658475248676,
    "output_throughput": 6216.802429194573,
    "total_throughput": 13343.46090444325,
    "itl": 79.81532194734316,
    "ttft": 1364747.2800395812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 203,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4937815519608584,
    "arrivals": 186986,
    "finished_requests": 103745,
    "scheduler_time": 249.64070485081095
}
#Debug simulation 
Total elapsed time: 108.30795251298696. Arrivals time: 0.5001111081801355 Scheduler time: 107.55761376360897 Scheduler overhead time: 0.10020278755109757 Adapter cache time: 0.017550187651067972 Engine time: 0.09468638186808676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 101.46960101299919,
    "estimated_duration": 3600.0623127702593,
    "input_throughput": 7073.449231606414,
    "output_throughput": 6191.842824755717,
    "total_throughput": 13265.29205636213,
    "itl": 81.90748631533964,
    "ttft": 1393132.6165218675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7802642657700918,
    "arrivals": 186004,
    "finished_requests": 103059,
    "scheduler_time": 250.35458100890276
}
#Debug simulation 
Total elapsed time: 101.46978594909888. Arrivals time: 0.48166146071162075 Scheduler time: 100.74885767954402 Scheduler overhead time: 0.09481305244844407 Adapter cache time: 0.015722534502856433 Engine time: 0.09158532740548253 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 103.08344170206692,
    "estimated_duration": 3600.0698480582782,
    "input_throughput": 7150.716260098315,
    "output_throughput": 6268.0008867524375,
    "total_throughput": 13418.717146850751,
    "itl": 81.76529715095826,
    "ttft": 1385176.5945252685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1247223641257744,
    "arrivals": 186004,
    "finished_requests": 104298,
    "scheduler_time": 247.0165100427281
}
#Debug simulation 
Total elapsed time: 103.08362082007807. Arrivals time: 0.5000196840846911 Scheduler time: 102.34236419957597 Scheduler overhead time: 0.09562133671715856 Adapter cache time: 0.016570563311688602 Engine time: 0.09252570010721684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 107.78596092306543,
    "estimated_duration": 3600.0479499321405,
    "input_throughput": 7149.359496860355,
    "output_throughput": 6255.446681043371,
    "total_throughput": 13404.806177903725,
    "itl": 80.1204519519132,
    "ttft": 1364642.3570225923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.149730947841891,
    "arrivals": 186004,
    "finished_requests": 104229,
    "scheduler_time": 247.5117113549595
}
#Debug simulation 
Total elapsed time: 107.78614156402182. Arrivals time: 0.5095820038113743 Scheduler time: 107.0289360806346 Scheduler overhead time: 0.0987513402942568 Adapter cache time: 0.017292066710069776 Engine time: 0.09351303451694548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 104.54789392091334,
    "estimated_duration": 3600.0277825432468,
    "input_throughput": 7150.799814609695,
    "output_throughput": 6268.074126933193,
    "total_throughput": 13418.873941542888,
    "itl": 81.76188726681643,
    "ttft": 1385158.2840413623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.064471678347326,
    "arrivals": 186004,
    "finished_requests": 104298,
    "scheduler_time": 247.01995563618934
}
#Debug simulation 
Total elapsed time: 104.5480816019699. Arrivals time: 0.5084025509422645 Scheduler time: 103.79650729510467 Scheduler overhead time: 0.09615465626120567 Adapter cache time: 0.016759817022830248 Engine time: 0.09310863260179758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 107.2054555080831,
    "estimated_duration": 3600.026833160014,
    "input_throughput": 7146.066735679947,
    "output_throughput": 6257.831411835662,
    "total_throughput": 13403.898147515609,
    "itl": 80.23186071501314,
    "ttft": 1364054.3608191852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1338362907385462,
    "arrivals": 186004,
    "finished_requests": 104218,
    "scheduler_time": 247.39045494093375
}
#Debug simulation 
Total elapsed time: 107.2056401220616. Arrivals time: 0.5048113215016201 Scheduler time: 106.45250741334166 Scheduler overhead time: 0.09870390384458005 Adapter cache time: 0.017360946163535118 Engine time: 0.09452787390910089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 109.48013028502464,
    "estimated_duration": 3600.085877147547,
    "input_throughput": 7152.127443248967,
    "output_throughput": 6264.288900204956,
    "total_throughput": 13416.416343453922,
    "itl": 81.56363704809398,
    "ttft": 1371925.9174396275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8490614850958796,
    "arrivals": 186004,
    "finished_requests": 104274,
    "scheduler_time": 247.26013409844583
}
#Debug simulation 
Total elapsed time: 109.48030127107631. Arrivals time: 0.5018280129879713 Scheduler time: 108.7331000227714 Scheduler overhead time: 0.09754028113093227 Adapter cache time: 0.016570020583458245 Engine time: 0.09350769978482276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_96_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 66, 17280, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 66, 135, 135, 135, 66, 66, 17280, 135, 135, 66, 17280, 17280, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 17280, 66, 66, 135, 66, 17280, 66, 66, 17280, 66, 135, 135, 66, 135, 17280, 17280, 17280, 135, 17280, 17280, 66, 66, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 17280, 66, 66, 17280, 66, 66, 135]
Prompts retrieved: 559392 . Total input tokens: 124740686 . Total output tokens: 109789483
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 108.93185563199222,
    "estimated_duration": 3600.093838586602,
    "input_throughput": 7145.9623424982965,
    "output_throughput": 6257.809382225652,
    "total_throughput": 13403.771724723949,
    "itl": 80.2350657252139,
    "ttft": 1364114.1067037373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 154,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1292263473570354,
    "arrivals": 186004,
    "finished_requests": 104218,
    "scheduler_time": 247.39619271446608
}
#Debug simulation 
Total elapsed time: 108.9320475179702. Arrivals time: 0.5096688049379736 Scheduler time: 108.17632241861429 Scheduler overhead time: 0.09818250697571784 Adapter cache time: 0.01696080749388784 Engine time: 0.09376341779716313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 102.12235817895271,
    "estimated_duration": 3600.0663159303836,
    "input_throughput": 7147.3352827252675,
    "output_throughput": 6242.318898559581,
    "total_throughput": 13389.654181284848,
    "itl": 82.21121631468488,
    "ttft": 1376321.9947473155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7802642657700918,
    "arrivals": 185665,
    "finished_requests": 104317,
    "scheduler_time": 248.747175708931
}
#Debug simulation 
Total elapsed time: 102.12253162299749. Arrivals time: 0.48947313078679144 Scheduler time: 101.39107980835252 Scheduler overhead time: 0.09624866908416152 Adapter cache time: 0.016480598133057356 Engine time: 0.09258499415591359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 109.6842351629166,
    "estimated_duration": 3600.005868373399,
    "input_throughput": 7073.656524761726,
    "output_throughput": 6177.691596388602,
    "total_throughput": 13251.34812115033,
    "itl": 80.94489399794159,
    "ttft": 1363111.5722003616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 116,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8460396230779595,
    "arrivals": 185665,
    "finished_requests": 103226,
    "scheduler_time": 251.6274819599492
}
#Debug simulation 
Total elapsed time: 109.68441211397294. Arrivals time: 0.5046701957471669 Scheduler time: 108.92967497743666 Scheduler overhead time: 0.10001243173610419 Adapter cache time: 0.016834454727359116 Engine time: 0.09531308780424297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 96.83948861993849,
    "estimated_duration": 3600.0884807612033,
    "input_throughput": 7118.635871577599,
    "output_throughput": 6224.835617168396,
    "total_throughput": 13343.471488745994,
    "itl": 80.14110274892964,
    "ttft": 1383713.0477893178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9597907582623887,
    "arrivals": 185665,
    "finished_requests": 103983,
    "scheduler_time": 249.63745128422238
}
#Debug simulation 
Total elapsed time: 96.83967259398196. Arrivals time: 0.4724065428599715 Scheduler time: 96.12730296084192 Scheduler overhead time: 0.0953313484787941 Adapter cache time: 0.016045247786678374 Engine time: 0.09127265587449074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 103.72729575296398,
    "estimated_duration": 3600.0582965361164,
    "input_throughput": 7089.456863672751,
    "output_throughput": 6197.294921992432,
    "total_throughput": 13286.751785665183,
    "itl": 81.10174123297689,
    "ttft": 1375997.1675093973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.808831509919837,
    "arrivals": 185665,
    "finished_requests": 103477,
    "scheduler_time": 250.86409301582432
}
#Debug simulation 
Total elapsed time: 103.72747065394651. Arrivals time: 0.48727954050991684 Scheduler time: 102.9974332251586 Scheduler overhead time: 0.09665502735879272 Adapter cache time: 0.01622464950196445 Engine time: 0.09298331080935895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 96.76621670997702,
    "estimated_duration": 3600.0122680604863,
    "input_throughput": 7118.774907344125,
    "output_throughput": 6224.4968437489515,
    "total_throughput": 13343.271751093076,
    "itl": 80.13814793318973,
    "ttft": 1383666.4827715706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9517131883744152,
    "arrivals": 185665,
    "finished_requests": 103976,
    "scheduler_time": 249.63475501003094
}
#Debug simulation 
Total elapsed time: 96.76639231201261. Arrivals time: 0.496198721928522 Scheduler time: 96.02789501438383 Scheduler overhead time: 0.0952349454164505 Adapter cache time: 0.016504199244081974 Engine time: 0.09320208197459579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 99.56170339800883,
    "estimated_duration": 3600.016818212648,
    "input_throughput": 7175.939809310649,
    "output_throughput": 6271.5857008716785,
    "total_throughput": 13447.525510182328,
    "itl": 81.96617127863728,
    "ttft": 1372506.9311994521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8235258013335977,
    "arrivals": 185665,
    "finished_requests": 104776,
    "scheduler_time": 247.48219303794284
}
#Debug simulation 
Total elapsed time: 99.56189284892753. Arrivals time: 0.48573002382181585 Scheduler time: 98.83733028394636 Scheduler overhead time: 0.09500076726544648 Adapter cache time: 0.01594993576873094 Engine time: 0.09128538670483977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_96_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 135, 17280, 135, 135, 135, 135, 17280, 135, 17280, 135, 33, 17280, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 33, 135, 135, 135, 33, 33, 17280, 135, 135, 33, 17280, 17280, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 17280, 33, 33, 135, 33, 17280, 33, 33, 17280, 33, 135, 135, 33, 135, 17280, 17280, 17280, 135, 17280, 17280, 33, 33, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 17280, 33, 33, 17280, 33, 33, 135]
Prompts retrieved: 558336 . Total input tokens: 124498388 . Total output tokens: 109587810
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 100.54811562295072,
    "estimated_duration": 3600.0087103162336,
    "input_throughput": 7054.888763802187,
    "output_throughput": 6164.078419146578,
    "total_throughput": 13218.967182948765,
    "itl": 79.47143116764093,
    "ttft": 1387861.1886995442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8669675733149053,
    "arrivals": 185665,
    "finished_requests": 102964,
    "scheduler_time": 252.29409799618304
}
#Debug simulation 
Total elapsed time: 100.5482981710229. Arrivals time: 0.4876431548036635 Scheduler time: 99.81962025247049 Scheduler overhead time: 0.09463814564514905 Adapter cache time: 0.01650554488878697 Engine time: 0.0929725815076381 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 110.53615001798607,
    "estimated_duration": 3600.100163537844,
    "input_throughput": 7158.171392285793,
    "output_throughput": 6246.839526236864,
    "total_throughput": 13405.010918522657,
    "itl": 82.07132426105596,
    "ttft": 1363194.1728976446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7207525844825424,
    "arrivals": 184921,
    "finished_requests": 104475,
    "scheduler_time": 246.931237934298
}
#Debug simulation 
Total elapsed time: 110.53632904298138. Arrivals time: 0.5067718506325036 Scheduler time: 109.7787265619263 Scheduler overhead time: 0.10046475939452648 Adapter cache time: 0.017041284241713583 Engine time: 0.09569003549404442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 108.95139457390178,
    "estimated_duration": 3600.087473092237,
    "input_throughput": 7192.237464652462,
    "output_throughput": 6282.780118276447,
    "total_throughput": 13475.01758292891,
    "itl": 81.75720707670253,
    "ttft": 1344996.4159203751,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9520731679257012,
    "arrivals": 184921,
    "finished_requests": 105033,
    "scheduler_time": 245.32881080745943
}
#Debug simulation 
Total elapsed time: 108.95157708693296. Arrivals time: 0.507872016984038 Scheduler time: 108.19662639882881 Scheduler overhead time: 0.09841165575198829 Adapter cache time: 0.016703489935025573 Engine time: 0.09473850834183395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 109.97850943903904,
    "estimated_duration": 3600.072310537102,
    "input_throughput": 7141.009341605294,
    "output_throughput": 6234.4628285123135,
    "total_throughput": 13375.472170117608,
    "itl": 80.00741943009861,
    "ttft": 1349554.8516531906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 123,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9232959627825769,
    "arrivals": 184921,
    "finished_requests": 104228,
    "scheduler_time": 247.4714630288556
}
#Debug simulation 
Total elapsed time: 109.97869826597162. Arrivals time: 0.5011960612609982 Scheduler time: 109.22874838695861 Scheduler overhead time: 0.09902769478503615 Adapter cache time: 0.01709215948358178 Engine time: 0.0948449649149552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 109.34066014003474,
    "estimated_duration": 3600.0901894019735,
    "input_throughput": 7128.7011296411865,
    "output_throughput": 6221.803572015734,
    "total_throughput": 13350.50470165692,
    "itl": 81.16247704497431,
    "ttft": 1366899.2064152823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 109,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7472115585347637,
    "arrivals": 184921,
    "finished_requests": 104065,
    "scheduler_time": 248.02703362384057
}
#Debug simulation 
Total elapsed time: 109.34085183008574. Arrivals time: 0.5117026072693989 Scheduler time: 108.5812831709627 Scheduler overhead time: 0.09911746205762029 Adapter cache time: 0.01662589784245938 Engine time: 0.0942898872308433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 109.45148549205624,
    "estimated_duration": 3600.0871031873944,
    "input_throughput": 7138.525058809468,
    "output_throughput": 6237.797129996571,
    "total_throughput": 13376.32218880604,
    "itl": 79.98816010293169,
    "ttft": 1347037.8427014102,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9528543674852701,
    "arrivals": 184921,
    "finished_requests": 104253,
    "scheduler_time": 247.33154799460436
}
#Debug simulation 
Total elapsed time: 109.4516674450133. Arrivals time: 0.5050425187218934 Scheduler time: 108.69646437535994 Scheduler overhead time: 0.09919703961350024 Adapter cache time: 0.017223061411641538 Engine time: 0.09563517314381897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 110.29543779895175,
    "estimated_duration": 3600.009415704682,
    "input_throughput": 7194.920904097099,
    "output_throughput": 6278.53274533051,
    "total_throughput": 13473.453649427609,
    "itl": 81.63404006371503,
    "ttft": 1344181.9847170808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8235258013335977,
    "arrivals": 184921,
    "finished_requests": 104999,
    "scheduler_time": 245.5024252736963
}
#Debug simulation 
Total elapsed time: 110.29563083394896. Arrivals time: 0.505235888180323 Scheduler time: 109.54260474105831 Scheduler overhead time: 0.09792356833349913 Adapter cache time: 0.017189072095789015 Engine time: 0.09485460072755814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_96_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 66, 17280, 66, 66, 66, 66, 17280, 66, 17280, 66, 33, 17280, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 33, 66, 66, 66, 33, 33, 17280, 66, 66, 33, 17280, 17280, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 17280, 33, 33, 66, 33, 17280, 33, 33, 17280, 33, 66, 66, 33, 66, 17280, 17280, 17280, 66, 17280, 17280, 33, 33, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 17280, 33, 33, 17280, 33, 33, 66]
Prompts retrieved: 556128 . Total input tokens: 124006175 . Total output tokens: 109143041
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 108.99349999905098,
    "estimated_duration": 3600.0787892283424,
    "input_throughput": 7138.541544394508,
    "output_throughput": 6237.8115354562715,
    "total_throughput": 13376.353079850778,
    "itl": 79.98805189918579,
    "ttft": 1347034.6089562562,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9441554460674526,
    "arrivals": 184921,
    "finished_requests": 104253,
    "scheduler_time": 247.3314273008628
}
#Debug simulation 
Total elapsed time: 108.99368203897029. Arrivals time: 0.49243755848146975 Scheduler time: 108.25358514720574 Scheduler overhead time: 0.09894794772844762 Adapter cache time: 0.01699107827153057 Engine time: 0.09377752582076937 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 108.76100976206362,
    "estimated_duration": 3600.0084111936612,
    "input_throughput": 6521.377263175806,
    "output_throughput": 5754.202388969261,
    "total_throughput": 12275.579652145067,
    "itl": 82.31936188243458,
    "ttft": 1403611.317865233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2497453070385367,
    "arrivals": 149510,
    "finished_requests": 95510,
    "scheduler_time": 253.47871727078297
}
#Debug simulation 
Total elapsed time: 108.76118828705512. Arrivals time: 0.5072109961183742 Scheduler time: 107.99151755159255 Scheduler overhead time: 0.10593505087308586 Adapter cache time: 0.017904804553836584 Engine time: 0.09897752793040127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 110.06418855302036,
    "estimated_duration": 3600.051279215417,
    "input_throughput": 6460.4474203680675,
    "output_throughput": 5698.722437217928,
    "total_throughput": 12159.169857585995,
    "itl": 80.84740969191876,
    "ttft": 1395426.7708337766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4239450889686138,
    "arrivals": 149510,
    "finished_requests": 94439,
    "scheduler_time": 256.5156621041384
}
#Debug simulation 
Total elapsed time: 110.06436834600754. Arrivals time: 0.5724754862021655 Scheduler time: 109.218432228663 Scheduler overhead time: 0.10964494070503861 Adapter cache time: 0.019238268840126693 Engine time: 0.10403832863084972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 110.48076866799966,
    "estimated_duration": 3600.06222555803,
    "input_throughput": 6424.343956003433,
    "output_throughput": 5662.555734530429,
    "total_throughput": 12086.89969053386,
    "itl": 79.14446632474025,
    "ttft": 1421182.540709709,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5440546673536337,
    "arrivals": 149510,
    "finished_requests": 94039,
    "scheduler_time": 257.27617574861944
}
#Debug simulation 
Total elapsed time: 110.48095175100025. Arrivals time: 0.6060601357603446 Scheduler time: 109.59477087575942 Scheduler overhead time: 0.11384612217079848 Adapter cache time: 0.0194173245690763 Engine time: 0.10595291969366372 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 109.80676309391856,
    "estimated_duration": 3600.001404781287,
    "input_throughput": 6496.9688536610365,
    "output_throughput": 5729.573875333844,
    "total_throughput": 12226.54272899488,
    "itl": 81.45142733167496,
    "ttft": 1412279.970979378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.383951056180521,
    "arrivals": 149510,
    "finished_requests": 95079,
    "scheduler_time": 254.31297273532445
}
#Debug simulation 
Total elapsed time: 109.80693993694149. Arrivals time: 0.6069173438008875 Scheduler time: 108.92234596447088 Scheduler overhead time: 0.11297621764242649 Adapter cache time: 0.018911068327724934 Engine time: 0.10514903930015862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 112.19058114895597,
    "estimated_duration": 3600.049750282602,
    "input_throughput": 6427.1236802168305,
    "output_throughput": 5667.256681216259,
    "total_throughput": 12094.38036143309,
    "itl": 79.3245380758709,
    "ttft": 1408922.5211270754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4054138412140331,
    "arrivals": 149510,
    "finished_requests": 93995,
    "scheduler_time": 257.23890344816994
}
#Debug simulation 
Total elapsed time: 112.19075697101653. Arrivals time: 0.6058853011345491 Scheduler time: 111.30196214059833 Scheduler overhead time: 0.11505221540573984 Adapter cache time: 0.01951693883165717 Engine time: 0.10664236661978066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 111.76144791196566,
    "estimated_duration": 3600.0201311222036,
    "input_throughput": 6493.330911656086,
    "output_throughput": 5724.755209514803,
    "total_throughput": 12218.086121170889,
    "itl": 80.58478037175473,
    "ttft": 1394185.3978278216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 198,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.264016346232962,
    "arrivals": 149510,
    "finished_requests": 95110,
    "scheduler_time": 254.52274394774227
}
#Debug simulation 
Total elapsed time: 111.76162601495162. Arrivals time: 0.6057684672996402 Scheduler time: 110.8772159467917 Scheduler overhead time: 0.11398097930941731 Adapter cache time: 0.019266045768745244 Engine time: 0.10457329091150314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [32 32 32]
Adapter prompts. [1080, 1080, 1080, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 8640, 4320, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 1080, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 4320]
Prompts retrieved: 449280 . Total input tokens: 100196291 . Total output tokens: 88073645
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 110.43145653000101,
    "estimated_duration": 3600.0391589877167,
    "input_throughput": 6431.862259662438,
    "output_throughput": 5676.212979234908,
    "total_throughput": 12108.075238897347,
    "itl": 79.34859196477427,
    "ttft": 1414966.076241714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5143850421160485,
    "arrivals": 149510,
    "finished_requests": 94229,
    "scheduler_time": 256.6547476704143
}
#Debug simulation 
Total elapsed time: 110.43163513706531. Arrivals time: 0.6119254296645522 Scheduler time: 109.54052440356463 Scheduler overhead time: 0.11327747255563736 Adapter cache time: 0.018598618917167187 Engine time: 0.10591602651402354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 113.79455981298815,
    "estimated_duration": 3600.089447031388,
    "input_throughput": 6559.802290321844,
    "output_throughput": 5722.84836338967,
    "total_throughput": 12282.650653711515,
    "itl": 81.24992664498947,
    "ttft": 1359595.2169358963,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2563577160704866,
    "arrivals": 143761,
    "finished_requests": 95411,
    "scheduler_time": 251.25447365459456
}
#Debug simulation 
Total elapsed time: 113.7947449060157. Arrivals time: 0.6020876413676888 Scheduler time: 112.91166921588592 Scheduler overhead time: 0.11448452237527817 Adapter cache time: 0.01904797286260873 Engine time: 0.10589781706221402 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 112.65128383296542,
    "estimated_duration": 3600.000504110602,
    "input_throughput": 6510.504921662624,
    "output_throughput": 5674.848927569025,
    "total_throughput": 12185.35384923165,
    "itl": 79.9496598164063,
    "ttft": 1377914.0378657735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3589915117109204,
    "arrivals": 143761,
    "finished_requests": 94627,
    "scheduler_time": 253.2768620001171
}
#Debug simulation 
Total elapsed time: 112.65146726602688. Arrivals time: 0.6010784099344164 Scheduler time: 111.76772871648427 Scheduler overhead time: 0.11420731001999229 Adapter cache time: 0.01937515928875655 Engine time: 0.10734556871466339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 109.90334284491837,
    "estimated_duration": 3600.0617229010436,
    "input_throughput": 6520.434038859738,
    "output_throughput": 5680.065391634976,
    "total_throughput": 12200.499430494714,
    "itl": 78.9093377116352,
    "ttft": 1377116.0140213468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4165462997695444,
    "arrivals": 143761,
    "finished_requests": 94751,
    "scheduler_time": 253.1577846209495
}
#Debug simulation 
Total elapsed time: 109.90352247096598. Arrivals time: 0.5938207864528522 Scheduler time: 109.03053300245665 Scheduler overhead time: 0.11360881733708084 Adapter cache time: 0.018934654537588358 Engine time: 0.10502611380070448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 111.25182012200821,
    "estimated_duration": 3600.038138006141,
    "input_throughput": 6540.875984453205,
    "output_throughput": 5700.787384259926,
    "total_throughput": 12241.66336871313,
    "itl": 80.50841344196894,
    "ttft": 1371024.2495720035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 188,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2904115000925949,
    "arrivals": 143761,
    "finished_requests": 95093,
    "scheduler_time": 252.24096002441706
}
#Debug simulation 
Total elapsed time: 111.25199523591436. Arrivals time: 0.6082289965124801 Scheduler time: 110.36403346783482 Scheduler overhead time: 0.11290166387334466 Adapter cache time: 0.019341653562150896 Engine time: 0.10542172577697784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 110.10408125002868,
    "estimated_duration": 3600.0711272544613,
    "input_throughput": 6519.230640284266,
    "output_throughput": 5679.6025070742335,
    "total_throughput": 12198.833147358499,
    "itl": 78.90440995263496,
    "ttft": 1377326.8957505976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.404947737879121,
    "arrivals": 143761,
    "finished_requests": 94734,
    "scheduler_time": 253.1831198615315
}
#Debug simulation 
Total elapsed time: 110.10426244197879. Arrivals time: 0.6082032002741471 Scheduler time: 109.21702926675789 Scheduler overhead time: 0.11311790463514626 Adapter cache time: 0.01860534376464784 Engine time: 0.10595192248001695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 110.79882532788906,
    "estimated_duration": 3600.07539083919,
    "input_throughput": 6540.808300825895,
    "output_throughput": 5700.728393695113,
    "total_throughput": 12241.536694521008,
    "itl": 80.50647770630803,
    "ttft": 1371036.0347695837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 188,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2001771368272571,
    "arrivals": 143761,
    "finished_requests": 95093,
    "scheduler_time": 252.24868314027182
}
#Debug simulation 
Total elapsed time: 110.79901585099287. Arrivals time: 0.5983521909220144 Scheduler time: 109.92116375092883 Scheduler overhead time: 0.11416156881023198 Adapter cache time: 0.019041121238842607 Engine time: 0.10579851293005049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 540, 8640, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 540, 4320, 4320, 4320, 540, 540, 8640, 4320, 4320, 540, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 8640, 540, 540, 4320, 540, 8640, 540, 540, 8640, 540, 4320, 4320, 540, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 540, 540, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 8640, 540, 540, 8640, 540, 540, 4320]
Prompts retrieved: 432000 . Total input tokens: 96400063 . Total output tokens: 84684380
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 110.01380340498872,
    "estimated_duration": 3600.0436396355108,
    "input_throughput": 6496.701523975964,
    "output_throughput": 5651.031497512552,
    "total_throughput": 12147.733021488517,
    "itl": 78.27644321343747,
    "ttft": 1382276.1143420204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 212,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5615987792983685,
    "arrivals": 143761,
    "finished_requests": 94456,
    "scheduler_time": 254.61286290518595
}
#Debug simulation 
Total elapsed time: 110.01397952099796. Arrivals time: 0.6012028041295707 Scheduler time: 109.13005973026156 Scheduler overhead time: 0.1145156987477094 Adapter cache time: 0.019534137099981308 Engine time: 0.10715477773919702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 110.93634252995253,
    "estimated_duration": 3600.076023600288,
    "input_throughput": 6528.972123342501,
    "output_throughput": 5600.044240131971,
    "total_throughput": 12129.016363474471,
    "itl": 80.99085010663923,
    "ttft": 1384065.873428766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.124109535431488,
    "arrivals": 140843,
    "finished_requests": 94104,
    "scheduler_time": 254.6070529876076
}
#Debug simulation 
Total elapsed time: 110.93652705196291. Arrivals time: 0.5917952685849741 Scheduler time: 110.06227438640781 Scheduler overhead time: 0.11506674846168607 Adapter cache time: 0.019394880742765963 Engine time: 0.10672322812024504 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 109.5016498080222,
    "estimated_duration": 3600.030530256001,
    "input_throughput": 6481.625587308755,
    "output_throughput": 5561.27589245092,
    "total_throughput": 12042.901479759676,
    "itl": 79.60931354623622,
    "ttft": 1399289.3642677881,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4020168842002767,
    "arrivals": 140843,
    "finished_requests": 93400,
    "scheduler_time": 256.23068753462275
}
#Debug simulation 
Total elapsed time: 109.50183215201832. Arrivals time: 0.5930878950748593 Scheduler time: 108.62600752245635 Scheduler overhead time: 0.11499344033654779 Adapter cache time: 0.019593253266066313 Engine time: 0.10634327249135822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 109.68462523701601,
    "estimated_duration": 3600.003846249991,
    "input_throughput": 6499.442500422039,
    "output_throughput": 5570.429326315626,
    "total_throughput": 12069.871826737664,
    "itl": 78.4705853226915,
    "ttft": 1392082.4405624259,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 177,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.330354216252456,
    "arrivals": 140843,
    "finished_requests": 93576,
    "scheduler_time": 256.1747397976535
}
#Debug simulation 
Total elapsed time: 109.68480367807206. Arrivals time: 0.5888523659668863 Scheduler time: 108.81297469884157 Scheduler overhead time: 0.11523718980606645 Adapter cache time: 0.019329615868628025 Engine time: 0.106742202071473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 111.88964513596147,
    "estimated_duration": 3600.0986536052687,
    "input_throughput": 6499.548554472912,
    "output_throughput": 5575.144997735021,
    "total_throughput": 12074.693552207933,
    "itl": 79.99280142502965,
    "ttft": 1387897.2488916076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2743100323388343,
    "arrivals": 140843,
    "finished_requests": 93595,
    "scheduler_time": 255.4894916532961
}
#Debug simulation 
Total elapsed time: 111.88982932595536. Arrivals time: 0.5912831084569916 Scheduler time: 111.01337948616128 Scheduler overhead time: 0.11580356897320598 Adapter cache time: 0.019263874040916562 Engine time: 0.1081011057831347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 109.93728938803542,
    "estimated_duration": 3600.005701775608,
    "input_throughput": 6488.517501091439,
    "output_throughput": 5564.470909065416,
    "total_throughput": 12052.988410156855,
    "itl": 78.45550127493591,
    "ttft": 1395570.452127112,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.334232725333424,
    "arrivals": 140843,
    "finished_requests": 93422,
    "scheduler_time": 256.2503193717336
}
#Debug simulation 
Total elapsed time: 109.93747235299088. Arrivals time: 0.6007506443420425 Scheduler time: 109.05362578469794 Scheduler overhead time: 0.1144923702813685 Adapter cache time: 0.019404255552217364 Engine time: 0.10715694248210639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 109.32512339693494,
    "estimated_duration": 3600.041584093096,
    "input_throughput": 6545.178284638023,
    "output_throughput": 5619.374256504938,
    "total_throughput": 12164.55254114296,
    "itl": 80.52769164071981,
    "ttft": 1385979.4988253077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 188,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2001771368272571,
    "arrivals": 140843,
    "finished_requests": 94303,
    "scheduler_time": 253.79692050830116
}
#Debug simulation 
Total elapsed time: 109.32529895694461. Arrivals time: 0.5821544030914083 Scheduler time: 108.46259987261146 Scheduler overhead time: 0.11295004177372903 Adapter cache time: 0.019527590251527727 Engine time: 0.10680709022562951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 270, 8640, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 270, 4320, 4320, 4320, 270, 270, 8640, 4320, 4320, 270, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 8640, 270, 270, 4320, 270, 8640, 270, 270, 8640, 270, 4320, 4320, 270, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 270, 270, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 8640, 270, 270, 8640, 270, 270, 4320]
Prompts retrieved: 423360 . Total input tokens: 94478793 . Total output tokens: 83015150
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 110.02290341700427,
    "estimated_duration": 3600.090511635221,
    "input_throughput": 6488.636028595194,
    "output_throughput": 5564.584816758031,
    "total_throughput": 12053.220845353224,
    "itl": 78.4547714658101,
    "ttft": 1395576.6589503975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3218056947365422,
    "arrivals": 140843,
    "finished_requests": 93425,
    "scheduler_time": 256.25697479809764
}
#Debug simulation 
Total elapsed time: 110.0230773879448. Arrivals time: 0.5921098558465019 Scheduler time: 109.14637455355842 Scheduler overhead time: 0.11587894079275429 Adapter cache time: 0.01936262275557965 Engine time: 0.10775668115820736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 115.67888926900923,
    "estimated_duration": 3600.0938148894006,
    "input_throughput": 6531.4339595126285,
    "output_throughput": 5663.187974624029,
    "total_throughput": 12194.621934136658,
    "itl": 80.10854322146633,
    "ttft": 1348051.9346606797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3026445792941361,
    "arrivals": 139377,
    "finished_requests": 94766,
    "scheduler_time": 249.58024078201498
}
#Debug simulation 
Total elapsed time: 115.67906754498836. Arrivals time: 0.5980883161537349 Scheduler time: 114.79616811033338 Scheduler overhead time: 0.11571859719697386 Adapter cache time: 0.01928691368084401 Engine time: 0.1077237460995093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 110.95432106289081,
    "estimated_duration": 3600.068904564814,
    "input_throughput": 6498.261733362004,
    "output_throughput": 5643.68003463424,
    "total_throughput": 12141.941767996244,
    "itl": 79.1229399644607,
    "ttft": 1364665.2213660688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4811265243124236,
    "arrivals": 139377,
    "finished_requests": 94287,
    "scheduler_time": 250.43889716856432
}
#Debug simulation 
Total elapsed time: 110.95450132992119. Arrivals time: 0.5941823528846726 Scheduler time: 110.07874667225406 Scheduler overhead time: 0.11471620702650398 Adapter cache time: 0.01975245028734207 Engine time: 0.1060498682782054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 115.2312295170268,
    "estimated_duration": 3600.0618567781,
    "input_throughput": 6439.816014924933,
    "output_throughput": 5595.683296961107,
    "total_throughput": 12035.49931188604,
    "itl": 77.2281635255891,
    "ttft": 1355381.4304173158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4718379655899503,
    "arrivals": 139377,
    "finished_requests": 93439,
    "scheduler_time": 252.87194210093242
}
#Debug simulation 
Total elapsed time: 115.23140445200261. Arrivals time: 0.5991228176280856 Scheduler time: 114.34708410338499 Scheduler overhead time: 0.11617105663754046 Adapter cache time: 0.01953358924947679 Engine time: 0.10751631006132811 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 110.32231028703973,
    "estimated_duration": 3600.048335360765,
    "input_throughput": 6498.298861772266,
    "output_throughput": 5643.712280314132,
    "total_throughput": 12142.011142086398,
    "itl": 79.11912285140957,
    "ttft": 1364666.7451492054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3867274981271467,
    "arrivals": 139377,
    "finished_requests": 94287,
    "scheduler_time": 250.44902144597611
}
#Debug simulation 
Total elapsed time: 110.32249086000957. Arrivals time: 0.5943217186722904 Scheduler time: 109.44773554464336 Scheduler overhead time: 0.11317274707835168 Adapter cache time: 0.019147292245179415 Engine time: 0.10656569537241012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 114.70556222496089,
    "estimated_duration": 3600.0945131482395,
    "input_throughput": 6447.4862854924795,
    "output_throughput": 5600.627129749403,
    "total_throughput": 12048.113415241884,
    "itl": 77.26072968611824,
    "ttft": 1355267.8165832881,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.460084155504594,
    "arrivals": 139377,
    "finished_requests": 93573,
    "scheduler_time": 252.68134627164392
}
#Debug simulation 
Total elapsed time: 114.70573077199515. Arrivals time: 0.5983418418327346 Scheduler time: 113.82117482169997 Scheduler overhead time: 0.11552048474550247 Adapter cache time: 0.019670984707772732 Engine time: 0.10908284201286733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 109.95813603396527,
    "estimated_duration": 3600.076834022389,
    "input_throughput": 6498.330474203015,
    "output_throughput": 5643.7156585068715,
    "total_throughput": 12142.046132709886,
    "itl": 79.11733773546014,
    "ttft": 1364767.4870074177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.289552029995244,
    "arrivals": 139377,
    "finished_requests": 94288,
    "scheduler_time": 250.4570581473837
}
#Debug simulation 
Total elapsed time: 109.95831442903727. Arrivals time: 0.5879294468322769 Scheduler time: 109.08942350686993 Scheduler overhead time: 0.11406660859938711 Adapter cache time: 0.019512850558385253 Engine time: 0.10632379353046417 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 135, 8640, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 135, 4320, 4320, 4320, 135, 135, 8640, 4320, 4320, 135, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 8640, 135, 135, 4320, 135, 8640, 135, 135, 8640, 135, 4320, 4320, 135, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 135, 135, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 8640, 135, 135, 8640, 135, 135, 4320]
Prompts retrieved: 419040 . Total input tokens: 93499750 . Total output tokens: 82168737
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 115.67710613703821,
    "estimated_duration": 3600.0842022513407,
    "input_throughput": 6447.50475155122,
    "output_throughput": 5600.643170343362,
    "total_throughput": 12048.14792189458,
    "itl": 77.25932724362406,
    "ttft": 1355262.776348182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4462073046714088,
    "arrivals": 139377,
    "finished_requests": 93573,
    "scheduler_time": 252.68436484265717
}
#Debug simulation 
Total elapsed time: 115.67728370404802. Arrivals time: 0.5885939025320113 Scheduler time: 114.80368568864651 Scheduler overhead time: 0.11515286820940673 Adapter cache time: 0.01936225953977555 Engine time: 0.10832126636523753 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 116.31758150609676,
    "estimated_duration": 3600.014466790021,
    "input_throughput": 6203.535904096858,
    "output_throughput": 5389.417231231272,
    "total_throughput": 11592.95313532813,
    "itl": 72.45492594937221,
    "ttft": 1417470.906349694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.130721944463438,
    "arrivals": 138774,
    "finished_requests": 90053,
    "scheduler_time": 262.3472270875694
}
#Debug simulation 
Total elapsed time: 116.31776756199542. Arrivals time: 0.5792717263102531 Scheduler time: 115.44431440345943 Scheduler overhead time: 0.11925516894552857 Adapter cache time: 0.020242617931216955 Engine time: 0.11146587203256786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 116.22351156105287,
    "estimated_duration": 3600.0625584497416,
    "input_throughput": 6451.394280770867,
    "output_throughput": 5611.793315252758,
    "total_throughput": 12063.187596023625,
    "itl": 80.02201593730102,
    "ttft": 1369137.3927575496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3659326165774854,
    "arrivals": 138774,
    "finished_requests": 93719,
    "scheduler_time": 252.32563995573554
}
#Debug simulation 
Total elapsed time: 116.22369089897256. Arrivals time: 0.5957596777006984 Scheduler time: 115.34153753658757 Scheduler overhead time: 0.11634150275494903 Adapter cache time: 0.01995689992327243 Engine time: 0.10821930668316782 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 117.82055261102505,
    "estimated_duration": 3600.091586529589,
    "input_throughput": 6358.892391976052,
    "output_throughput": 5537.459678690358,
    "total_throughput": 11896.35207066641,
    "itl": 77.71891001779034,
    "ttft": 1384198.939024906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.35287362997886,
    "arrivals": 138774,
    "finished_requests": 92441,
    "scheduler_time": 255.673857545351
}
#Debug simulation 
Total elapsed time: 117.8207362710964. Arrivals time: 0.5988695389823988 Scheduler time: 116.93342425860465 Scheduler overhead time: 0.11802964867092669 Adapter cache time: 0.019515640567988157 Engine time: 0.10886434616986662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 117.7010657179635,
    "estimated_duration": 3600.054511308108,
    "input_throughput": 6426.09380700571,
    "output_throughput": 5593.909741295157,
    "total_throughput": 12020.003548300867,
    "itl": 80.00267284467014,
    "ttft": 1375551.5674061545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.201867643971927,
    "arrivals": 138774,
    "finished_requests": 93452,
    "scheduler_time": 253.1934204125324
}
#Debug simulation 
Total elapsed time: 117.70124250894878. Arrivals time: 0.600314428913407 Scheduler time: 116.81458513764665 Scheduler overhead time: 0.11634673608932644 Adapter cache time: 0.019784065894782543 Engine time: 0.10869480739347637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 118.89916125196032,
    "estimated_duration": 3600.0123140958203,
    "input_throughput": 6350.705221335383,
    "output_throughput": 5531.851355625651,
    "total_throughput": 11882.556576961033,
    "itl": 77.7212691895084,
    "ttft": 1382990.4339287616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3406537165585923,
    "arrivals": 138774,
    "finished_requests": 92286,
    "scheduler_time": 255.88583699458144
}
#Debug simulation 
Total elapsed time: 118.89934056601487. Arrivals time: 0.5943496132967994 Scheduler time: 118.01610588119365 Scheduler overhead time: 0.11837196222040802 Adapter cache time: 0.019480454153381288 Engine time: 0.10901868727523834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 117.34182351501659,
    "estimated_duration": 3600.093777683533,
    "input_throughput": 6426.648145506676,
    "output_throughput": 5594.069555865424,
    "total_throughput": 12020.7177013721,
    "itl": 80.00068641530402,
    "ttft": 1375473.405379845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1171861645998407,
    "arrivals": 138774,
    "finished_requests": 93461,
    "scheduler_time": 253.20183950255134
}
#Debug simulation 
Total elapsed time: 117.34200358693488. Arrivals time: 0.5875770623097196 Scheduler time: 116.46742338791955 Scheduler overhead time: 0.11696381168439984 Adapter cache time: 0.01929343875963241 Engine time: 0.10876262525562197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 66, 8640, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 66, 4320, 4320, 4320, 66, 66, 8640, 4320, 4320, 66, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 8640, 66, 66, 4320, 66, 8640, 66, 66, 8640, 66, 4320, 4320, 66, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 66, 66, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 8640, 66, 66, 8640, 66, 66, 4320]
Prompts retrieved: 416832 . Total input tokens: 92995973 . Total output tokens: 81735433
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 118.24330370803364,
    "estimated_duration": 3600.0091320239135,
    "input_throughput": 6390.807955274701,
    "output_throughput": 5557.490902461161,
    "total_throughput": 11948.298857735863,
    "itl": 78.00717448373082,
    "ttft": 1382351.0756962677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3008416944742223,
    "arrivals": 138774,
    "finished_requests": 92895,
    "scheduler_time": 254.83296962415562
}
#Debug simulation 
Total elapsed time: 118.2434821210336. Arrivals time: 0.588585896184668 Scheduler time: 117.36285694944672 Scheduler overhead time: 0.11820818192791194 Adapter cache time: 0.020032966043800116 Engine time: 0.11055565695278347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 103.0729269539006,
    "estimated_duration": 3600.0346692399935,
    "input_throughput": 6669.0502191937285,
    "output_throughput": 5793.260875568989,
    "total_throughput": 12462.311094762717,
    "itl": 82.95575995121688,
    "ttft": 1294532.8338732936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5539161225082334,
    "arrivals": 138381,
    "finished_requests": 96846,
    "scheduler_time": 242.85561652792643
}
#Debug simulation 
Total elapsed time: 103.07310183090158. Arrivals time: 0.5905977126676589 Scheduler time: 102.2094021528028 Scheduler overhead time: 0.11038846499286592 Adapter cache time: 0.018931315164081752 Engine time: 0.10334772174246609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 102.70123906002846,
    "estimated_duration": 3600.0367746850457,
    "input_throughput": 6640.524388001971,
    "output_throughput": 5767.25191975752,
    "total_throughput": 12407.77630775949,
    "itl": 81.71786171068469,
    "ttft": 1297414.7082219427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5985396900679938,
    "arrivals": 138381,
    "finished_requests": 96315,
    "scheduler_time": 244.58502407733548
}
#Debug simulation 
Total elapsed time: 102.7014177480014. Arrivals time: 0.5794753518421203 Scheduler time: 101.84838982345536 Scheduler overhead time: 0.11077287059742957 Adapter cache time: 0.018491611466743052 Engine time: 0.10296416154596955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 100.47330594598316,
    "estimated_duration": 3600.011652703652,
    "input_throughput": 6593.8319900083025,
    "output_throughput": 5745.252514520845,
    "total_throughput": 12339.084504529148,
    "itl": 80.08013761293891,
    "ttft": 1293891.650167705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5810174214327752,
    "arrivals": 138381,
    "finished_requests": 95663,
    "scheduler_time": 246.53467098508622
}
#Debug simulation 
Total elapsed time: 100.47348088095896. Arrivals time: 0.5797939912881702 Scheduler time: 99.62109843688086 Scheduler overhead time: 0.11001267039682716 Adapter cache time: 0.01900007773656398 Engine time: 0.10334564989898354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 103.09169198991731,
    "estimated_duration": 3600.0796892808576,
    "input_throughput": 6642.1710250465785,
    "output_throughput": 5771.081418519997,
    "total_throughput": 12413.252443566575,
    "itl": 81.69730842583206,
    "ttft": 1293591.9231116448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.483874533209017,
    "arrivals": 138381,
    "finished_requests": 96428,
    "scheduler_time": 244.438889380445
}
#Debug simulation 
Total elapsed time: 103.09186185488943. Arrivals time: 0.5758274645777419 Scheduler time: 102.24327214364894 Scheduler overhead time: 0.11025878693908453 Adapter cache time: 0.019069360801950097 Engine time: 0.10342543572187424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 99.98042842699215,
    "estimated_duration": 3600.0298903088,
    "input_throughput": 6594.979965003421,
    "output_throughput": 5745.191742901303,
    "total_throughput": 12340.171707904723,
    "itl": 80.0707482911475,
    "ttft": 1294785.4670902307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 212,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5728864860488145,
    "arrivals": 138381,
    "finished_requests": 95681,
    "scheduler_time": 246.54146044719417
}
#Debug simulation 
Total elapsed time: 99.98060171096586. Arrivals time: 0.5760088303359225 Scheduler time: 99.13083176093642 Scheduler overhead time: 0.11121944850310683 Adapter cache time: 0.01909063325729221 Engine time: 0.10294983396306634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 101.22663603106048,
    "estimated_duration": 3600.067400611514,
    "input_throughput": 6653.0704386066645,
    "output_throughput": 5764.915400326859,
    "total_throughput": 12417.985838933522,
    "itl": 81.79309419077995,
    "ttft": 1299543.669995145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.468301816331218,
    "arrivals": 138381,
    "finished_requests": 96433,
    "scheduler_time": 244.1850102947377
}
#Debug simulation 
Total elapsed time: 101.22680687101092. Arrivals time: 0.5887928802985698 Scheduler time: 100.36654719465878 Scheduler overhead time: 0.10974924394395202 Adapter cache time: 0.01902797247748822 Engine time: 0.10268882033415139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 4320, 33, 8640, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 33, 4320, 4320, 4320, 33, 33, 8640, 4320, 4320, 33, 8640, 8640, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 8640, 33, 33, 4320, 33, 8640, 33, 33, 8640, 33, 4320, 4320, 33, 4320, 8640, 8640, 8640, 4320, 8640, 8640, 33, 33, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 8640, 33, 33, 8640, 33, 33, 4320]
Prompts retrieved: 415776 . Total input tokens: 92753965 . Total output tokens: 81534472
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 141.50525809894316,
    "estimated_duration": 3600.040985161635,
    "input_throughput": 6598.468211309393,
    "output_throughput": 5721.778469995713,
    "total_throughput": 12320.246681305107,
    "itl": 79.82302748039379,
    "ttft": 1314595.9459125362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 218,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6133175841346439,
    "arrivals": 138381,
    "finished_requests": 95696,
    "scheduler_time": 246.15031001502857
}
#Debug simulation 
Total elapsed time: 141.50542656390462. Arrivals time: 0.7761928396066651 Scheduler time: 140.3266534343129 Scheduler overhead time: 0.1639830080093816 Adapter cache time: 0.02830439549870789 Engine time: 0.15952609258238226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_96_slots_16_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_96_slots_16_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 119.61717432003934,
    "estimated_duration": 3600.0089181453045,
    "input_throughput": 6152.1375373176415,
    "output_throughput": 5381.3550023039315,
    "total_throughput": 11533.492539621573,
    "itl": 74.52225455219443,
    "ttft": 1003580.2873557624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.87131175604183,
    "arrivals": 109384,
    "finished_requests": 89547,
    "scheduler_time": 195.80490409089413
}
#Debug simulation 
Total elapsed time: 119.61735317995772. Arrivals time: 0.6143419389845803 Scheduler time: 118.63350250723306 Scheduler overhead time: 0.1490556396311149 Adapter cache time: 0.026819807128049433 Engine time: 0.14266877435147762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_96_slots_16_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_96_slots_16_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 131.38099171908107,
    "estimated_duration": 3600.040445143576,
    "input_throughput": 6033.012498317586,
    "output_throughput": 5281.721772223498,
    "total_throughput": 11314.734270541085,
    "itl": 72.994100253298,
    "ttft": 1063307.8789564054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8405809795670234,
    "arrivals": 109384,
    "finished_requests": 87891,
    "scheduler_time": 202.4107239678078
}
#Debug simulation 
Total elapsed time: 131.38117858604528. Arrivals time: 0.6287049303064123 Scheduler time: 130.3688908860786 Scheduler overhead time: 0.15611263958271593 Adapter cache time: 0.030438364716246724 Engine time: 0.14615074859466404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_96_slots_16_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_96_slots_16_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 138.68284062296152,
    "estimated_duration": 3600.0508172384184,
    "input_throughput": 6037.907269507427,
    "output_throughput": 5275.264423786127,
    "total_throughput": 11313.171693293554,
    "itl": 72.04389199757303,
    "ttft": 1152317.707556231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6099041021382463,
    "arrivals": 109384,
    "finished_requests": 87940,
    "scheduler_time": 218.82768524537835
}
#Debug simulation 
Total elapsed time: 138.68301462789532. Arrivals time: 0.6234112537931651 Scheduler time: 137.67653385503218 Scheduler overhead time: 0.15549042541533709 Adapter cache time: 0.027422377723269165 Engine time: 0.14831679547205567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_96_slots_16_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_96_slots_16_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 134.60355970892124,
    "estimated_duration": 3600.079708291392,
    "input_throughput": 6022.546653637892,
    "output_throughput": 5283.849398164583,
    "total_throughput": 11306.396051802476,
    "itl": 72.69940815045715,
    "ttft": 1054445.5945057757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5843551941635075,
    "arrivals": 109384,
    "finished_requests": 87734,
    "scheduler_time": 203.07096788835298
}
#Debug simulation 
Total elapsed time: 134.60373041999992. Arrivals time: 0.6316127628087997 Scheduler time: 133.58754074689932 Scheduler overhead time: 0.15499240858480334 Adapter cache time: 0.02835153427440673 Engine time: 0.14879174053203315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_96_slots_16_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_96_slots_16_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 154.79661758604925,
    "estimated_duration": 3600.0746048640563,
    "input_throughput": 5924.719163092296,
    "output_throughput": 5164.423808017743,
    "total_throughput": 11089.142971110039,
    "itl": 70.45791431971969,
    "ttft": 1227357.005122606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 214,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.585828137183564,
    "arrivals": 109384,
    "finished_requests": 86205,
    "scheduler_time": 227.35943004018534
}
#Debug simulation 
Total elapsed time: 154.79679459310137. Arrivals time: 0.6632017190568149 Scheduler time: 153.73342627531383 Scheduler overhead time: 0.16293467045761645 Adapter cache time: 0.02906006737612188 Engine time: 0.15409866243135184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_96_slots_16_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_96_slots_16_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 131.1985483940225,
    "estimated_duration": 3600.0010614693037,
    "input_throughput": 6031.660999326944,
    "output_throughput": 5280.867054033813,
    "total_throughput": 11312.528053360757,
    "itl": 72.9599210097043,
    "ttft": 1063760.139164246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.595980235142628,
    "arrivals": 109384,
    "finished_requests": 87873,
    "scheduler_time": 202.44899328064258
}
#Debug simulation 
Total elapsed time: 131.19872508605476. Arrivals time: 0.6276831987779588 Scheduler time: 130.19487855804618 Scheduler overhead time: 0.1521939052036032 Adapter cache time: 0.02708066231571138 Engine time: 0.1461919518187642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_96_slots_16_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_96_slots_16_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [32 32 32]
Adapter prompts. [540, 540, 540, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 540, 8640, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 540, 1080, 1080, 1080, 540, 540, 8640, 1080, 1080, 540, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 8640, 540, 540, 1080, 540, 8640, 540, 540, 8640, 540, 1080, 1080, 540, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 540, 540, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 8640, 540, 540, 8640, 540, 540, 1080]
Prompts retrieved: 328320 . Total input tokens: 73237179 . Total output tokens: 64409037
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 138.6978587739868,
    "estimated_duration": 3600.079995353246,
    "input_throughput": 6050.309723149014,
    "output_throughput": 5284.243134751055,
    "total_throughput": 11334.55285790007,
    "itl": 72.20496403397148,
    "ttft": 1148366.7406563368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.582202269453558,
    "arrivals": 109384,
    "finished_requests": 88077,
    "scheduler_time": 218.40874126738052
}
#Debug simulation 
Total elapsed time: 138.69805431296118. Arrivals time: 0.6265122162876651 Scheduler time: 137.6897027080413 Scheduler overhead time: 0.155514485668391 Adapter cache time: 0.027056655380874872 Engine time: 0.14843595144338906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_96_slots_16_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_96_slots_16_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 124.23400378297083,
    "estimated_duration": 3600.039624100674,
    "input_throughput": 6085.080801151591,
    "output_throughput": 5327.387751958719,
    "total_throughput": 11412.46855311031,
    "itl": 74.46967894634417,
    "ttft": 1009312.8065655011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5473037134762835,
    "arrivals": 106465,
    "finished_requests": 88460,
    "scheduler_time": 198.13672831183766
}
#Debug simulation 
Total elapsed time: 124.23420390102547. Arrivals time: 0.6200072575593367 Scheduler time: 123.23587048542686 Scheduler overhead time: 0.15508564235642552 Adapter cache time: 0.027481027063913643 Engine time: 0.146303927176632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_96_slots_16_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_96_slots_16_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 113.60527023894247,
    "estimated_duration": 3600.046109570867,
    "input_throughput": 6168.528214391987,
    "output_throughput": 5408.977665100287,
    "total_throughput": 11577.505879492273,
    "itl": 74.5496641211047,
    "ttft": 916934.4455591844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8605637794360543,
    "arrivals": 106465,
    "finished_requests": 89714,
    "scheduler_time": 188.7242949119451
}
#Debug simulation 
Total elapsed time: 113.6054867329076. Arrivals time: 0.5988661511801183 Scheduler time: 112.63610220351256 Scheduler overhead time: 0.1512457060161978 Adapter cache time: 0.02536030299961567 Engine time: 0.14342799258884043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_96_slots_16_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_96_slots_16_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 120.97501097095665,
    "estimated_duration": 3600.020064396049,
    "input_throughput": 6020.526444936941,
    "output_throughput": 5267.697863006614,
    "total_throughput": 11288.224307943556,
    "itl": 72.41048030607453,
    "ttft": 1032504.2638651112,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7594639248494102,
    "arrivals": 106465,
    "finished_requests": 87528,
    "scheduler_time": 198.5198762688489
}
#Debug simulation 
Total elapsed time: 120.97520574298687. Arrivals time: 0.6035378143424168 Scheduler time: 119.99586816492956 Scheduler overhead time: 0.15196860721334815 Adapter cache time: 0.026456830906681716 Engine time: 0.14574277424253523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_96_slots_16_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_96_slots_16_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 173.51528457098175,
    "estimated_duration": 3600.011373529936,
    "input_throughput": 5731.059116007661,
    "output_throughput": 5007.918345080774,
    "total_throughput": 10738.977461088434,
    "itl": 69.99744922055753,
    "ttft": 1238518.797425458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.318166441875509,
    "arrivals": 106465,
    "finished_requests": 83389,
    "scheduler_time": 229.26965222018947
}
#Debug simulation 
Total elapsed time: 173.51545844599605. Arrivals time: 0.6631618972169235 Scheduler time: 172.42423679528292 Scheduler overhead time: 0.17459060985129327 Adapter cache time: 0.03109418775420636 Engine time: 0.16605190304107964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_96_slots_16_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_96_slots_16_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 125.04107331100386,
    "estimated_duration": 3600.026080649391,
    "input_throughput": 5994.156296808675,
    "output_throughput": 5244.527560920961,
    "total_throughput": 11238.683857729637,
    "itl": 72.08248397579982,
    "ttft": 1043519.7648860078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7433087850734634,
    "arrivals": 106465,
    "finished_requests": 87154,
    "scheduler_time": 199.9179358358676
}
#Debug simulation 
Total elapsed time: 125.04124008701183. Arrivals time: 0.6098550776951015 Scheduler time: 124.04583635099698 Scheduler overhead time: 0.16014063020702451 Adapter cache time: 0.026779765030369163 Engine time: 0.14810660132206976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_96_slots_16_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_96_slots_16_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 111.99895286804531,
    "estimated_duration": 3600.0362646943486,
    "input_throughput": 6200.2492082937715,
    "output_throughput": 5442.300454621517,
    "total_throughput": 11642.549662915288,
    "itl": 74.7811895214323,
    "ttft": 903800.0729130715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.647051602667192,
    "arrivals": 106465,
    "finished_requests": 90181,
    "scheduler_time": 188.0764192552583
}
#Debug simulation 
Total elapsed time: 111.99916064098943. Arrivals time: 0.594120773486793 Scheduler time: 111.038642243715 Scheduler overhead time: 0.14957232063170522 Adapter cache time: 0.02652540081180632 Engine time: 0.14158460195176303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_96_slots_16_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_96_slots_16_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [32 32 32]
Adapter prompts. [270, 270, 270, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 270, 8640, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 270, 1080, 1080, 1080, 270, 270, 8640, 1080, 1080, 270, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 8640, 270, 270, 1080, 270, 8640, 270, 270, 8640, 270, 1080, 1080, 270, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 270, 270, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 8640, 270, 270, 8640, 270, 270, 1080]
Prompts retrieved: 319680 . Total input tokens: 71345069 . Total output tokens: 62690597
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 128.35318709502462,
    "estimated_duration": 3600.018074404472,
    "input_throughput": 6006.908730194996,
    "output_throughput": 5255.526113748697,
    "total_throughput": 11262.434843943693,
    "itl": 72.2355896284451,
    "ttft": 1037124.7991131135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7212006126716775,
    "arrivals": 106465,
    "finished_requests": 87349,
    "scheduler_time": 199.24693572998734
}
#Debug simulation 
Total elapsed time: 128.35335413599387. Arrivals time: 0.600371906766668 Scheduler time: 127.36370923893992 Scheduler overhead time: 0.1594883088255301 Adapter cache time: 0.027537715388461947 Engine time: 0.14995027717668563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 192.39622866502032,
    "estimated_duration": 3600.0658571000768,
    "input_throughput": 5409.341321240905,
    "output_throughput": 4762.999811845757,
    "total_throughput": 10172.341133086662,
    "itl": 67.54479901596162,
    "ttft": 1195608.4316791228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1968460347829373,
    "arrivals": 105012,
    "finished_requests": 79111,
    "scheduler_time": 222.96096055831816
}
#Debug simulation 
Total elapsed time: 192.39638694596943. Arrivals time: 0.6574869805481285 Scheduler time: 191.29188948625233 Scheduler overhead time: 0.18380294705275446 Adapter cache time: 0.03170856216456741 Engine time: 0.1732088808203116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 129.37331776402425,
    "estimated_duration": 3600.0679520472004,
    "input_throughput": 5929.32641392546,
    "output_throughput": 5224.11444742458,
    "total_throughput": 11153.44086135004,
    "itl": 72.57893103364327,
    "ttft": 1010585.1796401216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.727332476731392,
    "arrivals": 105012,
    "finished_requests": 86706,
    "scheduler_time": 196.9060517891145
}
#Debug simulation 
Total elapsed time: 129.37349418492522. Arrivals time: 0.6143834746908396 Scheduler time: 128.37934465089347 Scheduler overhead time: 0.15370016975793988 Adapter cache time: 0.027867805561982095 Engine time: 0.14724837057292461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 111.83830917393789,
    "estimated_duration": 3600.0678630529906,
    "input_throughput": 5979.3856168436205,
    "output_throughput": 5263.840216592346,
    "total_throughput": 11243.225833435967,
    "itl": 70.3194921876171,
    "ttft": 901341.0550072828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.157978146346293,
    "arrivals": 105012,
    "finished_requests": 87329,
    "scheduler_time": 181.48558818836352
}
#Debug simulation 
Total elapsed time: 111.8384798229672. Arrivals time: 0.5710420270916075 Scheduler time: 110.89883916429244 Scheduler overhead time: 0.15067360340617597 Adapter cache time: 0.02741655835416168 Engine time: 0.14140579919330776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 141.09633157600183,
    "estimated_duration": 3600.0077846032186,
    "input_throughput": 5596.264565361348,
    "output_throughput": 4927.1299011801975,
    "total_throughput": 10523.394466541546,
    "itl": 70.07027042826837,
    "ttft": 1167785.7668048635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.572418389329685,
    "arrivals": 105012,
    "finished_requests": 81783,
    "scheduler_time": 210.6945943648489
}
#Debug simulation 
Total elapsed time: 141.09650442691054. Arrivals time: 0.5563628500094637 Scheduler time: 140.16091746313032 Scheduler overhead time: 0.15584447886794806 Adapter cache time: 0.026770834461785853 Engine time: 0.14565652434248477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 104.8720327350311,
    "estimated_duration": 3600.0267330995466,
    "input_throughput": 6129.948368744456,
    "output_throughput": 5405.69069142573,
    "total_throughput": 11535.639060170186,
    "itl": 71.41123952504972,
    "ttft": 851173.9516942556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1137151396041802,
    "arrivals": 105012,
    "finished_requests": 89612,
    "scheduler_time": 177.19572192903112
}
#Debug simulation 
Total elapsed time: 104.87220931996126. Arrivals time: 0.5336122211301699 Scheduler time: 103.99097167258151 Scheduler overhead time: 0.14131613494828343 Adapter cache time: 0.025524873519316316 Engine time: 0.133480554795824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 88.30454011401162,
    "estimated_duration": 3600.0668536207445,
    "input_throughput": 6227.711293041974,
    "output_throughput": 5482.1693047590115,
    "total_throughput": 11709.880597800986,
    "itl": 74.47174707359996,
    "ttft": 818455.8323345901,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8704888355871594,
    "arrivals": 105012,
    "finished_requests": 91019,
    "scheduler_time": 175.35679286754066
}
#Debug simulation 
Total elapsed time: 88.30470749607775. Arrivals time: 0.5047400980256498 Scheduler time: 87.47573880304117 Scheduler overhead time: 0.13126009062398225 Adapter cache time: 0.023347307927906513 Engine time: 0.12461949337739497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_96_slots_16_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [32 32 32]
Adapter prompts. [135, 135, 135, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 135, 8640, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 135, 1080, 1080, 1080, 135, 135, 8640, 1080, 1080, 135, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 8640, 135, 135, 1080, 135, 8640, 135, 135, 8640, 135, 1080, 1080, 135, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 135, 135, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 8640, 135, 135, 8640, 135, 135, 1080]
Prompts retrieved: 315360 . Total input tokens: 70365562 . Total output tokens: 61863821
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 112.85049403796438,
    "estimated_duration": 3600.005658088813,
    "input_throughput": 6030.783854802594,
    "output_throughput": 5331.04467679882,
    "total_throughput": 11361.828531601413,
    "itl": 71.27907697612125,
    "ttft": 889195.4854622876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.778145505376164,
    "arrivals": 105012,
    "finished_requests": 88096,
    "scheduler_time": 181.8206903208674
}
#Debug simulation 
Total elapsed time: 112.8506703840103. Arrivals time: 0.5391336679458618 Scheduler time: 111.95120515138842 Scheduler overhead time: 0.14749215100891888 Adapter cache time: 0.026111010229215026 Engine time: 0.13743972324300557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 122.77560983994044,
    "estimated_duration": 3600.0271129998973,
    "input_throughput": 6168.3307661246035,
    "output_throughput": 5301.970624353057,
    "total_throughput": 11470.30139047766,
    "itl": 73.98603439401322,
    "ttft": 931915.5079399833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.679551894115282,
    "arrivals": 104295,
    "finished_requests": 89272,
    "scheduler_time": 196.17984446994106
}
#Debug simulation 
Total elapsed time: 122.77577885100618. Arrivals time: 0.5484149587573484 Scheduler time: 121.86453170282766 Scheduler overhead time: 0.14960626931861043 Adapter cache time: 0.026410446385852993 Engine time: 0.13782280613668263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 96.34883979998995,
    "estimated_duration": 3600.0943487228865,
    "input_throughput": 6316.114189636834,
    "output_throughput": 5443.839827962398,
    "total_throughput": 11759.954017599232,
    "itl": 72.97146266955022,
    "ttft": 757667.3104567324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3279971844377028,
    "arrivals": 104295,
    "finished_requests": 91529,
    "scheduler_time": 170.08972660171744
}
#Debug simulation 
Total elapsed time: 96.34902199706994. Arrivals time: 0.5170913846231997 Scheduler time: 95.48841717280447 Scheduler overhead time: 0.14015002583619207 Adapter cache time: 0.026260585873387754 Engine time: 0.1300673650112003 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 122.4929641749477,
    "estimated_duration": 3600.031556297694,
    "input_throughput": 6078.874492563019,
    "output_throughput": 5237.307147215708,
    "total_throughput": 11316.181639778726,
    "itl": 71.52180954128659,
    "ttft": 916369.8189914082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.171648597503089,
    "arrivals": 104295,
    "finished_requests": 88117,
    "scheduler_time": 187.91472579748174
}
#Debug simulation 
Total elapsed time: 122.49314177792985. Arrivals time: 0.598888339009136 Scheduler time: 121.5022896125447 Scheduler overhead time: 0.16281854396220297 Adapter cache time: 0.02922447945456952 Engine time: 0.14867661625612527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 123.2989073030185,
    "estimated_duration": 3600.0779764969766,
    "input_throughput": 6150.506779174091,
    "output_throughput": 5289.43265238077,
    "total_throughput": 11439.939431554862,
    "itl": 73.34666979140057,
    "ttft": 942386.9093892664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7911432530870648,
    "arrivals": 104295,
    "finished_requests": 88987,
    "scheduler_time": 196.97230499556517
}
#Debug simulation 
Total elapsed time: 123.29908954002894. Arrivals time: 0.5379638203885406 Scheduler time: 122.38849493616726 Scheduler overhead time: 0.15431407757569104 Adapter cache time: 0.02663946757093072 Engine time: 0.142070583649911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 113.13033015094697,
    "estimated_duration": 3600.084080644169,
    "input_throughput": 6070.859877275239,
    "output_throughput": 5233.002223833912,
    "total_throughput": 11303.86210110915,
    "itl": 71.50351366778654,
    "ttft": 922029.1431287865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.177234416464352,
    "arrivals": 104295,
    "finished_requests": 87985,
    "scheduler_time": 188.18791614853757
}
#Debug simulation 
Total elapsed time: 113.13050471397582. Arrivals time: 0.5303264170652255 Scheduler time: 112.24270262895152 Scheduler overhead time: 0.1470113875111565 Adapter cache time: 0.025871653575450182 Engine time: 0.13548540405463427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 136.14761222910602,
    "estimated_duration": 3600.05828133046,
    "input_throughput": 5973.079411384711,
    "output_throughput": 5148.535537916369,
    "total_throughput": 11121.614949301082,
    "itl": 71.04601403467507,
    "ttft": 969532.0289900476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 223,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4236143697472246,
    "arrivals": 104295,
    "finished_requests": 86497,
    "scheduler_time": 195.41193991008902
}
#Debug simulation 
Total elapsed time: 136.1477658210788. Arrivals time: 0.5544942826963961 Scheduler time: 135.20622995391022 Scheduler overhead time: 0.15865463006775826 Adapter cache time: 0.028712950297631323 Engine time: 0.14849150727968663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_96_slots_16_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [32 32 32]
Adapter prompts. [66, 66, 66, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 66, 8640, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 66, 1080, 1080, 1080, 66, 66, 8640, 1080, 1080, 66, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 8640, 66, 66, 1080, 66, 8640, 66, 66, 8640, 66, 1080, 1080, 66, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 66, 66, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 8640, 66, 66, 8640, 66, 66, 1080]
Prompts retrieved: 313152 . Total input tokens: 69872105 . Total output tokens: 61434027
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 115.06254076398909,
    "estimated_duration": 3600.055693764266,
    "input_throughput": 6068.406952103816,
    "output_throughput": 5233.079875023071,
    "total_throughput": 11301.486827126888,
    "itl": 71.50103040069195,
    "ttft": 921739.3525832206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1505696661770424,
    "arrivals": 104295,
    "finished_requests": 87965,
    "scheduler_time": 188.17408027250602
}
#Debug simulation 
Total elapsed time: 115.06270408199634. Arrivals time: 0.5336534620728344 Scheduler time: 114.16627030877862 Scheduler overhead time: 0.14834975998383015 Adapter cache time: 0.026989941601641476 Engine time: 0.1371055933414027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 301520,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [8]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 121.46808525198139,
    "estimated_duration": 3600.0445769367766,
    "input_throughput": 6115.667606186604,
    "output_throughput": 5292.041415834552,
    "total_throughput": 11407.709022021156,
    "itl": 73.59523609288755,
    "ttft": 863580.560995926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6663270760513822,
    "arrivals": 103971,
    "finished_requests": 88635,
    "scheduler_time": 183.0349455786416
}
#Debug simulation 
Total elapsed time: 121.4682372600073. Arrivals time: 0.5470473100431263 Scheduler time: 120.5566437211819 Scheduler overhead time: 0.15120747371111065 Adapter cache time: 0.026070787687785923 Engine time: 0.13843772129621357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 117.16125009290408,
    "estimated_duration": 3600.0395065617663,
    "input_throughput": 6123.666131946034,
    "output_throughput": 5317.547756103091,
    "total_throughput": 11441.213888049126,
    "itl": 73.30674324121625,
    "ttft": 865986.3133925229,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8328088376531402,
    "arrivals": 103971,
    "finished_requests": 88726,
    "scheduler_time": 184.29452632546915
}
#Debug simulation 
Total elapsed time: 117.16144765494391. Arrivals time: 0.5383832899387926 Scheduler time: 116.26754725293722 Scheduler overhead time: 0.1467026707250625 Adapter cache time: 0.02422308607492596 Engine time: 0.1356482314877212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 116.97169024997856,
    "estimated_duration": 3600.009076648081,
    "input_throughput": 6122.130397660242,
    "output_throughput": 5294.342206977491,
    "total_throughput": 11416.472604637733,
    "itl": 71.98386916817664,
    "ttft": 865127.2725684494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.954062575013382,
    "arrivals": 103971,
    "finished_requests": 88672,
    "scheduler_time": 183.60071458464463
}
#Debug simulation 
Total elapsed time: 116.97184347000439. Arrivals time: 0.5406559925759211 Scheduler time: 116.06853222567588 Scheduler overhead time: 0.15008278144523501 Adapter cache time: 0.025270904414355755 Engine time: 0.1376568686682731 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [32 64]
---Simulation End---
#Simulation results
{
    "duration": 123.53332803794183,
    "estimated_duration": 3600.0670220654997,
    "input_throughput": 5813.818151638621,
    "output_throughput": 5022.643992229269,
    "total_throughput": 10836.462143867891,
    "itl": 70.18651942112075,
    "ttft": 1028236.6488749344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.578802310270256,
    "arrivals": 103971,
    "finished_requests": 84069,
    "scheduler_time": 193.4986545643896
}
#Debug simulation 
Total elapsed time: 123.53348645393271. Arrivals time: 0.5409694154514 Scheduler time: 122.6241318678949 Scheduler overhead time: 0.1513004229636863 Adapter cache time: 0.025848976452834904 Engine time: 0.1402085575973615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [32 32 32]
---Simulation End---
#Simulation results
{
    "duration": 113.7538791790139,
    "estimated_duration": 3600.028258449217,
    "input_throughput": 6067.388762500771,
    "output_throughput": 5251.455167228011,
    "total_throughput": 11318.843929728782,
    "itl": 72.02522183750568,
    "ttft": 915403.0344424039,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8198234373144853,
    "arrivals": 103971,
    "finished_requests": 87917,
    "scheduler_time": 188.01450188471927
}
#Debug simulation 
Total elapsed time: 113.75403681804892. Arrivals time: 0.5288871699012816 Scheduler time: 112.86737505916972 Scheduler overhead time: 0.145380228292197 Adapter cache time: 0.02654781099408865 Engine time: 0.13628809584770352 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 296000,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [16]. Counts: [96]
---Simulation End---
#Simulation results
{
    "duration": 121.3938230869826,
    "estimated_duration": 3600.003213512548,
    "input_throughput": 5853.114219706669,
    "output_throughput": 5060.0438165238065,
    "total_throughput": 10913.158036230476,
    "itl": 70.66453908270795,
    "ttft": 993965.7646921062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5385249466774935,
    "arrivals": 103971,
    "finished_requests": 84650,
    "scheduler_time": 191.3529701090837
}
#Debug simulation 
Total elapsed time: 121.39398893795442. Arrivals time: 0.5332518169889227 Scheduler time: 120.49223918106873 Scheduler overhead time: 0.15437510469928384 Adapter cache time: 0.025983394240029156 Engine time: 0.13844551681540906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/llama/llama-3.1-8b-instruct",
    "adapter_slots": 16,
    "served_adapters": 96,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 284944,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/llama-3.1-8b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_96_slots_16_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [32 32 32]
Adapter prompts. [33, 33, 33, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 1080, 33, 8640, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 33, 1080, 1080, 1080, 33, 33, 8640, 1080, 1080, 33, 8640, 8640, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 8640, 33, 33, 1080, 33, 8640, 33, 33, 8640, 33, 1080, 1080, 33, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 33, 33, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 8640, 33, 33, 8640, 33, 33, 1080]
Prompts retrieved: 312096 . Total input tokens: 69643671 . Total output tokens: 61225545
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [64 32]
---Simulation End---
#Simulation results
{
    "duration": 113.67117014096584,
    "estimated_duration": 3600.000626478713,
    "input_throughput": 6093.647828460931,
    "output_throughput": 5274.391026585083,
    "total_throughput": 11368.038855046014,
    "itl": 72.20393202388101,
    "ttft": 892134.4617946058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.838093776516621,
    "arrivals": 103971,
    "finished_requests": 88314,
    "scheduler_time": 186.70128612541268
}
#Debug simulation 
Total elapsed time: 113.67130978195928. Arrivals time: 0.5290597231360152 Scheduler time: 112.79278062586673 Scheduler overhead time: 0.14459670800715685 Adapter cache time: 0.02482947614043951 Engine time: 0.1318510518176481 
