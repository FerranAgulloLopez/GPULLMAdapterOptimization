INFO 06-01 00:47:00 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:01 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 59.25401523220353,
    "estimated_duration": 3600.158165903821,
    "input_throughput": 6482.319366138499,
    "output_throughput": 5716.131917452477,
    "total_throughput": 12198.451283590975,
    "itl": 150.27413475237628,
    "ttft": 1998533.4450393787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5411991033772945,
    "arrivals": 744176,
    "finished_requests": 94320,
    "scheduler_time": 129.98953084532457
}
#Debug simulation 
Total elapsed time: 59.254247865173966. Arrivals time: 0.768702335190028 Scheduler time: 58.32985604181886 Scheduler overhead time: 0.06182483024895191 Adapter cache time: 0.012795977760106325 Engine time: 0.058836908312514424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 59.191954276990145,
    "estimated_duration": 3600.123693459772,
    "input_throughput": 6483.669170146758,
    "output_throughput": 5717.1432852130665,
    "total_throughput": 12200.812455359825,
    "itl": 150.23620172322475,
    "ttft": 1998604.4935514708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6069000666216028,
    "arrivals": 744176,
    "finished_requests": 94336,
    "scheduler_time": 130.02037688064485
}
#Debug simulation 
Total elapsed time: 59.192153478972614. Arrivals time: 0.4364704447798431 Scheduler time: 58.601193889044225 Scheduler overhead time: 0.062061311677098274 Adapter cache time: 0.012273788219317794 Engine time: 0.058355910470709205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 112.90133975981735,
    "estimated_duration": 3600.0946692031716,
    "input_throughput": 6503.422312831043,
    "output_throughput": 5761.829314502774,
    "total_throughput": 12265.251627333817,
    "itl": 149.34651564636823,
    "ttft": 2015938.2706916907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3948029189673248,
    "arrivals": 743225,
    "finished_requests": 94823,
    "scheduler_time": 131.1139730619795
}
#Debug simulation 
Total elapsed time: 112.90150080784224. Arrivals time: 0.5572063503786922 Scheduler time: 112.17618786916137 Scheduler overhead time: 0.06862643407657743 Adapter cache time: 0.01287462655454874 Engine time: 0.0631841633003205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 112.99906820314936,
    "estimated_duration": 3600.081717872502,
    "input_throughput": 6496.111430998423,
    "output_throughput": 5755.603240096737,
    "total_throughput": 12251.71467109516,
    "itl": 149.50402867199776,
    "ttft": 2014944.4835285197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41786563513334835,
    "arrivals": 743225,
    "finished_requests": 94717,
    "scheduler_time": 130.97315613556685
}
#Debug simulation 
Total elapsed time: 112.9992339280434. Arrivals time: 0.4875724900048226 Scheduler time: 112.34031372563913 Scheduler overhead time: 0.07120227860286832 Adapter cache time: 0.012685546185821295 Engine time: 0.06360465474426746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 112.77304675290361,
    "estimated_duration": 3600.0824322674985,
    "input_throughput": 6496.110141919745,
    "output_throughput": 5755.602097963401,
    "total_throughput": 12251.712239883147,
    "itl": 149.50404279621318,
    "ttft": 2014944.821240057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 743225,
    "finished_requests": 94717,
    "scheduler_time": 130.97316246851733
}
#Debug simulation 
Total elapsed time: 112.77320633106865. Arrivals time: 0.5487883051391691 Scheduler time: 112.05531391943805 Scheduler overhead time: 0.06930595124140382 Adapter cache time: 0.013109284453094006 Engine time: 0.06309956242330372 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 113.13427815702744,
    "estimated_duration": 3600.104270129281,
    "input_throughput": 6503.40496920086,
    "output_throughput": 5761.8139485874135,
    "total_throughput": 12265.218917788274,
    "itl": 149.34672578518925,
    "ttft": 2015943.4924503772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4036946820537562,
    "arrivals": 743225,
    "finished_requests": 94823,
    "scheduler_time": 131.11407634495154
}
#Debug simulation 
Total elapsed time: 113.13443839503452. Arrivals time: 0.5543133364990354 Scheduler time: 112.41115802596323 Scheduler overhead time: 0.06911236210726202 Adapter cache time: 0.013120600953698158 Engine time: 0.06341497972607613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 112.3142286639195,
    "estimated_duration": 3600.0877621687023,
    "input_throughput": 6496.100524480517,
    "output_throughput": 5755.593576840424,
    "total_throughput": 12251.694101320942,
    "itl": 149.50418389699811,
    "ttft": 2014947.1231067926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42385535620152986,
    "arrivals": 743225,
    "finished_requests": 94717,
    "scheduler_time": 130.97321071069905
}
#Debug simulation 
Total elapsed time: 112.31439541000873. Arrivals time: 0.558096386725083 Scheduler time: 111.58587939850986 Scheduler overhead time: 0.0701758679933846 Adapter cache time: 0.012961678672581911 Engine time: 0.06341296178288758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 113.36629165685736,
    "estimated_duration": 3600.0811767359955,
    "input_throughput": 6503.446686507019,
    "output_throughput": 5761.850908819425,
    "total_throughput": 12265.297595326443,
    "itl": 149.34644842339264,
    "ttft": 2015939.3840845944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3887065383372833,
    "arrivals": 743225,
    "finished_requests": 94823,
    "scheduler_time": 131.11312536898998
}
#Debug simulation 
Total elapsed time: 113.36645240499638. Arrivals time: 0.5546884823124856 Scheduler time: 112.64348572911695 Scheduler overhead time: 0.06847629486583173 Adapter cache time: 0.012932651210576296 Engine time: 0.06335377390496433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 113.12717349990271,
    "estimated_duration": 3600.09385531478,
    "input_throughput": 6496.089529853427,
    "output_throughput": 5755.583835518715,
    "total_throughput": 12251.673365372142,
    "itl": 149.50431049414527,
    "ttft": 2014950.3124473093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 743225,
    "finished_requests": 94717,
    "scheduler_time": 130.9732771665311
}
#Debug simulation 
Total elapsed time: 113.12732975604013. Arrivals time: 0.5642824675887823 Scheduler time: 112.3934568297118 Scheduler overhead time: 0.06965209334157407 Adapter cache time: 0.013047812739387155 Engine time: 0.06367751816287637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 56.03381216409616,
    "estimated_duration": 3600.0250748167105,
    "input_throughput": 6511.36742462647,
    "output_throughput": 5730.530085557903,
    "total_throughput": 12241.897510184373,
    "itl": 149.95635044040435,
    "ttft": 2001165.8716640384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4957990145171055,
    "arrivals": 742746,
    "finished_requests": 94573,
    "scheduler_time": 130.28901213221795
}
#Debug simulation 
Total elapsed time: 56.03398515214212. Arrivals time: 0.4522660542279482 Scheduler time: 55.42404918372631 Scheduler overhead time: 0.06357826851308346 Adapter cache time: 0.012035606196150184 Engine time: 0.05988819641061127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 56.046626368071884,
    "estimated_duration": 3600.0906273894657,
    "input_throughput": 6502.049371177587,
    "output_throughput": 5724.0100688639395,
    "total_throughput": 12226.059440041527,
    "itl": 150.13164558293346,
    "ttft": 2001176.8531515726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.520493364983704,
    "arrivals": 742746,
    "finished_requests": 94468,
    "scheduler_time": 130.13791855099979
}
#Debug simulation 
Total elapsed time: 56.046806587139145. Arrivals time: 0.4493763940408826 Scheduler time: 55.43835016153753 Scheduler overhead time: 0.06347870011813939 Adapter cache time: 0.011754508828744292 Engine time: 0.06133055896498263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 55.99522931012325,
    "estimated_duration": 3600.0933189775683,
    "input_throughput": 6502.044509959507,
    "output_throughput": 5724.005789342262,
    "total_throughput": 12226.050299301769,
    "itl": 150.13150978032243,
    "ttft": 2001177.948577204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5216995077393968,
    "arrivals": 742746,
    "finished_requests": 94468,
    "scheduler_time": 130.13801135188862
}
#Debug simulation 
Total elapsed time: 55.99540797108784. Arrivals time: 0.4453768541570753 Scheduler time: 55.39344857563265 Scheduler overhead time: 0.06293045356869698 Adapter cache time: 0.011878166580572724 Engine time: 0.05945519497618079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 55.83125970908441,
    "estimated_duration": 3600.069650262108,
    "input_throughput": 6502.087257755069,
    "output_throughput": 5724.043421909818,
    "total_throughput": 12226.130679664888,
    "itl": 150.13102906948092,
    "ttft": 2001167.8945286928,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4996550037432465,
    "arrivals": 742746,
    "finished_requests": 94468,
    "scheduler_time": 130.13777978487863
}
#Debug simulation 
Total elapsed time: 55.831431651022285. Arrivals time: 0.447169252904132 Scheduler time: 55.225877054268494 Scheduler overhead time: 0.06402198132127523 Adapter cache time: 0.011614989256486297 Engine time: 0.0598282921127975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 55.88336724205874,
    "estimated_duration": 3600.099771908476,
    "input_throughput": 6502.032855492509,
    "output_throughput": 5723.995529456088,
    "total_throughput": 12226.028384948597,
    "itl": 150.13169870176029,
    "ttft": 2001180.7033337427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5281129508465537,
    "arrivals": 742746,
    "finished_requests": 94468,
    "scheduler_time": 130.1380508396976
}
#Debug simulation 
Total elapsed time: 55.88354598986916. Arrivals time: 0.43553986586630344 Scheduler time: 55.29232474579476 Scheduler overhead time: 0.06230956013314426 Adapter cache time: 0.011713310144841671 Engine time: 0.05960087385028601 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 56.008899769047275,
    "estimated_duration": 3600.0135833911263,
    "input_throughput": 6511.388209240883,
    "output_throughput": 5730.5483777000045,
    "total_throughput": 12241.936586940888,
    "itl": 149.95602838899907,
    "ttft": 2001160.7346962385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48438814777415307,
    "arrivals": 742746,
    "finished_requests": 94573,
    "scheduler_time": 130.28893157336236
}
#Debug simulation 
Total elapsed time: 56.00907236803323. Arrivals time: 0.4933001338504255 Scheduler time: 55.35764174791984 Scheduler overhead time: 0.0643361413385719 Adapter cache time: 0.01185599248856306 Engine time: 0.05981307476758957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 56.01309340912849,
    "estimated_duration": 3600.1063634453385,
    "input_throughput": 6502.020950736116,
    "output_throughput": 5723.985049230305,
    "total_throughput": 12226.00599996642,
    "itl": 150.13187293885747,
    "ttft": 2001183.8892666576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5346521477401258,
    "arrivals": 742746,
    "finished_requests": 94468,
    "scheduler_time": 130.13810317967082
}
#Debug simulation 
Total elapsed time: 56.01327324309386. Arrivals time: 0.4955839242320508 Scheduler time: 55.358093441231176 Scheduler overhead time: 0.06624213769100606 Adapter cache time: 0.011687735328450799 Engine time: 0.0592646743170917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 38.3143786911387,
    "estimated_duration": 3600.014791362117,
    "input_throughput": 6346.257536168526,
    "output_throughput": 5596.333395168395,
    "total_throughput": 11942.590931336921,
    "itl": 153.02109815301475,
    "ttft": 1985970.5526545644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9854770535463454,
    "arrivals": 679344,
    "finished_requests": 92281,
    "scheduler_time": 127.49510784338003
}
#Debug simulation 
Total elapsed time: 38.31448401208036. Arrivals time: 0.8024780252017081 Scheduler time: 37.37186559964903 Scheduler overhead time: 0.05483800545334816 Adapter cache time: 0.012643908616155386 Engine time: 0.05207160394638777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 38.28863295610063,
    "estimated_duration": 3600.10294461597,
    "input_throughput": 6350.077026044215,
    "output_throughput": 5600.102361003626,
    "total_throughput": 11950.179387047841,
    "itl": 152.87591286444209,
    "ttft": 1986348.5196498309,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.033650960356468,
    "arrivals": 679344,
    "finished_requests": 92344,
    "scheduler_time": 127.61847199953637
}
#Debug simulation 
Total elapsed time: 38.28875809418969. Arrivals time: 0.473477580351755 Scheduler time: 37.67441201698966 Scheduler overhead time: 0.054707269417122006 Adapter cache time: 0.012973438017070293 Engine time: 0.053266815608367324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 38.029666813090444,
    "estimated_duration": 3600.1049210696256,
    "input_throughput": 6350.073539858888,
    "output_throughput": 5600.099286553568,
    "total_throughput": 11950.172826412456,
    "itl": 152.87598834472672,
    "ttft": 1986349.2916066034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0356174489110763,
    "arrivals": 679344,
    "finished_requests": 92344,
    "scheduler_time": 127.61848196462023
}
#Debug simulation 
Total elapsed time: 38.029789610998705. Arrivals time: 0.46840892639011145 Scheduler time: 37.422215309459716 Scheduler overhead time: 0.05398075492121279 Adapter cache time: 0.012917176820337772 Engine time: 0.0525358859449625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 38.1462638201192,
    "estimated_duration": 3600.0590768376246,
    "input_throughput": 6350.154403599835,
    "output_throughput": 5600.170599897444,
    "total_throughput": 11950.325003497279,
    "itl": 152.8743540355684,
    "ttft": 1986330.6549099095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.989931261283349,
    "arrivals": 679344,
    "finished_requests": 92344,
    "scheduler_time": 127.61832392025273
}
#Debug simulation 
Total elapsed time: 38.146390749141574. Arrivals time: 0.4692292164545506 Scheduler time: 37.537170277675614 Scheduler overhead time: 0.05461179185658693 Adapter cache time: 0.013003104599192739 Engine time: 0.05229724012315273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 38.055517208063975,
    "estimated_duration": 3600.1184206828825,
    "input_throughput": 6350.049728548558,
    "output_throughput": 5600.078287473611,
    "total_throughput": 11950.128016022169,
    "itl": 152.87646805194782,
    "ttft": 1986354.788623225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0490731040574663,
    "arrivals": 679344,
    "finished_requests": 92344,
    "scheduler_time": 127.6185259227472
}
#Debug simulation 
Total elapsed time: 38.05565601703711. Arrivals time: 0.4774973455350846 Scheduler time: 37.43908482603729 Scheduler overhead time: 0.053630979964509606 Adapter cache time: 0.012921163346618414 Engine time: 0.05253505171276629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 38.07599726994522,
    "estimated_duration": 3600.169815611707,
    "input_throughput": 6346.074815950887,
    "output_throughput": 5596.093526654056,
    "total_throughput": 11942.168342604944,
    "itl": 153.01890407833247,
    "ttft": 1986007.118935406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9627961949585019,
    "arrivals": 679344,
    "finished_requests": 92283,
    "scheduler_time": 127.50128390326411
}
#Debug simulation 
Total elapsed time: 38.0761379620526. Arrivals time: 0.41041640215553343 Scheduler time: 37.524020235287026 Scheduler overhead time: 0.055601118598133326 Adapter cache time: 0.012586446711793542 Engine time: 0.05319393891841173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 38.08838293305598,
    "estimated_duration": 3600.131443730375,
    "input_throughput": 6350.026757998597,
    "output_throughput": 5600.058029856178,
    "total_throughput": 11950.084787854774,
    "itl": 152.87689787662225,
    "ttft": 1986360.700571643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0620257440581968,
    "arrivals": 679344,
    "finished_requests": 92344,
    "scheduler_time": 127.61859633025566
}
#Debug simulation 
Total elapsed time: 38.08852186612785. Arrivals time: 0.48812701972201467 Scheduler time: 37.461678395746276 Scheduler overhead time: 0.05384340928867459 Adapter cache time: 0.012776367831975222 Engine time: 0.05178026342764497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 25.356350492918864,
    "estimated_duration": 3600.0499926170155,
    "input_throughput": 6330.549588683032,
    "output_throughput": 5605.847985830166,
    "total_throughput": 11936.397574513197,
    "itl": 153.8397853393612,
    "ttft": 1969821.1945569278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.181348269158048,
    "arrivals": 634153,
    "finished_requests": 92344,
    "scheduler_time": 127.0679141534454
}
#Debug simulation 
Total elapsed time: 25.356477170949802. Arrivals time: 0.447362368227914 Scheduler time: 24.77496597566642 Scheduler overhead time: 0.05193583597429097 Adapter cache time: 0.013010741211473942 Engine time: 0.049301958875730634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 25.12021207297221,
    "estimated_duration": 3600.112586956682,
    "input_throughput": 6332.728338163586,
    "output_throughput": 5605.476082363092,
    "total_throughput": 11938.20442052668,
    "itl": 153.81852533447486,
    "ttft": 1969864.7152738764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 383,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2522412363789113,
    "arrivals": 634153,
    "finished_requests": 92365,
    "scheduler_time": 127.09083369987496
}
#Debug simulation 
Total elapsed time: 25.12038053991273. Arrivals time: 0.4564171729143709 Scheduler time: 24.531695984536782 Scheduler overhead time: 0.05050116661004722 Adapter cache time: 0.012417420512065291 Engine time: 0.04949850309640169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 25.041082347976044,
    "estimated_duration": 3600.114382527459,
    "input_throughput": 6332.725179691179,
    "output_throughput": 5605.4732866105205,
    "total_throughput": 11938.1984663017,
    "itl": 153.8185900838973,
    "ttft": 1969865.4562967375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 383,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2540265626460385,
    "arrivals": 634153,
    "finished_requests": 92365,
    "scheduler_time": 127.09084394435371
}
#Debug simulation 
Total elapsed time: 25.04119746387005. Arrivals time: 0.46151748951524496 Scheduler time: 24.4482269580476 Scheduler overhead time: 0.05053740134462714 Adapter cache time: 0.012404674896970391 Engine time: 0.04889411525800824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 22.877682113088667,
    "estimated_duration": 3600.15995841312,
    "input_throughput": 6325.929753976401,
    "output_throughput": 5601.053351220594,
    "total_throughput": 11926.983105196996,
    "itl": 153.96471256197677,
    "ttft": 1967421.019848348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 315,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9851769466488658,
    "arrivals": 634153,
    "finished_requests": 92301,
    "scheduler_time": 127.00619611931346
}
#Debug simulation 
Total elapsed time: 22.877804480027407. Arrivals time: 0.46124883042648435 Scheduler time: 22.2835241954308 Scheduler overhead time: 0.05249693221412599 Adapter cache time: 0.011819515377283096 Engine time: 0.04926806641742587 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 25.41895305295475,
    "estimated_duration": 3600.130869581204,
    "input_throughput": 6332.696178528673,
    "output_throughput": 5605.447615949455,
    "total_throughput": 11938.143794478128,
    "itl": 153.81920735427806,
    "ttft": 1969870.8386970577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 383,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.270500308666383,
    "arrivals": 634153,
    "finished_requests": 92365,
    "scheduler_time": 127.09088665162137
}
#Debug simulation 
Total elapsed time: 25.419105018954724. Arrivals time: 0.46677896939218044 Scheduler time: 24.81520195165649 Scheduler overhead time: 0.052932972786948085 Adapter cache time: 0.013133896049112082 Engine time: 0.05108095589093864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 25.31407010415569,
    "estimated_duration": 3600.022370680545,
    "input_throughput": 6330.598161169688,
    "output_throughput": 5605.89099788981,
    "total_throughput": 11936.489159059498,
    "itl": 153.83882710628927,
    "ttft": 1969812.4647979091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1541594138322357,
    "arrivals": 634153,
    "finished_requests": 92344,
    "scheduler_time": 127.06782610391186
}
#Debug simulation 
Total elapsed time: 25.31417555711232. Arrivals time: 0.44797169347293675 Scheduler time: 24.732073046965525 Scheduler overhead time: 0.05177300050854683 Adapter cache time: 0.012500797165557742 Engine time: 0.049997977912425995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 25.040094639873132,
    "estimated_duration": 3600.148088828973,
    "input_throughput": 6332.665889701144,
    "output_throughput": 5605.4208054991705,
    "total_throughput": 11938.086695200314,
    "itl": 153.81975110700722,
    "ttft": 1969876.883963667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 383,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2869740546867272,
    "arrivals": 634153,
    "finished_requests": 92365,
    "scheduler_time": 127.09095570230366
}
#Debug simulation 
Total elapsed time: 25.040221214992926. Arrivals time: 0.37873627501539886 Scheduler time: 24.52906824555248 Scheduler overhead time: 0.05057628499343991 Adapter cache time: 0.01281247055158019 Engine time: 0.04930392582900822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 22.662048052996397,
    "estimated_duration": 3600.09817792282,
    "input_throughput": 6320.818731985103,
    "output_throughput": 5598.474542610679,
    "total_throughput": 11919.293274595782,
    "itl": 153.91416700226628,
    "ttft": 1964864.0026844393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 263,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8049082766543132,
    "arrivals": 626696,
    "finished_requests": 92158,
    "scheduler_time": 126.8827264656607
}
#Debug simulation 
Total elapsed time: 22.662232934962958. Arrivals time: 0.45204669842496514 Scheduler time: 22.08245886559598 Scheduler overhead time: 0.049921410623937845 Adapter cache time: 0.010726895881816745 Engine time: 0.0472506801597774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 22.286911068018526,
    "estimated_duration": 3600.037169400283,
    "input_throughput": 6319.654750617479,
    "output_throughput": 5601.30383413769,
    "total_throughput": 11920.958584755168,
    "itl": 153.9825818990393,
    "ttft": 1964986.4802574245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9557608235022105,
    "arrivals": 626696,
    "finished_requests": 92178,
    "scheduler_time": 126.86657657098216
}
#Debug simulation 
Total elapsed time: 22.28703390620649. Arrivals time: 0.4511759001761675 Scheduler time: 21.70846336800605 Scheduler overhead time: 0.04860597429797053 Adapter cache time: 0.011534776072949171 Engine time: 0.04781579156406224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 22.302873267792165,
    "estimated_duration": 3600.0389291007405,
    "input_throughput": 6319.651661567728,
    "output_throughput": 5601.301096218152,
    "total_throughput": 11920.95275778588,
    "itl": 153.98264925001698,
    "ttft": 1964987.0881700711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9575142841227402,
    "arrivals": 626696,
    "finished_requests": 92178,
    "scheduler_time": 126.86658281080103
}
#Debug simulation 
Total elapsed time: 22.302975739818066. Arrivals time: 0.3677997621707618 Scheduler time: 21.808139208471403 Scheduler overhead time: 0.04934227233752608 Adapter cache time: 0.010805788449943066 Engine time: 0.047485993476584554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 23.855917077045888,
    "estimated_duration": 3600.0008300635186,
    "input_throughput": 6320.095209422092,
    "output_throughput": 5599.314820059178,
    "total_throughput": 11919.41002948127,
    "itl": 153.9661698763154,
    "ttft": 1965000.1159884178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.106283456652893,
    "arrivals": 626696,
    "finished_requests": 92204,
    "scheduler_time": 126.87658615945297
}
#Debug simulation 
Total elapsed time: 23.856090818997473. Arrivals time: 0.45478269807063043 Scheduler time: 23.271300790132955 Scheduler overhead time: 0.05016058590263128 Adapter cache time: 0.012022952316328883 Engine time: 0.04836554452776909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 22.702098719077185,
    "estimated_duration": 3600.051947970512,
    "input_throughput": 6319.628807808068,
    "output_throughput": 5601.2808402300225,
    "total_throughput": 11920.90964803809,
    "itl": 153.98311782175935,
    "ttft": 1964992.0383523395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9703411703370562,
    "arrivals": 626696,
    "finished_requests": 92178,
    "scheduler_time": 126.86663882305578
}
#Debug simulation 
Total elapsed time: 22.702219079015777. Arrivals time: 0.39346588496118784 Scheduler time: 22.17661179509014 Scheduler overhead time: 0.05150941060855985 Adapter cache time: 0.011884615290910006 Engine time: 0.04905813396908343 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 22.35118251107633,
    "estimated_duration": 3600.076621892177,
    "input_throughput": 6313.892560445837,
    "output_throughput": 5599.155550585311,
    "total_throughput": 11913.048111031148,
    "itl": 154.00805513119292,
    "ttft": 1965120.3368594581,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 318,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9508359937788932,
    "arrivals": 626696,
    "finished_requests": 92160,
    "scheduler_time": 126.85746587144378
}
#Debug simulation 
Total elapsed time: 22.351287715137005. Arrivals time: 0.433870849898085 Scheduler time: 21.789769114926457 Scheduler overhead time: 0.04898889665491879 Adapter cache time: 0.011547115631401539 Engine time: 0.047685636440292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 23.02530835615471,
    "estimated_duration": 3600.0755360922303,
    "input_throughput": 6319.315739884289,
    "output_throughput": 5599.625562824175,
    "total_throughput": 11918.941302708465,
    "itl": 153.96656253390648,
    "ttft": 1965315.867976505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 353,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1840934571996367,
    "arrivals": 626696,
    "finished_requests": 92217,
    "scheduler_time": 126.86152036779606
}
#Debug simulation 
Total elapsed time: 23.025482471100986. Arrivals time: 0.38916381797753274 Scheduler time: 22.506331227254122 Scheduler overhead time: 0.0499476392287761 Adapter cache time: 0.012071465607732534 Engine time: 0.04859807342290878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 21.41031355690211,
    "estimated_duration": 3600.044013760736,
    "input_throughput": 6326.291265591636,
    "output_throughput": 5599.074878793873,
    "total_throughput": 11925.366144385509,
    "itl": 153.99726571717775,
    "ttft": 1967983.3271744268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.61209754878655,
    "arrivals": 623022,
    "finished_requests": 92371,
    "scheduler_time": 126.90461419908513
}
#Debug simulation 
Total elapsed time: 21.41043478809297. Arrivals time: 0.3872077208943665 Scheduler time: 20.89591009169817 Scheduler overhead time: 0.04955411679111421 Adapter cache time: 0.010608157142996788 Engine time: 0.04763124673627317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 21.867939182091504,
    "estimated_duration": 3600.0862752931944,
    "input_throughput": 6317.4119342869835,
    "output_throughput": 5596.474489589599,
    "total_throughput": 11913.886423876582,
    "itl": 154.11495625167404,
    "ttft": 1968758.0318734301,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6949012480326952,
    "arrivals": 623022,
    "finished_requests": 92304,
    "scheduler_time": 126.88291178422934
}
#Debug simulation 
Total elapsed time: 21.868066682945937. Arrivals time: 0.4127858921419829 Scheduler time: 21.32634120527655 Scheduler overhead time: 0.050761469174176455 Adapter cache time: 0.010725663276389241 Engine time: 0.048176327953115106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 21.79878697404638,
    "estimated_duration": 3600.0875378264213,
    "input_throughput": 6317.409718801278,
    "output_throughput": 5596.4725269331575,
    "total_throughput": 11913.882245734436,
    "itl": 154.11500312902152,
    "ttft": 1968758.581291638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6961586590111286,
    "arrivals": 623022,
    "finished_requests": 92304,
    "scheduler_time": 126.88291690646871
}
#Debug simulation 
Total elapsed time: 21.798957647988573. Arrivals time: 0.38544420967809856 Scheduler time: 21.287057792767882 Scheduler overhead time: 0.04890613746829331 Adapter cache time: 0.01007257355377078 Engine time: 0.04807752766646445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 21.885260693030432,
    "estimated_duration": 3600.046279547388,
    "input_throughput": 6317.417120220837,
    "output_throughput": 5596.466110578175,
    "total_throughput": 11913.883230799012,
    "itl": 154.11374875178265,
    "ttft": 1968753.5773293246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6650737897865485,
    "arrivals": 623022,
    "finished_requests": 92303,
    "scheduler_time": 126.88237941624591
}
#Debug simulation 
Total elapsed time: 21.885385839035735. Arrivals time: 0.3877812947612256 Scheduler time: 21.369014208437875 Scheduler overhead time: 0.05093762627802789 Adapter cache time: 0.010376733727753162 Engine time: 0.04749829042702913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 22.20017251209356,
    "estimated_duration": 3600.096977494231,
    "input_throughput": 6317.393154178288,
    "output_throughput": 5596.457852650245,
    "total_throughput": 11913.851006828532,
    "itl": 154.1152842834733,
    "ttft": 1968762.2251918172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7053386854194131,
    "arrivals": 623022,
    "finished_requests": 92304,
    "scheduler_time": 126.88296347228119
}
#Debug simulation 
Total elapsed time: 22.20027794712223. Arrivals time: 0.4174939231015742 Scheduler time: 21.65502169635147 Scheduler overhead time: 0.049963159719482064 Adapter cache time: 0.00994736235588789 Engine time: 0.048225989332422614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 21.976854694075882,
    "estimated_duration": 3600.038943055195,
    "input_throughput": 6317.581103913992,
    "output_throughput": 5595.681968624512,
    "total_throughput": 11913.263072538504,
    "itl": 154.09558901874868,
    "ttft": 1968821.9444254797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6279105619294577,
    "arrivals": 623022,
    "finished_requests": 92295,
    "scheduler_time": 126.8833232221096
}
#Debug simulation 
Total elapsed time: 21.977016870165244. Arrivals time: 0.37731487210839987 Scheduler time: 21.472613004036248 Scheduler overhead time: 0.04961231327615678 Adapter cache time: 0.010296131949871778 Engine time: 0.04760959302075207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 21.904415217926726,
    "estimated_duration": 3600.116515949661,
    "input_throughput": 6317.493308685462,
    "output_throughput": 5596.522198861445,
    "total_throughput": 11914.015507546907,
    "itl": 154.1143158350927,
    "ttft": 1968781.1970915024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7140156966820382,
    "arrivals": 623022,
    "finished_requests": 92307,
    "scheduler_time": 126.8834229053685
}
#Debug simulation 
Total elapsed time: 21.904560865135863. Arrivals time: 0.3906202460639179 Scheduler time: 21.387195530813187 Scheduler overhead time: 0.04946326487697661 Adapter cache time: 0.010099099250510335 Engine time: 0.04762043012306094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 23.22962240409106,
    "estimated_duration": 3600.0863178215595,
    "input_throughput": 6358.492541326508,
    "output_throughput": 5593.753655380591,
    "total_throughput": 11952.2461967071,
    "itl": 153.46064400194962,
    "ttft": 1962040.7759155617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.514161940980702,
    "arrivals": 621138,
    "finished_requests": 92528,
    "scheduler_time": 126.94280459426186
}
#Debug simulation 
Total elapsed time: 23.22973310109228. Arrivals time: 0.4493315489962697 Scheduler time: 22.64893034216948 Scheduler overhead time: 0.05169668258167803 Adapter cache time: 0.010464789811521769 Engine time: 0.04975367081351578 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 21.987148731015623,
    "estimated_duration": 3600.0977642910048,
    "input_throughput": 6361.2916924521705,
    "output_throughput": 5596.240802079359,
    "total_throughput": 11957.53249453153,
    "itl": 153.40564530387755,
    "ttft": 1961885.864402286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7745556492265343,
    "arrivals": 621138,
    "finished_requests": 92566,
    "scheduler_time": 126.97915262991691
}
#Debug simulation 
Total elapsed time: 21.98727635294199. Arrivals time: 0.368619172135368 Scheduler time: 21.493186514358968 Scheduler overhead time: 0.04820761503651738 Adapter cache time: 0.010204247431829572 Engine time: 0.04766308283433318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 22.001328637823462,
    "estimated_duration": 3600.099486248788,
    "input_throughput": 6361.288649792993,
    "output_throughput": 5596.238125350441,
    "total_throughput": 11957.526775143433,
    "itl": 153.4057004076573,
    "ttft": 1961886.55387964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7762936001643574,
    "arrivals": 621138,
    "finished_requests": 92566,
    "scheduler_time": 126.9791603196936
}
#Debug simulation 
Total elapsed time: 22.001471252879128. Arrivals time: 0.3807773576118052 Scheduler time: 21.489902352681383 Scheduler overhead time: 0.05255663231946528 Adapter cache time: 0.010445897933095694 Engine time: 0.04809354990720749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 22.4934388929978,
    "estimated_duration": 3600.139521185374,
    "input_throughput": 6361.437623522801,
    "output_throughput": 5594.25430083574,
    "total_throughput": 11955.69192435854,
    "itl": 153.4454007842506,
    "ttft": 1961885.0626180554,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5682962774601772,
    "arrivals": 621138,
    "finished_requests": 92585,
    "scheduler_time": 126.94555799529452
}
#Debug simulation 
Total elapsed time: 22.49354463000782. Arrivals time: 0.4643966464791447 Scheduler time: 21.902787814149633 Scheduler overhead time: 0.04946505045518279 Adapter cache time: 0.009893385460600257 Engine time: 0.04776556114666164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 22.472399132093415,
    "estimated_duration": 3600.107080631154,
    "input_throughput": 6360.113043078093,
    "output_throughput": 5592.8266990519805,
    "total_throughput": 11952.939742130075,
    "itl": 153.44328792686565,
    "ttft": 1962097.874372841,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6051046296395376,
    "arrivals": 621138,
    "finished_requests": 92570,
    "scheduler_time": 126.94437160655738
}
#Debug simulation 
Total elapsed time: 22.472470307024196. Arrivals time: 0.6834276597946882 Scheduler time: 21.661816196981817 Scheduler overhead time: 0.04973843111656606 Adapter cache time: 0.00988017488270998 Engine time: 0.048480644123628736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 22.49449784704484,
    "estimated_duration": 3600.1315723809053,
    "input_throughput": 6360.144216856246,
    "output_throughput": 5592.986421516708,
    "total_throughput": 11953.130638372955,
    "itl": 153.44118333670573,
    "ttft": 1962098.3031204194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5471792039670988,
    "arrivals": 621138,
    "finished_requests": 92572,
    "scheduler_time": 126.94646961879904
}
#Debug simulation 
Total elapsed time: 22.494671897031367. Arrivals time: 0.7672544510569423 Scheduler time: 21.60066935257055 Scheduler overhead time: 0.049872907577082515 Adapter cache time: 0.009771311422809958 Engine time: 0.047708227299153805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 22.59541259892285,
    "estimated_duration": 3600.1475357573963,
    "input_throughput": 6360.064073091636,
    "output_throughput": 5592.871625402424,
    "total_throughput": 11952.935698494059,
    "itl": 153.44362496840708,
    "ttft": 1962106.9618518455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.612524103038014,
    "arrivals": 621138,
    "finished_requests": 92571,
    "scheduler_time": 126.9456774158349
}
#Debug simulation 
Total elapsed time: 22.595559076871723. Arrivals time: 0.7682720716111362 Scheduler time: 21.700680868001655 Scheduler overhead time: 0.04939216305501759 Adapter cache time: 0.009756304323673248 Engine time: 0.047944365069270134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 24.145914647961035,
    "estimated_duration": 3600.166465927785,
    "input_throughput": 6371.341219103536,
    "output_throughput": 5599.131370944874,
    "total_throughput": 11970.47259004841,
    "itl": 153.11600392289103,
    "ttft": 1964176.33024438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6641258404334067,
    "arrivals": 620208,
    "finished_requests": 92711,
    "scheduler_time": 127.10710519140261
}
#Debug simulation 
Total elapsed time: 24.14601572882384. Arrivals time: 0.38575456757098436 Scheduler time: 23.629500047070906 Scheduler overhead time: 0.05205373535864055 Adapter cache time: 0.010183223756030202 Engine time: 0.0490550787653774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 23.479747558943927,
    "estimated_duration": 3600.147790786395,
    "input_throughput": 6374.068880929875,
    "output_throughput": 5606.512058103889,
    "total_throughput": 11980.580939033764,
    "itl": 152.98245700900247,
    "ttft": 1966303.6852208523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7566153474571212,
    "arrivals": 620208,
    "finished_requests": 92806,
    "scheduler_time": 127.22099251589141
}
#Debug simulation 
Total elapsed time: 23.479884176980704. Arrivals time: 0.3802306002471596 Scheduler time: 22.969690486090258 Scheduler overhead time: 0.05069107306189835 Adapter cache time: 0.010346341179683805 Engine time: 0.049159361980855465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 23.52572283288464,
    "estimated_duration": 3600.1492170529195,
    "input_throughput": 6374.066355723135,
    "output_throughput": 5606.509836979156,
    "total_throughput": 11980.57619270229,
    "itl": 152.98250141212483,
    "ttft": 1966304.243285451,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7580324870720541,
    "arrivals": 620208,
    "finished_requests": 92806,
    "scheduler_time": 127.22100164279065
}
#Debug simulation 
Total elapsed time: 23.525881252950057. Arrivals time: 0.3809713367372751 Scheduler time: 23.014783507445827 Scheduler overhead time: 0.05138748208992183 Adapter cache time: 0.010397153906524181 Engine time: 0.04901613341644406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 20.01831118389964,
    "estimated_duration": 3600.1161810322233,
    "input_throughput": 6376.780038642458,
    "output_throughput": 5611.204190140326,
    "total_throughput": 11987.984228782785,
    "itl": 152.9450040025109,
    "ttft": 1967324.736047792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8586288144392921,
    "arrivals": 620208,
    "finished_requests": 92835,
    "scheduler_time": 127.28176217687789
}
#Debug simulation 
Total elapsed time: 20.018431338947266. Arrivals time: 0.3824245489668101 Scheduler time: 19.51054149447009 Scheduler overhead time: 0.048709599766880274 Adapter cache time: 0.010380910243839025 Engine time: 0.046754041220992804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 24.653270666021854,
    "estimated_duration": 3600.0330316602017,
    "input_throughput": 6371.130153053801,
    "output_throughput": 5599.033904059617,
    "total_throughput": 11970.16405711342,
    "itl": 153.118036019827,
    "ttft": 1964120.5828962578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7147664755955363,
    "arrivals": 620208,
    "finished_requests": 92704,
    "scheduler_time": 127.10072836662273
}
#Debug simulation 
Total elapsed time: 24.65344757400453. Arrivals time: 0.4029261108953506 Scheduler time: 24.11713410401717 Scheduler overhead time: 0.052096511237323284 Adapter cache time: 0.011032438836991787 Engine time: 0.05078603164292872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 24.06321939895861,
    "estimated_duration": 3600.1510990019647,
    "input_throughput": 6371.3684146087235,
    "output_throughput": 5599.15527034078,
    "total_throughput": 11970.523684949503,
    "itl": 153.1155140757663,
    "ttft": 1964169.6485745695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.648840913993773,
    "arrivals": 620208,
    "finished_requests": 92711,
    "scheduler_time": 127.10701094219819
}
#Debug simulation 
Total elapsed time: 24.063343949848786. Arrivals time: 0.3921721251681447 Scheduler time: 23.542755854316056 Scheduler overhead time: 0.050006806617602706 Adapter cache time: 0.009975587949156761 Engine time: 0.04926274996250868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 24.182537024840713,
    "estimated_duration": 3600.044158879699,
    "input_throughput": 6371.110460805448,
    "output_throughput": 5599.01659824989,
    "total_throughput": 11970.127059055338,
    "itl": 153.11839423283405,
    "ttft": 1964124.1023045182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7240722557902359,
    "arrivals": 620208,
    "finished_requests": 92704,
    "scheduler_time": 127.10083654706524
}
#Debug simulation 
Total elapsed time: 24.18264877703041. Arrivals time: 0.38134033186361194 Scheduler time: 23.66993757826276 Scheduler overhead time: 0.051395070273429155 Adapter cache time: 0.010449989698827267 Engine time: 0.04949502763338387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 18.79825372598134,
    "estimated_duration": 3600.035406854212,
    "input_throughput": 6366.8996022538895,
    "output_throughput": 5595.772464250043,
    "total_throughput": 11962.672066503932,
    "itl": 153.25818192010232,
    "ttft": 1961254.5795475605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5539482816518277,
    "arrivals": 619792,
    "finished_requests": 92777,
    "scheduler_time": 126.9766929046822
}
#Debug simulation 
Total elapsed time: 18.798408922972158. Arrivals time: 0.36437090556137264 Scheduler time: 18.314382188022137 Scheduler overhead time: 0.04578587878495455 Adapter cache time: 0.009361742762848735 Engine time: 0.04545892169699073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 19.023249464808032,
    "estimated_duration": 3600.1655214366406,
    "input_throughput": 6366.7614345835,
    "output_throughput": 5595.667999164949,
    "total_throughput": 11962.429433748448,
    "itl": 153.25921656175908,
    "ttft": 1961325.0531295764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5952635684772406,
    "arrivals": 619792,
    "finished_requests": 92779,
    "scheduler_time": 126.98048201501693
}
#Debug simulation 
Total elapsed time: 19.023370898794383. Arrivals time: 0.4566111413296312 Scheduler time: 18.44501759414561 Scheduler overhead time: 0.04713298054412007 Adapter cache time: 0.009640548843890429 Engine time: 0.04573757224716246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 19.025842061033472,
    "estimated_duration": 3600.1661819339947,
    "input_throughput": 6366.76026651823,
    "output_throughput": 5595.666972566808,
    "total_throughput": 11962.427239085038,
    "itl": 153.25922126668473,
    "ttft": 1961325.3079645694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5960763672925555,
    "arrivals": 619792,
    "finished_requests": 92779,
    "scheduler_time": 126.980483651238
}
#Debug simulation 
Total elapsed time: 19.025943184038624. Arrivals time: 0.4256088768597692 Scheduler time: 18.478140184422955 Scheduler overhead time: 0.04773327149450779 Adapter cache time: 0.0093709253706038 Engine time: 0.045965644996613264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 18.947400097968057,
    "estimated_duration": 3600.1123063041596,
    "input_throughput": 6366.763603419512,
    "output_throughput": 5595.652936916471,
    "total_throughput": 11962.416540335982,
    "itl": 153.25897875989227,
    "ttft": 1961297.6089752677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5699306587339386,
    "arrivals": 619792,
    "finished_requests": 92777,
    "scheduler_time": 126.97929263176782
}
#Debug simulation 
Total elapsed time: 18.947564753936604. Arrivals time: 0.42569429357536137 Scheduler time: 18.400227376725525 Scheduler overhead time: 0.04751584236510098 Adapter cache time: 0.009452630067244172 Engine time: 0.045691692968830466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 19.279255309840664,
    "estimated_duration": 3600.003087209668,
    "input_throughput": 6366.805095649376,
    "output_throughput": 5595.699090251019,
    "total_throughput": 11962.504185900396,
    "itl": 153.25960472547123,
    "ttft": 1961265.166723337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6038731020502762,
    "arrivals": 619792,
    "finished_requests": 92775,
    "scheduler_time": 126.97444081538096
}
#Debug simulation 
Total elapsed time: 19.279373564990237. Arrivals time: 0.3815907733514905 Scheduler time: 18.773250732570887 Scheduler overhead time: 0.04783030110411346 Adapter cache time: 0.010257970308884978 Engine time: 0.04734464059583843 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 18.937768229981884,
    "estimated_duration": 3600.022336463295,
    "input_throughput": 6366.922718184557,
    "output_throughput": 5595.792780494431,
    "total_throughput": 11962.715498678988,
    "itl": 153.25793904510718,
    "ttft": 1961247.5954441724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 181,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5411991033772945,
    "arrivals": 619792,
    "finished_requests": 92777,
    "scheduler_time": 126.97660157151319
}
#Debug simulation 
Total elapsed time: 18.93788213096559. Arrivals time: 0.3587125928606838 Scheduler time: 18.458730303216726 Scheduler overhead time: 0.046570260310545564 Adapter cache time: 0.009386983467265964 Engine time: 0.04543118737637997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 19.652196089038625,
    "estimated_duration": 3600.05880591533,
    "input_throughput": 6371.063151055493,
    "output_throughput": 5596.053866369486,
    "total_throughput": 11967.11701742498,
    "itl": 153.220358685277,
    "ttft": 1960746.9438902831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5736868704482919,
    "arrivals": 619792,
    "finished_requests": 92793,
    "scheduler_time": 126.98596664130935
}
#Debug simulation 
Total elapsed time: 19.652369493152946. Arrivals time: 0.4427681886591017 Scheduler time: 19.086079076630995 Scheduler overhead time: 0.04823494516313076 Adapter cache time: 0.009192229947075248 Engine time: 0.04665273614227772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 11.641673715785146,
    "estimated_duration": 3600.019864693579,
    "input_throughput": 6305.846315637551,
    "output_throughput": 5597.94133294743,
    "total_throughput": 11903.787648584981,
    "itl": 154.34080728864654,
    "ttft": 1953086.0271257812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4078243622090796,
    "arrivals": 572657,
    "finished_requests": 92308,
    "scheduler_time": 126.71650796632441
}
#Debug simulation 
Total elapsed time: 11.641770835965872. Arrivals time: 0.4176463021431118 Scheduler time: 11.113409975310788 Scheduler overhead time: 0.040213884552940726 Adapter cache time: 0.011983054224401712 Engine time: 0.040430719032883644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.452611231943592,
    "estimated_duration": 3600.1154224703823,
    "input_throughput": 6305.678939710928,
    "output_throughput": 5597.792746925684,
    "total_throughput": 11903.471686636613,
    "itl": 154.34434561053743,
    "ttft": 1953127.8609924512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4996361124608717,
    "arrivals": 572657,
    "finished_requests": 92308,
    "scheduler_time": 126.7169195712434
}
#Debug simulation 
Total elapsed time: 11.452739043859765. Arrivals time: 0.38259823620319366 Scheduler time: 10.960030172485858 Scheduler overhead time: 0.03999327635392547 Adapter cache time: 0.012001068331301212 Engine time: 0.040056812576949596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.565920384135097,
    "estimated_duration": 3600.1563599782708,
    "input_throughput": 6305.894725121611,
    "output_throughput": 5597.892975993309,
    "total_throughput": 11903.787701114921,
    "itl": 154.34474439107217,
    "ttft": 1953152.7978493124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5025419087708085,
    "arrivals": 572657,
    "finished_requests": 92312,
    "scheduler_time": 126.7185022578589
}
#Debug simulation 
Total elapsed time: 11.566019244957715. Arrivals time: 0.37881116033531725 Scheduler time: 11.076433131005615 Scheduler overhead time: 0.04023066791705787 Adapter cache time: 0.01204786659218371 Engine time: 0.04044500458985567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 11.498068788088858,
    "estimated_duration": 3600.042488141399,
    "input_throughput": 6305.806688331609,
    "output_throughput": 5597.906154269938,
    "total_throughput": 11903.712842601546,
    "itl": 154.34165946049248,
    "ttft": 1953097.5664908532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4309920989628828,
    "arrivals": 572657,
    "finished_requests": 92308,
    "scheduler_time": 126.7166069234512
}
#Debug simulation 
Total elapsed time: 11.498174303909764. Arrivals time: 0.3638114526402205 Scheduler time: 11.023013185244054 Scheduler overhead time: 0.04073923081159592 Adapter cache time: 0.012065832503139973 Engine time: 0.04037123452872038 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 11.473805699963123,
    "estimated_duration": 3600.0067520624593,
    "input_throughput": 6305.836506277236,
    "output_throughput": 5597.67811225771,
    "total_throughput": 11903.514618534948,
    "itl": 154.3460135668627,
    "ttft": 1953092.5994936135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5236685448885001,
    "arrivals": 572657,
    "finished_requests": 92304,
    "scheduler_time": 126.71247736209826
}
#Debug simulation 
Total elapsed time: 11.473930893931538. Arrivals time: 0.3864776222035289 Scheduler time: 10.976720665814355 Scheduler overhead time: 0.040303412824869156 Adapter cache time: 0.012146812630817294 Engine time: 0.0401319544762373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 11.501424766844139,
    "estimated_duration": 3600.1299344001036,
    "input_throughput": 6305.941289250414,
    "output_throughput": 5598.050727954698,
    "total_throughput": 11903.992017205113,
    "itl": 154.3393085316627,
    "ttft": 1953119.1052928418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3754231356549889,
    "arrivals": 572657,
    "finished_requests": 92313,
    "scheduler_time": 126.72138354782015
}
#Debug simulation 
Total elapsed time: 11.501527688931674. Arrivals time: 0.31824455596506596 Scheduler time: 11.073462870437652 Scheduler overhead time: 0.039935226552188396 Adapter cache time: 0.011875894153490663 Engine time: 0.039915950037539005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.50790346506983,
    "estimated_duration": 3600.046294619943,
    "input_throughput": 6305.806965295307,
    "output_throughput": 5597.680238200225,
    "total_throughput": 11903.487203495532,
    "itl": 154.3470448700466,
    "ttft": 1953118.2016211767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5407710598409174,
    "arrivals": 572657,
    "finished_requests": 92305,
    "scheduler_time": 126.71344032296355
}
#Debug simulation 
Total elapsed time: 11.508030597120523. Arrivals time: 0.41690758103504777 Scheduler time: 10.98067248496227 Scheduler overhead time: 0.040041049011051655 Adapter cache time: 0.01203507836908102 Engine time: 0.04021137207746506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 10.609777722042054,
    "estimated_duration": 3600.1589186871734,
    "input_throughput": 6340.726761118722,
    "output_throughput": 5592.611452647243,
    "total_throughput": 11933.338213765965,
    "itl": 153.95482936658277,
    "ttft": 1944851.856640337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3619170460500867,
    "arrivals": 564989,
    "finished_requests": 92259,
    "scheduler_time": 126.76338284521438
}
#Debug simulation 
Total elapsed time: 10.609879436902702. Arrivals time: 0.3718488293234259 Scheduler time: 10.12888463633135 Scheduler overhead time: 0.03930027247406542 Adapter cache time: 0.011621403275057673 Engine time: 0.040053061209619045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 10.594194295117632,
    "estimated_duration": 3600.040187661698,
    "input_throughput": 6340.997825034606,
    "output_throughput": 5592.650623457986,
    "total_throughput": 11933.648448492591,
    "itl": 153.95444757591835,
    "ttft": 1944989.4124095712,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4804700247361382,
    "arrivals": 564989,
    "finished_requests": 92255,
    "scheduler_time": 126.75659522603057
}
#Debug simulation 
Total elapsed time: 10.594348627142608. Arrivals time: 0.31781452521681786 Scheduler time: 10.167040715226904 Scheduler overhead time: 0.03940352611243725 Adapter cache time: 0.0118038987275213 Engine time: 0.04016055562533438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 10.47433270397596,
    "estimated_duration": 3600.042995723942,
    "input_throughput": 6340.992879005737,
    "output_throughput": 5592.646261145903,
    "total_throughput": 11933.639140151641,
    "itl": 153.9545552183123,
    "ttft": 1944990.6246997502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4832690531946806,
    "arrivals": 564989,
    "finished_requests": 92255,
    "scheduler_time": 126.75660425979818
}
#Debug simulation 
Total elapsed time: 10.47443283488974. Arrivals time: 0.31933084316551685 Scheduler time: 10.046108480310068 Scheduler overhead time: 0.03960940963588655 Adapter cache time: 0.011962419142946601 Engine time: 0.03937110770493746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 10.587904943153262,
    "estimated_duration": 3600.0563473002703,
    "input_throughput": 6340.804364664586,
    "output_throughput": 5592.66246349135,
    "total_throughput": 11933.466828155935,
    "itl": 153.95712306871215,
    "ttft": 1944790.641156788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3832811773102682,
    "arrivals": 564989,
    "finished_requests": 92256,
    "scheduler_time": 126.759241245854
}
#Debug simulation 
Total elapsed time: 10.588015145156533. Arrivals time: 0.3303956938907504 Scheduler time: 10.148197337985039 Scheduler overhead time: 0.03949566604569554 Adapter cache time: 0.01164441229775548 Engine time: 0.04008498718030751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 10.516031509032473,
    "estimated_duration": 3600.0642946796374,
    "input_throughput": 6340.955363973966,
    "output_throughput": 5592.613173535464,
    "total_throughput": 11933.56853750943,
    "itl": 153.95538571638,
    "ttft": 1944999.5173611452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.504521443098787,
    "arrivals": 564989,
    "finished_requests": 92255,
    "scheduler_time": 126.75665082561065
}
#Debug simulation 
Total elapsed time: 10.516132853925228. Arrivals time: 0.32031741458922625 Scheduler time: 10.086400909349322 Scheduler overhead time: 0.03978357114829123 Adapter cache time: 0.011756015243008733 Engine time: 0.03974395291879773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 10.627847429132089,
    "estimated_duration": 3600.128147715534,
    "input_throughput": 6340.78095650159,
    "output_throughput": 5592.65925374802,
    "total_throughput": 11933.44021024961,
    "itl": 153.9536592524778,
    "ttft": 1944838.7069386155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3305723812314578,
    "arrivals": 564989,
    "finished_requests": 92259,
    "scheduler_time": 126.76329368394217
}
#Debug simulation 
Total elapsed time: 10.627957283053547. Arrivals time: 0.41573113459162414 Scheduler time: 10.102895685704425 Scheduler overhead time: 0.039739109575748444 Adapter cache time: 0.011774966260418296 Engine time: 0.039804653730243444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 10.49858052493073,
    "estimated_duration": 3600.079509129571,
    "input_throughput": 6340.928566191397,
    "output_throughput": 5592.589538353822,
    "total_throughput": 11933.51810454522,
    "itl": 153.95583499716275,
    "ttft": 1945005.0495344568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.521120942905545,
    "arrivals": 564989,
    "finished_requests": 92255,
    "scheduler_time": 126.75667938806446
}
#Debug simulation 
Total elapsed time: 10.498710843967274. Arrivals time: 0.32094649551436305 Scheduler time: 10.06861452292651 Scheduler overhead time: 0.03961266600526869 Adapter cache time: 0.011803243774920702 Engine time: 0.03963969275355339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 9.57879715692252,
    "estimated_duration": 3600.0485983856815,
    "input_throughput": 6318.760532899609,
    "output_throughput": 5597.729710936824,
    "total_throughput": 11916.490243836433,
    "itl": 154.05816151114396,
    "ttft": 1944711.808770371,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3833404602576167,
    "arrivals": 561196,
    "finished_requests": 92075,
    "scheduler_time": 126.70983152918122
}
#Debug simulation 
Total elapsed time: 9.578897430794314. Arrivals time: 0.3725458784028888 Scheduler time: 9.098325121914968 Scheduler overhead time: 0.038982294499874115 Adapter cache time: 0.0116583660710603 Engine time: 0.03949006157927215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.536120631964877,
    "estimated_duration": 3600.041417329216,
    "input_throughput": 6316.458163659099,
    "output_throughput": 5597.510046134342,
    "total_throughput": 11913.96820979344,
    "itl": 154.0728276109611,
    "ttft": 1944418.3720051013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4743600410805142,
    "arrivals": 561196,
    "finished_requests": 92060,
    "scheduler_time": 126.7040507403374
}
#Debug simulation 
Total elapsed time: 9.53624483384192. Arrivals time: 0.3067373817320913 Scheduler time: 9.122439637081698 Scheduler overhead time: 0.03883256367407739 Adapter cache time: 0.011368167353793979 Engine time: 0.038948404137045145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.67798903095536,
    "estimated_duration": 3600.0435602425323,
    "input_throughput": 6316.454403809507,
    "output_throughput": 5597.506714236098,
    "total_throughput": 11913.961118045605,
    "itl": 154.07291939278872,
    "ttft": 1944419.297550702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4764992291107861,
    "arrivals": 561196,
    "finished_requests": 92060,
    "scheduler_time": 126.70405446560238
}
#Debug simulation 
Total elapsed time: 9.678091319045052. Arrivals time: 0.315036135725677 Scheduler time: 9.255858153570443 Scheduler overhead time: 0.03886187868192792 Adapter cache time: 0.011456682812422514 Engine time: 0.038893792778253555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 9.58374913991429,
    "estimated_duration": 3600.1050593060804,
    "input_throughput": 6316.426500170829,
    "output_throughput": 5597.511091491375,
    "total_throughput": 11913.937591662205,
    "itl": 154.06918908979299,
    "ttft": 1944477.7589375477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4057160275825256,
    "arrivals": 561196,
    "finished_requests": 92062,
    "scheduler_time": 126.70857153182997
}
#Debug simulation 
Total elapsed time: 9.583877783967182. Arrivals time: 0.3068988227751106 Scheduler time: 9.169644378125668 Scheduler overhead time: 0.038913962664082646 Adapter cache time: 0.01154635800048709 Engine time: 0.038950551534071565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 9.590650271857157,
    "estimated_duration": 3600.148319260652,
    "input_throughput": 6316.2781039726515,
    "output_throughput": 5597.2341173261175,
    "total_throughput": 11913.51222129877,
    "itl": 154.07900230227668,
    "ttft": 1944474.64971141,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.496825616452847,
    "arrivals": 561196,
    "finished_requests": 92058,
    "scheduler_time": 126.70673864250993
}
#Debug simulation 
Total elapsed time: 9.590751045849174. Arrivals time: 0.3676487817429006 Scheduler time: 9.11481711268425 Scheduler overhead time: 0.03904504934325814 Adapter cache time: 0.01146935299038887 Engine time: 0.03980476642027497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 9.715900654904544,
    "estimated_duration": 3600.005832092613,
    "input_throughput": 6318.835596657109,
    "output_throughput": 5597.796209203911,
    "total_throughput": 11916.63180586102,
    "itl": 154.05657927237658,
    "ttft": 1944686.602139788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3515027332957723,
    "arrivals": 561196,
    "finished_requests": 92075,
    "scheduler_time": 126.70928020085253
}
#Debug simulation 
Total elapsed time: 9.716043469961733. Arrivals time: 0.32421562843956053 Scheduler time: 9.283968626055866 Scheduler overhead time: 0.039067939622327685 Adapter cache time: 0.01156475581228733 Engine time: 0.039203219348564744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.596290378132835,
    "estimated_duration": 3600.009892106489,
    "input_throughput": 6316.299310692945,
    "output_throughput": 5597.194619987438,
    "total_throughput": 11913.493930680383,
    "itl": 154.0786383627606,
    "ttft": 1944480.6017170027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5143053927645092,
    "arrivals": 561196,
    "finished_requests": 92053,
    "scheduler_time": 126.70122981043293
}
#Debug simulation 
Total elapsed time: 9.596385593060404. Arrivals time: 0.3659267029725015 Scheduler time: 9.122826667269692 Scheduler overhead time: 0.03902273532003164 Adapter cache time: 0.0114073536824435 Engine time: 0.03926576185040176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.89952667686157,
    "estimated_duration": 3600.0867135320436,
    "input_throughput": 6333.651885187306,
    "output_throughput": 5594.56302102228,
    "total_throughput": 11928.214906209585,
    "itl": 153.81559256746806,
    "ttft": 1947150.643446441,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3619170460500867,
    "arrivals": 559351,
    "finished_requests": 92112,
    "scheduler_time": 126.73653089600727
}
#Debug simulation 
Total elapsed time: 8.899639986921102. Arrivals time: 0.3670707799028605 Scheduler time: 8.42521921847947 Scheduler overhead time: 0.038876872044056654 Adapter cache time: 0.011484162416309118 Engine time: 0.039064529817551374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.891791332047433,
    "estimated_duration": 3600.0246288613735,
    "input_throughput": 6333.556114367905,
    "output_throughput": 5594.398948978838,
    "total_throughput": 11927.955063346742,
    "itl": 153.81768700849855,
    "ttft": 1947174.971606997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.44538766571321,
    "arrivals": 559351,
    "finished_requests": 92109,
    "scheduler_time": 126.73171103496021
}
#Debug simulation 
Total elapsed time: 8.891894001048058. Arrivals time: 0.3633885777089745 Scheduler time: 8.421364631736651 Scheduler overhead time: 0.03873075102455914 Adapter cache time: 0.011488564079627395 Engine time: 0.03894552355632186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.915974802104756,
    "estimated_duration": 3600.031826540313,
    "input_throughput": 6333.543451451116,
    "output_throughput": 5594.387763886752,
    "total_throughput": 11927.931215337869,
    "itl": 153.81771372912422,
    "ttft": 1947177.4232049754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.449132433664062,
    "arrivals": 559351,
    "finished_requests": 92109,
    "scheduler_time": 126.73182217158634
}
#Debug simulation 
Total elapsed time: 8.916083453921601. Arrivals time: 0.31248814845457673 Scheduler time: 8.496339991688728 Scheduler overhead time: 0.038901462219655514 Adapter cache time: 0.011420853203162551 Engine time: 0.03890757588669658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 8.896958462893963,
    "estimated_duration": 3600.109868758925,
    "input_throughput": 6333.611148334338,
    "output_throughput": 5594.527037849327,
    "total_throughput": 11928.138186183665,
    "itl": 153.81631276884175,
    "ttft": 1947162.727820016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3845069632655893,
    "arrivals": 559351,
    "finished_requests": 92112,
    "scheduler_time": 126.7366635240777
}
#Debug simulation 
Total elapsed time: 8.89708409504965. Arrivals time: 0.3081831745803356 Scheduler time: 8.481526101473719 Scheduler overhead time: 0.038855711463838816 Adapter cache time: 0.011455387342721224 Engine time: 0.039078725734725595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 8.898831102997065,
    "estimated_duration": 3600.050337800832,
    "input_throughput": 6333.510884719588,
    "output_throughput": 5594.358997853051,
    "total_throughput": 11927.869882572639,
    "itl": 153.8182941128895,
    "ttft": 1947185.1205414135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4678697478398723,
    "arrivals": 559351,
    "finished_requests": 92109,
    "scheduler_time": 126.73187262161403
}
#Debug simulation 
Total elapsed time: 8.89892736193724. Arrivals time: 0.3154213216621429 Scheduler time: 8.476072652963921 Scheduler overhead time: 0.03884669509716332 Adapter cache time: 0.011541597777977586 Engine time: 0.03909208136610687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.906522441888228,
    "estimated_duration": 3600.055605162844,
    "input_throughput": 6333.706614781188,
    "output_throughput": 5594.611364090014,
    "total_throughput": 11928.317978871202,
    "itl": 153.81449706575515,
    "ttft": 1947137.0637530137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3305723812314578,
    "arrivals": 559351,
    "finished_requests": 92112,
    "scheduler_time": 126.73643760543197
}
#Debug simulation 
Total elapsed time: 8.90663675405085. Arrivals time: 0.31816613976843655 Scheduler time: 8.480659700930119 Scheduler overhead time: 0.03876474313437939 Adapter cache time: 0.011463980190455914 Engine time: 0.0396438161842525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.88370753498748,
    "estimated_duration": 3600.1183762987894,
    "input_throughput": 6333.418687038095,
    "output_throughput": 5594.314101611774,
    "total_throughput": 11927.73278864987,
    "itl": 153.81970972745415,
    "ttft": 1947194.3480918782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4844692476466308,
    "arrivals": 559351,
    "finished_requests": 92110,
    "scheduler_time": 126.7339317636612
}
#Debug simulation 
Total elapsed time: 8.883833179017529. Arrivals time: 0.35894466657191515 Scheduler time: 8.417846921598539 Scheduler overhead time: 0.03889172920025885 Adapter cache time: 0.011497287079691887 Engine time: 0.038690150482580066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.571664858842269,
    "estimated_duration": 3600.140579063947,
    "input_throughput": 6320.441799502249,
    "output_throughput": 5594.298488543067,
    "total_throughput": 11914.740288045316,
    "itl": 154.0312397093136,
    "ttft": 1944807.5488834942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3741589970258181,
    "arrivals": 558425,
    "finished_requests": 92110,
    "scheduler_time": 126.70810281218498
}
#Debug simulation 
Total elapsed time: 8.571762586943805. Arrivals time: 0.3085318375378847 Scheduler time: 8.157233400270343 Scheduler overhead time: 0.038391033886000514 Adapter cache time: 0.011334766633808613 Engine time: 0.03843156713992357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.674043243052438,
    "estimated_duration": 3600.0869624384663,
    "input_throughput": 6319.898446172295,
    "output_throughput": 5593.787375169331,
    "total_throughput": 11913.685821341625,
    "itl": 154.03365001945508,
    "ttft": 1944828.9876794645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4659283685800681,
    "arrivals": 558425,
    "finished_requests": 92102,
    "scheduler_time": 126.70331163659137
}
#Debug simulation 
Total elapsed time: 8.674170197919011. Arrivals time: 0.3089907851535827 Scheduler time: 8.25815525231883 Scheduler overhead time: 0.038922657491639256 Adapter cache time: 0.011555345263332129 Engine time: 0.03860473306849599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.560536203207448,
    "estimated_duration": 3600.0895811122564,
    "input_throughput": 6319.8938491332365,
    "output_throughput": 5593.7833062971395,
    "total_throughput": 11913.677155430376,
    "itl": 154.0337366694373,
    "ttft": 1944830.2598817185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4683887064457026,
    "arrivals": 558425,
    "finished_requests": 92102,
    "scheduler_time": 126.70332100273731
}
#Debug simulation 
Total elapsed time: 8.560633483109996. Arrivals time: 0.30565499141812325 Scheduler time: 8.14877282269299 Scheduler overhead time: 0.038266552379354835 Adapter cache time: 0.011503905989229679 Engine time: 0.03858610475435853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 8.716514752944931,
    "estimated_duration": 3600.017652604197,
    "input_throughput": 6320.019843108665,
    "output_throughput": 5593.692849098371,
    "total_throughput": 11913.712692207037,
    "itl": 154.0312155137303,
    "ttft": 1944808.460612478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.402187498903364,
    "arrivals": 558425,
    "finished_requests": 92101,
    "scheduler_time": 126.70295496485352
}
#Debug simulation 
Total elapsed time: 8.716633049072698. Arrivals time: 0.3654407530557364 Scheduler time: 8.24493681336753 Scheduler overhead time: 0.03854936012066901 Adapter cache time: 0.011512767989188433 Engine time: 0.038355199387297034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 8.637482579099014,
    "estimated_duration": 3600.112529595727,
    "input_throughput": 6319.853563731505,
    "output_throughput": 5593.747649399559,
    "total_throughput": 11913.601213131064,
    "itl": 154.03455505047583,
    "ttft": 1944837.5222212772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4880062971264167,
    "arrivals": 558425,
    "finished_requests": 92102,
    "scheduler_time": 126.70344629537044
}
#Debug simulation 
Total elapsed time: 8.637582710012794. Arrivals time: 0.3145373547449708 Scheduler time: 8.216372096678242 Scheduler overhead time: 0.038528073113411665 Adapter cache time: 0.011514310725033283 Engine time: 0.03885616664774716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.670804182067513,
    "estimated_duration": 3600.065569871024,
    "input_throughput": 6320.207384671373,
    "output_throughput": 5594.374771546603,
    "total_throughput": 11914.582156217975,
    "itl": 154.02541936442162,
    "ttft": 1944795.2534254526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 447,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.336552481821262,
    "arrivals": 558425,
    "finished_requests": 92106,
    "scheduler_time": 126.70766212252757
}
#Debug simulation 
Total elapsed time: 8.670906534185633. Arrivals time: 0.3094936648849398 Scheduler time: 8.254638772457838 Scheduler overhead time: 0.038876514649018645 Adapter cache time: 0.011552267475053668 Engine time: 0.03842686326242983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.600035798968747,
    "estimated_duration": 3600.1449710233137,
    "input_throughput": 6319.917165317024,
    "output_throughput": 5593.7664072110465,
    "total_throughput": 11913.68357252807,
    "itl": 154.03573123397817,
    "ttft": 1944852.0782402907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5063663499429825,
    "arrivals": 558425,
    "finished_requests": 92103,
    "scheduler_time": 126.7040637177739
}
#Debug simulation 
Total elapsed time: 8.600159288849682. Arrivals time: 0.3081938438117504 Scheduler time: 8.185249564703554 Scheduler overhead time: 0.038617224199697375 Adapter cache time: 0.011567815439775586 Engine time: 0.03860649582929909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.362815876025707,
    "estimated_duration": 3600.1717939125815,
    "input_throughput": 6328.288566263127,
    "output_throughput": 5595.170217726872,
    "total_throughput": 11923.458783989998,
    "itl": 153.947665911411,
    "ttft": 1942264.988271379,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.202771683365578,
    "arrivals": 557951,
    "finished_requests": 92292,
    "scheduler_time": 126.69878681849593
}
#Debug simulation 
Total elapsed time: 8.362915019039065. Arrivals time: 0.29845626442693174 Scheduler time: 7.9589967366773635 Scheduler overhead time: 0.038317218190059066 Adapter cache time: 0.010596609907224774 Engine time: 0.038685648469254375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.41703571099788,
    "estimated_duration": 3600.1659930092046,
    "input_throughput": 6328.07099568138,
    "output_throughput": 5595.176733271393,
    "total_throughput": 11923.247728952772,
    "itl": 153.95200985769347,
    "ttft": 1942268.3429752628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2792815720988489,
    "arrivals": 557951,
    "finished_requests": 92290,
    "scheduler_time": 126.69643041806486
}
#Debug simulation 
Total elapsed time: 8.417161827906966. Arrivals time: 0.39561826433055103 Scheduler time: 7.9154897967819124 Scheduler overhead time: 0.03852375992573798 Adapter cache time: 0.010888980235904455 Engine time: 0.03868144960142672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.375780427828431,
    "estimated_duration": 3600.0074313331625,
    "input_throughput": 6327.986937394672,
    "output_throughput": 5595.127616872943,
    "total_throughput": 11923.114554267615,
    "itl": 153.95179292623448,
    "ttft": 1942256.9093952267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2821010186709536,
    "arrivals": 557951,
    "finished_requests": 92284,
    "scheduler_time": 126.69075673644257
}
#Debug simulation 
Total elapsed time: 8.375879836035892. Arrivals time: 0.3058189656585455 Scheduler time: 7.964680148055777 Scheduler overhead time: 0.038309725234285 Adapter cache time: 0.0107332409825176 Engine time: 0.03848390583880246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 8.40347279701382,
    "estimated_duration": 3600.051621802304,
    "input_throughput": 6328.687028269273,
    "output_throughput": 5595.172546419153,
    "total_throughput": 11923.859574688426,
    "itl": 153.94648760461598,
    "ttft": 1942264.075728685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2329614719282826,
    "arrivals": 557951,
    "finished_requests": 92294,
    "scheduler_time": 126.6941540565132
}
#Debug simulation 
Total elapsed time: 8.403582392958924. Arrivals time: 0.366221581120044 Scheduler time: 7.930641425540671 Scheduler overhead time: 0.03919311077333987 Adapter cache time: 0.010804658522829413 Engine time: 0.03881619800813496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 8.378459646133706,
    "estimated_duration": 3600.0238532983,
    "input_throughput": 6327.958071480137,
    "output_throughput": 5595.102093989093,
    "total_throughput": 11923.06016546923,
    "itl": 153.9524168712866,
    "ttft": 1942264.0777134746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2981975033320532,
    "arrivals": 557951,
    "finished_requests": 92284,
    "scheduler_time": 126.69081027429162
}
#Debug simulation 
Total elapsed time: 8.378584693185985. Arrivals time: 0.30568566475994885 Scheduler time: 7.967321127187461 Scheduler overhead time: 0.0385324468370527 Adapter cache time: 0.010698613012209535 Engine time: 0.038460428826510906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.395987189840525,
    "estimated_duration": 3600.103732369098,
    "input_throughput": 6328.180434125414,
    "output_throughput": 5595.273496951225,
    "total_throughput": 11923.453931076638,
    "itl": 153.9475466117532,
    "ttft": 1942237.0058114468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1750897658965502,
    "arrivals": 557951,
    "finished_requests": 92290,
    "scheduler_time": 126.6971698275052
}
#Debug simulation 
Total elapsed time: 8.396089956862852. Arrivals time: 0.31118874601088464 Scheduler time: 7.978721791878343 Scheduler overhead time: 0.0387062500230968 Adapter cache time: 0.010677085258066654 Engine time: 0.03882351820357144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.379202902084216,
    "estimated_duration": 3600.0397458174048,
    "input_throughput": 6327.930136456735,
    "output_throughput": 5595.077394187646,
    "total_throughput": 11923.007530644381,
    "itl": 153.95299622266896,
    "ttft": 1942270.250990649,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3141682342067382,
    "arrivals": 557951,
    "finished_requests": 92284,
    "scheduler_time": 126.69088436691695
}
#Debug simulation 
Total elapsed time: 8.379331260919571. Arrivals time: 0.2962276160251349 Scheduler time: 7.977910201065242 Scheduler overhead time: 0.03853909741155803 Adapter cache time: 0.010503183817490935 Engine time: 0.038339925929903984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_128_slots_64_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_128_slots_64_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.271353763062507,
    "estimated_duration": 3600.073422313013,
    "input_throughput": 6305.427789140885,
    "output_throughput": 5597.186400452254,
    "total_throughput": 11902.614189593138,
    "itl": 154.38363341928866,
    "ttft": 1926396.5395440543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1009,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0880321336282197,
    "arrivals": 518627,
    "finished_requests": 92051,
    "scheduler_time": 126.45594027729828
}
#Debug simulation 
Total elapsed time: 7.271451737964526. Arrivals time: 0.3513658738229424 Scheduler time: 6.8083202538546175 Scheduler overhead time: 0.03835540381260216 Adapter cache time: 0.017071983544155955 Engine time: 0.03853063681162894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_128_slots_64_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_128_slots_64_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.31998890102841,
    "estimated_duration": 3600.0292176144217,
    "input_throughput": 6303.92605953307,
    "output_throughput": 5596.762354432089,
    "total_throughput": 11900.688413965157,
    "itl": 154.39666707433216,
    "ttft": 1926478.4416060655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1012,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2939694273378732,
    "arrivals": 518627,
    "finished_requests": 92035,
    "scheduler_time": 126.4469694194161
}
#Debug simulation 
Total elapsed time: 7.320105798076838. Arrivals time: 0.3064767709001899 Scheduler time: 6.902015808504075 Scheduler overhead time: 0.038122717989608645 Adapter cache time: 0.017187208868563175 Engine time: 0.0384091017767787 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_128_slots_64_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_128_slots_64_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.2741344519890845,
    "estimated_duration": 3600.036535353069,
    "input_throughput": 6303.913245639959,
    "output_throughput": 5596.7509779797165,
    "total_throughput": 11900.664223619675,
    "itl": 154.3969760968046,
    "ttft": 1926481.444013686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1012,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.30127543136474,
    "arrivals": 518627,
    "finished_requests": 92035,
    "scheduler_time": 126.44698115400085
}
#Debug simulation 
Total elapsed time: 7.274231496965513. Arrivals time: 0.2919797084759921 Scheduler time: 6.8705063906963915 Scheduler overhead time: 0.038422986632212996 Adapter cache time: 0.01727282372303307 Engine time: 0.03829033765941858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_128_slots_64_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_128_slots_64_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.289497168036178,
    "estimated_duration": 3600.1226728537467,
    "input_throughput": 6305.341529377984,
    "output_throughput": 5597.1098296012415,
    "total_throughput": 11902.451358979226,
    "itl": 154.38507018322022,
    "ttft": 1926417.2987931473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1009,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1334104133117546,
    "arrivals": 518627,
    "finished_requests": 92051,
    "scheduler_time": 126.45618432723077
}
#Debug simulation 
Total elapsed time: 7.289596299873665. Arrivals time: 0.35094557725824416 Scheduler time: 6.827161800349131 Scheduler overhead time: 0.03824061038903892 Adapter cache time: 0.017117423005402088 Engine time: 0.03823568834923208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_128_slots_64_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_128_slots_64_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.234353461069986,
    "estimated_duration": 3600.0867100064606,
    "input_throughput": 6303.825387572199,
    "output_throughput": 5596.672975680589,
    "total_throughput": 11900.498363252787,
    "itl": 154.39875688484221,
    "ttft": 1926503.515483249,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1012,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.347930086124696,
    "arrivals": 518627,
    "finished_requests": 92035,
    "scheduler_time": 126.44715923942992
}
#Debug simulation 
Total elapsed time: 7.23448010510765. Arrivals time: 0.3393962434493005 Scheduler time: 6.7841221538838 Scheduler overhead time: 0.037815883522853255 Adapter cache time: 0.01747816218994558 Engine time: 0.03793115587905049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_128_slots_64_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_128_slots_64_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.191260827006772,
    "estimated_duration": 3600.159656931188,
    "input_throughput": 6304.242356669964,
    "output_throughput": 5596.455413084227,
    "total_throughput": 11900.697769754192,
    "itl": 154.37918270124166,
    "ttft": 1926476.2992291574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1010,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.019950797851127,
    "arrivals": 518627,
    "finished_requests": 92041,
    "scheduler_time": 126.46104370905421
}
#Debug simulation 
Total elapsed time: 7.191353403031826. Arrivals time: 0.33885455084964633 Scheduler time: 6.744301786413416 Scheduler overhead time: 0.037149904295802116 Adapter cache time: 0.016632501035928726 Engine time: 0.03741964092478156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_128_slots_64_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_128_slots_64_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.185199287021533,
    "estimated_duration": 3600.126169995497,
    "input_throughput": 6303.756293082469,
    "output_throughput": 5596.611632093217,
    "total_throughput": 11900.367925175686,
    "itl": 154.40025912073745,
    "ttft": 1926520.3425373759,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1012,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3837699152529646,
    "arrivals": 518627,
    "finished_requests": 92035,
    "scheduler_time": 126.44734023925207
}
#Debug simulation 
Total elapsed time: 7.185291124042124. Arrivals time: 0.2978530302643776 Scheduler time: 6.7793362063821405 Scheduler overhead time: 0.036813967395573854 Adapter cache time: 0.016776796197518706 Engine time: 0.03758858283981681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.783638857072219,
    "estimated_duration": 3600.021308971345,
    "input_throughput": 6319.420372125659,
    "output_throughput": 5592.965783236775,
    "total_throughput": 11912.386155362434,
    "itl": 153.84269965846622,
    "ttft": 1927153.9670796965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.629738464304336,
    "arrivals": 514902,
    "finished_requests": 92444,
    "scheduler_time": 126.50020868134447
}
#Debug simulation 
Total elapsed time: 6.783761579077691. Arrivals time: 0.2869064752012491 Scheduler time: 6.387562603224069 Scheduler overhead time: 0.03680138778872788 Adapter cache time: 0.01825221930630505 Engine time: 0.037249427288770676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.783960992004722,
    "estimated_duration": 3600.067648043379,
    "input_throughput": 6319.3081975458135,
    "output_throughput": 5592.720462056549,
    "total_throughput": 11912.028659602362,
    "itl": 153.85196331674854,
    "ttft": 1927136.5946398948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.862173713974191,
    "arrivals": 514902,
    "finished_requests": 92442,
    "scheduler_time": 126.4944032196315
}
#Debug simulation 
Total elapsed time: 6.7840738678351045. Arrivals time: 0.28851414239034057 Scheduler time: 6.385189831955358 Scheduler overhead time: 0.03708199108950794 Adapter cache time: 0.01830225600861013 Engine time: 0.03801390784792602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.799381033983082,
    "estimated_duration": 3600.076473028227,
    "input_throughput": 6319.292706819571,
    "output_throughput": 5592.706752438515,
    "total_throughput": 11911.999459258086,
    "itl": 153.8523290952572,
    "ttft": 1927140.0961381954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.870988454297129,
    "arrivals": 514902,
    "finished_requests": 92442,
    "scheduler_time": 126.49441346411024
}
#Debug simulation 
Total elapsed time: 6.7994955910835415. Arrivals time: 0.28562258975580335 Scheduler time: 6.404203009791672 Scheduler overhead time: 0.03697954979725182 Adapter cache time: 0.01860915869474411 Engine time: 0.03706187219358981 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.762029517907649,
    "estimated_duration": 3600.0877886496014,
    "input_throughput": 6319.303676906607,
    "output_throughput": 5592.862502820409,
    "total_throughput": 11912.166179727015,
    "itl": 153.8441342379685,
    "ttft": 1927175.9314878238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6855306533420893,
    "arrivals": 514902,
    "finished_requests": 92444,
    "scheduler_time": 126.50069717685126
}
#Debug simulation 
Total elapsed time: 6.762119319988415. Arrivals time: 0.2741723209619522 Scheduler time: 6.379191542277113 Scheduler overhead time: 0.036848562536761165 Adapter cache time: 0.01787584531120956 Engine time: 0.03708374802954495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.706824342021719,
    "estimated_duration": 3600.1521402258168,
    "input_throughput": 6319.198221043242,
    "output_throughput": 5592.6775357713295,
    "total_throughput": 11911.875756814572,
    "itl": 153.85526786992784,
    "ttft": 1927163.3857860605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9243080597370947,
    "arrivals": 514902,
    "finished_requests": 92443,
    "scheduler_time": 126.49544091549168
}
#Debug simulation 
Total elapsed time: 6.706940370146185. Arrivals time: 0.2727591071743518 Scheduler time: 6.325943823670968 Scheduler overhead time: 0.03642328246496618 Adapter cache time: 0.01764877955429256 Engine time: 0.03733885916881263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.709869466954842,
    "estimated_duration": 3600.096151202509,
    "input_throughput": 6319.594545384747,
    "output_throughput": 5593.125615068424,
    "total_throughput": 11912.720160453171,
    "itl": 153.84001141594126,
    "ttft": 1927190.3276736022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.543209599458989,
    "arrivals": 514902,
    "finished_requests": 92449,
    "scheduler_time": 126.50582313289574
}
#Debug simulation 
Total elapsed time: 6.7099590678699315. Arrivals time: 0.26904942211695015 Scheduler time: 6.33233055844903 Scheduler overhead time: 0.036706098821014166 Adapter cache time: 0.017761699156835675 Engine time: 0.03723192703910172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.819255696842447,
    "estimated_duration": 3600.043757524425,
    "input_throughput": 6319.350133578383,
    "output_throughput": 5592.757576326042,
    "total_throughput": 11912.107709904425,
    "itl": 153.856892184755,
    "ttft": 1927143.8552915824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.967315854691029,
    "arrivals": 514902,
    "finished_requests": 92442,
    "scheduler_time": 126.49013683015892
}
#Debug simulation 
Total elapsed time: 6.819350289879367. Arrivals time: 0.27449802681803703 Scheduler time: 6.435102657647803 Scheduler overhead time: 0.03702659532427788 Adapter cache time: 0.01789522240869701 Engine time: 0.0378393167629838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.6259906040504575,
    "estimated_duration": 3600.0070492541568,
    "input_throughput": 6335.916482365111,
    "output_throughput": 5591.517939991367,
    "total_throughput": 11927.434422356477,
    "itl": 153.95746243895698,
    "ttft": 1924119.8930935015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9082428490022263,
    "arrivals": 512974,
    "finished_requests": 92212,
    "scheduler_time": 126.50996759189638
}
#Debug simulation 
Total elapsed time: 6.626082133967429. Arrivals time: 0.27588403900153935 Scheduler time: 6.240953338332474 Scheduler overhead time: 0.0366643073502928 Adapter cache time: 0.018614345230162144 Engine time: 0.037198343547061086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.602448801975697,
    "estimated_duration": 3600.125933226377,
    "input_throughput": 6335.533096076775,
    "output_throughput": 5591.184134484077,
    "total_throughput": 11926.717230560853,
    "itl": 153.96827704315166,
    "ttft": 1924226.9637307667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.166268608714442,
    "arrivals": 512974,
    "finished_requests": 92210,
    "scheduler_time": 126.50527459644223
}
#Debug simulation 
Total elapsed time: 6.602552870055661. Arrivals time: 0.3180515640415251 Scheduler time: 6.175749109359458 Scheduler overhead time: 0.03635940630920231 Adapter cache time: 0.018476586788892746 Engine time: 0.03708748403005302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.630998458946124,
    "estimated_duration": 3600.0042817020344,
    "input_throughput": 6334.992187625295,
    "output_throughput": 5590.893072628266,
    "total_throughput": 11925.88526025356,
    "itl": 153.97129269187164,
    "ttft": 1924202.117339082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.177980279345005,
    "arrivals": 512974,
    "finished_requests": 92198,
    "scheduler_time": 126.49998237270806
}
#Debug simulation 
Total elapsed time: 6.631122533930466. Arrivals time: 0.3189016352407634 Scheduler time: 6.202918836381286 Scheduler overhead time: 0.03653711429797113 Adapter cache time: 0.01862248918041587 Engine time: 0.037220294354483485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.570411005988717,
    "estimated_duration": 3600.0749284592694,
    "input_throughput": 6335.797019025312,
    "output_throughput": 5591.412512243144,
    "total_throughput": 11927.209531268456,
    "itl": 153.95992563868677,
    "ttft": 1924148.7560844235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9747862335526785,
    "arrivals": 512974,
    "finished_requests": 92212,
    "scheduler_time": 126.51014153610113
}
#Debug simulation 
Total elapsed time: 6.570504687028006. Arrivals time: 0.31464487477205694 Scheduler time: 6.147645024349913 Scheduler overhead time: 0.03627959475852549 Adapter cache time: 0.018457813654094934 Engine time: 0.03670106711797416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.586265978170559,
    "estimated_duration": 3600.063486740282,
    "input_throughput": 6334.888005169583,
    "output_throughput": 5590.801127294684,
    "total_throughput": 11925.689132464267,
    "itl": 153.97360145267064,
    "ttft": 1924225.9241203368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2349467445909905,
    "arrivals": 512974,
    "finished_requests": 92198,
    "scheduler_time": 126.50010611476843
}
#Debug simulation 
Total elapsed time: 6.586375004146248. Arrivals time: 0.3112178328447044 Scheduler time: 6.166451808065176 Scheduler overhead time: 0.03655536822043359 Adapter cache time: 0.01849407679401338 Engine time: 0.03684756625443697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.615628611994907,
    "estimated_duration": 3600.0652923890684,
    "input_throughput": 6335.916475798988,
    "output_throughput": 5591.442755928924,
    "total_throughput": 11927.359231727913,
    "itl": 153.95372138639905,
    "ttft": 1924106.5403905434,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8182942265899795,
    "arrivals": 512974,
    "finished_requests": 92214,
    "scheduler_time": 126.51491871135214
}
#Debug simulation 
Total elapsed time: 6.615719415945932. Arrivals time: 0.26864325744099915 Scheduler time: 6.23842803039588 Scheduler overhead time: 0.03647790430113673 Adapter cache time: 0.01848720572888851 Engine time: 0.036868422059342265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.57133829803206,
    "estimated_duration": 3600.109126201631,
    "input_throughput": 6335.057688671478,
    "output_throughput": 5590.973299533444,
    "total_throughput": 11926.030988204922,
    "itl": 153.9734389586591,
    "ttft": 1924263.094201905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.273179639466141,
    "arrivals": 512974,
    "finished_requests": 92205,
    "scheduler_time": 126.501225313636
}
#Debug simulation 
Total elapsed time: 6.571428719907999. Arrivals time: 0.2664828533306718 Scheduler time: 6.196434149052948 Scheduler overhead time: 0.03644732991233468 Adapter cache time: 0.01872636703774333 Engine time: 0.03671515919268131 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.494913580827415,
    "estimated_duration": 3600.122450680532,
    "input_throughput": 6275.131001649561,
    "output_throughput": 5599.714808641916,
    "total_throughput": 11874.845810291477,
    "itl": 154.52904339122117,
    "ttft": 1923033.7971047992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.009238944552011,
    "arrivals": 512003,
    "finished_requests": 92068,
    "scheduler_time": 126.43722110316561
}
#Debug simulation 
Total elapsed time: 6.4949977258220315. Arrivals time: 0.2654149781446904 Scheduler time: 6.121280468767509 Scheduler overhead time: 0.036130246706306934 Adapter cache time: 0.018854426685720682 Engine time: 0.0366643937304616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.519565741997212,
    "estimated_duration": 3600.1262023524287,
    "input_throughput": 6274.736698185505,
    "output_throughput": 5599.339819484093,
    "total_throughput": 11874.076517669599,
    "itl": 154.54095761089312,
    "ttft": 1923100.5966251201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.267261957291029,
    "arrivals": 512003,
    "finished_requests": 92060,
    "scheduler_time": 126.42782981689228
}
#Debug simulation 
Total elapsed time: 6.51964992005378. Arrivals time: 0.3098909615073353 Scheduler time: 6.101140122395009 Scheduler overhead time: 0.03615255397744477 Adapter cache time: 0.018873746739700437 Engine time: 0.03690146771259606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.5233563899528235,
    "estimated_duration": 3600.13503310787,
    "input_throughput": 6274.721306911363,
    "output_throughput": 5599.3260848879945,
    "total_throughput": 11874.047391799357,
    "itl": 154.54139029746904,
    "ttft": 1923103.9678714385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.27671358108514,
    "arrivals": 512003,
    "finished_requests": 92060,
    "scheduler_time": 126.42781809514283
}
#Debug simulation 
Total elapsed time: 6.5234431938733906. Arrivals time: 0.30928265443071723 Scheduler time: 6.105806553037837 Scheduler overhead time: 0.03627638844773173 Adapter cache time: 0.018894981360062957 Engine time: 0.03653648728504777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.53876840416342,
    "estimated_duration": 3600.0479264425217,
    "input_throughput": 6275.062571826204,
    "output_throughput": 5599.696285132736,
    "total_throughput": 11874.75885695894,
    "itl": 154.5332702091277,
    "ttft": 1923025.6823400592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.091436087295823,
    "arrivals": 512003,
    "finished_requests": 92065,
    "scheduler_time": 126.4318500255971
}
#Debug simulation 
Total elapsed time: 6.538884794106707. Arrivals time: 0.26366795715875924 Scheduler time: 6.1665142686106265 Scheduler overhead time: 0.0362970270216465 Adapter cache time: 0.019008705392479897 Engine time: 0.03666477045044303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.5541560598649085,
    "estimated_duration": 3600.0131015375437,
    "input_throughput": 6274.534664985699,
    "output_throughput": 5599.234900392705,
    "total_throughput": 11873.769565378405,
    "itl": 154.54330994662223,
    "ttft": 1923069.3261070617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.329781678952262,
    "arrivals": 512003,
    "finished_requests": 92055,
    "scheduler_time": 126.42169918121289
}
#Debug simulation 
Total elapsed time: 6.554263637866825. Arrivals time: 0.2653398874681443 Scheduler time: 6.179962882772088 Scheduler overhead time: 0.03650435130111873 Adapter cache time: 0.019008876057341695 Engine time: 0.036634979071095586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.570989147992805,
    "estimated_duration": 3600.02740508265,
    "input_throughput": 6275.296673604446,
    "output_throughput": 5599.862648694801,
    "total_throughput": 11875.159322299247,
    "itl": 154.52526528671146,
    "ttft": 1922996.288075515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.916965886321748,
    "arrivals": 512003,
    "finished_requests": 92068,
    "scheduler_time": 126.43703297564103
}
#Debug simulation 
Total elapsed time: 6.571096431929618. Arrivals time: 0.3112482933793217 Scheduler time: 6.150695743970573 Scheduler overhead time: 0.03651548805646598 Adapter cache time: 0.019044981570914388 Engine time: 0.036852809600532055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.574001778848469,
    "estimated_duration": 3600.067749938736,
    "input_throughput": 6274.439418642718,
    "output_throughput": 5599.14990498249,
    "total_throughput": 11873.589323625207,
    "itl": 154.54561984104734,
    "ttft": 1923090.9296936358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.38360429953786,
    "arrivals": 512003,
    "finished_requests": 92055,
    "scheduler_time": 126.4218054990226
}
#Debug simulation 
Total elapsed time: 6.5740834719035774. Arrivals time: 0.26752608199603856 Scheduler time: 6.197936707641929 Scheduler overhead time: 0.03632985078729689 Adapter cache time: 0.018930641235783696 Engine time: 0.036645287182182074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.519087127177045,
    "estimated_duration": 3600.095883354373,
    "input_throughput": 6323.838791423379,
    "output_throughput": 5595.300695500164,
    "total_throughput": 11919.139486923543,
    "itl": 153.96924212975753,
    "ttft": 1924734.430933934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.822549192172106,
    "arrivals": 511517,
    "finished_requests": 92168,
    "scheduler_time": 126.56774108758349
}
#Debug simulation 
Total elapsed time: 6.519173731096089. Arrivals time: 0.2646919176913798 Scheduler time: 6.146026279544458 Scheduler overhead time: 0.036326048197224736 Adapter cache time: 0.018481590086594224 Engine time: 0.036937147146090865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.484293908113614,
    "estimated_duration": 3600.019255634337,
    "input_throughput": 6323.422566246474,
    "output_throughput": 5594.466743053913,
    "total_throughput": 11917.889309300388,
    "itl": 153.97762225245026,
    "ttft": 1924806.1356696405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.071255360727662,
    "arrivals": 511517,
    "finished_requests": 92156,
    "scheduler_time": 126.55644017104575
}
#Debug simulation 
Total elapsed time: 6.484376084990799. Arrivals time: 0.27410270110704005 Scheduler time: 6.102254042169079 Scheduler overhead time: 0.036276663187891245 Adapter cache time: 0.018401079578325152 Engine time: 0.03662895504385233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.493824763922021,
    "estimated_duration": 3600.027259824036,
    "input_throughput": 6323.408506943554,
    "output_throughput": 5594.454304488912,
    "total_throughput": 11917.862811432466,
    "itl": 153.97794532044648,
    "ttft": 1924809.3819593887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.079246977604865,
    "arrivals": 511517,
    "finished_requests": 92156,
    "scheduler_time": 126.55645274381513
}
#Debug simulation 
Total elapsed time: 6.493915597908199. Arrivals time: 0.27081893011927605 Scheduler time: 6.114246777724475 Scheduler overhead time: 0.03640659782104194 Adapter cache time: 0.018602252937853336 Engine time: 0.03708753804676235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.502608501818031,
    "estimated_duration": 3600.0093613811873,
    "input_throughput": 6323.616333949171,
    "output_throughput": 5594.844617924125,
    "total_throughput": 11918.460951873296,
    "itl": 153.97149053088964,
    "ttft": 1924753.122637847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.903322684848634,
    "arrivals": 511517,
    "finished_requests": 92161,
    "scheduler_time": 126.56183710398473
}
#Debug simulation 
Total elapsed time: 6.502693641930819. Arrivals time: 0.2660569667350501 Scheduler time: 6.1279898344073445 Scheduler overhead time: 0.036347867688164115 Adapter cache time: 0.01846891758032143 Engine time: 0.0370296910405159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.448714030906558,
    "estimated_duration": 3600.0926025128942,
    "input_throughput": 6323.293735308428,
    "output_throughput": 5594.352763576688,
    "total_throughput": 11917.646498885117,
    "itl": 153.97951124613527,
    "ttft": 1924823.0105507402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.130931783821428,
    "arrivals": 511517,
    "finished_requests": 92156,
    "scheduler_time": 126.55738504989696
}
#Debug simulation 
Total elapsed time: 6.4487972049973905. Arrivals time: 0.26714132679626346 Scheduler time: 6.073514701565728 Scheduler overhead time: 0.03611248405650258 Adapter cache time: 0.018467475194483995 Engine time: 0.036797905107960105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.391092521836981,
    "estimated_duration": 3600.001857057621,
    "input_throughput": 6324.003959988959,
    "output_throughput": 5595.44683581467,
    "total_throughput": 11919.450795803628,
    "itl": 153.96561143630896,
    "ttft": 1924698.3130802095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7345728183327216,
    "arrivals": 511517,
    "finished_requests": 92168,
    "scheduler_time": 126.56740617804887
}
#Debug simulation 
Total elapsed time: 6.39117376296781. Arrivals time: 0.2512414213269949 Scheduler time: 6.031424859073013 Scheduler overhead time: 0.036101980367675424 Adapter cache time: 0.01836681948043406 Engine time: 0.037431607488542795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.47766571585089,
    "estimated_duration": 3600.149473540475,
    "input_throughput": 6323.193847174598,
    "output_throughput": 5594.2643904153365,
    "total_throughput": 11917.458237589935,
    "itl": 153.9805456841538,
    "ttft": 1924829.5891739384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.202981215827208,
    "arrivals": 511517,
    "finished_requests": 92156,
    "scheduler_time": 126.55858669130038
}
#Debug simulation 
Total elapsed time: 6.4777469779364765. Arrivals time: 0.2725868127308786 Scheduler time: 6.097022844478488 Scheduler overhead time: 0.03626152314245701 Adapter cache time: 0.018436992075294256 Engine time: 0.03667181823402643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_128_slots_64_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.536737032001838,
    "estimated_duration": 3600.0246134950366,
    "input_throughput": 6415.344471097417,
    "output_throughput": 5653.041349693,
    "total_throughput": 12068.385820790416,
    "itl": 152.03357126610453,
    "ttft": 1906162.3586897093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.740695515351964,
    "arrivals": 507129,
    "finished_requests": 93639,
    "scheduler_time": 127.89190626858726
}
#Debug simulation 
Total elapsed time: 6.536818390944973. Arrivals time: 0.29429524345323443 Scheduler time: 6.13169667776674 Scheduler overhead time: 0.03627538378350437 Adapter cache time: 0.02102128602564335 Engine time: 0.0367988811340183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_64_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.504660769831389,
    "estimated_duration": 3600.1105261895696,
    "input_throughput": 6414.926661833249,
    "output_throughput": 5652.703952269831,
    "total_throughput": 12067.63061410308,
    "itl": 152.04778355470165,
    "ttft": 1906236.4901150365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.046312155020332,
    "arrivals": 507129,
    "finished_requests": 93632,
    "scheduler_time": 127.883804584131
}
#Debug simulation 
Total elapsed time: 6.504749386804178. Arrivals time: 0.25395304011180997 Scheduler time: 6.139981538290158 Scheduler overhead time: 0.03602997586131096 Adapter cache time: 0.02114826743490994 Engine time: 0.03691610740497708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_128_slots_64_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.519359668018296,
    "estimated_duration": 3600.1209413690503,
    "input_throughput": 6414.908103397679,
    "output_throughput": 5652.687598950824,
    "total_throughput": 12067.595702348503,
    "itl": 152.0482190836549,
    "ttft": 1906241.041823417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.056716903690193,
    "arrivals": 507129,
    "finished_requests": 93632,
    "scheduler_time": 127.88381501487298
}
#Debug simulation 
Total elapsed time: 6.51946129091084. Arrivals time: 0.2971908140461892 Scheduler time: 6.1113478424958885 Scheduler overhead time: 0.036170842591673136 Adapter cache time: 0.021207125624641776 Engine time: 0.03673790441825986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_64_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.603248287923634,
    "estimated_duration": 3600.1587283740255,
    "input_throughput": 6415.105483538166,
    "output_throughput": 5652.83075982357,
    "total_throughput": 12067.936243361735,
    "itl": 152.03629166846383,
    "ttft": 1906207.5194207095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.817498776693723,
    "arrivals": 507129,
    "finished_requests": 93639,
    "scheduler_time": 127.89366076765987
}
#Debug simulation 
Total elapsed time: 6.603336085798219. Arrivals time: 0.2559823647607118 Scheduler time: 6.2366056924220175 Scheduler overhead time: 0.03608627780340612 Adapter cache time: 0.021146498853340745 Engine time: 0.0368259041570127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_128_slots_64_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.5024869160261005,
    "estimated_duration": 3600.0202282892074,
    "input_throughput": 6414.930343592712,
    "output_throughput": 5652.701570977172,
    "total_throughput": 12067.631914569885,
    "itl": 152.0507161326941,
    "ttft": 1906223.5971062696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.1271390240825205,
    "arrivals": 507129,
    "finished_requests": 93630,
    "scheduler_time": 127.8777995639123
}
#Debug simulation 
Total elapsed time: 6.502590215066448. Arrivals time: 0.25340459612198174 Scheduler time: 6.138805394526571 Scheduler overhead time: 0.035899876384064555 Adapter cache time: 0.021209220867604017 Engine time: 0.036496077198535204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_64_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.500391484005377,
    "estimated_duration": 3600.085251732922,
    "input_throughput": 6415.584738967582,
    "output_throughput": 5653.263069312966,
    "total_throughput": 12068.847808280547,
    "itl": 152.02956801709553,
    "ttft": 1906123.5477164737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.631587906803342,
    "arrivals": 507129,
    "finished_requests": 93643,
    "scheduler_time": 127.89785274916218
}
#Debug simulation 
Total elapsed time: 6.500471166102216. Arrivals time: 0.25292736338451505 Scheduler time: 6.137360528111458 Scheduler overhead time: 0.03596973093226552 Adapter cache time: 0.02082562819123268 Engine time: 0.036626429529860616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_128_slots_64_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.613454154925421,
    "estimated_duration": 3600.114829277327,
    "input_throughput": 6414.838719084922,
    "output_throughput": 5652.572199783128,
    "total_throughput": 12067.41091886805,
    "itl": 152.05406941152998,
    "ttft": 1906243.538042671,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1549,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.184356996901279,
    "arrivals": 507129,
    "finished_requests": 93631,
    "scheduler_time": 127.87864159320995
}
#Debug simulation 
Total elapsed time: 6.6135337869636714. Arrivals time: 0.2540109280962497 Scheduler time: 6.248419743031263 Scheduler overhead time: 0.036225219490006566 Adapter cache time: 0.02113490109331906 Engine time: 0.03694704617373645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.589721959084272,
    "estimated_duration": 3600.040898278943,
    "input_throughput": 6495.300098167991,
    "output_throughput": 5755.4582254622355,
    "total_throughput": 12250.758323630227,
    "itl": 149.79746104957766,
    "ttft": 1908987.2909651233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.828670167659972,
    "arrivals": 505238,
    "finished_requests": 94705,
    "scheduler_time": 130.04148543801176
}
#Debug simulation 
Total elapsed time: 6.589806556934491. Arrivals time: 0.2967543052509427 Scheduler time: 6.183727083960548 Scheduler overhead time: 0.03641770966351032 Adapter cache time: 0.018768857000395656 Engine time: 0.03721119510009885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.635209384839982,
    "estimated_duration": 3600.0669450561877,
    "input_throughput": 6494.779224066644,
    "output_throughput": 5754.83548394866,
    "total_throughput": 12249.614708015304,
    "itl": 149.80838563525427,
    "ttft": 1909058.1429366476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.088657184112813,
    "arrivals": 505238,
    "finished_requests": 94698,
    "scheduler_time": 130.03321262896978
}
#Debug simulation 
Total elapsed time: 6.635286744916812. Arrivals time: 0.3192668203264475 Scheduler time: 6.206528926733881 Scheduler overhead time: 0.03629919467493892 Adapter cache time: 0.018971299519762397 Engine time: 0.03721662540920079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.6139473498333246,
    "estimated_duration": 3600.074781889367,
    "input_throughput": 6494.765085887745,
    "output_throughput": 5754.822956518427,
    "total_throughput": 12249.588042406172,
    "itl": 149.8087094403721,
    "ttft": 1909061.397927607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.0964880568161,
    "arrivals": 505238,
    "finished_requests": 94698,
    "scheduler_time": 130.03321858939378
}
#Debug simulation 
Total elapsed time: 6.614037757972255. Arrivals time: 0.2595427769701928 Scheduler time: 6.2455198732204735 Scheduler overhead time: 0.03624305408447981 Adapter cache time: 0.018760041799396276 Engine time: 0.03710666950792074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.628761057974771,
    "estimated_duration": 3600.0470650565285,
    "input_throughput": 6495.260638941906,
    "output_throughput": 5755.386978440699,
    "total_throughput": 12250.647617382605,
    "itl": 149.80165446249566,
    "ttft": 1908994.9075765174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.913778387820299,
    "arrivals": 505238,
    "finished_requests": 94704,
    "scheduler_time": 130.03882777678916
}
#Debug simulation 
Total elapsed time: 6.628844022983685. Arrivals time: 0.2980195013806224 Scheduler time: 6.2217786035034806 Scheduler overhead time: 0.03623631363734603 Adapter cache time: 0.018909210106357932 Engine time: 0.037028898717835546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.568385188002139,
    "estimated_duration": 3600.1322968716477,
    "input_throughput": 6494.661326840013,
    "output_throughput": 5754.73101863585,
    "total_throughput": 12249.392345475864,
    "itl": 149.81085198428252,
    "ttft": 1909081.9762726908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.150310677401719,
    "arrivals": 505238,
    "finished_requests": 94698,
    "scheduler_time": 130.03328665413395
}
#Debug simulation 
Total elapsed time: 6.568483758019283. Arrivals time: 0.31775113590992987 Scheduler time: 6.142247342970222 Scheduler overhead time: 0.035905897384509444 Adapter cache time: 0.018914025742560625 Engine time: 0.03680938552133739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.5844079160597175,
    "estimated_duration": 3600.1153943484774,
    "input_throughput": 6495.358186770531,
    "output_throughput": 5755.576899709318,
    "total_throughput": 12250.935086479849,
    "itl": 149.79479351209505,
    "ttft": 1908947.9723481133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7405529189225257,
    "arrivals": 505238,
    "finished_requests": 94708,
    "scheduler_time": 130.04739294613424
}
#Debug simulation 
Total elapsed time: 6.584486132021993. Arrivals time: 0.298044943716377 Scheduler time: 6.177478468976915 Scheduler overhead time: 0.036122877150774 Adapter cache time: 0.018752174684777856 Engine time: 0.03724285447970033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.575619064038619,
    "estimated_duration": 3600.06888468874,
    "input_throughput": 6494.178791809806,
    "output_throughput": 5754.499611968159,
    "total_throughput": 12248.678403777965,
    "itl": 149.81395222197898,
    "ttft": 1909075.613681635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.191270141713364,
    "arrivals": 505238,
    "finished_requests": 94692,
    "scheduler_time": 130.029334632559
}
#Debug simulation 
Total elapsed time: 6.57573041296564. Arrivals time: 0.25679643801413476 Scheduler time: 6.210225947434083 Scheduler overhead time: 0.03599204611964524 Adapter cache time: 0.01880701375193894 Engine time: 0.03711861162446439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.631897609913722,
    "estimated_duration": 3600.121287256949,
    "input_throughput": 6658.089016290748,
    "output_throughput": 5815.081306872043,
    "total_throughput": 12473.17032316279,
    "itl": 147.03781038203778,
    "ttft": 1892839.2061903172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.274721886008124,
    "arrivals": 504263,
    "finished_requests": 96372,
    "scheduler_time": 131.73212697284833
}
#Debug simulation 
Total elapsed time: 6.631977912969887. Arrivals time: 0.26011787774041295 Scheduler time: 6.262752411188558 Scheduler overhead time: 0.036540571600198746 Adapter cache time: 0.017885931069031358 Engine time: 0.03759417496621609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.60391933307983,
    "estimated_duration": 3600.053221064073,
    "input_throughput": 6657.806295684598,
    "output_throughput": 5814.889312612032,
    "total_throughput": 12472.69560829663,
    "itl": 147.04702849270274,
    "ttft": 1892867.763594209,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4784244180400865,
    "arrivals": 504263,
    "finished_requests": 96367,
    "scheduler_time": 131.72274202773698
}
#Debug simulation 
Total elapsed time: 6.604000594932586. Arrivals time: 0.30034407298080623 Scheduler time: 6.194541348377243 Scheduler overhead time: 0.036604691529646516 Adapter cache time: 0.01789161516353488 Engine time: 0.03756210766732693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.722240243805572,
    "estimated_duration": 3600.061707254654,
    "input_throughput": 6657.790601672197,
    "output_throughput": 5814.875605552841,
    "total_throughput": 12472.666207225038,
    "itl": 147.04737677242585,
    "ttft": 1892871.1192059517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.486905206944752,
    "arrivals": 504263,
    "finished_requests": 96367,
    "scheduler_time": 131.72274742937122
}
#Debug simulation 
Total elapsed time: 6.722322574816644. Arrivals time: 0.2613580799661577 Scheduler time: 6.351430637994781 Scheduler overhead time: 0.036737995920702815 Adapter cache time: 0.0179173001088202 Engine time: 0.03761671204119921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.702638849150389,
    "estimated_duration": 3600.0482075846376,
    "input_throughput": 6657.885844279071,
    "output_throughput": 5814.962409640964,
    "total_throughput": 12472.848253920036,
    "itl": 147.0405846288383,
    "ttft": 1892845.4219781219,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.333373079993721,
    "arrivals": 504263,
    "finished_requests": 96369,
    "scheduler_time": 131.72792147400096
}
#Debug simulation 
Total elapsed time: 6.702721486100927. Arrivals time: 0.27793224435299635 Scheduler time: 6.315791781293228 Scheduler overhead time: 0.03665515477769077 Adapter cache time: 0.017852395307272673 Engine time: 0.03739788429811597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.703256416134536,
    "estimated_duration": 3600.1105835244434,
    "input_throughput": 6657.700213346034,
    "output_throughput": 5814.796660914254,
    "total_throughput": 12472.496874260287,
    "itl": 147.04924901947314,
    "ttft": 1892890.3420515738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.531547801122075,
    "arrivals": 504263,
    "finished_requests": 96367,
    "scheduler_time": 131.72291278308114
}
#Debug simulation 
Total elapsed time: 6.7033484659623355. Arrivals time: 0.26418250519782305 Scheduler time: 6.329187149414793 Scheduler overhead time: 0.03693408519029617 Adapter cache time: 0.017996493261307478 Engine time: 0.03776689060032368 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.637453970964998,
    "estimated_duration": 3600.0406131989707,
    "input_throughput": 6658.238218790674,
    "output_throughput": 5815.211618237081,
    "total_throughput": 12473.449837027754,
    "itl": 147.0345961785231,
    "ttft": 1892809.689302763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1993538155452512,
    "arrivals": 504263,
    "finished_requests": 96372,
    "scheduler_time": 131.7319341511505
}
#Debug simulation 
Total elapsed time: 6.637559663970023. Arrivals time: 0.25898624141700566 Scheduler time: 6.269195930566639 Scheduler overhead time: 0.03669212176464498 Adapter cache time: 0.017976828617975116 Engine time: 0.03756584110669792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.67714143008925,
    "estimated_duration": 3600.0201582873397,
    "input_throughput": 6657.549665333575,
    "output_throughput": 5814.631607495911,
    "total_throughput": 12472.181272829486,
    "itl": 147.05065513273155,
    "ttft": 1892930.059458699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5727950430661903,
    "arrivals": 504263,
    "finished_requests": 96362,
    "scheduler_time": 131.71791523109266
}
#Debug simulation 
Total elapsed time: 6.6772219790145755. Arrivals time: 0.2640624768100679 Scheduler time: 6.303888636874035 Scheduler overhead time: 0.03667594539001584 Adapter cache time: 0.01788606634363532 Engine time: 0.03755940753035247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.60088275000453,
    "estimated_duration": 3600.1020701840507,
    "input_throughput": 6573.042246768917,
    "output_throughput": 5836.9833938974125,
    "total_throughput": 12410.025640666328,
    "itl": 147.89897528496314,
    "ttft": 1894846.280968569,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.800346285698531,
    "arrivals": 503772,
    "finished_requests": 95872,
    "scheduler_time": 131.80789951415161
}
#Debug simulation 
Total elapsed time: 6.600988877937198. Arrivals time: 0.2619381428230554 Scheduler time: 6.2317149844020605 Scheduler overhead time: 0.03652599034830928 Adapter cache time: 0.01656227489002049 Engine time: 0.0371844454202801 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.593713587149978,
    "estimated_duration": 3600.1562208966625,
    "input_throughput": 6572.937269401686,
    "output_throughput": 5836.745327336493,
    "total_throughput": 12409.682596738177,
    "itl": 147.9058553102775,
    "ttft": 1894880.424716253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 917,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9866247161710673,
    "arrivals": 503772,
    "finished_requests": 95869,
    "scheduler_time": 131.80360014531635
}
#Debug simulation 
Total elapsed time: 6.593793757027015. Arrivals time: 0.25810279604047537 Scheduler time: 6.228420122992247 Scheduler overhead time: 0.03636724431999028 Adapter cache time: 0.016627395525574684 Engine time: 0.03729543508961797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.575304293073714,
    "estimated_duration": 3600.1634650431483,
    "input_throughput": 6572.924043524337,
    "output_throughput": 5836.733582803623,
    "total_throughput": 12409.65762632796,
    "itl": 147.90604440694798,
    "ttft": 1894883.6343098034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 917,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.992918033543949,
    "arrivals": 503772,
    "finished_requests": 95869,
    "scheduler_time": 131.80365062429937
}
#Debug simulation 
Total elapsed time: 6.575386970071122. Arrivals time: 0.2579560505691916 Scheduler time: 6.210718279704452 Scheduler overhead time: 0.03626233013346791 Adapter cache time: 0.016575724352151155 Engine time: 0.03694633487612009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.671583825023845,
    "estimated_duration": 3600.0375018504415,
    "input_throughput": 6573.153748491999,
    "output_throughput": 5836.859474158043,
    "total_throughput": 12410.013222650041,
    "itl": 147.9015091639325,
    "ttft": 1894840.7153540715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 917,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8632289300020486,
    "arrivals": 503772,
    "finished_requests": 95868,
    "scheduler_time": 131.80334810382615
}
#Debug simulation 
Total elapsed time: 6.671669263159856. Arrivals time: 0.2588396032806486 Scheduler time: 6.304947142722085 Scheduler overhead time: 0.036659591598436236 Adapter cache time: 0.01668031862936914 Engine time: 0.03743977355770767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.618252021027729,
    "estimated_duration": 3600.0904126688406,
    "input_throughput": 6572.564654691234,
    "output_throughput": 5836.569527825588,
    "total_throughput": 12409.134182516822,
    "itl": 147.9061363429217,
    "ttft": 1894909.4887186189,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 914,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.020501870438459,
    "arrivals": 503772,
    "finished_requests": 95862,
    "scheduler_time": 131.8004618331166
}
#Debug simulation 
Total elapsed time: 6.618331252830103. Arrivals time: 0.2615973576903343 Scheduler time: 6.248787739081308 Scheduler overhead time: 0.03685831860639155 Adapter cache time: 0.016543607227504253 Engine time: 0.037504224106669426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.659622584935278,
    "estimated_duration": 3600.0276049455283,
    "input_throughput": 6573.177929939249,
    "output_throughput": 5837.037463582992,
    "total_throughput": 12410.215393522241,
    "itl": 147.8960100640738,
    "ttft": 1894824.5081735915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7358960198354305,
    "arrivals": 503772,
    "finished_requests": 95871,
    "scheduler_time": 131.80773026667575
}
#Debug simulation 
Total elapsed time: 6.659710120875388. Arrivals time: 0.2676332506816834 Scheduler time: 6.284178251167759 Scheduler overhead time: 0.036594717763364315 Adapter cache time: 0.016779680270701647 Engine time: 0.03746644081547856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.6607962909620255,
    "estimated_duration": 3600.127408204515,
    "input_throughput": 6572.497113873205,
    "output_throughput": 5836.509550221547,
    "total_throughput": 12409.006664094753,
    "itl": 147.90741942138268,
    "ttft": 1894922.1469389976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 914,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0577249912172864,
    "arrivals": 503772,
    "finished_requests": 95862,
    "scheduler_time": 131.80053082917283
}
#Debug simulation 
Total elapsed time: 6.660901963012293. Arrivals time: 0.26435547159053385 Scheduler time: 6.288405325496569 Scheduler overhead time: 0.03655800851993263 Adapter cache time: 0.016639617970213294 Engine time: 0.03760819975286722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.816760302055627,
    "estimated_duration": 3600.0173970156816,
    "input_throughput": 6718.636698825553,
    "output_throughput": 5959.8781433073555,
    "total_throughput": 12678.514842132909,
    "itl": 144.7999892574375,
    "ttft": 1883051.641212187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 874,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6748662881972836,
    "arrivals": 501405,
    "finished_requests": 98168,
    "scheduler_time": 134.59947914792323
}
#Debug simulation 
Total elapsed time: 6.816852821037173. Arrivals time: 0.3072019300889224 Scheduler time: 6.400927919894457 Scheduler overhead time: 0.0371727948077023 Adapter cache time: 0.015850378200411797 Engine time: 0.03829987603239715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.093768365914002,
    "estimated_duration": 3600.0843703013256,
    "input_throughput": 6718.4225457403845,
    "output_throughput": 5959.744215162428,
    "total_throughput": 12678.166760902814,
    "itl": 144.8067030904273,
    "ttft": 1883078.776491656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 873,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8387186904577577,
    "arrivals": 501405,
    "finished_requests": 98167,
    "scheduler_time": 134.59632002272065
}
#Debug simulation 
Total elapsed time: 7.093827608972788. Arrivals time: 0.5517815188504755 Scheduler time: 6.433105671778321 Scheduler overhead time: 0.03739448543637991 Adapter cache time: 0.015706669772043824 Engine time: 0.038383327424526215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.767677035881206,
    "estimated_duration": 3600.1107535748465,
    "input_throughput": 6718.373310038544,
    "output_throughput": 5959.700539405624,
    "total_throughput": 12678.073849444168,
    "itl": 144.80809133606937,
    "ttft": 1883086.980943561,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 873,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.845513304416071,
    "arrivals": 501405,
    "finished_requests": 98167,
    "scheduler_time": 134.59664128369747
}
#Debug simulation 
Total elapsed time: 6.7677799500525. Arrivals time: 0.30592490872368217 Scheduler time: 6.353432652540505 Scheduler overhead time: 0.037219158839434385 Adapter cache time: 0.015622802311554551 Engine time: 0.038187222788110375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.850989928934723,
    "estimated_duration": 3600.0786415465295,
    "input_throughput": 6718.5224013911,
    "output_throughput": 5959.776753871974,
    "total_throughput": 12678.299155263074,
    "itl": 144.8017084731114,
    "ttft": 1883074.5070930638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 874,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7150441920361166,
    "arrivals": 501405,
    "finished_requests": 98168,
    "scheduler_time": 134.60011545152875
}
#Debug simulation 
Total elapsed time: 6.851067634997889. Arrivals time: 0.3063347805291414 Scheduler time: 6.435804812936112 Scheduler overhead time: 0.03766454407013953 Adapter cache time: 0.015655657276511192 Engine time: 0.03815675084479153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.849279364105314,
    "estimated_duration": 3600.149780068128,
    "input_throughput": 6718.300481248948,
    "output_throughput": 5959.635934812129,
    "total_throughput": 12677.936416061077,
    "itl": 144.80959271375224,
    "ttft": 1883102.8296145818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 873,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.884496978204702,
    "arrivals": 501405,
    "finished_requests": 98167,
    "scheduler_time": 134.59669390306556
}
#Debug simulation 
Total elapsed time: 6.849359147017822. Arrivals time: 0.31145213916897774 Scheduler time: 6.429442222230136 Scheduler overhead time: 0.037218492943793535 Adapter cache time: 0.015589646762236953 Engine time: 0.03821682836860418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.85002185893245,
    "estimated_duration": 3600.1063794891584,
    "input_throughput": 6718.9053461893245,
    "output_throughput": 5959.9413845778045,
    "total_throughput": 12678.846730767129,
    "itl": 144.7976366142495,
    "ttft": 1883059.377039999,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 874,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6133039577444457,
    "arrivals": 501405,
    "finished_requests": 98171,
    "scheduler_time": 134.60498073513816
}
#Debug simulation 
Total elapsed time: 6.85011838003993. Arrivals time: 0.31285044527612627 Scheduler time: 6.428250431548804 Scheduler overhead time: 0.03745078109204769 Adapter cache time: 0.0158681427128613 Engine time: 0.038250668440014124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_128_slots_64_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.81949966493994,
    "estimated_duration": 3600.01914435013,
    "input_throughput": 6718.5398272003295,
    "output_throughput": 5959.78663993274,
    "total_throughput": 12678.32646713307,
    "itl": 144.81013156079842,
    "ttft": 1883085.7718281844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 873,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9158096710220236,
    "arrivals": 501405,
    "finished_requests": 98165,
    "scheduler_time": 134.59070876167374
}
#Debug simulation 
Total elapsed time: 6.81958144903183. Arrivals time: 0.3050017850473523 Scheduler time: 6.405860134866089 Scheduler overhead time: 0.03729838039726019 Adapter cache time: 0.015694414032623172 Engine time: 0.038296794053167105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.932929197093472,
    "estimated_duration": 3600.119279043589,
    "input_throughput": 6736.280695244813,
    "output_throughput": 6032.087638432123,
    "total_throughput": 12768.368333676935,
    "itl": 144.00978301507243,
    "ttft": 1875575.263393071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 677,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.07195020264251,
    "arrivals": 500396,
    "finished_requests": 98666,
    "scheduler_time": 135.92911138841703
}
#Debug simulation 
Total elapsed time: 6.933034793008119. Arrivals time: 0.26643675030209124 Scheduler time: 6.55722700082697 Scheduler overhead time: 0.03771883249282837 Adapter cache time: 0.014401879627257586 Engine time: 0.03962391917593777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.932109777117148,
    "estimated_duration": 3600.006421837846,
    "input_throughput": 6736.179650374051,
    "output_throughput": 6031.995628750607,
    "total_throughput": 12768.175279124658,
    "itl": 144.01564816310147,
    "ttft": 1875608.8224716187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 679,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.212069066944538,
    "arrivals": 500396,
    "finished_requests": 98659,
    "scheduler_time": 135.9197045893114
}
#Debug simulation 
Total elapsed time: 6.93219239404425. Arrivals time: 0.27122656791470945 Scheduler time: 6.55284997401759 Scheduler overhead time: 0.03761413274332881 Adapter cache time: 0.014293197076767683 Engine time: 0.03855697205290198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.884708573110402,
    "estimated_duration": 3600.0127094769346,
    "input_throughput": 6736.167885230454,
    "output_throughput": 6031.985093506829,
    "total_throughput": 12768.152978737284,
    "itl": 144.01590995238854,
    "ttft": 1875612.0803499578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 679,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.216624433379624,
    "arrivals": 500396,
    "finished_requests": 98659,
    "scheduler_time": 135.91975872774069
}
#Debug simulation 
Total elapsed time: 6.884803202934563. Arrivals time: 0.31331846048124135 Scheduler time: 6.463619163958356 Scheduler overhead time: 0.037634528474882245 Adapter cache time: 0.014288963517174125 Engine time: 0.03842557850293815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.887245659017935,
    "estimated_duration": 3600.0019485723337,
    "input_throughput": 6736.38024268773,
    "output_throughput": 6032.1653460804155,
    "total_throughput": 12768.545588768146,
    "itl": 144.0105882858124,
    "ttft": 1875557.9851748368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 677,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1108862571581097,
    "arrivals": 500396,
    "finished_requests": 98662,
    "scheduler_time": 135.9231202174972
}
#Debug simulation 
Total elapsed time: 6.887352708028629. Arrivals time: 0.2675596051849425 Scheduler time: 6.5122603750787675 Scheduler overhead time: 0.03720850474201143 Adapter cache time: 0.014211229281499982 Engine time: 0.03846291173249483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.920987732009962,
    "estimated_duration": 3600.0429428753555,
    "input_throughput": 6736.111314447623,
    "output_throughput": 6031.934436497595,
    "total_throughput": 12768.045750945217,
    "itl": 144.01703306470466,
    "ttft": 1875624.8331844045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 679,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2457993118278705,
    "arrivals": 500396,
    "finished_requests": 98659,
    "scheduler_time": 135.91984217531893
}
#Debug simulation 
Total elapsed time: 6.921098246006295. Arrivals time: 0.2695339086931199 Scheduler time: 6.54353288281709 Scheduler overhead time: 0.037595063680782914 Adapter cache time: 0.014182897051796317 Engine time: 0.038635809905827045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.8905181949958205,
    "estimated_duration": 3600.0267783631034,
    "input_throughput": 6736.453502445022,
    "output_throughput": 6032.236240718784,
    "total_throughput": 12768.689743163806,
    "itl": 144.00675698237777,
    "ttft": 1875537.1725156188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.02725409994364,
    "arrivals": 500396,
    "finished_requests": 98665,
    "scheduler_time": 135.9280811960699
}
#Debug simulation 
Total elapsed time: 6.890596919925883. Arrivals time: 0.31165636656805873 Scheduler time: 6.470753530040383 Scheduler overhead time: 0.037884107790887356 Adapter cache time: 0.014206682797521353 Engine time: 0.03852383606135845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_128_slots_64_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.867286378983408,
    "estimated_duration": 3600.0689290386417,
    "input_throughput": 6736.062691576233,
    "output_throughput": 6031.890896544252,
    "total_throughput": 12767.953588120487,
    "itl": 144.0180289509688,
    "ttft": 1875634.8808861973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 679,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2725848683342376,
    "arrivals": 500396,
    "finished_requests": 98659,
    "scheduler_time": 135.91995404252012
}
#Debug simulation 
Total elapsed time: 6.867373815970495. Arrivals time: 0.26484044664539397 Scheduler time: 6.494971975451335 Scheduler overhead time: 0.03727901470847428 Adapter cache time: 0.01419018767774105 Engine time: 0.03860178589820862 
