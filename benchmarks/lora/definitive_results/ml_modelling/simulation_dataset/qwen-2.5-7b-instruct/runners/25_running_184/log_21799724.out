INFO 06-01 00:46:59 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:00 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.571675139945,
    "estimated_duration": 3600.000888095917,
    "input_throughput": 7445.696218752108,
    "output_throughput": 6621.0078110861095,
    "total_throughput": 14066.704029838218,
    "itl": 129.7879388146607,
    "ttft": 833203.5206119651,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 130530,
    "finished_requests": 108619,
    "scheduler_time": 81.9249738365169
}
#Debug simulation 
Total elapsed time: 7.571861442178488. Arrivals time: 0.27334951516240835 Scheduler time: 7.176743120420724 Scheduler overhead time: 0.04231058619916439 Adapter cache time: 0.015298016369342804 Engine time: 0.04395332420244813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.593302900902927,
    "estimated_duration": 3600.0015252289,
    "input_throughput": 7445.694901003044,
    "output_throughput": 6621.00663929148,
    "total_throughput": 14066.701540294524,
    "itl": 129.78792049846098,
    "ttft": 833204.0094693626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 130530,
    "finished_requests": 108619,
    "scheduler_time": 81.92496090600008
}
#Debug simulation 
Total elapsed time: 7.593432074878365. Arrivals time: 0.27478158194571733 Scheduler time: 7.196221583057195 Scheduler overhead time: 0.04244050057604909 Adapter cache time: 0.015463179908692837 Engine time: 0.04430630337446928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.609332389198244,
    "estimated_duration": 3600.1208496842414,
    "input_throughput": 7445.635054820736,
    "output_throughput": 6621.118011105287,
    "total_throughput": 14066.753065926023,
    "itl": 129.78835791038193,
    "ttft": 833126.2926079835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 130530,
    "finished_requests": 108627,
    "scheduler_time": 81.92742835840026
}
#Debug simulation 
Total elapsed time: 7.609471006318927. Arrivals time: 0.2726276284083724 Scheduler time: 7.214731308165938 Scheduler overhead time: 0.042580073699355125 Adapter cache time: 0.015615711454302073 Engine time: 0.04373035812750459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.647063155192882,
    "estimated_duration": 3600.0089818797196,
    "input_throughput": 7445.679478834025,
    "output_throughput": 6620.992925288311,
    "total_throughput": 14066.672404122337,
    "itl": 129.7879456207507,
    "ttft": 833230.5316140238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 130530,
    "finished_requests": 108619,
    "scheduler_time": 81.92479422585856
}
#Debug simulation 
Total elapsed time: 7.647166267037392. Arrivals time: 0.2783770617097616 Scheduler time: 7.245468695182353 Scheduler overhead time: 0.04309781640768051 Adapter cache time: 0.01573458919301629 Engine time: 0.044271329417824745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.640165450051427,
    "estimated_duration": 3600.0896193716317,
    "input_throughput": 7445.699644743467,
    "output_throughput": 6621.175448449124,
    "total_throughput": 14066.875093192592,
    "itl": 129.7882279179088,
    "ttft": 833077.5354004693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 130530,
    "finished_requests": 108627,
    "scheduler_time": 81.92716322171529
}
#Debug simulation 
Total elapsed time: 7.6402749782428145. Arrivals time: 0.2783046052791178 Scheduler time: 7.239650986623019 Scheduler overhead time: 0.04261976387351751 Adapter cache time: 0.01558080967515707 Engine time: 0.043895306065678596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.637266573961824,
    "estimated_duration": 3600.0135145879,
    "input_throughput": 7445.670104121363,
    "output_throughput": 6620.984588922719,
    "total_throughput": 14066.654693044082,
    "itl": 129.7879214529934,
    "ttft": 833233.0688055172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 130530,
    "finished_requests": 108619,
    "scheduler_time": 81.92474168705964
}
#Debug simulation 
Total elapsed time: 7.637399411294609. Arrivals time: 0.2734420485794544 Scheduler time: 7.2408546674996614 Scheduler overhead time: 0.04262486565858126 Adapter cache time: 0.015575244557112455 Engine time: 0.04465778777375817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.673635947052389,
    "estimated_duration": 3600.0407112186936,
    "input_throughput": 7607.419525744443,
    "output_throughput": 6684.693293885948,
    "total_throughput": 14292.11281963039,
    "itl": 127.55875429317723,
    "ttft": 749216.2077719058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 129375,
    "finished_requests": 109848,
    "scheduler_time": 83.93634136633126
}
#Debug simulation 
Total elapsed time: 7.673742063809186. Arrivals time: 0.2729769856669009 Scheduler time: 7.273907704278827 Scheduler overhead time: 0.043067640624940395 Adapter cache time: 0.018849176354706287 Engine time: 0.0444432757794857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.801526654977351,
    "estimated_duration": 3600.115416191093,
    "input_throughput": 7607.548879356886,
    "output_throughput": 6684.961790881248,
    "total_throughput": 14292.510670238134,
    "itl": 127.55996325222569,
    "ttft": 749139.7658532624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 129375,
    "finished_requests": 109854,
    "scheduler_time": 83.93832078162274
}
#Debug simulation 
Total elapsed time: 7.8016507467255. Arrivals time: 0.2755944672971964 Scheduler time: 7.395159088075161 Scheduler overhead time: 0.04384699231013656 Adapter cache time: 0.01932460442185402 Engine time: 0.046206314116716385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.781108572147787,
    "estimated_duration": 3600.112440705131,
    "input_throughput": 7607.555166981306,
    "output_throughput": 6684.967315989225,
    "total_throughput": 14292.522482970531,
    "itl": 127.55984778793477,
    "ttft": 749116.030296915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2092868485860526,
    "arrivals": 129375,
    "finished_requests": 109854,
    "scheduler_time": 83.93838301667435
}
#Debug simulation 
Total elapsed time: 7.7812382178381085. Arrivals time: 0.2762334425933659 Scheduler time: 7.3771458035334945 Scheduler overhead time: 0.04354202514514327 Adapter cache time: 0.018831871915608644 Engine time: 0.04486291203647852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.7693434548564255,
    "estimated_duration": 3600.0744582668835,
    "input_throughput": 7607.348213899004,
    "output_throughput": 6684.630631663447,
    "total_throughput": 14291.978845562451,
    "itl": 127.55924583180433,
    "ttft": 749251.0318081449,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 129375,
    "finished_requests": 109848,
    "scheduler_time": 83.9364096126639
}
#Debug simulation 
Total elapsed time: 7.769510367885232. Arrivals time: 0.2752120331861079 Scheduler time: 7.366193704772741 Scheduler overhead time: 0.04347785096615553 Adapter cache time: 0.01896704314276576 Engine time: 0.044971329160034657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.815614950843155,
    "estimated_duration": 3600.1092999058797,
    "input_throughput": 7607.5676371036925,
    "output_throughput": 6684.999536161081,
    "total_throughput": 14292.567173264773,
    "itl": 127.55962832031598,
    "ttft": 749092.1425690902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2119276781007646,
    "arrivals": 129375,
    "finished_requests": 109855,
    "scheduler_time": 83.9385927437661
}
#Debug simulation 
Total elapsed time: 7.8157427427358925. Arrivals time: 0.2785671232268214 Scheduler time: 7.40855165431276 Scheduler overhead time: 0.043559140991419554 Adapter cache time: 0.019147949293255806 Engine time: 0.045240175910294056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.702025001402944,
    "estimated_duration": 3600.029961581846,
    "input_throughput": 7607.442241387957,
    "output_throughput": 6684.713254282421,
    "total_throughput": 14292.155495670379,
    "itl": 127.55868674367177,
    "ttft": 749215.1882748395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 129375,
    "finished_requests": 109848,
    "scheduler_time": 83.93619825860814
}
#Debug simulation 
Total elapsed time: 7.702157803345472. Arrivals time: 0.2727399468421936 Scheduler time: 7.300976435653865 Scheduler overhead time: 0.043587541207671165 Adapter cache time: 0.019201086834073067 Engine time: 0.0449668332003057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 17280, 17280, 135, 17280, 135, 270, 135, 17280, 270, 17280, 135, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 17280, 135, 135, 135, 17280, 270, 135, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 135, 270, 270, 17280, 17280, 270, 17280, 135, 17280, 270, 270, 270, 17280, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 17280, 270]
Prompts retrieved: 388665 . Total input tokens: 86782411 . Total output tokens: 77748172
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.782571418210864,
    "estimated_duration": 3600.129574281823,
    "input_throughput": 7607.524794566193,
    "output_throughput": 6684.961889128946,
    "total_throughput": 14292.486683695139,
    "itl": 127.56012874334382,
    "ttft": 749124.1400121354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 129375,
    "finished_requests": 109855,
    "scheduler_time": 83.93810086915421
}
#Debug simulation 
Total elapsed time: 7.782681394834071. Arrivals time: 0.2810041760094464 Scheduler time: 7.373155091889203 Scheduler overhead time: 0.04363924125209451 Adapter cache time: 0.01900828303769231 Engine time: 0.045265824533998966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.93335049925372,
    "estimated_duration": 3600.0373852122257,
    "input_throughput": 7633.880446044283,
    "output_throughput": 6811.9867590081485,
    "total_throughput": 14445.867205052433,
    "itl": 126.19921322049754,
    "ttft": 689986.2018190543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 128882,
    "finished_requests": 111220,
    "scheduler_time": 86.2120264827228
}
#Debug simulation 
Total elapsed time: 7.933475089259446. Arrivals time: 0.28552130749449134 Scheduler time: 7.519247316755354 Scheduler overhead time: 0.04479546099901199 Adapter cache time: 0.01691227313131094 Engine time: 0.04581789392977953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.85245089000091,
    "estimated_duration": 3600.098705327222,
    "input_throughput": 7633.95652439535,
    "output_throughput": 6812.057670449606,
    "total_throughput": 14446.014194844956,
    "itl": 126.20029817962225,
    "ttft": 689957.4812250219,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 128882,
    "finished_requests": 111222,
    "scheduler_time": 86.21498969108825
}
#Debug simulation 
Total elapsed time: 7.8525839522480965. Arrivals time: 0.2904372923076153 Scheduler time: 7.435487305279821 Scheduler overhead time: 0.04388847202062607 Adapter cache time: 0.0168470973148942 Engine time: 0.04505355516448617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.974778737872839,
    "estimated_duration": 3600.0991422746683,
    "input_throughput": 7633.955597854809,
    "output_throughput": 6812.056843663708,
    "total_throughput": 14446.012441518516,
    "itl": 126.20027982000644,
    "ttft": 689957.8625090956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2092868485860526,
    "arrivals": 128882,
    "finished_requests": 111222,
    "scheduler_time": 86.2149774688539
}
#Debug simulation 
Total elapsed time: 7.974888258147985. Arrivals time: 0.2776165874674916 Scheduler time: 7.569414752535522 Scheduler overhead time: 0.04447333049029112 Adapter cache time: 0.0167143396101892 Engine time: 0.04575120285153389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.823053275700659,
    "estimated_duration": 3600.08128018709,
    "input_throughput": 7633.890698871047,
    "output_throughput": 6812.003421968723,
    "total_throughput": 14445.89412083977,
    "itl": 126.20041718167931,
    "ttft": 690005.1174206744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 128882,
    "finished_requests": 111221,
    "scheduler_time": 86.21273738550512
}
#Debug simulation 
Total elapsed time: 7.823199100792408. Arrivals time: 0.27708368049934506 Scheduler time: 7.419076535385102 Scheduler overhead time: 0.04409143794327974 Adapter cache time: 0.017112184315919876 Engine time: 0.04505557660013437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.944389547221363,
    "estimated_duration": 3600.094557053007,
    "input_throughput": 7633.965320760142,
    "output_throughput": 6812.065519766545,
    "total_throughput": 14446.030840526686,
    "itl": 126.19975854972476,
    "ttft": 689955.8433674338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2119276781007646,
    "arrivals": 128882,
    "finished_requests": 111222,
    "scheduler_time": 86.21508400362859
}
#Debug simulation 
Total elapsed time: 7.944538451265544. Arrivals time: 0.2754420740529895 Scheduler time: 7.54123559826985 Scheduler overhead time: 0.0441318703815341 Adapter cache time: 0.01693255128338933 Engine time: 0.04579630820080638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.858609989285469,
    "estimated_duration": 3600.0212531013563,
    "input_throughput": 7633.914376567781,
    "output_throughput": 6811.860896341652,
    "total_throughput": 14445.775272909432,
    "itl": 126.19888119253636,
    "ttft": 690000.2704428559,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 128882,
    "finished_requests": 111219,
    "scheduler_time": 86.21235175925183
}
#Debug simulation 
Total elapsed time: 7.858719601295888. Arrivals time: 0.2716069435700774 Scheduler time: 7.460497688036412 Scheduler overhead time: 0.043739752378314734 Adapter cache time: 0.016626026947051287 Engine time: 0.045466160867363214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 17280, 17280, 66, 17280, 66, 270, 66, 17280, 270, 17280, 66, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 17280, 66, 66, 66, 17280, 270, 66, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 66, 270, 270, 17280, 17280, 270, 17280, 66, 17280, 270, 270, 270, 17280, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 17280, 270]
Prompts retrieved: 387216 . Total input tokens: 86467814 . Total output tokens: 77446717
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.859526923857629,
    "estimated_duration": 3600.109970587325,
    "input_throughput": 7633.932636651208,
    "output_throughput": 6812.036354544782,
    "total_throughput": 14445.968991195989,
    "itl": 126.20009686691897,
    "ttft": 689960.4863169729,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 128882,
    "finished_requests": 111222,
    "scheduler_time": 86.21504094476579
}
#Debug simulation 
Total elapsed time: 7.859655658714473. Arrivals time: 0.2745381211861968 Scheduler time: 7.458060946781188 Scheduler overhead time: 0.04408800834789872 Adapter cache time: 0.016717154532670975 Engine time: 0.04545025108382106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.944442660082132,
    "estimated_duration": 3600.1047323565695,
    "input_throughput": 7797.970638932924,
    "output_throughput": 6847.849391276616,
    "total_throughput": 14645.82003020954,
    "itl": 124.33861612843167,
    "ttft": 632504.2369087924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 128637,
    "finished_requests": 112446,
    "scheduler_time": 87.41871896150626
}
#Debug simulation 
Total elapsed time: 7.944581424817443. Arrivals time: 0.2762417853809893 Scheduler time: 7.541023935191333 Scheduler overhead time: 0.044575306586921215 Adapter cache time: 0.015866406727582216 Engine time: 0.045752903912216425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.011656568851322,
    "estimated_duration": 3600.127930670892,
    "input_throughput": 7797.920390781345,
    "output_throughput": 6847.805265466181,
    "total_throughput": 14645.725656247527,
    "itl": 124.33771080294441,
    "ttft": 632549.3505631728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 128637,
    "finished_requests": 112446,
    "scheduler_time": 87.4195043976283
}
#Debug simulation 
Total elapsed time: 8.011766355950385. Arrivals time: 0.2821638439781964 Scheduler time: 7.600695769768208 Scheduler overhead time: 0.04498053528368473 Adapter cache time: 0.0160659309476614 Engine time: 0.046445907559245825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.907692349050194,
    "estimated_duration": 3600.131655861935,
    "input_throughput": 7797.912321981099,
    "output_throughput": 6847.798179785634,
    "total_throughput": 14645.710501766733,
    "itl": 124.33778957914546,
    "ttft": 632552.1494180092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 128637,
    "finished_requests": 112446,
    "scheduler_time": 87.41945721790418
}
#Debug simulation 
Total elapsed time: 7.907882938161492. Arrivals time: 0.2755051492713392 Scheduler time: 7.505445360671729 Scheduler overhead time: 0.044271004386246204 Adapter cache time: 0.015553515404462814 Engine time: 0.04597880505025387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.970133655704558,
    "estimated_duration": 3600.109598425778,
    "input_throughput": 7797.960098846913,
    "output_throughput": 6847.840135416994,
    "total_throughput": 14645.800234263907,
    "itl": 124.33826454065454,
    "ttft": 632506.1313525434,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 128637,
    "finished_requests": 112446,
    "scheduler_time": 87.41866009809542
}
#Debug simulation 
Total elapsed time: 7.970275139901787. Arrivals time: 0.27605504309758544 Scheduler time: 7.566015921533108 Scheduler overhead time: 0.044928889721632004 Adapter cache time: 0.01592647982761264 Engine time: 0.046244166791439056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.942367422860116,
    "estimated_duration": 3600.112635811654,
    "input_throughput": 7797.953519771128,
    "output_throughput": 6847.834357949728,
    "total_throughput": 14645.787877720855,
    "itl": 124.33682553908541,
    "ttft": 632487.7163410486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 128637,
    "finished_requests": 112446,
    "scheduler_time": 87.42057991877898
}
#Debug simulation 
Total elapsed time: 7.94248108798638. Arrivals time: 0.2728789118118584 Scheduler time: 7.5420575751923025 Scheduler overhead time: 0.04463889868929982 Adapter cache time: 0.015575215220451355 Engine time: 0.04625372961163521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.920259304810315,
    "estimated_duration": 3600.024336845038,
    "input_throughput": 7798.065061027504,
    "output_throughput": 6847.9528728978075,
    "total_throughput": 14646.017933925312,
    "itl": 124.3361753193476,
    "ttft": 632426.5805995156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 128637,
    "finished_requests": 112445,
    "scheduler_time": 87.4181128317829
}
#Debug simulation 
Total elapsed time: 7.920376629102975. Arrivals time: 0.2720376756042242 Scheduler time: 7.520241274032742 Scheduler overhead time: 0.04472762392833829 Adapter cache time: 0.015859266743063927 Engine time: 0.04627000307664275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 17280, 17280, 33, 17280, 33, 270, 33, 17280, 270, 17280, 33, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 17280, 33, 33, 33, 17280, 270, 33, 17280, 17280, 17280, 17280, 270, 270, 270, 17280, 33, 270, 270, 17280, 17280, 270, 17280, 33, 17280, 270, 270, 270, 17280, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 17280, 270]
Prompts retrieved: 386523 . Total input tokens: 86317287 . Total output tokens: 77314538
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.849940849002451,
    "estimated_duration": 3600.013629830119,
    "input_throughput": 7797.997976281198,
    "output_throughput": 6847.88601791025,
    "total_throughput": 14645.883994191448,
    "itl": 124.33772706465648,
    "ttft": 632453.2614443176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 128637,
    "finished_requests": 112444,
    "scheduler_time": 87.4175578415382
}
#Debug simulation 
Total elapsed time: 7.850066865328699. Arrivals time: 0.27149775670841336 Scheduler time: 7.452179881744087 Scheduler overhead time: 0.0441834582015872 Adapter cache time: 0.015623921994119883 Engine time: 0.045652037020772696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.063491221982986,
    "estimated_duration": 3600.0428928845267,
    "input_throughput": 7891.929858989963,
    "output_throughput": 6990.242546759341,
    "total_throughput": 14882.172405749303,
    "itl": 122.18843041335134,
    "ttft": 511962.4625859428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 127906,
    "finished_requests": 114823,
    "scheduler_time": 91.09937552599585
}
#Debug simulation 
Total elapsed time: 8.063592745922506. Arrivals time: 0.2685702918097377 Scheduler time: 7.665595878846943 Scheduler overhead time: 0.04546101437881589 Adapter cache time: 0.015563435386866331 Engine time: 0.04688352858647704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.100979241076857,
    "estimated_duration": 3600.098348510735,
    "input_throughput": 7891.8560688079715,
    "output_throughput": 6990.19847899718,
    "total_throughput": 14882.054547805152,
    "itl": 122.1890686994421,
    "ttft": 511978.5877040283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 127906,
    "finished_requests": 114824,
    "scheduler_time": 91.09933784934985
}
#Debug simulation 
Total elapsed time: 8.101103355176747. Arrivals time: 0.27133421832695603 Scheduler time: 7.701013604644686 Scheduler overhead time: 0.04514248436316848 Adapter cache time: 0.015397874638438225 Engine time: 0.04670495958998799 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.125063044019043,
    "estimated_duration": 3600.0987162235283,
    "input_throughput": 7891.85526273662,
    "output_throughput": 6990.197765020812,
    "total_throughput": 14882.05302775743,
    "itl": 122.18905980342265,
    "ttft": 511978.8377294338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 127906,
    "finished_requests": 114824,
    "scheduler_time": 91.0992936081592
}
#Debug simulation 
Total elapsed time: 8.125193727668375. Arrivals time: 0.27346643479540944 Scheduler time: 7.722185871563852 Scheduler overhead time: 0.04560281662270427 Adapter cache time: 0.015426453202962875 Engine time: 0.046992023941129446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.104903475847095,
    "estimated_duration": 3600.05036289083,
    "input_throughput": 7891.913483450776,
    "output_throughput": 6990.228042196732,
    "total_throughput": 14882.141525647508,
    "itl": 122.18841947669209,
    "ttft": 511965.52388644905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 127906,
    "finished_requests": 114823,
    "scheduler_time": 91.09924748522535
}
#Debug simulation 
Total elapsed time: 8.105010266881436. Arrivals time: 0.2755010393448174 Scheduler time: 7.699989520013332 Scheduler overhead time: 0.04534697951748967 Adapter cache time: 0.015547401271760464 Engine time: 0.047082040924578905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.132870479021221,
    "estimated_duration": 3600.101290587174,
    "input_throughput": 7891.849619421711,
    "output_throughput": 6990.192766463952,
    "total_throughput": 14882.042385885663,
    "itl": 122.1889194861924,
    "ttft": 511981.5579513675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 127906,
    "finished_requests": 114824,
    "scheduler_time": 91.09922341225258
}
#Debug simulation 
Total elapsed time: 8.13299955194816. Arrivals time: 0.27824067743495107 Scheduler time: 7.725138340611011 Scheduler overhead time: 0.0453839018009603 Adapter cache time: 0.015661373268812895 Engine time: 0.047082163859158754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.120311403181404,
    "estimated_duration": 3600.0095796817795,
    "input_throughput": 7891.825666339294,
    "output_throughput": 6990.234732159917,
    "total_throughput": 14882.06039849921,
    "itl": 122.18719114402893,
    "ttft": 511987.06088289985,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 127906,
    "finished_requests": 114821,
    "scheduler_time": 91.09896986870002
}
#Debug simulation 
Total elapsed time: 8.120448036119342. Arrivals time: 0.27642057510092854 Scheduler time: 7.714259655680507 Scheduler overhead time: 0.045691643841564655 Adapter cache time: 0.01569231040775776 Engine time: 0.04671176942065358 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 17280, 17280, 66, 17280, 66, 135, 66, 17280, 135, 17280, 66, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 17280, 66, 66, 66, 17280, 135, 66, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 66, 135, 135, 17280, 17280, 135, 17280, 66, 17280, 135, 135, 135, 17280, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 17280, 135]
Prompts retrieved: 384381 . Total input tokens: 85824629 . Total output tokens: 76901894
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.22717950399965,
    "estimated_duration": 3600.100124043067,
    "input_throughput": 7891.936063181592,
    "output_throughput": 6990.322805727319,
    "total_throughput": 14882.25886890891,
    "itl": 122.18884693027233,
    "ttft": 511956.3355810707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 127906,
    "finished_requests": 114825,
    "scheduler_time": 91.09934036631711
}
#Debug simulation 
Total elapsed time: 8.227292383089662. Arrivals time: 0.2756120963022113 Scheduler time: 7.821103109046817 Scheduler overhead time: 0.04581880383193493 Adapter cache time: 0.015643203631043434 Engine time: 0.0475519192405045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.201197995338589,
    "estimated_duration": 3600.0051744192733,
    "input_throughput": 8058.646194773837,
    "output_throughput": 7084.56259486188,
    "total_throughput": 15143.208789635717,
    "itl": 120.10947436636198,
    "ttft": 442903.92135130306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 127669,
    "finished_requests": 116420,
    "scheduler_time": 93.1015288990607
}
#Debug simulation 
Total elapsed time: 8.201310457196087. Arrivals time: 0.27298172749578953 Scheduler time: 7.798872804734856 Scheduler overhead time: 0.04601243697106838 Adapter cache time: 0.0142454719170928 Engine time: 0.0473726661875844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.19927088310942,
    "estimated_duration": 3600.0282851239313,
    "input_throughput": 8058.594461571373,
    "output_throughput": 7084.517114876504,
    "total_throughput": 15143.111576447876,
    "itl": 120.10906124097154,
    "ttft": 442937.276592181,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 127669,
    "finished_requests": 116420,
    "scheduler_time": 93.10151420923187
}
#Debug simulation 
Total elapsed time: 8.199479843955487. Arrivals time: 0.2728063561953604 Scheduler time: 7.796183323953301 Scheduler overhead time: 0.046295147854834795 Adapter cache time: 0.014269692357629538 Engine time: 0.04771288391202688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.156221577897668,
    "estimated_duration": 3600.0287527764067,
    "input_throughput": 8058.593414740387,
    "output_throughput": 7084.51619458053,
    "total_throughput": 15143.109609320916,
    "itl": 120.10906004581666,
    "ttft": 442937.68572331493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 127669,
    "finished_requests": 116420,
    "scheduler_time": 93.10150165919806
}
#Debug simulation 
Total elapsed time: 8.156333781313151. Arrivals time: 0.2797982092015445 Scheduler time: 7.747469595167786 Scheduler overhead time: 0.04583007050678134 Adapter cache time: 0.014180855359882116 Engine time: 0.04726522695273161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.212748032063246,
    "estimated_duration": 3600.019653855159,
    "input_throughput": 8058.61378254776,
    "output_throughput": 7084.534100442478,
    "total_throughput": 15143.147882990237,
    "itl": 120.10929237989394,
    "ttft": 442933.5159110552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 127669,
    "finished_requests": 116420,
    "scheduler_time": 93.10149387990845
}
#Debug simulation 
Total elapsed time: 8.212880962062627. Arrivals time: 0.28719333792105317 Scheduler time: 7.79471680521965 Scheduler overhead time: 0.04675666755065322 Adapter cache time: 0.014491730369627476 Engine time: 0.04776695463806391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.210536791011691,
    "estimated_duration": 3600.0292749621212,
    "input_throughput": 8058.592245838126,
    "output_throughput": 7084.5151669685665,
    "total_throughput": 15143.107412806692,
    "itl": 120.10884205085932,
    "ttft": 442938.03778821876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 127669,
    "finished_requests": 116420,
    "scheduler_time": 93.10148085564853
}
#Debug simulation 
Total elapsed time: 8.21067335922271. Arrivals time: 0.28328814124688506 Scheduler time: 7.7976154941134155 Scheduler overhead time: 0.04588273447006941 Adapter cache time: 0.014368193224072456 Engine time: 0.04753821808844805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.26518236240372,
    "estimated_duration": 3600.005656370232,
    "input_throughput": 8058.645115922127,
    "output_throughput": 7084.561646415666,
    "total_throughput": 15143.206762337793,
    "itl": 120.11013271215253,
    "ttft": 442905.8157571276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 127669,
    "finished_requests": 116420,
    "scheduler_time": 93.10131482859161
}
#Debug simulation 
Total elapsed time: 8.265292355325073. Arrivals time: 0.2856830800883472 Scheduler time: 7.8492007539607584 Scheduler overhead time: 0.046532651875168085 Adapter cache time: 0.014225467573851347 Engine time: 0.047841318883001804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 17280, 17280, 33, 17280, 33, 135, 33, 17280, 135, 17280, 33, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 17280, 33, 33, 33, 17280, 135, 33, 17280, 17280, 17280, 17280, 135, 135, 135, 17280, 33, 135, 135, 17280, 17280, 135, 17280, 33, 17280, 135, 135, 135, 17280, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 17280, 135]
Prompts retrieved: 383688 . Total input tokens: 85672063 . Total output tokens: 76766353
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.20571005390957,
    "estimated_duration": 3600.0316483436864,
    "input_throughput": 8058.663876843326,
    "output_throughput": 7084.531329532674,
    "total_throughput": 15143.195206376,
    "itl": 120.10885530158129,
    "ttft": 442910.42319677165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 127669,
    "finished_requests": 116421,
    "scheduler_time": 93.10184662104014
}
#Debug simulation 
Total elapsed time: 8.205841748043895. Arrivals time: 0.2834914601407945 Scheduler time: 7.792220673989505 Scheduler overhead time: 0.04608731344342232 Adapter cache time: 0.014255140908062458 Engine time: 0.04758968111127615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.365526111796498,
    "estimated_duration": 3600.0266811077818,
    "input_throughput": 8210.841646013629,
    "output_throughput": 7208.505463635772,
    "total_throughput": 15419.347109649401,
    "itl": 117.96777850792068,
    "ttft": 335312.04762945365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 127143,
    "finished_requests": 118676,
    "scheduler_time": 96.17339016285045
}
#Debug simulation 
Total elapsed time: 8.365658422000706. Arrivals time: 0.2765699988231063 Scheduler time: 7.958199119661003 Scheduler overhead time: 0.04715316230431199 Adapter cache time: 0.012806997634470463 Engine time: 0.04863547533750534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.406930965837091,
    "estimated_duration": 3600.035940401396,
    "input_throughput": 8210.820805501238,
    "output_throughput": 7208.580811308929,
    "total_throughput": 15419.401616810168,
    "itl": 117.96737032428638,
    "ttft": 335306.1428417174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 127143,
    "finished_requests": 118677,
    "scheduler_time": 96.17430988553366
}
#Debug simulation 
Total elapsed time: 8.407044505234808. Arrivals time: 0.27685821428894997 Scheduler time: 7.999866681639105 Scheduler overhead time: 0.04699938325211406 Adapter cache time: 0.012828243896365166 Engine time: 0.04832942830398679 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.368619715794921,
    "estimated_duration": 3600.039838128785,
    "input_throughput": 8210.811915727076,
    "output_throughput": 7208.573006650056,
    "total_throughput": 15419.384922377132,
    "itl": 117.96746650472453,
    "ttft": 335309.46953617036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605262,
    "arrivals": 127143,
    "finished_requests": 118677,
    "scheduler_time": 96.17426980616737
}
#Debug simulation 
Total elapsed time: 8.36876073712483. Arrivals time: 0.27650948287919164 Scheduler time: 7.96182348811999 Scheduler overhead time: 0.04702615458518267 Adapter cache time: 0.012845668010413647 Engine time: 0.04834388615563512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.463745841756463,
    "estimated_duration": 3600.0235492418346,
    "input_throughput": 8210.849066869348,
    "output_throughput": 7208.6056230008035,
    "total_throughput": 15419.454689870152,
    "itl": 117.96746118401792,
    "ttft": 335255.2725126725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.200352315879427,
    "arrivals": 127143,
    "finished_requests": 118677,
    "scheduler_time": 96.17390337940145
}
#Debug simulation 
Total elapsed time: 8.46386112878099. Arrivals time: 0.2770501244813204 Scheduler time: 8.054826436564326 Scheduler overhead time: 0.04741375474259257 Adapter cache time: 0.012967411428689957 Engine time: 0.04903592402115464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.427800262812525,
    "estimated_duration": 3600.0462842602774,
    "input_throughput": 8210.797213701298,
    "output_throughput": 7208.560099202262,
    "total_throughput": 15419.357312903561,
    "itl": 117.96762168926156,
    "ttft": 335308.91921834025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076463,
    "arrivals": 127143,
    "finished_requests": 118677,
    "scheduler_time": 96.174239221333
}
#Debug simulation 
Total elapsed time: 8.427906365133822. Arrivals time: 0.2771115927025676 Scheduler time: 8.02038273960352 Scheduler overhead time: 0.047147410456091166 Adapter cache time: 0.012837151996791363 Engine time: 0.0482501070946455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.350453748833388,
    "estimated_duration": 3600.0204103517012,
    "input_throughput": 8210.855948206203,
    "output_throughput": 7208.5180198922135,
    "total_throughput": 15419.373968098416,
    "itl": 117.96796753746459,
    "ttft": 335282.09247794497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 127143,
    "finished_requests": 118676,
    "scheduler_time": 96.17343467337977
}
#Debug simulation 
Total elapsed time: 8.35059631895274. Arrivals time: 0.2736467821523547 Scheduler time: 7.944912082515657 Scheduler overhead time: 0.046912002842873335 Adapter cache time: 0.01321365823969245 Engine time: 0.04973774589598179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 66, 17280, 17280, 33, 17280, 33, 66, 33, 17280, 66, 17280, 33, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 17280, 33, 33, 33, 17280, 66, 33, 17280, 17280, 17280, 17280, 66, 66, 66, 17280, 33, 66, 66, 17280, 17280, 66, 17280, 33, 17280, 66, 66, 66, 17280, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 17280, 66]
Prompts retrieved: 382239 . Total input tokens: 85334118 . Total output tokens: 76480486
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.286312187556177,
    "estimated_duration": 3600.0498374469926,
    "input_throughput": 8210.789109786938,
    "output_throughput": 7208.55298447854,
    "total_throughput": 15419.342094265477,
    "itl": 117.96750284941362,
    "ttft": 335337.41026320553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2146942614018915,
    "arrivals": 127143,
    "finished_requests": 118677,
    "scheduler_time": 96.17431913989638
}
#Debug simulation 
Total elapsed time: 8.286418663803488. Arrivals time: 0.2728253551758826 Scheduler time: 7.881634916644543 Scheduler overhead time: 0.046529846265912056 Adapter cache time: 0.013085839804261923 Engine time: 0.050173734314739704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.584504049271345,
    "estimated_duration": 3600.006068473153,
    "input_throughput": 6465.252157164127,
    "output_throughput": 5674.382934766528,
    "total_throughput": 12139.635091930655,
    "itl": 149.29553736636302,
    "ttft": 392835.7363160184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 101151,
    "finished_requests": 93225,
    "scheduler_time": 76.0529560044353
}
#Debug simulation 
Total elapsed time: 6.58467983128503. Arrivals time: 0.22090460499748588 Scheduler time: 6.259862184058875 Scheduler overhead time: 0.03689315402880311 Adapter cache time: 0.00960087450221181 Engine time: 0.0396978217177093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.585061653051525,
    "estimated_duration": 3600.1625168791006,
    "input_throughput": 6465.315076992024,
    "output_throughput": 5674.469389706731,
    "total_throughput": 12139.784466698755,
    "itl": 149.29875377504027,
    "ttft": 392670.9642167849,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 101151,
    "finished_requests": 93230,
    "scheduler_time": 76.05633004430823
}
#Debug simulation 
Total elapsed time: 6.585174797102809. Arrivals time: 0.2209352138452232 Scheduler time: 6.260262756142765 Scheduler overhead time: 0.03714790474623442 Adapter cache time: 0.00958916638046503 Engine time: 0.03950913855805993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.5441857050172985,
    "estimated_duration": 3600.1620047662673,
    "input_throughput": 6465.3159966647545,
    "output_throughput": 5674.470196883906,
    "total_throughput": 12139.786193548662,
    "itl": 149.2986391046572,
    "ttft": 392670.4779587487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 101151,
    "finished_requests": 93230,
    "scheduler_time": 76.05632628047618
}
#Debug simulation 
Total elapsed time: 6.544298113323748. Arrivals time: 0.21595122292637825 Scheduler time: 6.225782380905002 Scheduler overhead time: 0.03669636696577072 Adapter cache time: 0.009483769070357084 Engine time: 0.03884395770728588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.561396395321935,
    "estimated_duration": 3600.094585747646,
    "input_throughput": 6465.169579750456,
    "output_throughput": 5674.378412409844,
    "total_throughput": 12139.5479921603,
    "itl": 149.29820127851985,
    "ttft": 392770.0368733281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 101151,
    "finished_requests": 93227,
    "scheduler_time": 76.05480952741327
}
#Debug simulation 
Total elapsed time: 6.561504559125751. Arrivals time: 0.21582014812156558 Scheduler time: 6.24239986250177 Scheduler overhead time: 0.03688974631950259 Adapter cache time: 0.00953541649505496 Engine time: 0.03919599670916796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.5721180331893265,
    "estimated_duration": 3600.1620350993135,
    "input_throughput": 6465.315942191448,
    "output_throughput": 5674.470149073845,
    "total_throughput": 12139.786091265294,
    "itl": 149.29815979845353,
    "ttft": 392670.2723107133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 101151,
    "finished_requests": 93230,
    "scheduler_time": 76.05633335742787
}
#Debug simulation 
Total elapsed time: 6.57223203824833. Arrivals time: 0.21295419847592711 Scheduler time: 6.2553114499896765 Scheduler overhead time: 0.03692144341766834 Adapter cache time: 0.009590474423021078 Engine time: 0.039708868600428104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.5533740390092134,
    "estimated_duration": 3600.165037288727,
    "input_throughput": 6465.347493494165,
    "output_throughput": 5674.509303991559,
    "total_throughput": 12139.856797485725,
    "itl": 149.2969649964887,
    "ttft": 392632.8098758378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 101151,
    "finished_requests": 93231,
    "scheduler_time": 76.05649306592517
}
#Debug simulation 
Total elapsed time: 6.553497530985624. Arrivals time: 0.2160068261437118 Scheduler time: 6.233990493696183 Scheduler overhead time: 0.036889113020151854 Adapter cache time: 0.009527541697025299 Engine time: 0.039466675370931625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 4320, 8640, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 1080, 1080, 8640, 4320, 1080, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 1080, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 8640, 4320]
Prompts retrieved: 303480 . Total input tokens: 67823698 . Total output tokens: 60750181
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.589406610932201,
    "estimated_duration": 3600.001527514365,
    "input_throughput": 6465.260312283888,
    "output_throughput": 5674.3900923021165,
    "total_throughput": 12139.650404586006,
    "itl": 149.29696256765038,
    "ttft": 392805.5926990976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 101151,
    "finished_requests": 93225,
    "scheduler_time": 76.05307874181159
}
#Debug simulation 
Total elapsed time: 6.589535791892558. Arrivals time: 0.21632127184420824 Scheduler time: 6.26876725256443 Scheduler overhead time: 0.03717320645228028 Adapter cache time: 0.009505677502602339 Engine time: 0.040010261349380016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.655269476119429,
    "estimated_duration": 3600.126332499901,
    "input_throughput": 6543.638979369247,
    "output_throughput": 5745.895029643538,
    "total_throughput": 12289.534009012785,
    "itl": 145.72289071208925,
    "ttft": 134681.26906947474,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 97469,
    "finished_requests": 94778,
    "scheduler_time": 79.1103022298338
}
#Debug simulation 
Total elapsed time: 6.655410361941904. Arrivals time: 0.20888077793642879 Scheduler time: 6.336894746404141 Scheduler overhead time: 0.03762847278267145 Adapter cache time: 0.013462619855999947 Engine time: 0.04048415971919894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.601501260884106,
    "estimated_duration": 3600.0873773710964,
    "input_throughput": 6543.680619552824,
    "output_throughput": 5745.956648170458,
    "total_throughput": 12289.637267723283,
    "itl": 145.72478999861858,
    "ttft": 134719.9398445964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 97469,
    "finished_requests": 94777,
    "scheduler_time": 79.10921161566591
}
#Debug simulation 
Total elapsed time: 6.6016448070295155. Arrivals time: 0.2070423918776214 Scheduler time: 6.285575158428401 Scheduler overhead time: 0.037740856409072876 Adapter cache time: 0.013051419984549284 Engine time: 0.04029707843437791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.629158752039075,
    "estimated_duration": 3600.089046465296,
    "input_throughput": 6543.677585733598,
    "output_throughput": 5745.953984196653,
    "total_throughput": 12289.631569930252,
    "itl": 145.72475890236248,
    "ttft": 134721.4169823933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2092868485860525,
    "arrivals": 97469,
    "finished_requests": 94777,
    "scheduler_time": 79.10921596484366
}
#Debug simulation 
Total elapsed time: 6.629297023639083. Arrivals time: 0.20686842687427998 Scheduler time: 6.313695496879518 Scheduler overhead time: 0.03757113590836525 Adapter cache time: 0.012974794022738934 Engine time: 0.04013483552262187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.618872215971351,
    "estimated_duration": 3600.0713461404994,
    "input_throughput": 6543.709758767824,
    "output_throughput": 5745.9822350955965,
    "total_throughput": 12289.69199386342,
    "itl": 145.72623893379597,
    "ttft": 134681.12800258005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 97469,
    "finished_requests": 94777,
    "scheduler_time": 79.10909159436724
}
#Debug simulation 
Total elapsed time: 6.618979126214981. Arrivals time: 0.2048687133938074 Scheduler time: 6.305033953394741 Scheduler overhead time: 0.03789711045101285 Adapter cache time: 0.013074697460979223 Engine time: 0.04003157280385494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.649574526119977,
    "estimated_duration": 3600.1242514813835,
    "input_throughput": 6543.642761859221,
    "output_throughput": 5745.898351005002,
    "total_throughput": 12289.541112864223,
    "itl": 145.72485505757425,
    "ttft": 134682.44577460308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076452,
    "arrivals": 97469,
    "finished_requests": 94778,
    "scheduler_time": 79.11005217756296
}
#Debug simulation 
Total elapsed time: 6.649680396076292. Arrivals time: 0.21160607133060694 Scheduler time: 6.329266514163464 Scheduler overhead time: 0.03778407024219632 Adapter cache time: 0.012988792732357979 Engine time: 0.04004945047199726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.611848745960742,
    "estimated_duration": 3600.126644643424,
    "input_throughput": 6543.638412012949,
    "output_throughput": 5745.894531454421,
    "total_throughput": 12289.53294346737,
    "itl": 145.7237800502404,
    "ttft": 134684.74372440833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 97469,
    "finished_requests": 94778,
    "scheduler_time": 79.10992756701296
}
#Debug simulation 
Total elapsed time: 6.611946294084191. Arrivals time: 0.21239220025017858 Scheduler time: 6.291230284143239 Scheduler overhead time: 0.037583716213703156 Adapter cache time: 0.012972726952284575 Engine time: 0.03988584643229842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 8640, 4320, 8640, 540, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 540, 540, 8640, 4320, 540, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 540, 4320, 4320, 8640, 8640, 4320, 8640, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 8640, 4320]
Prompts retrieved: 292140 . Total input tokens: 65294059 . Total output tokens: 58446438
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.638297616969794,
    "estimated_duration": 3600.12682188687,
    "input_throughput": 6543.638089852903,
    "output_throughput": 5745.894248569345,
    "total_throughput": 12289.532338422248,
    "itl": 145.72414528362347,
    "ttft": 134682.23912031192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189138,
    "arrivals": 97469,
    "finished_requests": 94778,
    "scheduler_time": 79.11030162690702
}
#Debug simulation 
Total elapsed time: 6.638439822010696. Arrivals time: 0.2063391488045454 Scheduler time: 6.322385623585433 Scheduler overhead time: 0.03806178783997893 Adapter cache time: 0.013061864767223597 Engine time: 0.040439764969050884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.589263951405883,
    "estimated_duration": 3600.0874648807303,
    "input_throughput": 6533.8890317153155,
    "output_throughput": 5735.319544711131,
    "total_throughput": 12269.208576426447,
    "itl": 116.03241680711595,
    "ttft": 30020.66695082438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 95579,
    "finished_requests": 94786,
    "scheduler_time": 78.49715081755826
}
#Debug simulation 
Total elapsed time: 6.589361438062042. Arrivals time: 0.19858232233673334 Scheduler time: 6.26161046884954 Scheduler overhead time: 0.045206520706415176 Adapter cache time: 0.01574878580868244 Engine time: 0.04681641282513738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.587147164158523,
    "estimated_duration": 3600.0697669290153,
    "input_throughput": 6533.92115232966,
    "output_throughput": 5735.347739555938,
    "total_throughput": 12269.268891885597,
    "itl": 116.03062748348405,
    "ttft": 30019.98930510668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 95579,
    "finished_requests": 94786,
    "scheduler_time": 78.49656092533486
}
#Debug simulation 
Total elapsed time: 6.587249808013439. Arrivals time: 0.1989611997269094 Scheduler time: 6.257856766227633 Scheduler overhead time: 0.04531297693029046 Adapter cache time: 0.01576923904940486 Engine time: 0.047959561459720135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.645914718974382,
    "estimated_duration": 3600.0721697814774,
    "input_throughput": 6533.916791292495,
    "output_throughput": 5735.343911523113,
    "total_throughput": 12269.260702815607,
    "itl": 116.03054722552838,
    "ttft": 30020.3069450181,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 95579,
    "finished_requests": 94786,
    "scheduler_time": 78.496666488384
}
#Debug simulation 
Total elapsed time: 6.646031978074461. Arrivals time: 0.20088189281523228 Scheduler time: 6.314575961790979 Scheduler overhead time: 0.045512961223721504 Adapter cache time: 0.015883968211710453 Engine time: 0.04771407134830952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.608229746110737,
    "estimated_duration": 3600.072298870456,
    "input_throughput": 6533.916557003688,
    "output_throughput": 5735.343705868997,
    "total_throughput": 12269.260262872684,
    "itl": 116.03171064943211,
    "ttft": 30020.030096741528,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942695,
    "arrivals": 95579,
    "finished_requests": 94786,
    "scheduler_time": 78.4968587250576
}
#Debug simulation 
Total elapsed time: 6.6083316039294. Arrivals time: 0.19846707629039884 Scheduler time: 6.278981016017497 Scheduler overhead time: 0.045624560210853815 Adapter cache time: 0.015923498198390007 Engine time: 0.047873014118522406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.599357074126601,
    "estimated_duration": 3600.0697770468864,
    "input_throughput": 6533.921133966301,
    "output_throughput": 5735.347723436942,
    "total_throughput": 12269.268857403244,
    "itl": 116.03031082416051,
    "ttft": 30019.954224482295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 95579,
    "finished_requests": 94786,
    "scheduler_time": 78.4965160488937
}
#Debug simulation 
Total elapsed time: 6.599450535140932. Arrivals time: 0.19963732827454805 Scheduler time: 6.272476633545011 Scheduler overhead time: 0.045043541584163904 Adapter cache time: 0.015305666252970695 Engine time: 0.045787818264216185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.552442821674049,
    "estimated_duration": 3600.07314159097,
    "input_throughput": 6533.915027516562,
    "output_throughput": 5735.34236331522,
    "total_throughput": 12269.257390831783,
    "itl": 116.03286113804792,
    "ttft": 30020.599913442817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 95579,
    "finished_requests": 94786,
    "scheduler_time": 78.4971184298279
}
#Debug simulation 
Total elapsed time: 6.55254248669371. Arrivals time: 0.19729630928486586 Scheduler time: 6.229058605618775 Scheduler overhead time: 0.04470930062234402 Adapter cache time: 0.015161284245550632 Engine time: 0.045132636558264494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 8640, 4320, 8640, 270, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 270, 270, 8640, 4320, 270, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 270, 4320, 4320, 8640, 8640, 4320, 8640, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 8640, 4320]
Prompts retrieved: 286470 . Total input tokens: 64041887 . Total output tokens: 57312412
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.564011613838375,
    "estimated_duration": 3600.00655766858,
    "input_throughput": 6534.035875543955,
    "output_throughput": 5735.448441341657,
    "total_throughput": 12269.484316885611,
    "itl": 116.03164600962597,
    "ttft": 29983.27251447101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 95579,
    "finished_requests": 94786,
    "scheduler_time": 78.49550127342619
}
#Debug simulation 
Total elapsed time: 6.564108399674296. Arrivals time: 0.1980960639193654 Scheduler time: 6.239044502843171 Scheduler overhead time: 0.04496837640181184 Adapter cache time: 0.015307532623410225 Engine time: 0.04549559345468879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.584380603861064,
    "estimated_duration": 3600.0766687876603,
    "input_throughput": 6447.272693171916,
    "output_throughput": 5755.381039420329,
    "total_throughput": 12202.653732592245,
    "itl": 103.34403750997328,
    "ttft": 26545.522037572442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 94646,
    "finished_requests": 93952,
    "scheduler_time": 78.03776305149324
}
#Debug simulation 
Total elapsed time: 6.584487001877278. Arrivals time: 0.19884187169373035 Scheduler time: 6.248265427537262 Scheduler overhead time: 0.04927100567147136 Adapter cache time: 0.013780690729618073 Engine time: 0.05094558047130704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.624718587845564,
    "estimated_duration": 3600.0698740812786,
    "input_throughput": 6447.2848616370975,
    "output_throughput": 5755.391902021791,
    "total_throughput": 12202.676763658888,
    "itl": 103.34607704353367,
    "ttft": 26545.53675018461,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 94646,
    "finished_requests": 93952,
    "scheduler_time": 78.03784532938377
}
#Debug simulation 
Total elapsed time: 6.624812264926732. Arrivals time: 0.20052469708025455 Scheduler time: 6.288220810703933 Scheduler overhead time: 0.04894319223240018 Adapter cache time: 0.013762930408120155 Engine time: 0.050063283648341894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.639127802103758,
    "estimated_duration": 3600.001889405507,
    "input_throughput": 6447.406616176231,
    "output_throughput": 5755.500590423747,
    "total_throughput": 12202.907206599979,
    "itl": 103.34396778697418,
    "ttft": 26507.587659290915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 94646,
    "finished_requests": 93952,
    "scheduler_time": 78.03600328517042
}
#Debug simulation 
Total elapsed time: 6.639223729260266. Arrivals time: 0.1987549695186317 Scheduler time: 6.304515599273145 Scheduler overhead time: 0.04902265826240182 Adapter cache time: 0.013709688559174538 Engine time: 0.05000151880085468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.610973357222974,
    "estimated_duration": 3600.0862918473676,
    "input_throughput": 6447.255459559984,
    "output_throughput": 5755.36565524037,
    "total_throughput": 12202.621114800355,
    "itl": 103.34413305672456,
    "ttft": 26545.608066675246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942695,
    "arrivals": 94646,
    "finished_requests": 93952,
    "scheduler_time": 78.03795078092666
}
#Debug simulation 
Total elapsed time: 6.611084979958832. Arrivals time: 0.20045260852202773 Scheduler time: 6.273388625588268 Scheduler overhead time: 0.049559881910681725 Adapter cache time: 0.013900276273488998 Engine time: 0.050421237014234066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.587772843893617,
    "estimated_duration": 3600.0698700083776,
    "input_throughput": 6447.284868931165,
    "output_throughput": 5755.391908533093,
    "total_throughput": 12202.676777464258,
    "itl": 103.34573039368638,
    "ttft": 26545.450220346447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 94646,
    "finished_requests": 93952,
    "scheduler_time": 78.0378054549522
}
#Debug simulation 
Total elapsed time: 6.587873911019415. Arrivals time: 0.19822981068864465 Scheduler time: 6.253713977057487 Scheduler overhead time: 0.04900180036202073 Adapter cache time: 0.013800421729683876 Engine time: 0.04983106907457113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.620026036165655,
    "estimated_duration": 3600.0060204325036,
    "input_throughput": 6447.399217741163,
    "output_throughput": 5755.493985954703,
    "total_throughput": 12202.893203695867,
    "itl": 103.34289038438186,
    "ttft": 26507.5092854196,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 94646,
    "finished_requests": 93952,
    "scheduler_time": 78.0358148619436
}
#Debug simulation 
Total elapsed time: 6.6201628958806396. Arrivals time: 0.20249977661296725 Scheduler time: 6.280169607605785 Scheduler overhead time: 0.04957492928951979 Adapter cache time: 0.013727434445172548 Engine time: 0.050701520405709743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 8640, 4320, 8640, 135, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 135, 135, 8640, 4320, 135, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 135, 4320, 4320, 8640, 8640, 4320, 8640, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 8640, 4320]
Prompts retrieved: 283635 . Total input tokens: 63409082 . Total output tokens: 56735307
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.610035912133753,
    "estimated_duration": 3600.0861828586735,
    "input_throughput": 6447.255654743632,
    "output_throughput": 5755.365829477807,
    "total_throughput": 12202.62148422144,
    "itl": 103.34584020375232,
    "ttft": 26545.298160326034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2146942614018914,
    "arrivals": 94646,
    "finished_requests": 93952,
    "scheduler_time": 78.03820681581382
}
#Debug simulation 
Total elapsed time: 6.6101339790038764. Arrivals time: 0.1999227935448289 Scheduler time: 6.273326817434281 Scheduler overhead time: 0.049349067732691765 Adapter cache time: 0.01378158014267683 Engine time: 0.050328831654042006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.565093148034066,
    "estimated_duration": 3600.028732903605,
    "input_throughput": 6447.211320249615,
    "output_throughput": 5718.576302416207,
    "total_throughput": 12165.787622665823,
    "itl": 93.31158897500984,
    "ttft": 23593.470871408328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 94180,
    "finished_requests": 93565,
    "scheduler_time": 76.78195898278388
}
#Debug simulation 
Total elapsed time: 6.5651940032839775. Arrivals time: 0.20048835454508662 Scheduler time: 6.219494561199099 Scheduler overhead time: 0.05316278850659728 Adapter cache time: 0.01227383129298687 Engine time: 0.054585047997534275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.537071844097227,
    "estimated_duration": 3600.0286953396326,
    "input_throughput": 6447.2113875220975,
    "output_throughput": 5718.576362085854,
    "total_throughput": 12165.787749607953,
    "itl": 93.3099217173366,
    "ttft": 23593.41669775846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 94180,
    "finished_requests": 93565,
    "scheduler_time": 76.78161789293263
}
#Debug simulation 
Total elapsed time: 6.537169340066612. Arrivals time: 0.19990615267306566 Scheduler time: 6.192767172586173 Scheduler overhead time: 0.05315918708220124 Adapter cache time: 0.01225219201296568 Engine time: 0.05398268857970834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.5753943040035665,
    "estimated_duration": 3600.0287193660647,
    "input_throughput": 6447.211344493695,
    "output_throughput": 5718.576323920329,
    "total_throughput": 12165.787668414023,
    "itl": 93.30991983919715,
    "ttft": 23593.389586025412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 94180,
    "finished_requests": 93565,
    "scheduler_time": 76.78158602616732
}
#Debug simulation 
Total elapsed time: 6.575498734600842. Arrivals time: 0.20448579639196396 Scheduler time: 6.225947441533208 Scheduler overhead time: 0.05328865675255656 Adapter cache time: 0.012364291120320559 Engine time: 0.05402299761772156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.5615198840387166,
    "estimated_duration": 3600.0294147527243,
    "input_throughput": 6447.210099141436,
    "output_throughput": 5718.575219312219,
    "total_throughput": 12165.785318453656,
    "itl": 93.31131620010201,
    "ttft": 23593.390913834635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 94180,
    "finished_requests": 93565,
    "scheduler_time": 76.78188650403833
}
#Debug simulation 
Total elapsed time: 6.561620536260307. Arrivals time: 0.20047606760635972 Scheduler time: 6.216106622479856 Scheduler overhead time: 0.05310393217951059 Adapter cache time: 0.012326246127486229 Engine time: 0.05437015136703849 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.565064667724073,
    "estimated_duration": 3600.02586627735,
    "input_throughput": 6447.216454030851,
    "output_throughput": 5718.580856000425,
    "total_throughput": 12165.797310031277,
    "itl": 93.3097488573032,
    "ttft": 23593.55573122575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 94180,
    "finished_requests": 93565,
    "scheduler_time": 76.78142520553493
}
#Debug simulation 
Total elapsed time: 6.5651761195622385. Arrivals time: 0.20232848031446338 Scheduler time: 6.217531205620617 Scheduler overhead time: 0.05312101310119033 Adapter cache time: 0.012227596715092659 Engine time: 0.054631642531603575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.5937560219317675,
    "estimated_duration": 3600.0680561966205,
    "input_throughput": 6447.2072854425915,
    "output_throughput": 5718.658836063846,
    "total_throughput": 12165.866121506439,
    "itl": 93.31058885254765,
    "ttft": 23555.283637668028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 94180,
    "finished_requests": 93567,
    "scheduler_time": 76.78228314385477
}
#Debug simulation 
Total elapsed time: 6.593856570776552. Arrivals time: 0.20041658449918032 Scheduler time: 6.24823409691453 Scheduler overhead time: 0.053200355265289545 Adapter cache time: 0.012416500598192215 Engine time: 0.054191207978874445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 8640, 4320, 8640, 66, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 66, 66, 8640, 4320, 66, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 66, 4320, 4320, 8640, 8640, 4320, 8640, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 8640, 4320]
Prompts retrieved: 282186 . Total input tokens: 63078113 . Total output tokens: 56436327
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.5419352310709655,
    "estimated_duration": 3600.047048568199,
    "input_throughput": 6447.178519300484,
    "output_throughput": 5718.547208483795,
    "total_throughput": 12165.72572778428,
    "itl": 93.31004598725292,
    "ttft": 23593.58651639598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 94180,
    "finished_requests": 93565,
    "scheduler_time": 76.78198628655446
}
#Debug simulation 
Total elapsed time: 6.5420342441648245. Arrivals time: 0.19968323595821857 Scheduler time: 6.197554528713226 Scheduler overhead time: 0.05322209559381008 Adapter cache time: 0.012215502094477415 Engine time: 0.054137520492076874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.549082844983786,
    "estimated_duration": 3600.032290832305,
    "input_throughput": 6426.517356223818,
    "output_throughput": 5698.945548973601,
    "total_throughput": 12125.462905197419,
    "itl": 90.0345150317001,
    "ttft": 23874.930895347617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 93946,
    "finished_requests": 93326,
    "scheduler_time": 76.23933097387864
}
#Debug simulation 
Total elapsed time: 6.549180979840457. Arrivals time: 0.1995563292875886 Scheduler time: 6.202117543667555 Scheduler overhead time: 0.0544314319267869 Adapter cache time: 0.011252803262323141 Engine time: 0.05590031109750271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.5623085498809814,
    "estimated_duration": 3600.0024700346125,
    "input_throughput": 6426.495590648182,
    "output_throughput": 5698.89497875893,
    "total_throughput": 12125.390569407113,
    "itl": 90.03512642763681,
    "ttft": 23913.26255264261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 93946,
    "finished_requests": 93325,
    "scheduler_time": 76.23900704694879
}
#Debug simulation 
Total elapsed time: 6.562400332652032. Arrivals time: 0.20263518439605832 Scheduler time: 6.211562163662165 Scheduler overhead time: 0.054875663947314024 Adapter cache time: 0.011327984277158976 Engine time: 0.05594512773677707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.560919898096472,
    "estimated_duration": 3600.0006469738546,
    "input_throughput": 6426.498845062019,
    "output_throughput": 5698.897864711689,
    "total_throughput": 12125.39670977371,
    "itl": 90.0350923578772,
    "ttft": 23913.280099566793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 93946,
    "finished_requests": 93325,
    "scheduler_time": 76.23894179734611
}
#Debug simulation 
Total elapsed time: 6.561015770770609. Arrivals time: 0.20503984717652202 Scheduler time: 6.207253277301788 Scheduler overhead time: 0.054979207925498486 Adapter cache time: 0.011300312355160713 Engine time: 0.05622435500845313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.573901349212974,
    "estimated_duration": 3600.050918107238,
    "input_throughput": 6426.484104331448,
    "output_throughput": 5698.916061661343,
    "total_throughput": 12125.40016599279,
    "itl": 90.03466830547556,
    "ttft": 23913.102936727548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 93946,
    "finished_requests": 93326,
    "scheduler_time": 76.23984119334035
}
#Debug simulation 
Total elapsed time: 6.574011884164065. Arrivals time: 0.2027377299964428 Scheduler time: 6.223789216019213 Scheduler overhead time: 0.054682997055351734 Adapter cache time: 0.01129495957866311 Engine time: 0.055377605836838484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.569635394960642,
    "estimated_duration": 3600.0043994273274,
    "input_throughput": 6426.492146420786,
    "output_throughput": 5698.891924483092,
    "total_throughput": 12125.38407090388,
    "itl": 90.03499312188927,
    "ttft": 23913.272773020883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 93946,
    "finished_requests": 93325,
    "scheduler_time": 76.23896442719771
}
#Debug simulation 
Total elapsed time: 6.56977111985907. Arrivals time: 0.20050526596605778 Scheduler time: 6.220547313801944 Scheduler overhead time: 0.05478376289829612 Adapter cache time: 0.011224656365811825 Engine time: 0.05665054125711322 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.536039089784026,
    "estimated_duration": 3600.031437488724,
    "input_throughput": 6426.518879551441,
    "output_throughput": 5698.946899839194,
    "total_throughput": 12125.465779390635,
    "itl": 90.03509590819031,
    "ttft": 23875.198904895256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 93946,
    "finished_requests": 93326,
    "scheduler_time": 76.23948098884183
}
#Debug simulation 
Total elapsed time: 6.5361536969430745. Arrivals time: 0.20028873812407255 Scheduler time: 6.186592387966812 Scheduler overhead time: 0.05496766697615385 Adapter cache time: 0.011246866546571255 Engine time: 0.05697909602895379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 8640, 4320, 8640, 33, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 33, 33, 8640, 4320, 33, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 33, 4320, 4320, 8640, 8640, 4320, 8640, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 8640, 4320]
Prompts retrieved: 281493 . Total input tokens: 62920289 . Total output tokens: 56298982
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.567643786780536,
    "estimated_duration": 3600.0122354941695,
    "input_throughput": 6426.5531577628635,
    "output_throughput": 5698.977297276808,
    "total_throughput": 12125.53045503967,
    "itl": 90.03545149821193,
    "ttft": 23875.0571462167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 93946,
    "finished_requests": 93326,
    "scheduler_time": 76.23913765736975
}
#Debug simulation 
Total elapsed time: 6.567747476976365. Arrivals time: 0.201762602198869 Scheduler time: 6.216570009011775 Scheduler overhead time: 0.05504936259239912 Adapter cache time: 0.011317259632050991 Engine time: 0.05681542633101344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_64_slots_64_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_64_slots_64_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.399492148309946,
    "estimated_duration": 3600.04598971395,
    "input_throughput": 5148.812002113567,
    "output_throughput": 4567.960811330265,
    "total_throughput": 9716.772813443831,
    "itl": 62.90586146617774,
    "ttft": 16916.729937947814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 74944,
    "finished_requests": 74593,
    "scheduler_time": 56.48601417085302
}
#Debug simulation 
Total elapsed time: 5.39960850821808. Arrivals time: 0.16894636675715446 Scheduler time: 5.012515070848167 Scheduler overhead time: 0.07200750382617116 Adapter cache time: 0.03802487067878246 Engine time: 0.07393768895417452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_64_slots_64_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_64_slots_64_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.348177156876773,
    "estimated_duration": 3600.0563963700133,
    "input_throughput": 5148.797118481273,
    "output_throughput": 4567.947606760158,
    "total_throughput": 9716.744725241431,
    "itl": 62.90671901040257,
    "ttft": 16916.756112667234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 74944,
    "finished_requests": 74593,
    "scheduler_time": 56.486442127494485
}
#Debug simulation 
Total elapsed time: 5.348272106144577. Arrivals time: 0.16793056251481175 Scheduler time: 4.966634378302842 Scheduler overhead time: 0.07053628098219633 Adapter cache time: 0.03769257152453065 Engine time: 0.07165878126397729 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_64_slots_64_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_64_slots_64_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.35926173068583,
    "estimated_duration": 3600.055237369481,
    "input_throughput": 5148.798776083228,
    "output_throughput": 4567.9490773636235,
    "total_throughput": 9716.747853446852,
    "itl": 62.90677844894871,
    "ttft": 16916.75848325493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 74944,
    "finished_requests": 74593,
    "scheduler_time": 56.48642974722079
}
#Debug simulation 
Total elapsed time: 5.3593593798577785. Arrivals time: 0.17390015628188848 Scheduler time: 4.969854100607336 Scheduler overhead time: 0.07141781598329544 Adapter cache time: 0.037711808923631907 Engine time: 0.07249215198680758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_64_slots_64_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_64_slots_64_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.398786551784724,
    "estimated_duration": 3600.0550644523682,
    "input_throughput": 5148.799023389284,
    "output_throughput": 4567.949296770425,
    "total_throughput": 9716.748320159708,
    "itl": 62.90576474407973,
    "ttft": 16916.79112799197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 74944,
    "finished_requests": 74593,
    "scheduler_time": 56.48624014742925
}
#Debug simulation 
Total elapsed time: 5.398882629815489. Arrivals time: 0.17007724335417151 Scheduler time: 5.011644283309579 Scheduler overhead time: 0.07172496616840363 Adapter cache time: 0.0379361710511148 Engine time: 0.07329479046165943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_64_slots_64_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_64_slots_64_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.348687265999615,
    "estimated_duration": 3600.0528262158637,
    "input_throughput": 5148.8022245172915,
    "output_throughput": 4567.9521367706575,
    "total_throughput": 9716.754361287949,
    "itl": 62.906823426062296,
    "ttft": 16916.77837529748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 74944,
    "finished_requests": 74593,
    "scheduler_time": 56.486364825417475
}
#Debug simulation 
Total elapsed time: 5.348781845998019. Arrivals time: 0.16966532776132226 Scheduler time: 4.964028392918408 Scheduler overhead time: 0.07104191370308399 Adapter cache time: 0.03793258685618639 Engine time: 0.07214547926560044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_64_slots_64_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_64_slots_64_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.383723367936909,
    "estimated_duration": 3600.0364610511383,
    "input_throughput": 5148.82563011261,
    "output_throughput": 4567.9729019184515,
    "total_throughput": 9716.798532031062,
    "itl": 62.90603840631758,
    "ttft": 16916.69001491839,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 74944,
    "finished_requests": 74593,
    "scheduler_time": 56.485851763907725
}
#Debug simulation 
Total elapsed time: 5.383822006173432. Arrivals time: 0.1712015150114894 Scheduler time: 4.9965215073898435 Scheduler overhead time: 0.07090403838083148 Adapter cache time: 0.038061972707509995 Engine time: 0.0728849358856678 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_64_slots_64_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_64_slots_64_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 8640, 1080, 8640, 540, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 540, 540, 8640, 1080, 540, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 540, 1080, 1080, 8640, 8640, 1080, 8640, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 8640, 1080]
Prompts retrieved: 224100 . Total input tokens: 50148211 . Total output tokens: 44756551
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.384970485698432,
    "estimated_duration": 3600.045985647256,
    "input_throughput": 5148.812007929782,
    "output_throughput": 4567.960816490337,
    "total_throughput": 9716.772824420119,
    "itl": 62.906403974265864,
    "ttft": 16916.802920636146,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 74944,
    "finished_requests": 74593,
    "scheduler_time": 56.48613856201663
}
#Debug simulation 
Total elapsed time: 5.38507262384519. Arrivals time: 0.17856429144740105 Scheduler time: 4.991446244996041 Scheduler overhead time: 0.07059933478012681 Adapter cache time: 0.037981560453772545 Engine time: 0.07254767371341586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_64_slots_64_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_64_slots_64_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.185523912776262,
    "estimated_duration": 3600.027882912381,
    "input_throughput": 5036.94654312802,
    "output_throughput": 4428.352923506512,
    "total_throughput": 9465.299466634531,
    "itl": 54.981635369588275,
    "ttft": 13241.888821400298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 73090,
    "finished_requests": 72822,
    "scheduler_time": 52.89413218623384
}
#Debug simulation 
Total elapsed time: 5.18561992701143. Arrivals time: 0.17425717739388347 Scheduler time: 4.781638836488128 Scheduler overhead time: 0.07736056437715888 Adapter cache time: 0.03757715318351984 Engine time: 0.07791163213551044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_64_slots_64_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_64_slots_64_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.154407674912363,
    "estimated_duration": 3600.057408281708,
    "input_throughput": 5036.9549547419465,
    "output_throughput": 4428.396325937835,
    "total_throughput": 9465.351280679783,
    "itl": 54.9814827283807,
    "ttft": 13241.828763232801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 73090,
    "finished_requests": 72823,
    "scheduler_time": 52.8946990065222
}
#Debug simulation 
Total elapsed time: 5.154505810700357. Arrivals time: 0.16525448160246015 Scheduler time: 4.759743859991431 Scheduler overhead time: 0.07755124196410179 Adapter cache time: 0.03756092628464103 Engine time: 0.07754335552453995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_64_slots_64_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_64_slots_64_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.2171362191438675,
    "estimated_duration": 3600.0580672130154,
    "input_throughput": 5036.954032810341,
    "output_throughput": 4428.395515392858,
    "total_throughput": 9465.3495482032,
    "itl": 54.981493483203074,
    "ttft": 13241.83945752232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 73090,
    "finished_requests": 72823,
    "scheduler_time": 52.894711140946335
}
#Debug simulation 
Total elapsed time: 5.217234333045781. Arrivals time: 0.16757875541225076 Scheduler time: 4.816618053242564 Scheduler overhead time: 0.07848964631557465 Adapter cache time: 0.03783014742657542 Engine time: 0.07922122813761234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_64_slots_64_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_64_slots_64_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.238956455141306,
    "estimated_duration": 3600.0581082773006,
    "input_throughput": 5036.953975356014,
    "output_throughput": 4428.395464880092,
    "total_throughput": 9465.349440236105,
    "itl": 54.981828292662485,
    "ttft": 13241.826797568854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.200352315879427,
    "arrivals": 73090,
    "finished_requests": 72823,
    "scheduler_time": 52.89475546993508
}
#Debug simulation 
Total elapsed time: 5.239106418099254. Arrivals time: 0.16726904036477208 Scheduler time: 4.840470829047263 Scheduler overhead time: 0.07769217388704419 Adapter cache time: 0.037719827610999346 Engine time: 0.07880405802279711 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_64_slots_64_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_64_slots_64_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.1875222572125494,
    "estimated_duration": 3600.059322684845,
    "input_throughput": 5036.952276240982,
    "output_throughput": 4428.3939710500235,
    "total_throughput": 9465.346247291007,
    "itl": 54.98152705511158,
    "ttft": 13241.826685149017,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 73090,
    "finished_requests": 72823,
    "scheduler_time": 52.894690000970435
}
#Debug simulation 
Total elapsed time: 5.187622540164739. Arrivals time: 0.16559069184586406 Scheduler time: 4.791368410456926 Scheduler overhead time: 0.07774123596027493 Adapter cache time: 0.037614261731505394 Engine time: 0.0781601625494659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_64_slots_64_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_64_slots_64_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.202783681917936,
    "estimated_duration": 3600.0258351527027,
    "input_throughput": 5036.949408234134,
    "output_throughput": 4428.35544243359,
    "total_throughput": 9465.304850667724,
    "itl": 54.9817297591708,
    "ttft": 13241.970396465285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 73090,
    "finished_requests": 72822,
    "scheduler_time": 52.89420883271282
}
#Debug simulation 
Total elapsed time: 5.202895322814584. Arrivals time: 0.1700138202868402 Scheduler time: 4.801462572999299 Scheduler overhead time: 0.07825249573215842 Adapter cache time: 0.03765619499608874 Engine time: 0.07812274619936943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_64_slots_64_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_64_slots_64_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 8640, 1080, 8640, 270, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 270, 270, 8640, 1080, 270, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 270, 1080, 1080, 8640, 8640, 1080, 8640, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 8640, 1080]
Prompts retrieved: 218430 . Total input tokens: 48876022 . Total output tokens: 43612074
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.1991482437588274,
    "estimated_duration": 3600.0258382266907,
    "input_throughput": 5036.949403933187,
    "output_throughput": 4428.355438652308,
    "total_throughput": 9465.304842585494,
    "itl": 54.98197046996944,
    "ttft": 13241.94744139302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 73090,
    "finished_requests": 72822,
    "scheduler_time": 52.894272838580235
}
#Debug simulation 
Total elapsed time: 5.199271838646382. Arrivals time: 0.1694195168092847 Scheduler time: 4.798532756045461 Scheduler overhead time: 0.07824280532076955 Adapter cache time: 0.03770578745752573 Engine time: 0.07802469050511718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.074622570071369,
    "estimated_duration": 3600.0248115652284,
    "input_throughput": 4989.68338837365,
    "output_throughput": 4357.476634495625,
    "total_throughput": 9347.160022869275,
    "itl": 51.240623276861974,
    "ttft": 13709.92915876427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 72131,
    "finished_requests": 71858,
    "scheduler_time": 51.03005373529429
}
#Debug simulation 
Total elapsed time: 5.074711031746119. Arrivals time: 0.16001522867009044 Scheduler time: 4.682345955632627 Scheduler overhead time: 0.08046375075355172 Adapter cache time: 0.033953520469367504 Engine time: 0.08008287195116282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.093767988029867,
    "estimated_duration": 3600.0405513958617,
    "input_throughput": 4989.66157284954,
    "output_throughput": 4357.457583059056,
    "total_throughput": 9347.119155908595,
    "itl": 51.24043967657926,
    "ttft": 13709.833848070739,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 72131,
    "finished_requests": 71858,
    "scheduler_time": 51.03028065429353
}
#Debug simulation 
Total elapsed time: 5.0938859903253615. Arrivals time: 0.16487094201147556 Scheduler time: 4.695989401079714 Scheduler overhead time: 0.08046847488731146 Adapter cache time: 0.0340955900028348 Engine time: 0.08036490343511105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.116699588019401,
    "estimated_duration": 3600.0405614369297,
    "input_throughput": 4989.661558932605,
    "output_throughput": 4357.457570905434,
    "total_throughput": 9347.11912983804,
    "itl": 51.24041462770803,
    "ttft": 13709.8399608305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 72131,
    "finished_requests": 71858,
    "scheduler_time": 51.03026038927839
}
#Debug simulation 
Total elapsed time: 5.116815977264196. Arrivals time: 0.16466244077309966 Scheduler time: 4.719820767175406 Scheduler overhead time: 0.07976194703951478 Adapter cache time: 0.03391886595636606 Engine time: 0.08044114010408521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.106915968004614,
    "estimated_duration": 3600.0247638968785,
    "input_throughput": 4989.683454442633,
    "output_throughput": 4357.476692193484,
    "total_throughput": 9347.160146636117,
    "itl": 51.240629751134705,
    "ttft": 13709.992639897675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942695,
    "arrivals": 72131,
    "finished_requests": 71858,
    "scheduler_time": 51.03004613737747
}
#Debug simulation 
Total elapsed time: 5.107009695842862. Arrivals time: 0.16499842889606953 Scheduler time: 4.709853340871632 Scheduler overhead time: 0.0796855348162353 Adapter cache time: 0.03417053399607539 Engine time: 0.08032593037933111 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.111438112799078,
    "estimated_duration": 3600.046777783903,
    "input_throughput": 4989.6876648524085,
    "output_throughput": 4357.503379346824,
    "total_throughput": 9347.191044199231,
    "itl": 51.24074546571528,
    "ttft": 13660.000730902479,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 72131,
    "finished_requests": 71859,
    "scheduler_time": 51.030345371222374
}
#Debug simulation 
Total elapsed time: 5.1115304278209805. Arrivals time: 0.16718296334147453 Scheduler time: 4.710222635418177 Scheduler overhead time: 0.08099234849214554 Adapter cache time: 0.03431026404723525 Engine time: 0.0804078858345747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.10261196596548,
    "estimated_duration": 3600.040561038256,
    "input_throughput": 4989.661559485167,
    "output_throughput": 4357.457571387986,
    "total_throughput": 9347.119130873152,
    "itl": 51.2399331033993,
    "ttft": 13709.846524927632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 72131,
    "finished_requests": 71858,
    "scheduler_time": 51.03010747958392
}
#Debug simulation 
Total elapsed time: 5.102710306178778. Arrivals time: 0.1638754433952272 Scheduler time: 4.706355825532228 Scheduler overhead time: 0.07977080531418324 Adapter cache time: 0.03428340842947364 Engine time: 0.08042871672660112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 8640, 1080, 8640, 135, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 135, 135, 8640, 1080, 135, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 135, 1080, 1080, 8640, 8640, 1080, 8640, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 8640, 1080]
Prompts retrieved: 215595 . Total input tokens: 48244017 . Total output tokens: 43061389
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.078351195435971,
    "estimated_duration": 3600.017829688415,
    "input_throughput": 4989.69306536871,
    "output_throughput": 4357.4850853885155,
    "total_throughput": 9347.178150757225,
    "itl": 51.24100278419355,
    "ttft": 13710.033294344588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 72131,
    "finished_requests": 71858,
    "scheduler_time": 51.030097312246966
}
#Debug simulation 
Total elapsed time: 5.078450917266309. Arrivals time: 0.16388637572526932 Scheduler time: 4.681282938923687 Scheduler overhead time: 0.07996552158147097 Adapter cache time: 0.03444157587364316 Engine time: 0.0807475782930851 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.0931552750989795,
    "estimated_duration": 3600.0408064641992,
    "input_throughput": 4961.101820826711,
    "output_throughput": 4362.49082838172,
    "total_throughput": 9323.59264920843,
    "itl": 49.98883346388257,
    "ttft": 14493.714028034437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 71695,
    "finished_requests": 71408,
    "scheduler_time": 50.80588921345645
}
#Debug simulation 
Total elapsed time: 5.0932568167336285. Arrivals time: 0.16416532173752785 Scheduler time: 4.6965266559273005 Scheduler overhead time: 0.08137856423854828 Adapter cache time: 0.03141304478049278 Engine time: 0.08113210182636976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.110903368797153,
    "estimated_duration": 3600.048846079713,
    "input_throughput": 4961.090741712825,
    "output_throughput": 4362.481086083645,
    "total_throughput": 9323.57182779647,
    "itl": 49.98829289758895,
    "ttft": 14493.701971399663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 71695,
    "finished_requests": 71408,
    "scheduler_time": 50.806006553864506
}
#Debug simulation 
Total elapsed time: 5.111001146957278. Arrivals time: 0.16437857411801815 Scheduler time: 4.7145702904090285 Scheduler overhead time: 0.08099770918488503 Adapter cache time: 0.03186230780556798 Engine time: 0.08053515246137977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.089082216843963,
    "estimated_duration": 3600.0488587408004,
    "input_throughput": 4961.09072426506,
    "output_throughput": 4362.481070741144,
    "total_throughput": 9323.571795006204,
    "itl": 49.988273839359785,
    "ttft": 14493.71190505142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 71695,
    "finished_requests": 71408,
    "scheduler_time": 50.80601464348058
}
#Debug simulation 
Total elapsed time: 5.089216276071966. Arrivals time: 0.16491535492241383 Scheduler time: 4.692307726945728 Scheduler overhead time: 0.08084299461916089 Adapter cache time: 0.03155162185430527 Engine time: 0.08088097302243114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.105479864869267,
    "estimated_duration": 3600.024651784589,
    "input_throughput": 4961.124083177162,
    "output_throughput": 4362.510404537011,
    "total_throughput": 9323.634487714173,
    "itl": 49.988364572872996,
    "ttft": 14443.654653476402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 71695,
    "finished_requests": 71408,
    "scheduler_time": 50.805529307490495
}
#Debug simulation 
Total elapsed time: 5.105578070972115. Arrivals time: 0.17275133775547147 Scheduler time: 4.693494810722768 Scheduler overhead time: 0.08509039413183928 Adapter cache time: 0.032342694234102964 Engine time: 0.08250084891915321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.098765273112804,
    "estimated_duration": 3600.0591135838636,
    "input_throughput": 4961.076592495221,
    "output_throughput": 4362.468644123321,
    "total_throughput": 9323.545236618542,
    "itl": 49.98853845066746,
    "ttft": 14493.671536682672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 71695,
    "finished_requests": 71408,
    "scheduler_time": 50.80616826423649
}
#Debug simulation 
Total elapsed time: 5.098865038715303. Arrivals time: 0.16452429071068764 Scheduler time: 4.702315009199083 Scheduler overhead time: 0.08106709411367774 Adapter cache time: 0.03177262283861637 Engine time: 0.08056736644357443 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.085920860990882,
    "estimated_duration": 3600.0011586491614,
    "input_throughput": 4961.1564588222855,
    "output_throughput": 4362.538873707776,
    "total_throughput": 9323.695332530062,
    "itl": 49.988303339112825,
    "ttft": 14393.664035495942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 71695,
    "finished_requests": 71408,
    "scheduler_time": 50.80513271142813
}
#Debug simulation 
Total elapsed time: 5.086019888985902. Arrivals time: 0.16421616030856967 Scheduler time: 4.690080917440355 Scheduler overhead time: 0.08053376665338874 Adapter cache time: 0.03166778618469834 Engine time: 0.08116208203136921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_64_slots_64_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 8640, 1080, 8640, 66, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 66, 66, 8640, 1080, 66, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 66, 1080, 1080, 8640, 8640, 1080, 8640, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 8640, 1080]
Prompts retrieved: 214146 . Total input tokens: 47928592 . Total output tokens: 42768674
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.0935972151346505,
    "estimated_duration": 3600.04090664182,
    "input_throughput": 4961.101682775119,
    "output_throughput": 4362.490706987557,
    "total_throughput": 9323.592389762676,
    "itl": 49.98916318313204,
    "ttft": 14493.7504432735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 71695,
    "finished_requests": 71408,
    "scheduler_time": 50.806066146149675
}
#Debug simulation 
Total elapsed time: 5.093694726005197. Arrivals time: 0.16529735689982772 Scheduler time: 4.695591507013887 Scheduler overhead time: 0.08115694811567664 Adapter cache time: 0.03171479143202305 Engine time: 0.08146667014807463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.107274265959859,
    "estimated_duration": 3599.9955409562695,
    "input_throughput": 4886.919108607226,
    "output_throughput": 4383.114873472471,
    "total_throughput": 9270.033982079698,
    "itl": 49.667033203869444,
    "ttft": 15191.126492590467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 71480,
    "finished_requests": 71180,
    "scheduler_time": 51.01164827573293
}
#Debug simulation 
Total elapsed time: 5.107375629711896. Arrivals time: 0.1637966730631888 Scheduler time: 4.7120851362124085 Scheduler overhead time: 0.08161965850740671 Adapter cache time: 0.030088180676102638 Engine time: 0.08129147486761212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.077201390173286,
    "estimated_duration": 3599.9746405236597,
    "input_throughput": 4886.947480674726,
    "output_throughput": 4383.140320595349,
    "total_throughput": 9270.087801270074,
    "itl": 49.66737196087905,
    "ttft": 15191.050855067395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 71480,
    "finished_requests": 71180,
    "scheduler_time": 51.01138410498707
}
#Debug simulation 
Total elapsed time: 5.077322486322373. Arrivals time: 0.16344479424878955 Scheduler time: 4.684066996444017 Scheduler overhead time: 0.0808253069408238 Adapter cache time: 0.03009458025917411 Engine time: 0.08062632661312819 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.106988147832453,
    "estimated_duration": 3599.986464803915,
    "input_throughput": 4886.931429326431,
    "output_throughput": 4383.125924019124,
    "total_throughput": 9270.057353345554,
    "itl": 49.66762536786061,
    "ttft": 15191.139634615096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 71480,
    "finished_requests": 71180,
    "scheduler_time": 51.01166319674234
}
#Debug simulation 
Total elapsed time: 5.107095933984965. Arrivals time: 0.16397440107539296 Scheduler time: 4.709817327093333 Scheduler overhead time: 0.08137897029519081 Adapter cache time: 0.030401759315282106 Engine time: 0.08292380534112453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 5.063208870124072,
    "estimated_duration": 3599.99553983174,
    "input_throughput": 4886.919110133751,
    "output_throughput": 4383.1148748416235,
    "total_throughput": 9270.033984975375,
    "itl": 49.6669863977332,
    "ttft": 15191.134265554878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 71480,
    "finished_requests": 71180,
    "scheduler_time": 51.011614878408
}
#Debug simulation 
Total elapsed time: 5.063306942116469. Arrivals time: 0.16323949955403805 Scheduler time: 4.6706050536595285 Scheduler overhead time: 0.08071976387873292 Adapter cache time: 0.029970829375088215 Engine time: 0.08021916775032878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 5.1048861891031265,
    "estimated_duration": 3599.9951630496294,
    "input_throughput": 4886.919621607687,
    "output_throughput": 4383.115333586483,
    "total_throughput": 9270.03495519417,
    "itl": 49.667441429567845,
    "ttft": 15191.12617872595,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 71480,
    "finished_requests": 71180,
    "scheduler_time": 51.011800351441394
}
#Debug simulation 
Total elapsed time: 5.104984311852604. Arrivals time: 0.16366294817999005 Scheduler time: 4.709781018085778 Scheduler overhead time: 0.08182855090126395 Adapter cache time: 0.030049616005271673 Engine time: 0.08093540789559484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 5.119574062060565,
    "estimated_duration": 3599.994794256026,
    "input_throughput": 4886.920122237494,
    "output_throughput": 4383.1157826051585,
    "total_throughput": 9270.035904842653,
    "itl": 49.66710162732944,
    "ttft": 15191.178690312518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 71480,
    "finished_requests": 71180,
    "scheduler_time": 51.01165965812092
}
#Debug simulation 
Total elapsed time: 5.1196950282901525. Arrivals time: 0.16501976316794753 Scheduler time: 4.72131413128227 Scheduler overhead time: 0.08219112595543265 Adapter cache time: 0.030360945034772158 Engine time: 0.08207203634083271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 8640, 1080, 8640, 33, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 33, 33, 8640, 1080, 33, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 33, 1080, 1080, 8640, 8640, 1080, 8640, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 8640, 1080]
Prompts retrieved: 213453 . Total input tokens: 47767348 . Total output tokens: 42626957
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 5.0838518110103905,
    "estimated_duration": 3599.9937460292053,
    "input_throughput": 4886.921545184617,
    "output_throughput": 4383.117058857243,
    "total_throughput": 9270.03860404186,
    "itl": 49.667429963424794,
    "ttft": 15191.141862217204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 71480,
    "finished_requests": 71180,
    "scheduler_time": 51.011768033951896
}
#Debug simulation 
Total elapsed time: 5.083971941843629. Arrivals time: 0.16348412539809942 Scheduler time: 4.68928690161556 Scheduler overhead time: 0.08116268273442984 Adapter cache time: 0.030242618173360825 Engine time: 0.08129433775320649 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_64_slots_64_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_64_slots_64_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.9685215461067855,
    "estimated_duration": 3600.0398684452566,
    "input_throughput": 4826.427938283877,
    "output_throughput": 4223.826000728976,
    "total_throughput": 9050.253939012853,
    "itl": 47.00033568301421,
    "ttft": 11443.400226365755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 69367,
    "finished_requests": 69148,
    "scheduler_time": 47.89375370458161
}
#Debug simulation 
Total elapsed time: 4.968617702834308. Arrivals time: 0.16493556136265397 Scheduler time: 4.557850590441376 Scheduler overhead time: 0.08414118504151702 Adapter cache time: 0.03871132526546717 Engine time: 0.08274350361898541 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_64_slots_64_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_64_slots_64_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.974699106998742,
    "estimated_duration": 3600.0400708247976,
    "input_throughput": 4826.427666961822,
    "output_throughput": 4223.82576328274,
    "total_throughput": 9050.253430244562,
    "itl": 46.99998460990044,
    "ttft": 11443.355763401523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 69367,
    "finished_requests": 69148,
    "scheduler_time": 47.89375403238097
}
#Debug simulation 
Total elapsed time: 4.974838342983276. Arrivals time: 0.16002203850075603 Scheduler time: 4.567083860747516 Scheduler overhead time: 0.08487505791708827 Adapter cache time: 0.03881663270294666 Engine time: 0.08403527969494462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_64_slots_64_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_64_slots_64_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.961421612184495,
    "estimated_duration": 3600.012476100145,
    "input_throughput": 4826.456051291933,
    "output_throughput": 4223.857584091606,
    "total_throughput": 9050.31363538354,
    "itl": 47.00050801122598,
    "ttft": 11443.569971860792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2092868485860525,
    "arrivals": 69367,
    "finished_requests": 69147,
    "scheduler_time": 47.8935385367448
}
#Debug simulation 
Total elapsed time: 4.961520488373935. Arrivals time: 0.16052120085805655 Scheduler time: 4.555132327135652 Scheduler overhead time: 0.08323574252426624 Adapter cache time: 0.03870307933539152 Engine time: 0.08416479686275125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_64_slots_64_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_64_slots_64_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.954956698231399,
    "estimated_duration": 3600.0399985802696,
    "input_throughput": 4826.427763817132,
    "output_throughput": 4223.825848045214,
    "total_throughput": 9050.253611862347,
    "itl": 47.00022193825719,
    "ttft": 11443.424274469617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 69367,
    "finished_requests": 69148,
    "scheduler_time": 47.89374573789029
}
#Debug simulation 
Total elapsed time: 4.955058993306011. Arrivals time: 0.167504389770329 Scheduler time: 4.541267899796367 Scheduler overhead time: 0.08461345825344324 Adapter cache time: 0.03852808056399226 Engine time: 0.08331955689936876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_64_slots_64_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_64_slots_64_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.928897699806839,
    "estimated_duration": 3600.0148638952132,
    "input_throughput": 4826.452850030718,
    "output_throughput": 4223.854782518088,
    "total_throughput": 9050.307632548805,
    "itl": 47.000521938417855,
    "ttft": 11443.545561856816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 69367,
    "finished_requests": 69147,
    "scheduler_time": 47.893578698000915
}
#Debug simulation 
Total elapsed time: 4.928991173859686. Arrivals time: 0.16186496475711465 Scheduler time: 4.5220089592039585 Scheduler overhead time: 0.08352663507685065 Adapter cache time: 0.038503671530634165 Engine time: 0.08339133346453309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_64_slots_64_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_64_slots_64_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.982218086253852,
    "estimated_duration": 3600.017058579679,
    "input_throughput": 4826.449907672134,
    "output_throughput": 4223.852207522379,
    "total_throughput": 9050.302115194514,
    "itl": 47.000446412477615,
    "ttft": 11443.561555089516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 69367,
    "finished_requests": 69147,
    "scheduler_time": 47.89343816857997
}
#Debug simulation 
Total elapsed time: 4.982315497007221. Arrivals time: 0.15993316238746047 Scheduler time: 4.577315689064562 Scheduler overhead time: 0.08347739325836301 Adapter cache time: 0.03888414008542895 Engine time: 0.08305620914325118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_64_slots_64_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_64_slots_64_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 8640, 8640, 270, 8640, 270, 540, 270, 8640, 540, 8640, 270, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 8640, 270, 270, 270, 8640, 540, 270, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 270, 540, 540, 8640, 8640, 540, 8640, 270, 8640, 540, 540, 540, 8640, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 8640, 540]
Prompts retrieved: 207090 . Total input tokens: 46352097 . Total output tokens: 41334926
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.926489655394107,
    "estimated_duration": 3600.0255475500157,
    "input_throughput": 4826.43852675565,
    "output_throughput": 4223.842247549701,
    "total_throughput": 9050.28077430535,
    "itl": 47.000781762432034,
    "ttft": 11443.562437927078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2146942614018914,
    "arrivals": 69367,
    "finished_requests": 69147,
    "scheduler_time": 47.893676935178966
}
#Debug simulation 
Total elapsed time: 4.926585339009762. Arrivals time: 0.15717218909412622 Scheduler time: 4.524909748695791 Scheduler overhead time: 0.08346488187089562 Adapter cache time: 0.0385293778963387 Engine time: 0.08289456274360418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.865380144678056,
    "estimated_duration": 3600.01941988423,
    "input_throughput": 4728.306993562662,
    "output_throughput": 4159.996448152939,
    "total_throughput": 8888.3034417156,
    "itl": 44.336351864686364,
    "ttft": 10388.412590915887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 68405,
    "finished_requests": 68208,
    "scheduler_time": 46.137794709659225
}
#Debug simulation 
Total elapsed time: 4.86547478986904. Arrivals time: 0.15754152508452535 Scheduler time: 4.460046905092895 Scheduler overhead time: 0.08651223639026284 Adapter cache time: 0.03436600137501955 Engine time: 0.08602473419159651 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.854016928933561,
    "estimated_duration": 3600.0048453786058,
    "input_throughput": 4728.273913811871,
    "output_throughput": 4159.97023426923,
    "total_throughput": 8888.244148081101,
    "itl": 44.336605534426994,
    "ttft": 10441.091925597288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 68405,
    "finished_requests": 68207,
    "scheduler_time": 46.13774700594873
}
#Debug simulation 
Total elapsed time: 4.854109849780798. Arrivals time: 0.1585633927024901 Scheduler time: 4.448851251974702 Scheduler overhead time: 0.08587973192334175 Adapter cache time: 0.034334493800997734 Engine time: 0.08556677307933569 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.881521922070533,
    "estimated_duration": 3600.006509580304,
    "input_throughput": 4728.2717280376355,
    "output_throughput": 4159.968311208949,
    "total_throughput": 8888.240039246584,
    "itl": 44.33654352964012,
    "ttft": 10441.08817216067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 68405,
    "finished_requests": 68207,
    "scheduler_time": 46.13775505458986
}
#Debug simulation 
Total elapsed time: 4.881621869280934. Arrivals time: 0.16010260628536344 Scheduler time: 4.472681459505111 Scheduler overhead time: 0.08684639958664775 Adapter cache time: 0.03449391247704625 Engine time: 0.08632013248279691 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.875047835055739,
    "estimated_duration": 3600.02054343167,
    "input_throughput": 4728.305517882966,
    "output_throughput": 4159.99514983997,
    "total_throughput": 8888.300667722937,
    "itl": 44.33624006220558,
    "ttft": 10388.483345311655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 68405,
    "finished_requests": 68208,
    "scheduler_time": 46.13780684408338
}
#Debug simulation 
Total elapsed time: 4.875143331009895. Arrivals time: 0.15950181568041444 Scheduler time: 4.466543089598417 Scheduler overhead time: 0.08646687865257263 Adapter cache time: 0.03445988800376654 Engine time: 0.08699615113437176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.893926900811493,
    "estimated_duration": 3600.0069901532383,
    "input_throughput": 4728.271096850133,
    "output_throughput": 4159.967755885533,
    "total_throughput": 8888.238852735667,
    "itl": 44.33663392930314,
    "ttft": 10440.977535779604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 68405,
    "finished_requests": 68207,
    "scheduler_time": 46.13769438246934
}
#Debug simulation 
Total elapsed time: 4.894018955994397. Arrivals time: 0.15790725592523813 Scheduler time: 4.487441934645176 Scheduler overhead time: 0.08783835219219327 Adapter cache time: 0.03427822794765234 Engine time: 0.08539273450151086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.843546328134835,
    "estimated_duration": 3600.006969350479,
    "input_throughput": 4728.271124172605,
    "output_throughput": 4159.967779924044,
    "total_throughput": 8888.23890409665,
    "itl": 44.33620987412159,
    "ttft": 10441.089582336735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 68405,
    "finished_requests": 68207,
    "scheduler_time": 46.13754405448543
}
#Debug simulation 
Total elapsed time: 4.843642760999501. Arrivals time: 0.15849630907177925 Scheduler time: 4.4379345597699285 Scheduler overhead time: 0.08654715539887547 Adapter cache time: 0.03445726539939642 Engine time: 0.08521080994978547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 8640, 8640, 135, 8640, 135, 540, 135, 8640, 540, 8640, 135, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 8640, 135, 135, 135, 8640, 540, 135, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 135, 540, 540, 8640, 8640, 540, 8640, 135, 8640, 540, 540, 540, 8640, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 8640, 540]
Prompts retrieved: 204255 . Total input tokens: 45706051 . Total output tokens: 40770430
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.854133300948888,
    "estimated_duration": 3600.006965347101,
    "input_throughput": 4728.2711294306655,
    "output_throughput": 4159.9677845501255,
    "total_throughput": 8888.238913980791,
    "itl": 44.336577546529334,
    "ttft": 10441.052756228666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 68405,
    "finished_requests": 68207,
    "scheduler_time": 46.137686456752874
}
#Debug simulation 
Total elapsed time: 4.854231303092092. Arrivals time: 0.16025755368173122 Scheduler time: 4.446820674464107 Scheduler overhead time: 0.08587435446679592 Adapter cache time: 0.03438522294163704 Engine time: 0.08602211391553283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.81965200882405,
    "estimated_duration": 3600.0155515082633,
    "input_throughput": 4670.352324715897,
    "output_throughput": 4133.379922141107,
    "total_throughput": 8803.732246857004,
    "itl": 43.047335174689536,
    "ttft": 10083.867542156178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 67954,
    "finished_requests": 67764,
    "scheduler_time": 45.28803267516624
}
#Debug simulation 
Total elapsed time: 4.819784956052899. Arrivals time: 0.15736238937824965 Scheduler time: 4.412632205057889 Scheduler overhead time: 0.08783840434625745 Adapter cache time: 0.03247995022684336 Engine time: 0.08755216095596552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.851545809302479,
    "estimated_duration": 3600.00486244061,
    "input_throughput": 4670.366191839379,
    "output_throughput": 4133.392194896093,
    "total_throughput": 8803.758386735472,
    "itl": 43.04780252497694,
    "ttft": 10083.862373227059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 67954,
    "finished_requests": 67764,
    "scheduler_time": 45.28806424061928
}
#Debug simulation 
Total elapsed time: 4.851640770211816. Arrivals time: 0.15922277979552746 Scheduler time: 4.439713970758021 Scheduler overhead time: 0.08886487921699882 Adapter cache time: 0.03275743452832103 Engine time: 0.08908895822241902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.8172557502985,
    "estimated_duration": 3600.007285218247,
    "input_throughput": 4670.3630487183045,
    "output_throughput": 4133.389413154451,
    "total_throughput": 8803.752461872757,
    "itl": 43.04777510317087,
    "ttft": 10083.86699686322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605248,
    "arrivals": 67954,
    "finished_requests": 67764,
    "scheduler_time": 45.28810464772493
}
#Debug simulation 
Total elapsed time: 4.817355546168983. Arrivals time: 0.15810101525858045 Scheduler time: 4.409217528067529 Scheduler overhead time: 0.08840919844806194 Adapter cache time: 0.03249449981376529 Engine time: 0.08705547731369734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.848227126989514,
    "estimated_duration": 3600.024593668646,
    "input_throughput": 4670.340594219711,
    "output_throughput": 4133.369540355315,
    "total_throughput": 8803.710134575027,
    "itl": 43.047339830213375,
    "ttft": 10083.911961211285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 67954,
    "finished_requests": 67764,
    "scheduler_time": 45.288146093690905
}
#Debug simulation 
Total elapsed time: 4.848326527979225. Arrivals time: 0.15971476072445512 Scheduler time: 4.437703093979508 Scheduler overhead time: 0.08821344142779708 Adapter cache time: 0.0327682183124125 Engine time: 0.0880419653840363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.8282975647598505,
    "estimated_duration": 3600.011695337067,
    "input_throughput": 4670.357327388009,
    "output_throughput": 4133.384349632445,
    "total_throughput": 8803.741677020455,
    "itl": 43.047792765016354,
    "ttft": 10083.82003014072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2119276781007645,
    "arrivals": 67954,
    "finished_requests": 67764,
    "scheduler_time": 45.28814088709789
}
#Debug simulation 
Total elapsed time: 4.828406906686723. Arrivals time: 0.15897567197680473 Scheduler time: 4.41947075445205 Scheduler overhead time: 0.08825170435011387 Adapter cache time: 0.03262708941474557 Engine time: 0.08714876277372241 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.825540784280747,
    "estimated_duration": 3600.011490702169,
    "input_throughput": 4670.35759286441,
    "output_throughput": 4133.384584585775,
    "total_throughput": 8803.742177450185,
    "itl": 43.047471971927784,
    "ttft": 10083.815454747102,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 67954,
    "finished_requests": 67764,
    "scheduler_time": 45.287996189944224
}
#Debug simulation 
Total elapsed time: 4.825657842215151. Arrivals time: 0.15999141335487366 Scheduler time: 4.414485946763307 Scheduler overhead time: 0.08865643385797739 Adapter cache time: 0.032808837946504354 Engine time: 0.08767838915809989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_64_slots_64_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 8640, 8640, 66, 8640, 66, 540, 66, 8640, 540, 8640, 66, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 8640, 66, 66, 66, 8640, 540, 66, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 66, 540, 540, 8640, 8640, 540, 8640, 66, 8640, 540, 540, 540, 8640, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 8640, 540]
Prompts retrieved: 202806 . Total input tokens: 45392141 . Total output tokens: 40468373
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.817537073045969,
    "estimated_duration": 3600.0115598219154,
    "input_throughput": 4670.357503194162,
    "output_throughput": 4133.384505225336,
    "total_throughput": 8803.742008419498,
    "itl": 43.047668290110586,
    "ttft": 10083.784775514205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189135,
    "arrivals": 67954,
    "finished_requests": 67764,
    "scheduler_time": 45.288124748840616
}
#Debug simulation 
Total elapsed time: 4.817636730149388. Arrivals time: 0.157392259221524 Scheduler time: 4.40920878527686 Scheduler overhead time: 0.08822356723248959 Adapter cache time: 0.032605704851448536 Engine time: 0.08804992167279124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.816061801742762,
    "estimated_duration": 3600.001188226846,
    "input_throughput": 4702.409559019645,
    "output_throughput": 4111.986976118524,
    "total_throughput": 8814.396535138168,
    "itl": 42.53952705534125,
    "ttft": 12566.572001573915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 67703,
    "finished_requests": 67465,
    "scheduler_time": 44.85769733677396
}
#Debug simulation 
Total elapsed time: 4.816154601983726. Arrivals time: 0.1578273307532072 Scheduler time: 4.407035198062658 Scheduler overhead time: 0.0892666419968009 Adapter cache time: 0.03077973611652851 Engine time: 0.08863228419795632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.804536235053092,
    "estimated_duration": 3600.0015653064033,
    "input_throughput": 4702.40906646916,
    "output_throughput": 4111.986545411425,
    "total_throughput": 8814.395611880585,
    "itl": 42.53995083725565,
    "ttft": 12566.59593756625,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 67703,
    "finished_requests": 67465,
    "scheduler_time": 44.8578864448661
}
#Debug simulation 
Total elapsed time: 4.804633202962577. Arrivals time: 0.15834040939807892 Scheduler time: 4.395767806563526 Scheduler overhead time: 0.08899579988792539 Adapter cache time: 0.03075370704755187 Engine time: 0.08836701652035117 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.805393035989255,
    "estimated_duration": 3600.0004548133693,
    "input_throughput": 4702.410517022452,
    "output_throughput": 4111.987813836936,
    "total_throughput": 8814.398330859389,
    "itl": 42.539930996237906,
    "ttft": 12566.593468245033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 67703,
    "finished_requests": 67465,
    "scheduler_time": 44.85784607873551
}
#Debug simulation 
Total elapsed time: 4.805489215999842. Arrivals time: 0.1573462630622089 Scheduler time: 4.399163586087525 Scheduler overhead time: 0.0884856847114861 Adapter cache time: 0.030447025783360004 Engine time: 0.08771883882582188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.811270514968783,
    "estimated_duration": 3600.0021583630837,
    "input_throughput": 4702.408291804316,
    "output_throughput": 4111.9858680115285,
    "total_throughput": 8814.394159815845,
    "itl": 42.539556444694014,
    "ttft": 12566.583382347026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 67703,
    "finished_requests": 67465,
    "scheduler_time": 44.85770938924823
}
#Debug simulation 
Total elapsed time: 4.811405363027006. Arrivals time: 0.15757170086726546 Scheduler time: 4.401576888281852 Scheduler overhead time: 0.08990348549559712 Adapter cache time: 0.030875960364937782 Engine time: 0.08889394579455256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.832266794051975,
    "estimated_duration": 3600.0000380055267,
    "input_throughput": 4702.411061467331,
    "output_throughput": 4111.9882899227,
    "total_throughput": 8814.399351390031,
    "itl": 42.539865224984666,
    "ttft": 12566.606975645389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 67703,
    "finished_requests": 67465,
    "scheduler_time": 44.85783807106929
}
#Debug simulation 
Total elapsed time: 4.832362757064402. Arrivals time: 0.15817581536248326 Scheduler time: 4.4219490848481655 Scheduler overhead time: 0.09018793981522322 Adapter cache time: 0.030843333341181278 Engine time: 0.08870438858866692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.796200160868466,
    "estimated_duration": 3600.000408837132,
    "input_throughput": 4702.410577077763,
    "output_throughput": 4111.987866351854,
    "total_throughput": 8814.398443429616,
    "itl": 42.53948635437643,
    "ttft": 12566.684072162325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 67703,
    "finished_requests": 67465,
    "scheduler_time": 44.85767310890053
}
#Debug simulation 
Total elapsed time: 4.796300301793963. Arrivals time: 0.1573320897296071 Scheduler time: 4.389501801226288 Scheduler overhead time: 0.08880507061257958 Adapter cache time: 0.03046213649213314 Engine time: 0.08812922891229391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 8640, 8640, 33, 8640, 33, 540, 33, 8640, 540, 8640, 33, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 8640, 33, 33, 33, 8640, 540, 33, 8640, 8640, 8640, 8640, 540, 540, 540, 8640, 33, 540, 540, 8640, 8640, 540, 8640, 33, 8640, 540, 540, 540, 8640, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 8640, 540]
Prompts retrieved: 202113 . Total input tokens: 45238201 . Total output tokens: 40334129
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.831910042092204,
    "estimated_duration": 3600.002156526055,
    "input_throughput": 4702.408294203887,
    "output_throughput": 4111.985870109815,
    "total_throughput": 8814.394164313702,
    "itl": 42.53995079028339,
    "ttft": 12566.589363086054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 67703,
    "finished_requests": 67465,
    "scheduler_time": 44.85787443336684
}
#Debug simulation 
Total elapsed time: 4.832008301280439. Arrivals time: 0.1635212223045528 Scheduler time: 4.4176675216294825 Scheduler overhead time: 0.08902921108528972 Adapter cache time: 0.030704939272254705 Engine time: 0.08870593830943108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.728536309674382,
    "estimated_duration": 3600.0125639670955,
    "input_throughput": 4608.385027889485,
    "output_throughput": 4042.6342245768396,
    "total_throughput": 8651.019252466325,
    "itl": 40.70459278410476,
    "ttft": 9915.12267467868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 66533,
    "finished_requests": 66350,
    "scheduler_time": 43.12143118582486
}
#Debug simulation 
Total elapsed time: 4.728636035695672. Arrivals time: 0.15065313503146172 Scheduler time: 4.320856315549463 Scheduler overhead time: 0.09248177101835608 Adapter cache time: 0.03017064742743969 Engine time: 0.09067172463983297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.7322702230885625,
    "estimated_duration": 3600.0413686818474,
    "input_throughput": 4608.34815519759,
    "output_throughput": 4042.6018785802917,
    "total_throughput": 8650.950033777883,
    "itl": 40.704738832832916,
    "ttft": 9969.041039676487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 66533,
    "finished_requests": 66350,
    "scheduler_time": 43.12182774091265
}
#Debug simulation 
Total elapsed time: 4.732365996111184. Arrivals time: 0.15563413174822927 Scheduler time: 4.320766623597592 Scheduler overhead time: 0.09170008497312665 Adapter cache time: 0.03010812494903803 Engine time: 0.09058326715603471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.721703303977847,
    "estimated_duration": 3600.043370457566,
    "input_throughput": 4608.34559276195,
    "output_throughput": 4042.599630723405,
    "total_throughput": 8650.945223485354,
    "itl": 40.704733681676814,
    "ttft": 9969.083725043036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 66533,
    "finished_requests": 66350,
    "scheduler_time": 43.1218562184686
}
#Debug simulation 
Total elapsed time: 4.721802573185414. Arrivals time: 0.1551500093191862 Scheduler time: 4.311013124417514 Scheduler overhead time: 0.09175113402307034 Adapter cache time: 0.030226476956158876 Engine time: 0.09004339203238487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.765789004974067,
    "estimated_duration": 3600.0300291711187,
    "input_throughput": 4608.362670746884,
    "output_throughput": 4042.6146121205684,
    "total_throughput": 8650.977282867452,
    "itl": 40.70462218564758,
    "ttft": 9915.182789427106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 66533,
    "finished_requests": 66350,
    "scheduler_time": 43.12164985130856
}
#Debug simulation 
Total elapsed time: 4.76587922219187. Arrivals time: 0.1551674990914762 Scheduler time: 4.352573734242469 Scheduler overhead time: 0.09341027168557048 Adapter cache time: 0.03029887517914176 Engine time: 0.09063904453068972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.744829618372023,
    "estimated_duration": 3600.0035485469093,
    "input_throughput": 4608.396568580167,
    "output_throughput": 4042.644348468581,
    "total_throughput": 8651.040917048747,
    "itl": 40.70487351234265,
    "ttft": 9915.126842054256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 66533,
    "finished_requests": 66350,
    "scheduler_time": 43.12145478458698
}
#Debug simulation 
Total elapsed time: 4.744923638179898. Arrivals time: 0.15582421980798244 Scheduler time: 4.33341619418934 Scheduler overhead time: 0.09173266822472215 Adapter cache time: 0.030216993764042854 Engine time: 0.08993147825822234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.741962887812406,
    "estimated_duration": 3600.0446529562155,
    "input_throughput": 4608.343951060924,
    "output_throughput": 4042.5981905666667,
    "total_throughput": 8650.94214162759,
    "itl": 40.70449658082654,
    "ttft": 9969.092899387504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 66533,
    "finished_requests": 66350,
    "scheduler_time": 43.12172753664766
}
#Debug simulation 
Total elapsed time: 4.742059508804232. Arrivals time: 0.15403264435008168 Scheduler time: 4.330959306564182 Scheduler overhead time: 0.09203281672671437 Adapter cache time: 0.030292231123894453 Engine time: 0.09113330487161875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 8640, 8640, 135, 8640, 135, 270, 135, 8640, 270, 8640, 135, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 8640, 135, 135, 135, 8640, 270, 135, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 135, 270, 270, 8640, 8640, 270, 8640, 135, 8640, 270, 270, 270, 8640, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 8640, 270]
Prompts retrieved: 198585 . Total input tokens: 44460915 . Total output tokens: 39638584
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.765512781217694,
    "estimated_duration": 3600.0071292924194,
    "input_throughput": 4608.391984840544,
    "output_throughput": 4042.6403274541553,
    "total_throughput": 8651.0323122947,
    "itl": 40.70491791454123,
    "ttft": 9915.063455572905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 66533,
    "finished_requests": 66350,
    "scheduler_time": 43.12149994756166
}
#Debug simulation 
Total elapsed time: 4.765603585168719. Arrivals time: 0.15427069133147597 Scheduler time: 4.35427910881117 Scheduler overhead time: 0.09218981163576245 Adapter cache time: 0.030348099768161774 Engine time: 0.09082390740513802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.722334155347198,
    "estimated_duration": 3600.007983086531,
    "input_throughput": 4557.673226583403,
    "output_throughput": 4018.9510878793612,
    "total_throughput": 8576.624314462764,
    "itl": 39.54815987256359,
    "ttft": 9217.215888847877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 66085,
    "finished_requests": 65917,
    "scheduler_time": 42.29397822645875
}
#Debug simulation 
Total elapsed time: 4.722427695989609. Arrivals time: 0.15397324645891786 Scheduler time: 4.3095043250359595 Scheduler overhead time: 0.09402563329786062 Adapter cache time: 0.02750192955136299 Engine time: 0.09272454353049397 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.729610709007829,
    "estimated_duration": 3600.0246453566365,
    "input_throughput": 4557.879074845749,
    "output_throughput": 4019.0338748546724,
    "total_throughput": 8576.912949700421,
    "itl": 39.548315351647034,
    "ttft": 9162.783682300362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 66085,
    "finished_requests": 65918,
    "scheduler_time": 42.294180384910334
}
#Debug simulation 
Total elapsed time: 4.729706182610244. Arrivals time: 0.1530177011154592 Scheduler time: 4.319019029848278 Scheduler overhead time: 0.09351481217890978 Adapter cache time: 0.02756979549303651 Engine time: 0.09206133056432009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.739150868263096,
    "estimated_duration": 3600.0326170284793,
    "input_throughput": 4557.868982182667,
    "output_throughput": 4019.024975374422,
    "total_throughput": 8576.89395755709,
    "itl": 39.54848244514907,
    "ttft": 9162.72479933031,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 66085,
    "finished_requests": 65918,
    "scheduler_time": 42.294338255348535
}
#Debug simulation 
Total elapsed time: 4.73929307423532. Arrivals time: 0.1545834536664188 Scheduler time: 4.32631260342896 Scheduler overhead time: 0.0941510982811451 Adapter cache time: 0.027710585854947567 Engine time: 0.09180462267249823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.704722612630576,
    "estimated_duration": 3600.0122114956566,
    "input_throughput": 4557.6678733496,
    "output_throughput": 4018.9463674038584,
    "total_throughput": 8576.614240753459,
    "itl": 39.548202323560794,
    "ttft": 9217.278642869243,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.200352315879427,
    "arrivals": 66085,
    "finished_requests": 65917,
    "scheduler_time": 42.29401871551393
}
#Debug simulation 
Total elapsed time: 4.704832633957267. Arrivals time: 0.15358321415260434 Scheduler time: 4.294422633480281 Scheduler overhead time: 0.09282150072976947 Adapter cache time: 0.027717669494450092 Engine time: 0.09158699354156852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.724619197193533,
    "estimated_duration": 3600.0390520846945,
    "input_throughput": 4557.860835009068,
    "output_throughput": 4019.0177913824505,
    "total_throughput": 8576.878626391519,
    "itl": 39.548477813756286,
    "ttft": 9162.693611400233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 66085,
    "finished_requests": 65918,
    "scheduler_time": 42.294439416524504
}
#Debug simulation 
Total elapsed time: 4.724718400277197. Arrivals time: 0.1549481302499771 Scheduler time: 4.31006238097325 Scheduler overhead time: 0.09413088858127594 Adapter cache time: 0.027674964163452387 Engine time: 0.09323331993073225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.739259175956249,
    "estimated_duration": 3600.0050339354166,
    "input_throughput": 4557.676960263481,
    "output_throughput": 4018.9543802342246,
    "total_throughput": 8576.631340497706,
    "itl": 39.54818511955595,
    "ttft": 9162.906096738125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 66085,
    "finished_requests": 65917,
    "scheduler_time": 42.29392977071167
}
#Debug simulation 
Total elapsed time: 4.739355499390513. Arrivals time: 0.15282411826774478 Scheduler time: 4.328081245534122 Scheduler overhead time: 0.09401746978983283 Adapter cache time: 0.027597958222031593 Engine time: 0.09212897531688213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_64_slots_64_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 8640, 8640, 66, 8640, 66, 270, 66, 8640, 270, 8640, 66, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 8640, 66, 66, 66, 8640, 270, 66, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 66, 270, 270, 8640, 8640, 270, 8640, 66, 8640, 270, 270, 270, 8640, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 8640, 270]
Prompts retrieved: 197136 . Total input tokens: 44133992 . Total output tokens: 39360049
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.699280810076743,
    "estimated_duration": 3600.0062859180516,
    "input_throughput": 4557.675375229468,
    "output_throughput": 4018.952982553027,
    "total_throughput": 8576.628357782496,
    "itl": 39.54844924845337,
    "ttft": 9162.906510407935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 66085,
    "finished_requests": 65917,
    "scheduler_time": 42.29411095308771
}
#Debug simulation 
Total elapsed time: 4.699378397781402. Arrivals time: 0.15268941782414913 Scheduler time: 4.28860146086663 Scheduler overhead time: 0.09325966844335198 Adapter cache time: 0.02752883266657591 Engine time: 0.09275988675653934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.688989047892392,
    "estimated_duration": 3600.0177094727287,
    "input_throughput": 4569.352244216956,
    "output_throughput": 4022.3235463252363,
    "total_throughput": 8591.675790542193,
    "itl": 39.150412551180295,
    "ttft": 7828.476892269444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 65845,
    "finished_requests": 65703,
    "scheduler_time": 42.152035075705236
}
#Debug simulation 
Total elapsed time: 4.689107378013432. Arrivals time: 0.15292422100901604 Scheduler time: 4.277915241662413 Scheduler overhead time: 0.09405531780794263 Adapter cache time: 0.025874505750834942 Engine time: 0.09333776729181409 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.736712451092899,
    "estimated_duration": 3600.0384725829445,
    "input_throughput": 4569.390334375376,
    "output_throughput": 4022.4636792867936,
    "total_throughput": 8591.85401366217,
    "itl": 39.15062898068904,
    "ttft": 7773.73555626814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 65845,
    "finished_requests": 65704,
    "scheduler_time": 42.15234244014072
}
#Debug simulation 
Total elapsed time: 4.736829719040543. Arrivals time: 0.15402328176423907 Scheduler time: 4.322928105015308 Scheduler overhead time: 0.09449497144669294 Adapter cache time: 0.025986019987612963 Engine time: 0.09431747021153569 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.716131072025746,
    "estimated_duration": 3600.000702474312,
    "input_throughput": 4569.315219492621,
    "output_throughput": 4022.2897706791,
    "total_throughput": 8591.60499017172,
    "itl": 39.15057703052848,
    "ttft": 7828.502747715492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 65845,
    "finished_requests": 65702,
    "scheduler_time": 42.151989707855165
}
#Debug simulation 
Total elapsed time: 4.716219385154545. Arrivals time: 0.1541358740068972 Scheduler time: 4.304204223677516 Scheduler overhead time: 0.09404382621869445 Adapter cache time: 0.02587998192757368 Engine time: 0.09297790704295039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.716511524282396,
    "estimated_duration": 3600.0230546346115,
    "input_throughput": 4569.345459836114,
    "output_throughput": 4022.3175741494547,
    "total_throughput": 8591.66303398557,
    "itl": 39.150431661991476,
    "ttft": 7828.461091126333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 65845,
    "finished_requests": 65703,
    "scheduler_time": 42.15211605381564
}
#Debug simulation 
Total elapsed time: 4.716612834017724. Arrivals time: 0.15440035425126553 Scheduler time: 4.304183021653444 Scheduler overhead time: 0.0944753005169332 Adapter cache time: 0.025976378470659256 Engine time: 0.09257619408890605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.708359939046204,
    "estimated_duration": 3600.008773136931,
    "input_throughput": 4569.363586763213,
    "output_throughput": 4022.3335309769864,
    "total_throughput": 8591.6971177402,
    "itl": 39.150654708552196,
    "ttft": 7828.420850576321,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 65845,
    "finished_requests": 65703,
    "scheduler_time": 42.15209474993978
}
#Debug simulation 
Total elapsed time: 4.708463291171938. Arrivals time: 0.1526269754394889 Scheduler time: 4.296842989046127 Scheduler overhead time: 0.0948089836165309 Adapter cache time: 0.02595140878111124 Engine time: 0.09317936282604933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.728093020152301,
    "estimated_duration": 3600.008820088808,
    "input_throughput": 4569.363527168859,
    "output_throughput": 4022.3334785170846,
    "total_throughput": 8591.697005685945,
    "itl": 39.150462133104334,
    "ttft": 7828.4100768826065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 65845,
    "finished_requests": 65703,
    "scheduler_time": 42.15193779543731
}
#Debug simulation 
Total elapsed time: 4.728188695851713. Arrivals time: 0.1542768618091941 Scheduler time: 4.316939732991159 Scheduler overhead time: 0.09397160448133945 Adapter cache time: 0.025750269182026386 Engine time: 0.0925666238181293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_64_slots_64_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 270, 8640, 8640, 33, 8640, 33, 270, 33, 8640, 270, 8640, 33, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 8640, 33, 33, 33, 8640, 270, 33, 8640, 8640, 8640, 8640, 270, 270, 270, 8640, 33, 270, 270, 8640, 8640, 270, 8640, 33, 8640, 270, 270, 270, 8640, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 8640, 270]
Prompts retrieved: 196443 . Total input tokens: 43962358 . Total output tokens: 39214084
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.707736494019628,
    "estimated_duration": 3600.0112577176087,
    "input_throughput": 4569.360433175164,
    "output_throughput": 4022.330754926731,
    "total_throughput": 8591.691188101895,
    "itl": 39.15056475296366,
    "ttft": 7828.414344811425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 65845,
    "finished_requests": 65703,
    "scheduler_time": 42.15209891767285
}
#Debug simulation 
Total elapsed time: 4.707828220911324. Arrivals time: 0.15322047378867865 Scheduler time: 4.296821952331811 Scheduler overhead time: 0.09448056016117334 Adapter cache time: 0.026009551715105772 Engine time: 0.09258047211915255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.682331818155944,
    "estimated_duration": 3599.951398638521,
    "input_throughput": 4483.177746817291,
    "output_throughput": 3978.6470465731413,
    "total_throughput": 8461.824793390433,
    "itl": 38.18149610436644,
    "ttft": 8293.566902530274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 65172,
    "finished_requests": 65023,
    "scheduler_time": 41.08632895373298
}
#Debug simulation 
Total elapsed time: 4.68243240006268. Arrivals time: 0.1561629083007574 Scheduler time: 4.265568463131785 Scheduler overhead time: 0.09625349286943674 Adapter cache time: 0.024088840000331402 Engine time: 0.09468648070469499 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.692594259046018,
    "estimated_duration": 3599.937139054865,
    "input_throughput": 4483.1955049740745,
    "output_throughput": 3978.662806251215,
    "total_throughput": 8461.858311225289,
    "itl": 38.181723747300374,
    "ttft": 8293.558376634188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 65172,
    "finished_requests": 65023,
    "scheduler_time": 41.08628858760239
}
#Debug simulation 
Total elapsed time: 4.692719127982855. Arrivals time: 0.1500567365437746 Scheduler time: 4.277350071351975 Scheduler overhead time: 0.0957505889236927 Adapter cache time: 0.023937511257827282 Engine time: 0.0998272723518312 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.669286843854934,
    "estimated_duration": 3599.937756671056,
    "input_throughput": 4483.1947358235175,
    "output_throughput": 3978.662123659811,
    "total_throughput": 8461.856859483329,
    "itl": 38.1816723199206,
    "ttft": 8293.55099355519,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2092868485860526,
    "arrivals": 65172,
    "finished_requests": 65023,
    "scheduler_time": 41.08628846467781
}
#Debug simulation 
Total elapsed time: 4.6693823570385575. Arrivals time: 0.1527515910565853 Scheduler time: 4.25338920392096 Scheduler overhead time: 0.09803142584860325 Adapter cache time: 0.024084128439426422 Engine time: 0.09511123178526759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 4.683367902878672,
    "estimated_duration": 3599.9561353733593,
    "input_throughput": 4483.17184796091,
    "output_throughput": 3978.6418115660003,
    "total_throughput": 8461.81365952691,
    "itl": 38.181526153703985,
    "ttft": 8293.563846812785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.200352315879427,
    "arrivals": 65172,
    "finished_requests": 65023,
    "scheduler_time": 41.086357431289045
}
#Debug simulation 
Total elapsed time: 4.683468922041357. Arrivals time: 0.15112502267584205 Scheduler time: 4.271682616788894 Scheduler overhead time: 0.09661587700247765 Adapter cache time: 0.024057182017713785 Engine time: 0.09409426851198077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 4.706917338073254,
    "estimated_duration": 3599.938959409471,
    "input_throughput": 4483.19323798964,
    "output_throughput": 3978.660794390112,
    "total_throughput": 8461.854032379752,
    "itl": 38.18158236292338,
    "ttft": 8293.595900721924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 65172,
    "finished_requests": 65023,
    "scheduler_time": 41.08628454279448
}
#Debug simulation 
Total elapsed time: 4.707010760437697. Arrivals time: 0.15154575230553746 Scheduler time: 4.292852468788624 Scheduler overhead time: 0.09698340110480785 Adapter cache time: 0.024027327075600624 Engine time: 0.09569935034960508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.647819944657385,
    "estimated_duration": 3599.9415157929798,
    "input_throughput": 4483.190054393126,
    "output_throughput": 3978.6579690712015,
    "total_throughput": 8461.848023464327,
    "itl": 38.18140427040108,
    "ttft": 8293.569244887387,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 65172,
    "finished_requests": 65023,
    "scheduler_time": 41.086179459734986
}
#Debug simulation 
Total elapsed time: 4.647920269053429. Arrivals time: 0.1518362695351243 Scheduler time: 4.236095412634313 Scheduler overhead time: 0.09549797372892499 Adapter cache time: 0.02397851273417473 Engine time: 0.09453500946983695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_64_slots_64_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 8640, 8640, 66, 8640, 66, 135, 66, 8640, 135, 8640, 66, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 8640, 66, 66, 66, 8640, 135, 66, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 66, 135, 135, 8640, 8640, 135, 8640, 66, 8640, 135, 135, 135, 8640, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 8640, 135]
Prompts retrieved: 194301 . Total input tokens: 43486059 . Total output tokens: 38779715
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.696732339914888,
    "estimated_duration": 3599.9480801969858,
    "input_throughput": 4483.181879422238,
    "output_throughput": 3978.6507141003717,
    "total_throughput": 8461.83259352261,
    "itl": 38.18171753629691,
    "ttft": 8293.580532493806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 65172,
    "finished_requests": 65023,
    "scheduler_time": 41.086410683879905
}
#Debug simulation 
Total elapsed time: 4.696827393956482. Arrivals time: 0.1569874668493867 Scheduler time: 4.278702985029668 Scheduler overhead time: 0.09637836180627346 Adapter cache time: 0.024051064159721136 Engine time: 0.09484319435432553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_64_slots_64_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43331592 . Total output tokens: 38638110
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 4.686247774865478,
    "estimated_duration": 3599.9874706080514,
    "input_throughput": 4473.281124303473,
    "output_throughput": 3978.504402289174,
    "total_throughput": 8451.785526592646,
    "itl": 37.73985246901944,
    "ttft": 9271.760384145025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 64890,
    "finished_requests": 64724,
    "scheduler_time": 40.914927587634985
}
#Debug simulation 
Total elapsed time: 4.686338258907199. Arrivals time: 0.1495030466467142 Scheduler time: 4.275537406094372 Scheduler overhead time: 0.0965913007967174 Adapter cache time: 0.02216809242963791 Engine time: 0.0962620759382844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_64_slots_64_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43331592 . Total output tokens: 38638110
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.649370031896979,
    "estimated_duration": 3600.018017789034,
    "input_throughput": 4473.38038877108,
    "output_throughput": 3978.576476346776,
    "total_throughput": 8451.956865117856,
    "itl": 37.73995958121612,
    "ttft": 9105.35981330214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 64890,
    "finished_requests": 64727,
    "scheduler_time": 40.91540896076666
}
#Debug simulation 
Total elapsed time: 4.649460211861879. Arrivals time: 0.15217456594109535 Scheduler time: 4.238259483128786 Scheduler overhead time: 0.09609751310199499 Adapter cache time: 0.022180605679750443 Engine time: 0.0943929455243051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_64_slots_64_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [21 21 22]
Adapter prompts. [33, 135, 8640, 8640, 33, 8640, 33, 135, 33, 8640, 135, 8640, 33, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 8640, 33, 33, 33, 8640, 135, 33, 8640, 8640, 8640, 8640, 135, 135, 135, 8640, 33, 135, 135, 8640, 8640, 135, 8640, 33, 8640, 135, 135, 135, 8640, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 8640, 135]
Prompts retrieved: 193608 . Total input tokens: 43331592 . Total output tokens: 38638110
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 4.664489171933383,
    "estimated_duration": 3599.98074444784,
    "input_throughput": 4473.289482127486,
    "output_throughput": 3978.5118356783805,
    "total_throughput": 8451.801317805866,
    "itl": 37.74015169523391,
    "ttft": 9271.816378606765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 64890,
    "finished_requests": 64724,
    "scheduler_time": 40.915035922491136
}
#Debug simulation 
Total elapsed time: 4.664580585900694. Arrivals time: 0.15422267839312553 Scheduler time: 4.249734341632575 Scheduler overhead time: 0.09702861867845058 Adapter cache time: 0.022322678938508034 Engine time: 0.09518967848271132 
