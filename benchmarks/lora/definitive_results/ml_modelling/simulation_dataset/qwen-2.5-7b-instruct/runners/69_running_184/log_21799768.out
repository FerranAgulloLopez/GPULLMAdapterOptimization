INFO 06-01 00:47:09 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:10 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 146.08546600164846,
    "estimated_duration": 3600.0704707080035,
    "input_throughput": 8113.021741559395,
    "output_throughput": 7146.963430118612,
    "total_throughput": 15259.985171678009,
    "itl": 107.21492044078927,
    "ttft": 1879586.8172576874,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9299056417145779,
    "arrivals": 775108,
    "finished_requests": 117767,
    "scheduler_time": 267.34985415215726
}
#Debug simulation 
Total elapsed time: 146.08570724679157. Arrivals time: 0.9855279698967934 Scheduler time: 144.7822618293576 Scheduler overhead time: 0.12191390339285135 Adapter cache time: 0.02892301231622696 Engine time: 0.12916068080812693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 145.55209762090817,
    "estimated_duration": 3600.0532190017593,
    "input_throughput": 8112.783123827961,
    "output_throughput": 7146.797681822666,
    "total_throughput": 15259.580805650627,
    "itl": 107.21712752759353,
    "ttft": 1879577.9796250418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0409866232797558,
    "arrivals": 775108,
    "finished_requests": 117763,
    "scheduler_time": 267.3425037285087
}
#Debug simulation 
Total elapsed time: 145.55231567984447. Arrivals time: 0.9457205259241164 Scheduler time: 144.28900287253782 Scheduler overhead time: 0.12340446654707193 Adapter cache time: 0.02820761315524578 Engine time: 0.12861858727410436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 172.1347301308997,
    "estimated_duration": 3600.030459875426,
    "input_throughput": 7933.352041968081,
    "output_throughput": 7044.9231701438475,
    "total_throughput": 14978.275212111928,
    "itl": 103.67714887262129,
    "ttft": 1880586.457868421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7865453501907167,
    "arrivals": 774530,
    "finished_requests": 115576,
    "scheduler_time": 273.9678061295424
}
#Debug simulation 
Total elapsed time: 172.13490010704845. Arrivals time: 0.9904643846675754 Scheduler time: 170.8007983714342 Scheduler overhead time: 0.13560245838016272 Adapter cache time: 0.029933208599686623 Engine time: 0.13764777034521103 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 143.3049342990853,
    "estimated_duration": 3600.0807351805684,
    "input_throughput": 8103.988811944429,
    "output_throughput": 7174.292439501235,
    "total_throughput": 15278.281251445664,
    "itl": 106.7146566639332,
    "ttft": 1884600.3556641259,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9505979135492868,
    "arrivals": 774530,
    "finished_requests": 118160,
    "scheduler_time": 265.32441019084825
}
#Debug simulation 
Total elapsed time: 143.30512099293992. Arrivals time: 1.0244084596633911 Scheduler time: 141.95393152628094 Scheduler overhead time: 0.12608052790164948 Adapter cache time: 0.027752919122576714 Engine time: 0.13398197991773486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 145.04118752013892,
    "estimated_duration": 3600.08236373477,
    "input_throughput": 8103.985145976905,
    "output_throughput": 7174.289194096571,
    "total_throughput": 15278.274340073476,
    "itl": 106.71468956236853,
    "ttft": 1884600.9365591696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9521017414145221,
    "arrivals": 774530,
    "finished_requests": 118160,
    "scheduler_time": 265.3244348785928
}
#Debug simulation 
Total elapsed time: 145.04136741301045. Arrivals time: 1.0249266307801008 Scheduler time: 143.68766977312043 Scheduler overhead time: 0.1270105023868382 Adapter cache time: 0.027920369058847427 Engine time: 0.13440346159040928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 170.53693487402052,
    "estimated_duration": 3600.0481930591327,
    "input_throughput": 7933.31296371645,
    "output_throughput": 7044.88846813152,
    "total_throughput": 14978.20143184797,
    "itl": 103.67746454729458,
    "ttft": 1880593.3220815354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8048079091310524,
    "arrivals": 774530,
    "finished_requests": 115576,
    "scheduler_time": 273.9678769857606
}
#Debug simulation 
Total elapsed time: 170.53713289601728. Arrivals time: 1.0245406907051802 Scheduler time: 169.16295054228976 Scheduler overhead time: 0.1380501464009285 Adapter cache time: 0.02884223498404026 Engine time: 0.14157224399968982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 142.92596051376313,
    "estimated_duration": 3600.0943477442897,
    "input_throughput": 8103.958169396361,
    "output_throughput": 7174.265312291903,
    "total_throughput": 15278.223481688263,
    "itl": 106.71495055487168,
    "ttft": 1884605.8221765975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.964174104910349,
    "arrivals": 774530,
    "finished_requests": 118160,
    "scheduler_time": 265.3245466017877
}
#Debug simulation 
Total elapsed time: 142.92622719379142. Arrivals time: 0.9942932231351733 Scheduler time: 141.61032167309895 Scheduler overhead time: 0.12658517668023705 Adapter cache time: 0.026654826011508703 Engine time: 0.1297703175805509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 169.29140718793496,
    "estimated_duration": 3600.0120035513646,
    "input_throughput": 7933.392714198072,
    "output_throughput": 7044.9592876303695,
    "total_throughput": 14978.352001828442,
    "itl": 103.67667511711485,
    "ttft": 1880579.085319207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7684429257898602,
    "arrivals": 774530,
    "finished_requests": 115576,
    "scheduler_time": 273.9674522298399
}
#Debug simulation 
Total elapsed time: 169.29157362878323. Arrivals time: 1.0057555879466236 Scheduler time: 167.94080048892647 Scheduler overhead time: 0.13550014980137348 Adapter cache time: 0.030197203624993563 Engine time: 0.13890662230551243 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 144.52556053129956,
    "estimated_duration": 3600.106744740124,
    "input_throughput": 8103.930263352792,
    "output_throughput": 7174.240607652985,
    "total_throughput": 15278.170871005777,
    "itl": 106.71517177227656,
    "ttft": 1884610.674826108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 291,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9768752373382501,
    "arrivals": 774530,
    "finished_requests": 118160,
    "scheduler_time": 265.32464261952583
}
#Debug simulation 
Total elapsed time: 144.5258104270324. Arrivals time: 0.9695704593323171 Scheduler time: 143.24615635117516 Scheduler overhead time: 0.12107851728796959 Adapter cache time: 0.026849236339330673 Engine time: 0.12486393703147769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 157.01232899772003,
    "estimated_duration": 3600.020986668444,
    "input_throughput": 7867.734689572407,
    "output_throughput": 6984.372894800491,
    "total_throughput": 14852.107584372898,
    "itl": 103.39529260284168,
    "ttft": 1857459.6256901042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9946585167781437,
    "arrivals": 716829,
    "finished_requests": 114589,
    "scheduler_time": 277.5292626779232
}
#Debug simulation 
Total elapsed time: 157.0125316749327. Arrivals time: 0.9612635406665504 Scheduler time: 155.7144702631049 Scheduler overhead time: 0.1319376383908093 Adapter cache time: 0.029670909978449345 Engine time: 0.13500793324783444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 156.16323135001585,
    "estimated_duration": 3600.086252523376,
    "input_throughput": 7867.592055647863,
    "output_throughput": 6984.246275315243,
    "total_throughput": 14851.838330963106,
    "itl": 103.39642150043835,
    "ttft": 1857482.8482291696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0596143393078876,
    "arrivals": 716829,
    "finished_requests": 114589,
    "scheduler_time": 277.5296727488797
}
#Debug simulation 
Total elapsed time: 156.1634179619141. Arrivals time: 0.9851558590307832 Scheduler time: 154.83773187361658 Scheduler overhead time: 0.13306465046480298 Adapter cache time: 0.030356257688254118 Engine time: 0.1374828196130693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 156.13520231097937,
    "estimated_duration": 3600.0879191823315,
    "input_throughput": 7867.5884133499385,
    "output_throughput": 6984.243041961819,
    "total_throughput": 14851.831455311756,
    "itl": 103.39639404988077,
    "ttft": 1857483.2862579518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.061651837173855,
    "arrivals": 716829,
    "finished_requests": 114589,
    "scheduler_time": 277.5297020642746
}
#Debug simulation 
Total elapsed time: 156.1353823458776. Arrivals time: 0.9760749288834631 Scheduler time: 154.82130090193823 Scheduler overhead time: 0.13208398967981339 Adapter cache time: 0.030311455484479666 Engine time: 0.13542716903612018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 156.69130564294755,
    "estimated_duration": 3600.0424972523247,
    "input_throughput": 7867.687679136525,
    "output_throughput": 6984.331162532297,
    "total_throughput": 14852.018841668822,
    "itl": 103.39568707038546,
    "ttft": 1857466.5144373016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0167118308716487,
    "arrivals": 716829,
    "finished_requests": 114589,
    "scheduler_time": 277.5293201791535
}
#Debug simulation 
Total elapsed time: 156.69148101424798. Arrivals time: 0.9770607678219676 Scheduler time: 155.375873696059 Scheduler overhead time: 0.1334699308499694 Adapter cache time: 0.029645173344761133 Engine time: 0.13480549724772573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 127.31467317370698,
    "estimated_duration": 3600.100872698705,
    "input_throughput": 7867.56010499444,
    "output_throughput": 6984.2179119696875,
    "total_throughput": 14851.778016964126,
    "itl": 103.39657975235158,
    "ttft": 1857487.8915035324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.074855984747415,
    "arrivals": 716829,
    "finished_requests": 114589,
    "scheduler_time": 277.5297515488305
}
#Debug simulation 
Total elapsed time: 127.31484167464077. Arrivals time: 0.8075494929216802 Scheduler time: 126.2355873011984 Scheduler overhead time: 0.10644634161144495 Adapter cache time: 0.02270717266947031 Engine time: 0.10688828537240624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 159.84602572117,
    "estimated_duration": 3600.1074842187086,
    "input_throughput": 7867.739817259095,
    "output_throughput": 6984.310082468769,
    "total_throughput": 14852.049899727863,
    "itl": 103.39461434465716,
    "ttft": 1857489.1418057925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9717663458432084,
    "arrivals": 716829,
    "finished_requests": 114592,
    "scheduler_time": 277.53722196142166
}
#Debug simulation 
Total elapsed time: 159.84620468132198. Arrivals time: 1.0320254317484796 Scheduler time: 158.4671084592119 Scheduler overhead time: 0.13640982517972589 Adapter cache time: 0.029696902725845575 Engine time: 0.13999596750363708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 155.45474411686882,
    "estimated_duration": 3600.0052341239716,
    "input_throughput": 7867.670783232154,
    "output_throughput": 6984.402345214516,
    "total_throughput": 14852.07312844667,
    "itl": 103.397665518441,
    "ttft": 1857388.0928934077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.088688901253049,
    "arrivals": 716829,
    "finished_requests": 114587,
    "scheduler_time": 277.5217104952535
}
#Debug simulation 
Total elapsed time: 155.4550334098749. Arrivals time: 0.9811416859738529 Scheduler time: 154.1378337754868 Scheduler overhead time: 0.13222677865996957 Adapter cache time: 0.029259670060127974 Engine time: 0.1333652134053409 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 157.39207760198042,
    "estimated_duration": 3600.050462762745,
    "input_throughput": 7909.396075006225,
    "output_throughput": 7042.05378847512,
    "total_throughput": 14951.449863481344,
    "itl": 105.88575319904098,
    "ttft": 1848909.3098168848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9946585167781437,
    "arrivals": 707393,
    "finished_requests": 115435,
    "scheduler_time": 273.9954283806444
}
#Debug simulation 
Total elapsed time: 157.3922624080442. Arrivals time: 0.9715933729894459 Scheduler time: 156.0914679542184 Scheduler overhead time: 0.12797589879482985 Adapter cache time: 0.02958190767094493 Engine time: 0.13260124251246452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 144.85896060895175,
    "estimated_duration": 3600.0371411830256,
    "input_throughput": 7998.151094223984,
    "output_throughput": 7103.951708563719,
    "total_throughput": 15102.102802787704,
    "itl": 106.97726739416002,
    "ttft": 1857049.1154371183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1285343252774378,
    "arrivals": 707393,
    "finished_requests": 116685,
    "scheduler_time": 269.4522680223776
}
#Debug simulation 
Total elapsed time: 144.85916084470227. Arrivals time: 1.0696854442358017 Scheduler time: 143.4592602136545 Scheduler overhead time: 0.12874728720635176 Adapter cache time: 0.02895616553723812 Engine time: 0.13293848186731339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 144.87532697431743,
    "estimated_duration": 3600.0392331069306,
    "input_throughput": 7998.1464466292255,
    "output_throughput": 7103.947580573595,
    "total_throughput": 15102.09402720282,
    "itl": 106.97730716526193,
    "ttft": 1857049.8203079526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1306244454160395,
    "arrivals": 707393,
    "finished_requests": 116685,
    "scheduler_time": 269.4522698261294
}
#Debug simulation 
Total elapsed time: 144.87556974124163. Arrivals time: 1.049537098966539 Scheduler time: 143.49940499057993 Scheduler overhead time: 0.12698651757091284 Adapter cache time: 0.028137211222201586 Engine time: 0.1313961478881538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 156.84831731393933,
    "estimated_duration": 3600.072857409455,
    "input_throughput": 7909.346873743416,
    "output_throughput": 7042.0099826098085,
    "total_throughput": 14951.356856353224,
    "itl": 105.8860895644285,
    "ttft": 1848917.1626758284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0175290215085295,
    "arrivals": 707393,
    "finished_requests": 115435,
    "scheduler_time": 273.99565279264755
}
#Debug simulation 
Total elapsed time: 156.8485023761168. Arrivals time: 0.9827391356229782 Scheduler time: 155.53253526380286 Scheduler overhead time: 0.1296986471861601 Adapter cache time: 0.029904954601079226 Engine time: 0.1351469629444182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 145.4736352339387,
    "estimated_duration": 3600.0549343883104,
    "input_throughput": 7998.111563509339,
    "output_throughput": 7103.916597413087,
    "total_throughput": 15102.028160922426,
    "itl": 106.97791289490995,
    "ttft": 1857056.2050468859,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1448346232809177,
    "arrivals": 707393,
    "finished_requests": 116685,
    "scheduler_time": 269.45286058243437
}
#Debug simulation 
Total elapsed time: 145.4738203450106. Arrivals time: 0.9701563906855881 Scheduler time: 144.17733968095854 Scheduler overhead time: 0.12815558910369873 Adapter cache time: 0.028939472511410713 Engine time: 0.13021788140758872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 156.72351557109505,
    "estimated_duration": 3600.0261972000703,
    "input_throughput": 7909.449387381098,
    "output_throughput": 7042.101254629032,
    "total_throughput": 14951.55064201013,
    "itl": 105.88528276526186,
    "ttft": 1848900.206138807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9717663458432084,
    "arrivals": 707393,
    "finished_requests": 115435,
    "scheduler_time": 273.99485529749387
}
#Debug simulation 
Total elapsed time: 156.72372566303238. Arrivals time: 0.9948599059134722 Scheduler time: 155.39943369058892 Scheduler overhead time: 0.13002340402454138 Adapter cache time: 0.02906707627698779 Engine time: 0.13112864270806313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 145.31212710170075,
    "estimated_duration": 3600.0705715349923,
    "input_throughput": 7998.076823178223,
    "output_throughput": 7103.885741077456,
    "total_throughput": 15101.962564255678,
    "itl": 106.9781276868531,
    "ttft": 1857062.4587434072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1595478162914554,
    "arrivals": 707393,
    "finished_requests": 116685,
    "scheduler_time": 269.4530842660575
}
#Debug simulation 
Total elapsed time: 145.31231855088845. Arrivals time: 1.0454887533560395 Scheduler time: 143.94251964753494 Scheduler overhead time: 0.12698656832799315 Adapter cache time: 0.026079575065523386 Engine time: 0.13182844361290336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 154.88211448630318,
    "estimated_duration": 3600.037156730321,
    "input_throughput": 8028.487135463506,
    "output_throughput": 7169.754054272816,
    "total_throughput": 15198.241189736322,
    "itl": 106.72129994112363,
    "ttft": 1843639.4719136015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 301,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9212068109237577,
    "arrivals": 702770,
    "finished_requests": 116883,
    "scheduler_time": 267.9315555783387
}
#Debug simulation 
Total elapsed time: 154.88228561496362. Arrivals time: 0.9901963137090206 Scheduler time: 153.5693160179071 Scheduler overhead time: 0.12655637599527836 Adapter cache time: 0.02686567697674036 Engine time: 0.13063024636358023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 166.27245371975005,
    "estimated_duration": 3600.061781427993,
    "input_throughput": 7907.435129823866,
    "output_throughput": 7027.9057794292075,
    "total_throughput": 14935.340909253073,
    "itl": 105.01498230836184,
    "ttft": 1862231.412750693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8101764866337217,
    "arrivals": 702770,
    "finished_requests": 115143,
    "scheduler_time": 274.15939545192043
}
#Debug simulation 
Total elapsed time: 166.2726292819716. Arrivals time: 1.1138828499242663 Scheduler time: 164.8560856548138 Scheduler overhead time: 0.11934956209734082 Adapter cache time: 0.02311446378007531 Engine time: 0.12243874277919531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 168.22589732380584,
    "estimated_duration": 3600.0639731062515,
    "input_throughput": 7907.43031586673,
    "output_throughput": 7027.901500919599,
    "total_throughput": 14935.331816786329,
    "itl": 105.01505625882493,
    "ttft": 1862232.5499988573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8114502535760442,
    "arrivals": 702770,
    "finished_requests": 115143,
    "scheduler_time": 274.15941301600805
}
#Debug simulation 
Total elapsed time: 168.22608651267365. Arrivals time: 0.8169453414157033 Scheduler time: 167.10390068823472 Scheduler overhead time: 0.11993974586948752 Adapter cache time: 0.023631430231034756 Engine time: 0.12444474129006267 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 149.4551658281125,
    "estimated_duration": 3600.019460453229,
    "input_throughput": 8028.518822606932,
    "output_throughput": 7169.6004100907085,
    "total_throughput": 15198.11923269764,
    "itl": 106.72171191941226,
    "ttft": 1843651.3913639884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 301,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9416818612464736,
    "arrivals": 702770,
    "finished_requests": 116882,
    "scheduler_time": 267.92628826099366
}
#Debug simulation 
Total elapsed time: 149.45539012784138. Arrivals time: 0.8115580566227436 Scheduler time: 148.34797080373392 Scheduler overhead time: 0.11339585529640317 Adapter cache time: 0.023969565983861685 Engine time: 0.12215713877230883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 165.7772032287903,
    "estimated_duration": 3600.0748127419083,
    "input_throughput": 7907.406507010507,
    "output_throughput": 7027.880340278317,
    "total_throughput": 14935.286847288824,
    "itl": 105.01516916107502,
    "ttft": 1862237.2059333809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8223908329941372,
    "arrivals": 702770,
    "finished_requests": 115143,
    "scheduler_time": 274.1595121494116
}
#Debug simulation 
Total elapsed time: 165.77739596972242. Arrivals time: 0.7941036787815392 Scheduler time: 164.67669513169676 Scheduler overhead time: 0.12178723327815533 Adapter cache time: 0.02412419021129608 Engine time: 0.12260756734758615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 151.1105660921894,
    "estimated_duration": 3600.0156524711324,
    "input_throughput": 8028.535092662841,
    "output_throughput": 7169.796881933689,
    "total_throughput": 15198.33197459653,
    "itl": 106.72092307350971,
    "ttft": 1843631.150336012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 301,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9000051387655561,
    "arrivals": 702770,
    "finished_requests": 116883,
    "scheduler_time": 267.9313530298358
}
#Debug simulation 
Total elapsed time: 151.1107253972441. Arrivals time: 0.7868821606971323 Scheduler time: 150.03400922799483 Scheduler overhead time: 0.11280141258612275 Adapter cache time: 0.023972530383616686 Engine time: 0.11686011031270027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 166.42500223591924,
    "estimated_duration": 3600.0851267242974,
    "input_throughput": 7907.383852865234,
    "output_throughput": 7027.860205911626,
    "total_throughput": 14935.24405877686,
    "itl": 105.01571307949452,
    "ttft": 1862241.2857718964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8325768896937407,
    "arrivals": 702770,
    "finished_requests": 115143,
    "scheduler_time": 274.1600402294274
}
#Debug simulation 
Total elapsed time: 166.4251751927659. Arrivals time: 0.802017770241946 Scheduler time: 165.31034380104393 Scheduler overhead time: 0.12553991097956896 Adapter cache time: 0.02448980323970318 Engine time: 0.12419793289154768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 146.23428656579927,
    "estimated_duration": 3600.0905314771207,
    "input_throughput": 8050.489493692942,
    "output_throughput": 7112.702243488771,
    "total_throughput": 15163.191737181713,
    "itl": 106.80502756910276,
    "ttft": 1845450.6730017061,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9762955903145472,
    "arrivals": 700393,
    "finished_requests": 116832,
    "scheduler_time": 268.8234212350425
}
#Debug simulation 
Total elapsed time: 146.2344202939421. Arrivals time: 0.7819094406440854 Scheduler time: 145.15881078364328 Scheduler overhead time: 0.11565260542556643 Adapter cache time: 0.023043882567435503 Engine time: 0.1184065961278975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 156.01827104389668,
    "estimated_duration": 3600.056318384545,
    "input_throughput": 7986.846442697427,
    "output_throughput": 7077.851774117231,
    "total_throughput": 15064.698216814657,
    "itl": 106.1111548832904,
    "ttft": 1846757.3818151327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8671362714236646,
    "arrivals": 700393,
    "finished_requests": 115918,
    "scheduler_time": 272.5747696568753
}
#Debug simulation 
Total elapsed time: 156.01840889407322. Arrivals time: 0.769735055975616 Scheduler time: 154.9596466673538 Scheduler overhead time: 0.11393158184364438 Adapter cache time: 0.022407888434827328 Engine time: 0.11580218328163028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 178.35515192896128,
    "estimated_duration": 3600.0568984576003,
    "input_throughput": 7986.845155785984,
    "output_throughput": 7077.850633671061,
    "total_throughput": 15064.695789457046,
    "itl": 106.1111644471945,
    "ttft": 1846757.488686653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8682487864233597,
    "arrivals": 700393,
    "finished_requests": 115918,
    "scheduler_time": 272.57473740782353
}
#Debug simulation 
Total elapsed time: 178.35531983803958. Arrivals time: 0.8520393543876708 Scheduler time: 177.1666102730669 Scheduler overhead time: 0.13484218856319785 Adapter cache time: 0.02803858555853367 Engine time: 0.13376351678743958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 157.86161350319162,
    "estimated_duration": 3600.001021272548,
    "input_throughput": 8011.395227272774,
    "output_throughput": 7112.170482370981,
    "total_throughput": 15123.565709643754,
    "itl": 106.28555474973632,
    "ttft": 1847314.5653366337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8956053208676196,
    "arrivals": 700393,
    "finished_requests": 116274,
    "scheduler_time": 271.12471518851606
}
#Debug simulation 
Total elapsed time: 157.86175389913842. Arrivals time: 0.7967010717839003 Scheduler time: 156.75922475522384 Scheduler overhead time: 0.11999504407867789 Adapter cache time: 0.024095038883388042 Engine time: 0.12288672709837556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 167.05928662978113,
    "estimated_duration": 3600.0036938309468,
    "input_throughput": 7986.659305175193,
    "output_throughput": 7077.563015744085,
    "total_throughput": 15064.222320919278,
    "itl": 106.11362824385084,
    "ttft": 1846674.3405249866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8796923809871121,
    "arrivals": 700393,
    "finished_requests": 115913,
    "scheduler_time": 272.5685030347106
}
#Debug simulation 
Total elapsed time: 167.0594226210378. Arrivals time: 0.8136502560228109 Scheduler time: 165.94238527305424 Scheduler overhead time: 0.12038683285936713 Adapter cache time: 0.02437017112970352 Engine time: 0.12154481559991837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 144.98340408876538,
    "estimated_duration": 3600.0679217652,
    "input_throughput": 8050.540053641317,
    "output_throughput": 7112.746913798387,
    "total_throughput": 15163.286967439704,
    "itl": 106.80472477850944,
    "ttft": 1845442.4449217482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 319,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9538260440737953,
    "arrivals": 700393,
    "finished_requests": 116832,
    "scheduler_time": 268.82318103073055
}
#Debug simulation 
Total elapsed time: 144.98358245799318. Arrivals time: 0.7986186048947275 Scheduler time: 143.89503639936447 Scheduler overhead time: 0.11551245488226414 Adapter cache time: 0.023539892863482237 Engine time: 0.11492288718000054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 169.07117309700698,
    "estimated_duration": 3600.015529600334,
    "input_throughput": 7986.6330474391,
    "output_throughput": 7077.539746843439,
    "total_throughput": 15064.17279428254,
    "itl": 106.11373536237271,
    "ttft": 1846679.1557546798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8912617293372794,
    "arrivals": 700393,
    "finished_requests": 115913,
    "scheduler_time": 272.56866941717465
}
#Debug simulation 
Total elapsed time: 169.07131571974605. Arrivals time: 0.8112542806193233 Scheduler time: 167.948878927622 Scheduler overhead time: 0.12340106861665845 Adapter cache time: 0.024131962098181248 Engine time: 0.1261381502263248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 143.99774632090703,
    "estimated_duration": 3600.09572387268,
    "input_throughput": 8119.014393472197,
    "output_throughput": 7159.218525521493,
    "total_throughput": 15278.23291899369,
    "itl": 106.8376893824075,
    "ttft": 1844807.1560007385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9579326638509507,
    "arrivals": 699127,
    "finished_requests": 118238,
    "scheduler_time": 266.23214985019536
}
#Debug simulation 
Total elapsed time: 143.9978764820844. Arrivals time: 0.8011059681884944 Scheduler time: 142.91537753818557 Scheduler overhead time: 0.10980796255171299 Adapter cache time: 0.023226427845656872 Engine time: 0.11204131273552775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 142.245890062768,
    "estimated_duration": 3600.0381495439146,
    "input_throughput": 8118.42396828564,
    "output_throughput": 7159.034690581024,
    "total_throughput": 15277.458658866664,
    "itl": 106.8393598691508,
    "ttft": 1844813.7097995577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0241423310875022,
    "arrivals": 699127,
    "finished_requests": 118231,
    "scheduler_time": 266.22440192454945
}
#Debug simulation 
Total elapsed time: 142.24602594878525. Arrivals time: 0.8398782168515027 Scheduler time: 141.10991159453988 Scheduler overhead time: 0.11750753410160542 Adapter cache time: 0.024344826582819223 Engine time: 0.11846715118736029 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 144.35465440806001,
    "estimated_duration": 3600.038070548569,
    "input_throughput": 8118.424146427564,
    "output_throughput": 7159.034847671145,
    "total_throughput": 15277.45899409871,
    "itl": 106.83926776920417,
    "ttft": 1844813.1599174084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0254668584838564,
    "arrivals": 699127,
    "finished_requests": 118231,
    "scheduler_time": 266.22449898049706
}
#Debug simulation 
Total elapsed time: 144.3548891399987. Arrivals time: 0.7813765914179385 Scheduler time: 143.28277314500883 Scheduler overhead time: 0.11603531055152416 Adapter cache time: 0.023270396050065756 Engine time: 0.11600256664678454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 142.1011071470566,
    "estimated_duration": 3600.118979089801,
    "input_throughput": 8118.961948138133,
    "output_throughput": 7159.172280055108,
    "total_throughput": 15278.134228193241,
    "itl": 106.83850962120401,
    "ttft": 1844816.5130596876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9791968460590614,
    "arrivals": 699127,
    "finished_requests": 118238,
    "scheduler_time": 266.232940422109
}
#Debug simulation 
Total elapsed time: 142.10125409485772. Arrivals time: 0.8349105403758585 Scheduler time: 140.97366804862395 Scheduler overhead time: 0.11587720224633813 Adapter cache time: 0.024089972022920847 Engine time: 0.11675485083833337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 145.19777961075306,
    "estimated_duration": 3600.051836057959,
    "input_throughput": 8118.3931040290345,
    "output_throughput": 7159.007473687129,
    "total_throughput": 15277.400577716164,
    "itl": 106.83954886539668,
    "ttft": 1844818.244782675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0392997749894908,
    "arrivals": 699127,
    "finished_requests": 118231,
    "scheduler_time": 266.2245316119742
}
#Debug simulation 
Total elapsed time: 145.19792352616787. Arrivals time: 0.8053959663957357 Scheduler time: 144.09514769539237 Scheduler overhead time: 0.11819309880957007 Adapter cache time: 0.0232694149017334 Engine time: 0.11891002487391233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 144.16507260361686,
    "estimated_duration": 3600.0735395687266,
    "input_throughput": 8119.0644243066035,
    "output_throughput": 7159.262641920253,
    "total_throughput": 15278.327066226857,
    "itl": 106.83735031550542,
    "ttft": 1844798.7501143129,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9358857423043823,
    "arrivals": 699127,
    "finished_requests": 118238,
    "scheduler_time": 266.2319124291589
}
#Debug simulation 
Total elapsed time: 144.16520211705938. Arrivals time: 0.8062859415076673 Scheduler time: 143.07427082024515 Scheduler overhead time: 0.11183951934799552 Adapter cache time: 0.02360050519928336 Engine time: 0.11414156062528491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 144.35660665808246,
    "estimated_duration": 3600.0646548219756,
    "input_throughput": 8118.364196835589,
    "output_throughput": 7158.981982581775,
    "total_throughput": 15277.346179417364,
    "itl": 106.83976023749474,
    "ttft": 1844822.9045288267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.052629676349466,
    "arrivals": 699127,
    "finished_requests": 118231,
    "scheduler_time": 266.2246207061266
}
#Debug simulation 
Total elapsed time: 144.35673677595332. Arrivals time: 0.8239981275983155 Scheduler time: 143.2401364510879 Scheduler overhead time: 0.11576616298407316 Adapter cache time: 0.023761695250868797 Engine time: 0.11666698521003127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 160.68120283493772,
    "estimated_duration": 3600.0377842756607,
    "input_throughput": 8072.983880042131,
    "output_throughput": 7142.273370659477,
    "total_throughput": 15215.257250701608,
    "itl": 106.75647575869147,
    "ttft": 1854268.7749343556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8110292521421787,
    "arrivals": 698509,
    "finished_requests": 117687,
    "scheduler_time": 267.3175039710213
}
#Debug simulation 
Total elapsed time: 160.68133074138314. Arrivals time: 0.8488239292055368 Scheduler time: 159.5292252842337 Scheduler overhead time: 0.12088979361578822 Adapter cache time: 0.024258519522845745 Engine time: 0.12108443956822157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 160.89923002570868,
    "estimated_duration": 3600.0916745312793,
    "input_throughput": 8072.863034462565,
    "output_throughput": 7142.166457010482,
    "total_throughput": 15215.029491473048,
    "itl": 106.75747748854411,
    "ttft": 1854290.1182270716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8638675088761415,
    "arrivals": 698509,
    "finished_requests": 117687,
    "scheduler_time": 267.3183616092954
}
#Debug simulation 
Total elapsed time: 160.89935884764418. Arrivals time: 0.8382036322727799 Scheduler time: 159.75750948721543 Scheduler overhead time: 0.12073862971737981 Adapter cache time: 0.024376599118113518 Engine time: 0.1222257181070745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 160.36708160489798,
    "estimated_duration": 3600.0929986141314,
    "input_throughput": 8072.860065333846,
    "output_throughput": 7142.163830183859,
    "total_throughput": 15215.023895517706,
    "itl": 106.7574735670261,
    "ttft": 1854290.4182332847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8655508064664943,
    "arrivals": 698509,
    "finished_requests": 117687,
    "scheduler_time": 267.3184025488681
}
#Debug simulation 
Total elapsed time: 160.36721472116187. Arrivals time: 0.8538197572343051 Scheduler time: 159.20875079836696 Scheduler overhead time: 0.12053023185580969 Adapter cache time: 0.025233176536858082 Engine time: 0.12192414328455925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 160.70997489988804,
    "estimated_duration": 3600.05574466104,
    "input_throughput": 8072.943604581991,
    "output_throughput": 7142.23773843839,
    "total_throughput": 15215.18134302038,
    "itl": 106.75699687159073,
    "ttft": 1854276.098220351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8283197161718296,
    "arrivals": 698509,
    "finished_requests": 117687,
    "scheduler_time": 267.3178737765974
}
#Debug simulation 
Total elapsed time: 160.7101178029552. Arrivals time: 0.8582046232186258 Scheduler time: 159.5476926532574 Scheduler overhead time: 0.12117071496322751 Adapter cache time: 0.023824868723750114 Engine time: 0.12209275737404823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 162.66502816183493,
    "estimated_duration": 3600.1039786086217,
    "input_throughput": 8072.835443834144,
    "output_throughput": 7142.142047224264,
    "total_throughput": 15214.977491058407,
    "itl": 106.75762166974567,
    "ttft": 1854294.669504028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8764913858845873,
    "arrivals": 698509,
    "finished_requests": 117687,
    "scheduler_time": 267.3185420025312
}
#Debug simulation 
Total elapsed time: 162.66515980707482. Arrivals time: 0.9040931146591902 Scheduler time: 161.4570506857708 Scheduler overhead time: 0.12043643649667501 Adapter cache time: 0.025044777896255255 Engine time: 0.12195948278531432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 125.49684112379327,
    "estimated_duration": 3600.018858628304,
    "input_throughput": 8073.02632049926,
    "output_throughput": 7142.310918281434,
    "total_throughput": 15215.337238780694,
    "itl": 106.75618565956206,
    "ttft": 1854261.3531363602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7923633281490776,
    "arrivals": 698509,
    "finished_requests": 117687,
    "scheduler_time": 267.31732870177177
}
#Debug simulation 
Total elapsed time: 125.49697652086616. Arrivals time: 0.7025119941681623 Scheduler time: 124.55293748807162 Scheduler overhead time: 0.09421389643102884 Adapter cache time: 0.018432696349918842 Engine time: 0.0960140572860837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 160.55537579301745,
    "estimated_duration": 3600.115535028774,
    "input_throughput": 8072.809529921854,
    "output_throughput": 7142.119120850518,
    "total_throughput": 15214.928650772372,
    "itl": 106.75788139601754,
    "ttft": 1854299.392955842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8875577190890951,
    "arrivals": 698509,
    "finished_requests": 117687,
    "scheduler_time": 267.3186319351675
}
#Debug simulation 
Total elapsed time: 160.55552886193618. Arrivals time: 0.8334098183549941 Scheduler time: 159.4206095780246 Scheduler overhead time: 0.12014210084453225 Adapter cache time: 0.023851521778851748 Engine time: 0.12006317032501101 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 157.79334912495688,
    "estimated_duration": 3600.0586065159064,
    "input_throughput": 8090.355792342935,
    "output_throughput": 7210.441505873665,
    "total_throughput": 15300.797298216601,
    "itl": 110.07586957917303,
    "ttft": 1834291.8987513871,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7436985217756582,
    "arrivals": 650482,
    "finished_requests": 117986,
    "scheduler_time": 264.73993912093215
}
#Debug simulation 
Total elapsed time: 157.7934804400429. Arrivals time: 0.8228636234998703 Scheduler time: 156.663646990899 Scheduler overhead time: 0.12221083045005798 Adapter cache time: 0.02429953310638666 Engine time: 0.12420217972248793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 150.90216497704387,
    "estimated_duration": 3600.0668012434066,
    "input_throughput": 8066.562539886755,
    "output_throughput": 7183.069489451841,
    "total_throughput": 15249.632029338596,
    "itl": 109.77052195846849,
    "ttft": 1835291.144636168,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7995719544752535,
    "arrivals": 650482,
    "finished_requests": 117661,
    "scheduler_time": 265.7359935615939
}
#Debug simulation 
Total elapsed time: 150.9022938851267. Arrivals time: 0.8222985095344484 Scheduler time: 149.7793167498894 Scheduler overhead time: 0.11923257261514664 Adapter cache time: 0.02399039873853326 Engine time: 0.12022331869229674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 155.0713128503412,
    "estimated_duration": 3600.0681999959875,
    "input_throughput": 8066.559405744693,
    "output_throughput": 7183.066698577772,
    "total_throughput": 15249.626104322466,
    "itl": 109.77054697972032,
    "ttft": 1835291.7281241843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8009707070514598,
    "arrivals": 650482,
    "finished_requests": 117661,
    "scheduler_time": 265.7359935615939
}
#Debug simulation 
Total elapsed time: 155.0714553850703. Arrivals time: 0.8229476339183748 Scheduler time: 153.9384750588797 Scheduler overhead time: 0.12548166140913963 Adapter cache time: 0.024291123263537884 Engine time: 0.1211167648434639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 153.6361180478707,
    "estimated_duration": 3600.125471074489,
    "input_throughput": 8035.927145440708,
    "output_throughput": 7141.934970484812,
    "total_throughput": 15177.86211592552,
    "itl": 109.78192762599936,
    "ttft": 1843934.4840371648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7179476213920878,
    "arrivals": 650482,
    "finished_requests": 117212,
    "scheduler_time": 267.280666934547
}
#Debug simulation 
Total elapsed time: 153.63626552792266. Arrivals time: 0.8154666465707123 Scheduler time: 152.51648321328685 Scheduler overhead time: 0.12054631765931845 Adapter cache time: 0.02438895497471094 Engine time: 0.12167423870414495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_160_slots_16_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 156.04706488130614,
    "estimated_duration": 3600.078978169294,
    "input_throughput": 8066.535255503604,
    "output_throughput": 7183.045193400187,
    "total_throughput": 15249.580448903791,
    "itl": 109.77066539160059,
    "ttft": 1835296.15974494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8117855326831379,
    "arrivals": 650482,
    "finished_requests": 117661,
    "scheduler_time": 265.7360569478549
}
#Debug simulation 
Total elapsed time: 156.0472059668973. Arrivals time: 0.8372526844032109 Scheduler time: 154.89718662342057 Scheduler overhead time: 0.12385633448138833 Adapter cache time: 0.02411848120391369 Engine time: 0.12658077804371715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_160_slots_16_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_160_slots_16_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 154.29973585903645,
    "estimated_duration": 3600.0417496229843,
    "input_throughput": 8090.393674753968,
    "output_throughput": 7210.475268160005,
    "total_throughput": 15300.868942913972,
    "itl": 110.0756594592483,
    "ttft": 1834284.6382212641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7265822216612297,
    "arrivals": 650482,
    "finished_requests": 117986,
    "scheduler_time": 264.73979837376356
}
#Debug simulation 
Total elapsed time: 154.29986439831555. Arrivals time: 0.8193182735703886 Scheduler time: 153.18297484423965 Scheduler overhead time: 0.11780651286244392 Adapter cache time: 0.023757354822009802 Engine time: 0.11905992357060313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_160_slots_16_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_160_slots_16_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 156.4077212982811,
    "estimated_duration": 3600.089744254556,
    "input_throughput": 8066.511132492096,
    "output_throughput": 7183.0237124698515,
    "total_throughput": 15249.534844961947,
    "itl": 109.77079153278035,
    "ttft": 1835301.1799461367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8215943280234967,
    "arrivals": 650482,
    "finished_requests": 117661,
    "scheduler_time": 265.73621392913475
}
#Debug simulation 
Total elapsed time: 156.40785232093185. Arrivals time: 0.8530069384723902 Scheduler time: 155.2410359289497 Scheduler overhead time: 0.1256547225639224 Adapter cache time: 0.024191622156649828 Engine time: 0.12535096844658256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 160.98283493658528,
    "estimated_duration": 3600.0021322808866,
    "input_throughput": 8150.289617025897,
    "output_throughput": 7175.627138763168,
    "total_throughput": 15325.916755789065,
    "itl": 109.43468104376029,
    "ttft": 1830492.728866323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7651219359831875,
    "arrivals": 645743,
    "finished_requests": 118312,
    "scheduler_time": 266.66811862749444
}
#Debug simulation 
Total elapsed time: 160.98297644779086. Arrivals time: 0.8187113995663822 Scheduler time: 159.85078289033845 Scheduler overhead time: 0.12695906590670347 Adapter cache time: 0.02418602118268609 Engine time: 0.12456393614411354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 151.14145884336904,
    "estimated_duration": 3600.064558670341,
    "input_throughput": 8180.2424706730635,
    "output_throughput": 7205.437451816358,
    "total_throughput": 15385.67992248942,
    "itl": 109.52117743076433,
    "ttft": 1826125.506321074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8850765731930776,
    "arrivals": 645743,
    "finished_requests": 118689,
    "scheduler_time": 265.535748221042
}
#Debug simulation 
Total elapsed time: 151.14160294737667. Arrivals time: 0.9172879145480692 Scheduler time: 149.9152040691115 Scheduler overhead time: 0.12072568712756038 Adapter cache time: 0.02483044657856226 Engine time: 0.12624955968931317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 149.8065415979363,
    "estimated_duration": 3600.065068674104,
    "input_throughput": 8180.241311817775,
    "output_throughput": 7205.436431056969,
    "total_throughput": 15385.677742874743,
    "itl": 109.52115983238282,
    "ttft": 1826125.3766167806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8865098995156631,
    "arrivals": 645743,
    "finished_requests": 118689,
    "scheduler_time": 265.5357252456992
}
#Debug simulation 
Total elapsed time: 149.8066808450967. Arrivals time: 0.8298948262818158 Scheduler time: 148.66715597268194 Scheduler overhead time: 0.1237623249180615 Adapter cache time: 0.02403854625299573 Engine time: 0.12420621421188116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_160_slots_16_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_160_slots_16_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 147.59285351401195,
    "estimated_duration": 3600.0244920350324,
    "input_throughput": 8180.333513051395,
    "output_throughput": 7205.517645058169,
    "total_throughput": 15385.851158109564,
    "itl": 109.52054515684463,
    "ttft": 1826109.2493402117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8458514226228021,
    "arrivals": 645743,
    "finished_requests": 118689,
    "scheduler_time": 265.5354069291933
}
#Debug simulation 
Total elapsed time: 147.59299971396104. Arrivals time: 0.8175899852067232 Scheduler time: 146.47721230611205 Scheduler overhead time: 0.11906572058796883 Adapter cache time: 0.023417002987116575 Engine time: 0.11868731956928968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_160_slots_16_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_160_slots_16_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 148.42390498612076,
    "estimated_duration": 3600.0780496626885,
    "input_throughput": 8180.211815896402,
    "output_throughput": 7205.410450040234,
    "total_throughput": 15385.622265936636,
    "itl": 109.52137984142172,
    "ttft": 1826130.875140737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.89858226301149,
    "arrivals": 645743,
    "finished_requests": 118689,
    "scheduler_time": 265.5358335621532
}
#Debug simulation 
Total elapsed time: 148.4240469718352. Arrivals time: 0.8901333250105381 Scheduler time: 147.23306280793622 Scheduler overhead time: 0.11808862164616585 Adapter cache time: 0.024487062357366085 Engine time: 0.12121055833995342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_160_slots_16_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_160_slots_16_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 150.32860816270113,
    "estimated_duration": 3600.0729535395944,
    "input_throughput": 8180.464501710854,
    "output_throughput": 7205.436760523332,
    "total_throughput": 15385.901262234185,
    "itl": 109.51690958608275,
    "ttft": 1826186.9664391503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8103036299184907,
    "arrivals": 645743,
    "finished_requests": 118693,
    "scheduler_time": 265.5416417138607
}
#Debug simulation 
Total elapsed time: 150.32875783881173. Arrivals time: 0.8087028479203582 Scheduler time: 149.21381165459752 Scheduler overhead time: 0.12126249400898814 Adapter cache time: 0.025233214255422354 Engine time: 0.12280605593696237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_160_slots_16_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_160_slots_16_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 151.3934092950076,
    "estimated_duration": 3600.0033017078695,
    "input_throughput": 8180.328330818215,
    "output_throughput": 7205.543391500189,
    "total_throughput": 15385.871722318405,
    "itl": 109.52175583355691,
    "ttft": 1826052.5943452602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9095228424295827,
    "arrivals": 645743,
    "finished_requests": 118688,
    "scheduler_time": 265.5287666692469
}
#Debug simulation 
Total elapsed time: 151.39363390114158. Arrivals time: 1.1515856701880693 Scheduler time: 149.9316718201153 Scheduler overhead time: 0.1233438029885292 Adapter cache time: 0.02452658535912633 Engine time: 0.12401360273361206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 154.3087185616605,
    "estimated_duration": 3600.0063349568272,
    "input_throughput": 8144.881500702036,
    "output_throughput": 7227.560059366405,
    "total_throughput": 15372.44156006844,
    "itl": 109.76034679103688,
    "ttft": 1813586.6188464742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8752994947647664,
    "arrivals": 643390,
    "finished_requests": 118641,
    "scheduler_time": 264.69005562837117
}
#Debug simulation 
Total elapsed time: 154.30886106705293. Arrivals time: 0.8507961700670421 Scheduler time: 153.15079836873338 Scheduler overhead time: 0.12139443447813392 Adapter cache time: 0.025362517684698105 Engine time: 0.1221245639026165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 163.03181580081582,
    "estimated_duration": 3600.1201596381347,
    "input_throughput": 8126.199044128735,
    "output_throughput": 7175.787433327411,
    "total_throughput": 15301.986477456147,
    "itl": 109.99243817866697,
    "ttft": 1834191.1772765662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7446551462775124,
    "arrivals": 643390,
    "finished_requests": 118395,
    "scheduler_time": 265.2162315036394
}
#Debug simulation 
Total elapsed time: 163.0319592389278. Arrivals time: 0.8560893307439983 Scheduler time: 161.85536616574973 Scheduler overhead time: 0.1287164781242609 Adapter cache time: 0.025051411241292953 Engine time: 0.12880527041852474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 117.27950909314677,
    "estimated_duration": 3600.1213745277155,
    "input_throughput": 8126.19630187826,
    "output_throughput": 7175.78501180089,
    "total_throughput": 15301.98131367915,
    "itl": 109.99244500021793,
    "ttft": 1834191.6980168559,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7458584116771851,
    "arrivals": 643390,
    "finished_requests": 118395,
    "scheduler_time": 265.2162431278172
}
#Debug simulation 
Total elapsed time: 117.27965905703604. Arrivals time: 0.607014718465507 Scheduler time: 116.46211330592632 Scheduler overhead time: 0.08277021953836083 Adapter cache time: 0.01625662436708808 Engine time: 0.08175107417628169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 159.66165370261297,
    "estimated_duration": 3600.0441277570008,
    "input_throughput": 8122.824043887342,
    "output_throughput": 7174.364003169065,
    "total_throughput": 15297.188047056407,
    "itl": 110.06077904851165,
    "ttft": 1834779.4281731185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7301865348243175,
    "arrivals": 643390,
    "finished_requests": 118360,
    "scheduler_time": 265.2635149461634
}
#Debug simulation 
Total elapsed time: 159.66180794173852. Arrivals time: 0.8574793874286115 Scheduler time: 158.49514445429668 Scheduler overhead time: 0.12481486285105348 Adapter cache time: 0.02360220765694976 Engine time: 0.12288295151665807 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 159.85006313072518,
    "estimated_duration": 3600.007191943138,
    "input_throughput": 8126.227376842994,
    "output_throughput": 7175.877330971742,
    "total_throughput": 15302.104707814737,
    "itl": 109.99216829149037,
    "ttft": 1834181.8635605897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7555414532311293,
    "arrivals": 643390,
    "finished_requests": 118391,
    "scheduler_time": 265.2076132634219
}
#Debug simulation 
Total elapsed time: 159.8502075620927. Arrivals time: 0.8404471557587385 Scheduler time: 158.70155929680914 Scheduler overhead time: 0.12230314267799258 Adapter cache time: 0.024223132524639368 Engine time: 0.12424011807888746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 155.19396433327347,
    "estimated_duration": 3600.0277037704404,
    "input_throughput": 8144.833154836668,
    "output_throughput": 7227.5171584788295,
    "total_throughput": 15372.350313315497,
    "itl": 109.76007610727372,
    "ttft": 1813577.8005114244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8551543843420234,
    "arrivals": 643390,
    "finished_requests": 118641,
    "scheduler_time": 264.6953642690972
}
#Debug simulation 
Total elapsed time: 155.19409043109044. Arrivals time: 0.8519652346149087 Scheduler time: 154.02784940227866 Scheduler overhead time: 0.12518869154155254 Adapter cache time: 0.02556010987609625 Engine time: 0.1257524136453867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 163.02140236878768,
    "estimated_duration": 3600.016462719911,
    "input_throughput": 8126.206450149798,
    "output_throughput": 7175.858851623779,
    "total_throughput": 15302.065301773577,
    "itl": 109.99222525071676,
    "ttft": 1834185.9681815444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7652244947850733,
    "arrivals": 643390,
    "finished_requests": 118391,
    "scheduler_time": 265.2077011915439
}
#Debug simulation 
Total elapsed time: 163.021594167687. Arrivals time: 0.836204492021352 Scheduler time: 161.87498885719106 Scheduler overhead time: 0.1239886935800314 Adapter cache time: 0.02551834052428603 Engine time: 0.12426739139482379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 165.6515980088152,
    "estimated_duration": 3600.0923339998562,
    "input_throughput": 8175.685862839643,
    "output_throughput": 7212.121965536519,
    "total_throughput": 15387.807828376162,
    "itl": 109.61643797443246,
    "ttft": 1832301.3951261826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7283960830559945,
    "arrivals": 642182,
    "finished_requests": 118725,
    "scheduler_time": 264.0080329965095
}
#Debug simulation 
Total elapsed time: 165.651740539819. Arrivals time: 0.9556135893799365 Scheduler time: 164.37421733886003 Scheduler overhead time: 0.12882932694628835 Adapter cache time: 0.024885812774300575 Engine time: 0.1297191409394145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 169.8520441823639,
    "estimated_duration": 3600.0143601829363,
    "input_throughput": 8163.404103339914,
    "output_throughput": 7219.856478205611,
    "total_throughput": 15383.260581545526,
    "itl": 108.91996567230461,
    "ttft": 1825631.1079187642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7582497287308827,
    "arrivals": 642182,
    "finished_requests": 118529,
    "scheduler_time": 264.86490429198676
}
#Debug simulation 
Total elapsed time: 169.85217111604288. Arrivals time: 0.8560706749558449 Scheduler time: 168.67546213511378 Scheduler overhead time: 0.13020834373310208 Adapter cache time: 0.025025779847055674 Engine time: 0.1271584527567029 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 165.0940630240366,
    "estimated_duration": 3600.0161346381783,
    "input_throughput": 8163.400079581503,
    "output_throughput": 7219.852919523734,
    "total_throughput": 15383.252999105238,
    "itl": 108.91996844521849,
    "ttft": 1825632.1618474268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7593814770504868,
    "arrivals": 642182,
    "finished_requests": 118529,
    "scheduler_time": 264.8649607185368
}
#Debug simulation 
Total elapsed time: 165.09419683925807. Arrivals time: 0.8805120997130871 Scheduler time: 163.89826029259712 Scheduler overhead time: 0.1282214829698205 Adapter cache time: 0.024729978293180466 Engine time: 0.12492404412478209 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 165.91873943805695,
    "estimated_duration": 3600.0780157151903,
    "input_throughput": 8196.933475103493,
    "output_throughput": 7227.84058745758,
    "total_throughput": 15424.774062561073,
    "itl": 110.05774268863598,
    "ttft": 1833441.4816331428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7311336085270176,
    "arrivals": 642182,
    "finished_requests": 119126,
    "scheduler_time": 262.32224710520245
}
#Debug simulation 
Total elapsed time: 165.91888320585713. Arrivals time: 0.8655151100829244 Scheduler time: 164.73230600589886 Scheduler overhead time: 0.13065010262653232 Adapter cache time: 0.02509607980027795 Engine time: 0.12615921115502715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 167.36525566503406,
    "estimated_duration": 3600.026088159863,
    "input_throughput": 8163.377509028479,
    "output_throughput": 7219.8329577343375,
    "total_throughput": 15383.210466762815,
    "itl": 108.92014104975472,
    "ttft": 1825636.3956668011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7693160261772607,
    "arrivals": 642182,
    "finished_requests": 118529,
    "scheduler_time": 264.86507972968036
}
#Debug simulation 
Total elapsed time: 167.36548546608537. Arrivals time: 0.8810284775681794 Scheduler time: 166.16755768004805 Scheduler overhead time: 0.12664584536105394 Adapter cache time: 0.025021005421876907 Engine time: 0.12640998605638742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 167.68108190689236,
    "estimated_duration": 3600.076041341818,
    "input_throughput": 8175.7228630731,
    "output_throughput": 7212.1546050240095,
    "total_throughput": 15387.87746809711,
    "itl": 109.61623549270622,
    "ttft": 1832294.8233197306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7116319701867188,
    "arrivals": 642182,
    "finished_requests": 118725,
    "scheduler_time": 264.007904219819
}
#Debug simulation 
Total elapsed time: 167.68122450169176. Arrivals time: 0.9500161553733051 Scheduler time: 166.4085558615625 Scheduler overhead time: 0.12898804061114788 Adapter cache time: 0.02625430515035987 Engine time: 0.1296748979948461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 169.50905187800527,
    "estimated_duration": 3600.0358929051995,
    "input_throughput": 8163.355275961936,
    "output_throughput": 7219.813294423851,
    "total_throughput": 15383.168570385787,
    "itl": 108.92029503000688,
    "ttft": 1825640.3336996045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7792505753040345,
    "arrivals": 642182,
    "finished_requests": 118529,
    "scheduler_time": 264.8651500030562
}
#Debug simulation 
Total elapsed time: 169.50919386278838. Arrivals time: 0.8759114192798734 Scheduler time: 168.31457524280995 Scheduler overhead time: 0.12831357959657907 Adapter cache time: 0.02500485721975565 Engine time: 0.127050690818578 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 156.77258705999702,
    "estimated_duration": 3600.0529044172295,
    "input_throughput": 8256.720606391136,
    "output_throughput": 7275.212252537655,
    "total_throughput": 15531.932858928792,
    "itl": 110.15342445969476,
    "ttft": 1822215.2705025268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7436985217756582,
    "arrivals": 641592,
    "finished_requests": 119914,
    "scheduler_time": 260.34792335869264
}
#Debug simulation 
Total elapsed time: 156.7727424632758. Arrivals time: 0.8904108088463545 Scheduler time: 155.57109245751053 Scheduler overhead time: 0.1251078136265278 Adapter cache time: 0.02512522740289569 Engine time: 0.12339548207819462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 154.72025158209726,
    "estimated_duration": 3600.102150162398,
    "input_throughput": 8256.60766282955,
    "output_throughput": 7275.112735014626,
    "total_throughput": 15531.720397844178,
    "itl": 110.15414646239871,
    "ttft": 1822235.9098359307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7940004492038895,
    "arrivals": 641592,
    "finished_requests": 119914,
    "scheduler_time": 260.3483677551198
}
#Debug simulation 
Total elapsed time: 154.72040017182007. Arrivals time: 0.8499977216124535 Scheduler time: 153.56424628570676 Scheduler overhead time: 0.12459778552874923 Adapter cache time: 0.02316825883463025 Engine time: 0.12147699762135744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 154.87252560909837,
    "estimated_duration": 3600.1033766425567,
    "input_throughput": 8256.60484997547,
    "output_throughput": 7275.110256535403,
    "total_throughput": 15531.715106510874,
    "itl": 110.1541648112906,
    "ttft": 1822236.319620188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7952209168486335,
    "arrivals": 641592,
    "finished_requests": 119914,
    "scheduler_time": 260.3483737676256
}
#Debug simulation 
Total elapsed time: 154.8726564538665. Arrivals time: 0.8720552120357752 Scheduler time: 153.68275521462783 Scheduler overhead time: 0.1287953914143145 Adapter cache time: 0.02435696078464389 Engine time: 0.12695407774299383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 155.78307187510654,
    "estimated_duration": 3600.068446430784,
    "input_throughput": 8256.684960940087,
    "output_throughput": 7275.180844399414,
    "total_throughput": 15531.865805339501,
    "itl": 110.15350975416632,
    "ttft": 1822221.623284403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7600870377733391,
    "arrivals": 641592,
    "finished_requests": 119914,
    "scheduler_time": 260.3480772420239
}
#Debug simulation 
Total elapsed time: 155.78320736903697. Arrivals time: 0.875615207478404 Scheduler time: 154.59420261066407 Scheduler overhead time: 0.12671833345666528 Adapter cache time: 0.023328044917434454 Engine time: 0.1255399459041655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 152.89716431731358,
    "estimated_duration": 3600.113987033192,
    "input_throughput": 8256.580515800748,
    "output_throughput": 7275.088815058267,
    "total_throughput": 15531.669330859015,
    "itl": 110.15438687836722,
    "ttft": 1822240.9090523089,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.805658481121067,
    "arrivals": 641592,
    "finished_requests": 119914,
    "scheduler_time": 260.34844655541633
}
#Debug simulation 
Total elapsed time: 152.89730117330328. Arrivals time: 0.8932255986146629 Scheduler time: 151.6914951405488 Scheduler overhead time: 0.12670750357210636 Adapter cache time: 0.024568024091422558 Engine time: 0.12256343150511384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 157.47269448591396,
    "estimated_duration": 3600.0346351533376,
    "input_throughput": 8256.762507156804,
    "output_throughput": 7275.249172397041,
    "total_throughput": 15532.011679553843,
    "itl": 110.15311680185032,
    "ttft": 1822207.2308642103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7265822216612297,
    "arrivals": 641592,
    "finished_requests": 119914,
    "scheduler_time": 260.34767074210004
}
#Debug simulation 
Total elapsed time: 157.47283119289204. Arrivals time: 0.8752302024513483 Scheduler time: 156.27840188564733 Scheduler overhead time: 0.13015280617401004 Adapter cache time: 0.02402047347277403 Engine time: 0.1256839050911367 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 157.81830262299627,
    "estimated_duration": 3600.1254271867815,
    "input_throughput": 8256.554278784528,
    "output_throughput": 7275.065696937773,
    "total_throughput": 15531.619975722302,
    "itl": 110.15455476460804,
    "ttft": 1822246.1244810454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8159702916070852,
    "arrivals": 641592,
    "finished_requests": 119914,
    "scheduler_time": 260.3485745127225
}
#Debug simulation 
Total elapsed time: 157.81843156693503. Arrivals time: 0.9536895607598126 Scheduler time: 156.5400508302264 Scheduler overhead time: 0.1309557193890214 Adapter cache time: 0.02542383037507534 Engine time: 0.12922681076452136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_160_slots_16_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_160_slots_16_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 159.17093505803496,
    "estimated_duration": 3600.0970018671837,
    "input_throughput": 8156.369393594273,
    "output_throughput": 7233.968969861881,
    "total_throughput": 15390.338363456154,
    "itl": 111.15178904719471,
    "ttft": 1830820.7336121753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7773638869589184,
    "arrivals": 636231,
    "finished_requests": 118990,
    "scheduler_time": 261.74052336861735
}
#Debug simulation 
Total elapsed time: 159.1711420211941. Arrivals time: 0.8905248106457293 Scheduler time: 157.95403245510533 Scheduler overhead time: 0.13280089246109128 Adapter cache time: 0.02559501165524125 Engine time: 0.129929362796247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_160_slots_16_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_160_slots_16_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 152.19333671405911,
    "estimated_duration": 3600.023618592251,
    "input_throughput": 8142.502412654338,
    "output_throughput": 7261.16569485783,
    "total_throughput": 15403.668107512167,
    "itl": 110.99457573574985,
    "ttft": 1813588.4659656696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8974453696911269,
    "arrivals": 636231,
    "finished_requests": 118774,
    "scheduler_time": 262.9712017265229
}
#Debug simulation 
Total elapsed time: 152.19346681702882. Arrivals time: 0.895055175293237 Scheduler time: 150.9789634309709 Scheduler overhead time: 0.12871184293180704 Adapter cache time: 0.025215632282197475 Engine time: 0.12713162414729595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_160_slots_16_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_160_slots_16_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 151.2113561378792,
    "estimated_duration": 3600.025198252894,
    "input_throughput": 8142.498839793068,
    "output_throughput": 7261.162508719667,
    "total_throughput": 15403.661348512736,
    "itl": 110.99460873994762,
    "ttft": 1813589.0352613851,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8990212224051402,
    "arrivals": 636231,
    "finished_requests": 118774,
    "scheduler_time": 262.9712055344432
}
#Debug simulation 
Total elapsed time: 151.2114941580221. Arrivals time: 0.8849636372178793 Scheduler time: 150.02042701467872 Scheduler overhead time: 0.12404575385153294 Adapter cache time: 0.023588219191879034 Engine time: 0.12091475958004594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_160_slots_16_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_160_slots_16_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 154.79267190303653,
    "estimated_duration": 3600.1119596004164,
    "input_throughput": 8156.335505537761,
    "output_throughput": 7233.938914191592,
    "total_throughput": 15390.274419729352,
    "itl": 111.15210186070472,
    "ttft": 1830826.6907132224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7925689956988228,
    "arrivals": 636231,
    "finished_requests": 118990,
    "scheduler_time": 261.7404760702385
}
#Debug simulation 
Total elapsed time: 154.79280235618353. Arrivals time: 0.8780567548237741 Scheduler time: 153.60458122054115 Scheduler overhead time: 0.12575973104685545 Adapter cache time: 0.02474111132323742 Engine time: 0.12209564028307796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_160_slots_16_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_160_slots_16_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 152.34183088317513,
    "estimated_duration": 3600.003221045058,
    "input_throughput": 8142.548547912289,
    "output_throughput": 7261.206836479334,
    "total_throughput": 15403.755384391623,
    "itl": 110.99492406523558,
    "ttft": 1813594.8295741724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9109678321145522,
    "arrivals": 636231,
    "finished_requests": 118774,
    "scheduler_time": 262.9664842995432
}
#Debug simulation 
Total elapsed time: 152.34197921538725. Arrivals time: 1.0032044970430434 Scheduler time: 151.0258534331806 Scheduler overhead time: 0.12613328453153372 Adapter cache time: 0.02516265818849206 Engine time: 0.12414790829643607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_160_slots_16_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_160_slots_16_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 158.37273994507268,
    "estimated_duration": 3600.078264132526,
    "input_throughput": 8156.411845972875,
    "output_throughput": 7234.006621318637,
    "total_throughput": 15390.418467291513,
    "itl": 111.15149937931055,
    "ttft": 1830812.6581853111,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7594727749051536,
    "arrivals": 636231,
    "finished_requests": 118990,
    "scheduler_time": 261.740176938872
}
#Debug simulation 
Total elapsed time: 158.37288209004328. Arrivals time: 0.9275995828211308 Scheduler time: 157.1203805054538 Scheduler overhead time: 0.13005956402048469 Adapter cache time: 0.025468367151916027 Engine time: 0.13083620928227901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_160_slots_16_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_160_slots_16_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 151.82206613291055,
    "estimated_duration": 3600.0142207311596,
    "input_throughput": 8142.523668711096,
    "output_throughput": 7261.184650179219,
    "total_throughput": 15403.708318890316,
    "itl": 110.99519982607038,
    "ttft": 1813599.0727998368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9221599191054748,
    "arrivals": 636231,
    "finished_requests": 118774,
    "scheduler_time": 262.9665920144009
}
#Debug simulation 
Total elapsed time: 151.82220591325313. Arrivals time: 0.8858632370829582 Scheduler time: 150.619822550565 Scheduler overhead time: 0.12734660971909761 Adapter cache time: 0.025276390369981527 Engine time: 0.12513868464156985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 150.47294979076833,
    "estimated_duration": 3600.0343676134053,
    "input_throughput": 8233.517509348132,
    "output_throughput": 7257.177107817429,
    "total_throughput": 15490.69461716556,
    "itl": 110.85669251854232,
    "ttft": 1826130.5098244476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7773638869589184,
    "arrivals": 633808,
    "finished_requests": 119454,
    "scheduler_time": 260.6286946015649
}
#Debug simulation 
Total elapsed time: 150.47309062210843. Arrivals time: 0.8875403068959713 Scheduler time: 149.2699209479615 Scheduler overhead time: 0.12897700909525156 Adapter cache time: 0.02410612814128399 Engine time: 0.1253287009894848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 148.0389510979876,
    "estimated_duration": 3600.089486074282,
    "input_throughput": 8233.391451700267,
    "output_throughput": 7257.065998236948,
    "total_throughput": 15490.457449937216,
    "itl": 110.85742858638955,
    "ttft": 1826155.3920455303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8309769556322174,
    "arrivals": 633808,
    "finished_requests": 119454,
    "scheduler_time": 260.62941200291664
}
#Debug simulation 
Total elapsed time: 148.03908896632493. Arrivals time: 0.9651708197779953 Scheduler time: 146.76326467236504 Scheduler overhead time: 0.12583099957555532 Adapter cache time: 0.024059873074293137 Engine time: 0.12345625273883343 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 148.19507858576253,
    "estimated_duration": 3600.090578011116,
    "input_throughput": 8233.388954445489,
    "output_throughput": 7257.063797109643,
    "total_throughput": 15490.452751555133,
    "itl": 110.8574523543755,
    "ttft": 1826155.8564495298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8320720991306048,
    "arrivals": 633808,
    "finished_requests": 119454,
    "scheduler_time": 260.62940879624693
}
#Debug simulation 
Total elapsed time: 148.19521380215883. Arrivals time: 0.9716585273854434 Scheduler time: 146.91184714064002 Scheduler overhead time: 0.1252239253371954 Adapter cache time: 0.02504354389384389 Engine time: 0.1240917295217514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 147.54279647534713,
    "estimated_duration": 3600.0514685140715,
    "input_throughput": 8233.478398639218,
    "output_throughput": 7257.142634903383,
    "total_throughput": 15490.6210335426,
    "itl": 110.85695395494176,
    "ttft": 1826137.9468186093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7942033769725845,
    "arrivals": 633808,
    "finished_requests": 119454,
    "scheduler_time": 260.6288559736075
}
#Debug simulation 
Total elapsed time: 147.54299382213503. Arrivals time: 0.9759545023553073 Scheduler time: 146.26049685431644 Scheduler overhead time: 0.1242852439172566 Adapter cache time: 0.02336478140205145 Engine time: 0.12236245349049568 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 148.4163276227191,
    "estimated_duration": 3600.1031111074785,
    "input_throughput": 8233.360291417244,
    "output_throughput": 7257.038532977736,
    "total_throughput": 15490.39882439498,
    "itl": 110.85835938608287,
    "ttft": 1826161.3258324529,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8433899399079423,
    "arrivals": 633808,
    "finished_requests": 119454,
    "scheduler_time": 260.6303239360969
}
#Debug simulation 
Total elapsed time: 148.41646099090576. Arrivals time: 0.9584615514613688 Scheduler time: 147.1487395730801 Scheduler overhead time: 0.12541297171264887 Adapter cache time: 0.02422298351302743 Engine time: 0.12203559558838606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 153.7516805822961,
    "estimated_duration": 3600.0034496858398,
    "input_throughput": 8198.470754958758,
    "output_throughput": 7231.691125815979,
    "total_throughput": 15430.161880774738,
    "itl": 110.79600961944023,
    "ttft": 1825486.7521089257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.765452875494958,
    "arrivals": 633808,
    "finished_requests": 119058,
    "scheduler_time": 261.9881310666969
}
#Debug simulation 
Total elapsed time: 153.75182133400813. Arrivals time: 0.8384988778270781 Scheduler time: 152.60134372347966 Scheduler overhead time: 0.12700225552543998 Adapter cache time: 0.02366038504987955 Engine time: 0.12450621835887432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 150.62210485339165,
    "estimated_duration": 3600.1141035441055,
    "input_throughput": 8233.33515202204,
    "output_throughput": 7257.016374642229,
    "total_throughput": 15490.351526664268,
    "itl": 110.85848527619075,
    "ttft": 1826166.2015517005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8540790117532054,
    "arrivals": 633808,
    "finished_requests": 119454,
    "scheduler_time": 260.63042722372114
}
#Debug simulation 
Total elapsed time: 150.62224341230467. Arrivals time: 0.9906221134588122 Scheduler time: 149.31225307751447 Scheduler overhead time: 0.13046114379540086 Adapter cache time: 0.025126060470938683 Engine time: 0.12583713000640273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 151.04138062568381,
    "estimated_duration": 3600.038794744155,
    "input_throughput": 8213.410656343041,
    "output_throughput": 7257.215405051412,
    "total_throughput": 15470.626061394452,
    "itl": 110.89289018445554,
    "ttft": 1824643.1024199217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7314565707999272,
    "arrivals": 632594,
    "finished_requests": 119526,
    "scheduler_time": 260.807217000275
}
#Debug simulation 
Total elapsed time: 151.04153058491647. Arrivals time: 0.8932922082021832 Scheduler time: 149.8412367636338 Scheduler overhead time: 0.12275111209601164 Adapter cache time: 0.024502733256667852 Engine time: 0.12304276367649436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 148.840181783773,
    "estimated_duration": 3600.0146751476113,
    "input_throughput": 8236.340036248386,
    "output_throughput": 7269.553977283977,
    "total_throughput": 15505.894013532363,
    "itl": 110.83601094614475,
    "ttft": 1823748.1375723057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7782330070924979,
    "arrivals": 632594,
    "finished_requests": 119747,
    "scheduler_time": 260.2773976332875
}
#Debug simulation 
Total elapsed time: 148.84032028308138. Arrivals time: 0.8419088958762586 Scheduler time: 147.69813271518797 Scheduler overhead time: 0.12139330198988318 Adapter cache time: 0.023730541113764048 Engine time: 0.11855926411226392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 151.10411049006507,
    "estimated_duration": 3600.0157629514656,
    "input_throughput": 8236.337547503052,
    "output_throughput": 7269.551780668918,
    "total_throughput": 15505.889328171968,
    "itl": 110.83603801565978,
    "ttft": 1823748.5675454768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.779328827615831,
    "arrivals": 632594,
    "finished_requests": 119747,
    "scheduler_time": 260.2773896166131
}
#Debug simulation 
Total elapsed time: 151.10426392499357. Arrivals time: 0.8660117681138217 Scheduler time: 149.93087610276416 Scheduler overhead time: 0.1246048309840262 Adapter cache time: 0.02418477274477482 Engine time: 0.12189883412793279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 149.72447361517698,
    "estimated_duration": 3600.1080296116575,
    "input_throughput": 8236.69449808112,
    "output_throughput": 7269.649072953823,
    "total_throughput": 15506.343571034942,
    "itl": 110.83520639263622,
    "ttft": 1823748.6268201887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7439110003435071,
    "arrivals": 632594,
    "finished_requests": 119752,
    "scheduler_time": 260.28593683770356
}
#Debug simulation 
Total elapsed time: 149.72460579220206. Arrivals time: 0.8585233660414815 Scheduler time: 148.5582323251292 Scheduler overhead time: 0.12485178094357252 Adapter cache time: 0.023532690480351448 Engine time: 0.12159588700160384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 150.52726491214707,
    "estimated_duration": 3600.0278675741883,
    "input_throughput": 8236.309853895586,
    "output_throughput": 7269.527337752112,
    "total_throughput": 15505.837191647697,
    "itl": 110.83626499591738,
    "ttft": 1823754.099536934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7898921456746792,
    "arrivals": 632594,
    "finished_requests": 119747,
    "scheduler_time": 260.27754269896843
}
#Debug simulation 
Total elapsed time: 150.52740704501048. Arrivals time: 0.8951968140900135 Scheduler time: 149.31738228769973 Scheduler overhead time: 0.12758353864774108 Adapter cache time: 0.025142414961010218 Engine time: 0.12468131212517619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 151.5256505422294,
    "estimated_duration": 3600.021417798118,
    "input_throughput": 8213.450301660996,
    "output_throughput": 7257.2504349098035,
    "total_throughput": 15470.7007365708,
    "itl": 110.89260650965599,
    "ttft": 1824635.5969149368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7146220204816209,
    "arrivals": 632594,
    "finished_requests": 119526,
    "scheduler_time": 260.8070747588384
}
#Debug simulation 
Total elapsed time: 151.52579519199207. Arrivals time: 0.8736667041666806 Scheduler time: 150.34533537458628 Scheduler overhead time: 0.12401091353967786 Adapter cache time: 0.02347181038931012 Engine time: 0.12175870873034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 147.41943520167843,
    "estimated_duration": 3600.0389087908334,
    "input_throughput": 8236.284593368198,
    "output_throughput": 7269.505042319124,
    "total_throughput": 15505.789635687323,
    "itl": 110.83631745870058,
    "ttft": 1823759.2865837114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.799826694801453,
    "arrivals": 632594,
    "finished_requests": 119747,
    "scheduler_time": 260.27774901926654
}
#Debug simulation 
Total elapsed time: 147.4196194340475. Arrivals time: 0.878572050947696 Scheduler time: 146.23638221248984 Scheduler overhead time: 0.12237088149413466 Adapter cache time: 0.024266925174742937 Engine time: 0.12084272131323814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 159.42257486423478,
    "estimated_duration": 3600.0585349912785,
    "input_throughput": 8266.975303520197,
    "output_throughput": 7287.138457614983,
    "total_throughput": 15554.11376113518,
    "itl": 111.11710361984225,
    "ttft": 1821685.1488773855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.685549254640936,
    "arrivals": 632045,
    "finished_requests": 120225,
    "scheduler_time": 258.99351921283727
}
#Debug simulation 
Total elapsed time: 159.42271893611178. Arrivals time: 0.8573144734837115 Scheduler time: 158.25796471722424 Scheduler overhead time: 0.12339212233200669 Adapter cache time: 0.023921547457575798 Engine time: 0.12269296543672681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 165.8504064050503,
    "estimated_duration": 3600.0217977976654,
    "input_throughput": 8259.816931717158,
    "output_throughput": 7288.8858662057455,
    "total_throughput": 15548.702797922902,
    "itl": 111.18009251299958,
    "ttft": 1820754.3623454678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7532167018437785,
    "arrivals": 632045,
    "finished_requests": 120212,
    "scheduler_time": 259.0919206920498
}
#Debug simulation 
Total elapsed time: 165.8505568150431. Arrivals time: 1.1912556067109108 Scheduler time: 164.34204399818555 Scheduler overhead time: 0.12868885789066553 Adapter cache time: 0.02458098577335477 Engine time: 0.12515902565792203 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 165.838724904228,
    "estimated_duration": 3600.023229209054,
    "input_throughput": 8259.813647517232,
    "output_throughput": 7288.882968059379,
    "total_throughput": 15548.696615576611,
    "itl": 111.18012682491772,
    "ttft": 1820754.9582913802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7546517207287287,
    "arrivals": 632045,
    "finished_requests": 120212,
    "scheduler_time": 259.09191708454637
}
#Debug simulation 
Total elapsed time: 165.83887358708307. Arrivals time: 0.9447909719310701 Scheduler time: 164.5708745936863 Scheduler overhead time: 0.1306149917654693 Adapter cache time: 0.026183954440057278 Engine time: 0.12898704037070274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 150.77725478634238,
    "estimated_duration": 3600.018328262287,
    "input_throughput": 8267.272632016335,
    "output_throughput": 7284.204581441078,
    "total_throughput": 15551.477213457412,
    "itl": 111.08808827599553,
    "ttft": 1821584.1735383666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7114290424180235,
    "arrivals": 632045,
    "finished_requests": 120190,
    "scheduler_time": 259.3403302935541
}
#Debug simulation 
Total elapsed time: 150.77739694993943. Arrivals time: 0.8999785012565553 Scheduler time: 149.56427846802399 Scheduler overhead time: 0.12684194883331656 Adapter cache time: 0.02445551985874772 Engine time: 0.12329305056482553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 165.78615851607174,
    "estimated_duration": 3600.033071773329,
    "input_throughput": 8259.791065017265,
    "output_throughput": 7288.863040103809,
    "total_throughput": 15548.654105121073,
    "itl": 111.18026728717527,
    "ttft": 1820759.0912712682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7639575009234281,
    "arrivals": 632045,
    "finished_requests": 120212,
    "scheduler_time": 259.0920537143109
}
#Debug simulation 
Total elapsed time: 165.78630237607285. Arrivals time: 0.8862217548303306 Scheduler time: 164.58588773244992 Scheduler overhead time: 0.12783068278804421 Adapter cache time: 0.023920759558677673 Engine time: 0.12457440374419093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 117.96847314480692,
    "estimated_duration": 3600.0432582651024,
    "input_throughput": 8267.010384298108,
    "output_throughput": 7287.169380470859,
    "total_throughput": 15554.179764768967,
    "itl": 111.11685345523296,
    "ttft": 1821678.949443541,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6697712660580882,
    "arrivals": 632045,
    "finished_requests": 120225,
    "scheduler_time": 258.9934202437254
}
#Debug simulation 
Total elapsed time: 117.9686056417413. Arrivals time: 0.6806797962635756 Scheduler time: 117.06513297976926 Scheduler overhead time: 0.08842261414974928 Adapter cache time: 0.016589300241321325 Engine time: 0.08658363949507475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 176.3047348689288,
    "estimated_duration": 3600.043387197101,
    "input_throughput": 8259.767397734418,
    "output_throughput": 7288.842154880219,
    "total_throughput": 15548.609552614638,
    "itl": 111.1803818030865,
    "ttft": 1820763.581346881,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.773892050050202,
    "arrivals": 632045,
    "finished_requests": 120212,
    "scheduler_time": 259.09223532845624
}
#Debug simulation 
Total elapsed time: 176.30487287882715. Arrivals time: 0.8935697954148054 Scheduler time: 175.06029249960557 Scheduler overhead time: 0.14390196138992906 Adapter cache time: 0.02835887996479869 Engine time: 0.1382324192672968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 150.43415898410603,
    "estimated_duration": 3600.0078678354807,
    "input_throughput": 8215.009268238502,
    "output_throughput": 7267.849671598468,
    "total_throughput": 15482.85893983697,
    "itl": 111.0744938098835,
    "ttft": 1817224.836558948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8140897398861114,
    "arrivals": 629108,
    "finished_requests": 119736,
    "scheduler_time": 260.16097816738915
}
#Debug simulation 
Total elapsed time: 150.43431222671643. Arrivals time: 0.9777356223203242 Scheduler time: 149.14487921679392 Scheduler overhead time: 0.12409675447270274 Adapter cache time: 0.024848629254847765 Engine time: 0.12506392505019903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 151.7338231881149,
    "estimated_duration": 3600.1139247120686,
    "input_throughput": 8192.265749578692,
    "output_throughput": 7253.294908462779,
    "total_throughput": 15445.56065804147,
    "itl": 111.23747606341553,
    "ttft": 1820908.7319087791,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8782792819663924,
    "arrivals": 629108,
    "finished_requests": 119442,
    "scheduler_time": 260.68429422465977
}
#Debug simulation 
Total elapsed time: 151.73399393679574. Arrivals time: 0.8901010905392468 Scheduler time: 150.5290518128313 Scheduler overhead time: 0.12722548795863986 Adapter cache time: 0.02492727292701602 Engine time: 0.12518820632249117 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 150.30533422576264,
    "estimated_duration": 3600.1144927876803,
    "input_throughput": 8192.264456890254,
    "output_throughput": 7253.293763938084,
    "total_throughput": 15445.558220828338,
    "itl": 111.23753722698791,
    "ttft": 1820908.4670768655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8797483668290123,
    "arrivals": 629108,
    "finished_requests": 119442,
    "scheduler_time": 260.68419352404646
}
#Debug simulation 
Total elapsed time: 150.30547420401126. Arrivals time: 0.9976729578338563 Scheduler time: 149.0005673407577 Scheduler overhead time: 0.12453258177265525 Adapter cache time: 0.024675885681062937 Engine time: 0.12088599940761924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 146.3035461907275,
    "estimated_duration": 3600.124938751452,
    "input_throughput": 8214.908788764622,
    "output_throughput": 7267.698606336165,
    "total_throughput": 15482.607395100787,
    "itl": 111.07390374771165,
    "ttft": 1817231.1481370854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8309011711482914,
    "arrivals": 629108,
    "finished_requests": 119740,
    "scheduler_time": 260.16580084641174
}
#Debug simulation 
Total elapsed time: 146.30370149901137. Arrivals time: 0.9595543113537133 Scheduler time: 145.04596431273967 Scheduler overhead time: 0.12051545875146985 Adapter cache time: 0.023730984423309565 Engine time: 0.11754389246925712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 150.92105852393433,
    "estimated_duration": 3600.1270584245094,
    "input_throughput": 8192.23586317167,
    "output_throughput": 7253.268447538476,
    "total_throughput": 15445.504310710146,
    "itl": 111.23759772564678,
    "ttft": 1820914.284892018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8915692227520092,
    "arrivals": 629108,
    "finished_requests": 119442,
    "scheduler_time": 260.6844397453621
}
#Debug simulation 
Total elapsed time: 150.92122290283442. Arrivals time: 0.9019644744694233 Scheduler time: 149.70797393657267 Scheduler overhead time: 0.12500404519960284 Adapter cache time: 0.0248117595911026 Engine time: 0.12357928697019815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 148.21912757027894,
    "estimated_duration": 3600.115129480049,
    "input_throughput": 8214.931172012646,
    "output_throughput": 7267.718408710684,
    "total_throughput": 15482.649580723331,
    "itl": 111.07420902383566,
    "ttft": 1817235.8468523417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7953533784439798,
    "arrivals": 629108,
    "finished_requests": 119740,
    "scheduler_time": 260.16953956285175
}
#Debug simulation 
Total elapsed time: 148.21927714906633. Arrivals time: 0.8397477497346699 Scheduler time: 147.0862698070705 Scheduler overhead time: 0.11638381611555815 Adapter cache time: 0.022885070648044348 Engine time: 0.11693425523117185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 149.91997029585764,
    "estimated_duration": 3600.0070600495155,
    "input_throughput": 8192.301433873952,
    "output_throughput": 7253.42549734912,
    "total_throughput": 15445.726931223073,
    "itl": 111.23738883385391,
    "ttft": 1820892.7543250485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9025098021701021,
    "arrivals": 629108,
    "finished_requests": 119440,
    "scheduler_time": 260.67563921473277
}
#Debug simulation 
Total elapsed time: 149.9201663560234. Arrivals time: 0.8888274356722832 Scheduler time: 148.71886086976156 Scheduler overhead time: 0.12529679527506232 Adapter cache time: 0.02538266172632575 Engine time: 0.12312291376292706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 152.78317543910816,
    "estimated_duration": 3600.044339726979,
    "input_throughput": 8244.959005769091,
    "output_throughput": 7315.028514898001,
    "total_throughput": 15559.987520667093,
    "itl": 111.58820257610529,
    "ttft": 1814957.0887187603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7987873011664477,
    "arrivals": 627971,
    "finished_requests": 120363,
    "scheduler_time": 257.49911525663936
}
#Debug simulation 
Total elapsed time: 152.78334572119638. Arrivals time: 0.8876492865383625 Scheduler time: 151.59187586046755 Scheduler overhead time: 0.12243019277229905 Adapter cache time: 0.0235925423912704 Engine time: 0.12084975140169263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 158.71334098884836,
    "estimated_duration": 3600.0954996887763,
    "input_throughput": 8164.880626789235,
    "output_throughput": 7241.440401304273,
    "total_throughput": 15406.321028093509,
    "itl": 111.39430686492922,
    "ttft": 1822313.0192115628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.836139865585141,
    "arrivals": 627971,
    "finished_requests": 119140,
    "scheduler_time": 261.6222145552651
}
#Debug simulation 
Total elapsed time: 158.71348860720173. Arrivals time: 0.8516295682638884 Scheduler time: 157.54074831213802 Scheduler overhead time: 0.13032027520239353 Adapter cache time: 0.026258768048137426 Engine time: 0.12634266493842006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 156.2539839898236,
    "estimated_duration": 3600.096426260693,
    "input_throughput": 8164.8785253596625,
    "output_throughput": 7241.438537544385,
    "total_throughput": 15406.317062904049,
    "itl": 111.39427808100558,
    "ttft": 1822313.064249355,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8374846418388229,
    "arrivals": 627971,
    "finished_requests": 119140,
    "scheduler_time": 261.62229654382315
}
#Debug simulation 
Total elapsed time: 156.25411563087255. Arrivals time: 0.8315351852215827 Scheduler time: 155.12064694566652 Scheduler overhead time: 0.1210035658441484 Adapter cache time: 0.02340128645300865 Engine time: 0.12080840021371841 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 154.18555002985522,
    "estimated_duration": 3600.063146022139,
    "input_throughput": 8244.915935098841,
    "output_throughput": 7314.990302072344,
    "total_throughput": 15559.906237171186,
    "itl": 111.58849048046832,
    "ttft": 1814966.4681111027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8163595149922207,
    "arrivals": 627971,
    "finished_requests": 120363,
    "scheduler_time": 257.49935473672355
}
#Debug simulation 
Total elapsed time: 154.1856810930185. Arrivals time: 0.8863712684251368 Scheduler time: 152.9882942410186 Scheduler overhead time: 0.123338065110147 Adapter cache time: 0.025571045465767384 Engine time: 0.12505055405199528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 156.0417848401703,
    "estimated_duration": 3600.1083213150987,
    "input_throughput": 8164.85154792854,
    "output_throughput": 7241.414611235038,
    "total_throughput": 15406.266159163577,
    "itl": 111.39449171499027,
    "ttft": 1822318.3770107063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8480479598976712,
    "arrivals": 627971,
    "finished_requests": 119140,
    "scheduler_time": 261.62242781721164
}
#Debug simulation 
Total elapsed time: 156.04192468617111. Arrivals time: 0.8615714660845697 Scheduler time: 154.87287626834586 Scheduler overhead time: 0.12518502352759242 Adapter cache time: 0.024379095993936062 Engine time: 0.12108635809272528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 154.5895579168573,
    "estimated_duration": 3600.027275671601,
    "input_throughput": 8244.998086705511,
    "output_throughput": 7315.063187982985,
    "total_throughput": 15560.061274688494,
    "itl": 111.58793830193024,
    "ttft": 1814950.5348202758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7804031269694689,
    "arrivals": 627971,
    "finished_requests": 120363,
    "scheduler_time": 257.4990348352911
}
#Debug simulation 
Total elapsed time: 154.58969481801614. Arrivals time: 0.8831457868218422 Scheduler time: 153.39265464385971 Scheduler overhead time: 0.1272879345342517 Adapter cache time: 0.02443087473511696 Engine time: 0.12477472424507141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 155.62563509354368,
    "estimated_duration": 3600.1207387834224,
    "input_throughput": 8164.823385877092,
    "output_throughput": 7241.389634284797,
    "total_throughput": 15406.21302016189,
    "itl": 111.39462934838704,
    "ttft": 1822324.1316872127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8592400468885938,
    "arrivals": 627971,
    "finished_requests": 119140,
    "scheduler_time": 261.62265281274847
}
#Debug simulation 
Total elapsed time: 155.62581703066826. Arrivals time: 0.8400251572020352 Scheduler time: 154.47913920041174 Scheduler overhead time: 0.1237184596247971 Adapter cache time: 0.02347548631951213 Engine time: 0.12167999986559153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 154.90094500314444,
    "estimated_duration": 3600.044523512743,
    "input_throughput": 8130.739441921902,
    "output_throughput": 7196.508496156185,
    "total_throughput": 15327.247938078086,
    "itl": 111.23553109992947,
    "ttft": 1825388.859288934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.599855597810819,
    "arrivals": 627360,
    "finished_requests": 118684,
    "scheduler_time": 264.1689851182187
}
#Debug simulation 
Total elapsed time: 154.90107568819076. Arrivals time: 0.9450889462605119 Scheduler time: 153.65632129227743 Scheduler overhead time: 0.12085132906213403 Adapter cache time: 0.022780250757932663 Engine time: 0.11915448820218444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 112.04247329197824,
    "estimated_duration": 3600.053744366735,
    "input_throughput": 8216.535668748258,
    "output_throughput": 7269.840357509367,
    "total_throughput": 15486.376026257625,
    "itl": 111.20739179239851,
    "ttft": 1816374.638239125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7990334760909936,
    "arrivals": 627360,
    "finished_requests": 119873,
    "scheduler_time": 260.0966934314682
}
#Debug simulation 
Total elapsed time: 112.04260730510578. Arrivals time: 0.6313274712301791 Scheduler time: 111.20141857815906 Scheduler overhead time: 0.08260557148605585 Adapter cache time: 0.016014262568205595 Engine time: 0.08106700703501701 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 162.06290278909728,
    "estimated_duration": 3600.0544606120557,
    "input_throughput": 8216.534034035425,
    "output_throughput": 7269.838911145376,
    "total_throughput": 15486.372945180801,
    "itl": 111.20738948338516,
    "ttft": 1816374.871604868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7999506731703916,
    "arrivals": 627360,
    "finished_requests": 119873,
    "scheduler_time": 260.09669255685907
}
#Debug simulation 
Total elapsed time: 162.06304591940716. Arrivals time: 0.8781929966062307 Scheduler time: 160.85819420590997 Scheduler overhead time: 0.13080021319910884 Adapter cache time: 0.025686667766422033 Engine time: 0.13061146577820182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 154.27614092174917,
    "estimated_duration": 3600.0172021434823,
    "input_throughput": 8216.619071261055,
    "output_throughput": 7269.914150526021,
    "total_throughput": 15486.533221787076,
    "itl": 111.20667049573326,
    "ttft": 1816360.6272300906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7622598974313609,
    "arrivals": 627360,
    "finished_requests": 119873,
    "scheduler_time": 260.09632455538036
}
#Debug simulation 
Total elapsed time: 154.2762885936536. Arrivals time: 0.9889363572001457 Scheduler time: 152.96341764042154 Scheduler overhead time: 0.12981235655024648 Adapter cache time: 0.026562989689409733 Engine time: 0.12801652029156685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 153.78678288124502,
    "estimated_duration": 3600.01977461447,
    "input_throughput": 8198.8735751211,
    "output_throughput": 7260.14595372577,
    "total_throughput": 15459.019528846871,
    "itl": 111.4653319581239,
    "ttft": 1819333.693982835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7933129258826409,
    "arrivals": 627360,
    "finished_requests": 119650,
    "scheduler_time": 260.63063726099364
}
#Debug simulation 
Total elapsed time: 153.78691584989429. Arrivals time: 0.8753554704599082 Scheduler time: 152.60661585768685 Scheduler overhead time: 0.12272627465426922 Adapter cache time: 0.02512412890791893 Engine time: 0.11991855828091502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 167.5334198488854,
    "estimated_duration": 3600.030702717534,
    "input_throughput": 8130.770656457,
    "output_throughput": 7196.53612410671,
    "total_throughput": 15327.306780563711,
    "itl": 111.23536276286688,
    "ttft": 1825382.9768290771,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5860498578008272,
    "arrivals": 627360,
    "finished_requests": 118684,
    "scheduler_time": 264.1688700244081
}
#Debug simulation 
Total elapsed time: 167.53356734104455. Arrivals time: 0.8750545512884855 Scheduler time: 166.33571728598326 Scheduler overhead time: 0.13086610054597259 Adapter cache time: 0.024917189963161945 Engine time: 0.12803294835612178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 153.99161801394075,
    "estimated_duration": 3600.029465647313,
    "input_throughput": 8198.851504314778,
    "output_throughput": 7260.126409909933,
    "total_throughput": 15458.977914224712,
    "itl": 111.46551814366128,
    "ttft": 1819337.5424801675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 239,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8028702136501704,
    "arrivals": 627360,
    "finished_requests": 119650,
    "scheduler_time": 260.6306709674969
}
#Debug simulation 
Total elapsed time: 153.9917721101083. Arrivals time: 0.9115544627420604 Scheduler time: 152.76664674840868 Scheduler overhead time: 0.12544693006202579 Adapter cache time: 0.025346608832478523 Engine time: 0.12432596227154136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 160.39434923697263,
    "estimated_duration": 3600.119872833898,
    "input_throughput": 8264.190374466649,
    "output_throughput": 7294.52155139714,
    "total_throughput": 15558.711925863789,
    "itl": 111.66578948549017,
    "ttft": 1817942.228055451,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8110292521421787,
    "arrivals": 625657,
    "finished_requests": 120090,
    "scheduler_time": 258.7493517622728
}
#Debug simulation 
Total elapsed time: 160.3945018579252. Arrivals time: 0.9492092998698354 Scheduler time: 159.12696917448193 Scheduler overhead time: 0.12974306754767895 Adapter cache time: 0.025536343455314636 Engine time: 0.12607461959123611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 158.8997474592179,
    "estimated_duration": 3600.051192513768,
    "input_throughput": 8264.054428411082,
    "output_throughput": 7294.353217700687,
    "total_throughput": 15558.40764611177,
    "itl": 111.6663483834575,
    "ttft": 1817924.8945441467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8642761041945819,
    "arrivals": 625657,
    "finished_requests": 120086,
    "scheduler_time": 258.7413697181081
}
#Debug simulation 
Total elapsed time: 158.89989154506475. Arrivals time: 0.8604910355061293 Scheduler time: 157.73282536817715 Scheduler overhead time: 0.1231064205057919 Adapter cache time: 0.02404259843751788 Engine time: 0.12189086293801665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 155.25180414691567,
    "estimated_duration": 3600.05280406271,
    "input_throughput": 8264.050729040851,
    "output_throughput": 7294.349952413246,
    "total_throughput": 15558.400681454097,
    "itl": 111.66639351700817,
    "ttft": 1817925.3847845518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8658880539611025,
    "arrivals": 625657,
    "finished_requests": 120086,
    "scheduler_time": 258.74136931727435
}
#Debug simulation 
Total elapsed time: 155.25194519665092. Arrivals time: 1.0206570415757596 Scheduler time: 153.9134127162397 Scheduler overhead time: 0.12683332432061434 Adapter cache time: 0.0265910173766315 Engine time: 0.12593443039804697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 159.89561284845695,
    "estimated_duration": 3600.011378127744,
    "input_throughput": 8264.145824859197,
    "output_throughput": 7294.4338897220505,
    "total_throughput": 15558.579714581247,
    "itl": 111.66568071177662,
    "ttft": 1817908.1305377851,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8266853348980681,
    "arrivals": 625657,
    "finished_requests": 120086,
    "scheduler_time": 258.740846757235
}
#Debug simulation 
Total elapsed time: 159.89575470611453. Arrivals time: 1.0097475619986653 Scheduler time: 158.55413460126147 Scheduler overhead time: 0.1349727506749332 Adapter cache time: 0.02735981671139598 Engine time: 0.13060669926926494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 160.03856395743787,
    "estimated_duration": 3600.063801917678,
    "input_throughput": 8264.025483146232,
    "output_throughput": 7294.327668862932,
    "total_throughput": 15558.353152009164,
    "itl": 111.66649635091143,
    "ttft": 1817929.5377701083,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8774574023112699,
    "arrivals": 625657,
    "finished_requests": 120086,
    "scheduler_time": 258.7414980939648
}
#Debug simulation 
Total elapsed time: 160.038709460292. Arrivals time: 0.8995601506903768 Scheduler time: 158.81438355939463 Scheduler overhead time: 0.13118245033547282 Adapter cache time: 0.026063526049256325 Engine time: 0.12855342775583267 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 159.8375041638501,
    "estimated_duration": 3600.0997552878457,
    "input_throughput": 8264.236555195448,
    "output_throughput": 7294.562313565751,
    "total_throughput": 15558.798868761198,
    "itl": 111.66548268421191,
    "ttft": 1817934.0093968343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7923633281490776,
    "arrivals": 625657,
    "finished_requests": 120090,
    "scheduler_time": 258.74910060313164
}
#Debug simulation 
Total elapsed time: 159.8376521631144. Arrivals time: 0.9504656079225242 Scheduler time: 158.57383280154318 Scheduler overhead time: 0.12489222269505262 Adapter cache time: 0.026638480369001627 Engine time: 0.12387207755818963 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_160_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 154.25320661487058,
    "estimated_duration": 3600.0751954300954,
    "input_throughput": 8263.999329171149,
    "output_throughput": 7294.304583785994,
    "total_throughput": 15558.303912957143,
    "itl": 111.66687478803756,
    "ttft": 1817934.0502840686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.888020720370118,
    "arrivals": 625657,
    "finished_requests": 120086,
    "scheduler_time": 258.7419281340113
}
#Debug simulation 
Total elapsed time: 154.25334380287677. Arrivals time: 1.0028188321739435 Scheduler time: 152.94540904462337 Scheduler overhead time: 0.12210518773645163 Adapter cache time: 0.025217704009264708 Engine time: 0.12030633725225925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 156.69612409407273,
    "estimated_duration": 3600.097002235027,
    "input_throughput": 8222.444556805724,
    "output_throughput": 7323.029347162821,
    "total_throughput": 15545.473903968545,
    "itl": 111.7833196147439,
    "ttft": 1815688.237821321,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7865453501907167,
    "arrivals": 625114,
    "finished_requests": 120030,
    "scheduler_time": 256.88404667556176
}
#Debug simulation 
Total elapsed time: 156.69626523321494. Arrivals time: 0.9115248844027519 Scheduler time: 155.46957189124078 Scheduler overhead time: 0.1254271874204278 Adapter cache time: 0.025649940595030785 Engine time: 0.1254107104614377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 157.08606509724632,
    "estimated_duration": 3600.020014723972,
    "input_throughput": 8222.361508806667,
    "output_throughput": 7323.1490081094435,
    "total_throughput": 15545.51051691611,
    "itl": 111.78459980120938,
    "ttft": 1815643.7253627074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8383127252431624,
    "arrivals": 625114,
    "finished_requests": 120028,
    "scheduler_time": 256.8755294801836
}
#Debug simulation 
Total elapsed time: 157.08628998091444. Arrivals time: 0.9144666232168674 Scheduler time: 155.86051387107 Scheduler overhead time: 0.12520514708012342 Adapter cache time: 0.025157976895570755 Engine time: 0.12333745416253805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 153.14104598388076,
    "estimated_duration": 3600.0216583449064,
    "input_throughput": 8222.357754816612,
    "output_throughput": 7323.145664662609,
    "total_throughput": 15545.50341947922,
    "itl": 111.78463147529327,
    "ttft": 1815644.2430413133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8398536656983238,
    "arrivals": 625114,
    "finished_requests": 120028,
    "scheduler_time": 256.87553212207365
}
#Debug simulation 
Total elapsed time: 153.1411932320334. Arrivals time: 0.8732182532548904 Scheduler time: 151.96644289791584 Scheduler overhead time: 0.12027092091739178 Adapter cache time: 0.025236938148736954 Engine time: 0.11939146183431149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 158.17987622180954,
    "estimated_duration": 3600.1118733983044,
    "input_throughput": 8222.41059194023,
    "output_throughput": 7322.999097556994,
    "total_throughput": 15545.409689497223,
    "itl": 111.78360564335895,
    "ttft": 1815693.8056042043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8023563372204101,
    "arrivals": 625114,
    "finished_requests": 120030,
    "scheduler_time": 256.8841072375784
}
#Debug simulation 
Total elapsed time: 158.18002230580896. Arrivals time: 0.8637171075679362 Scheduler time: 157.0017780852504 Scheduler overhead time: 0.12436194671317935 Adapter cache time: 0.02669618744403124 Engine time: 0.12518123909831047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 155.92060997523367,
    "estimated_duration": 3600.0327270160656,
    "input_throughput": 8222.332474331393,
    "output_throughput": 7323.123148897515,
    "total_throughput": 15545.455623228909,
    "itl": 111.78483830225815,
    "ttft": 1815648.62215695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8509199989028314,
    "arrivals": 625114,
    "finished_requests": 120028,
    "scheduler_time": 256.87563449861784
}
#Debug simulation 
Total elapsed time: 155.92074368381873. Arrivals time: 0.8789608385413885 Scheduler time: 154.730981182307 Scheduler overhead time: 0.12507730768993497 Adapter cache time: 0.02538225520402193 Engine time: 0.12247279798611999 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 113.98802360286936,
    "estimated_duration": 3600.0764559298996,
    "input_throughput": 8222.49148382431,
    "output_throughput": 7323.07114105172,
    "total_throughput": 15545.562624876029,
    "itl": 111.78236239247335,
    "ttft": 1815679.9023960163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7684429257898602,
    "arrivals": 625114,
    "finished_requests": 120030,
    "scheduler_time": 256.88320341207674
}
#Debug simulation 
Total elapsed time: 113.98815354611725. Arrivals time: 0.6218334282748401 Scheduler time: 113.15819603996351 Scheduler overhead time: 0.08139279345050454 Adapter cache time: 0.016259273048490286 Engine time: 0.08075897162780166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 160.8330839187838,
    "estimated_duration": 3600.0179424890666,
    "input_throughput": 8240.828649728348,
    "output_throughput": 7328.639862766497,
    "total_throughput": 15569.468512494845,
    "itl": 111.66492849217578,
    "ttft": 1821053.8650211683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8442879638075865,
    "arrivals": 625114,
    "finished_requests": 120259,
    "scheduler_time": 256.5339895999981
}
#Debug simulation 
Total elapsed time: 160.83322359900922. Arrivals time: 0.8571599274873734 Scheduler time: 159.66343330219388 Scheduler overhead time: 0.12559755984693766 Adapter cache time: 0.02571668801829219 Engine time: 0.12334768241271377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 149.60769531689584,
    "estimated_duration": 3600.1014386173033,
    "input_throughput": 8261.666096670704,
    "output_throughput": 7346.140782676803,
    "total_throughput": 15607.806879347507,
    "itl": 112.14777244377015,
    "ttft": 1809644.8299884235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7926663256785822,
    "arrivals": 623927,
    "finished_requests": 120302,
    "scheduler_time": 255.60602024643873
}
#Debug simulation 
Total elapsed time: 149.6078457031399. Arrivals time: 0.8753644274547696 Scheduler time: 148.4298584666103 Scheduler overhead time: 0.1201855936087668 Adapter cache time: 0.024933294858783484 Engine time: 0.12181728100404143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 149.5320837632753,
    "estimated_duration": 3600.0227848540976,
    "input_throughput": 8261.696044016955,
    "output_throughput": 7345.922951171485,
    "total_throughput": 15607.61899518844,
    "itl": 112.14875917660942,
    "ttft": 1809646.4646599717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8447014211514071,
    "arrivals": 623927,
    "finished_requests": 120298,
    "scheduler_time": 255.59758053459967
}
#Debug simulation 
Total elapsed time: 149.53223704406992. Arrivals time: 0.8883912628516555 Scheduler time: 148.33436112618074 Scheduler overhead time: 0.12191808456555009 Adapter cache time: 0.025752795860171318 Engine time: 0.12420349102467299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 149.42857281304896,
    "estimated_duration": 3600.0243667951045,
    "input_throughput": 8261.692413620483,
    "output_throughput": 7345.919723188681,
    "total_throughput": 15607.612136809164,
    "itl": 112.14880312641097,
    "ttft": 1809646.9933139624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8462779508903664,
    "arrivals": 623927,
    "finished_requests": 120298,
    "scheduler_time": 255.5975859458549
}
#Debug simulation 
Total elapsed time: 149.42874655965716. Arrivals time: 0.8642534697428346 Scheduler time: 148.25873377826065 Scheduler overhead time: 0.1232790038920939 Adapter cache time: 0.024852476082742214 Engine time: 0.11959891114383936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 149.1743069528602,
    "estimated_duration": 3600.118775595266,
    "input_throughput": 8261.62631122695,
    "output_throughput": 7346.1054060993065,
    "total_throughput": 15607.731717326256,
    "itl": 112.14814915173552,
    "ttft": 1809651.5519125974,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8111966050392969,
    "arrivals": 623927,
    "finished_requests": 120302,
    "scheduler_time": 255.60612744655216
}
#Debug simulation 
Total elapsed time: 149.17445584991947. Arrivals time: 0.8832444488070905 Scheduler time: 147.9908334389329 Scheduler overhead time: 0.11981896916404366 Adapter cache time: 0.024015942122787237 Engine time: 0.12040806235745549 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 147.64407022483647,
    "estimated_duration": 3600.0839314874866,
    "input_throughput": 8287.433173168425,
    "output_throughput": 7374.286684763721,
    "total_throughput": 15661.719857932147,
    "itl": 112.18490260039508,
    "ttft": 1811070.9591283281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8931437100283846,
    "arrivals": 623927,
    "finished_requests": 120680,
    "scheduler_time": 254.27641076295686
}
#Debug simulation 
Total elapsed time: 147.64421691000462. Arrivals time: 0.8758403779938817 Scheduler time: 146.46450922638178 Scheduler overhead time: 0.12101752730086446 Adapter cache time: 0.02426766837015748 Engine time: 0.1212405851110816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 149.32227885071188,
    "estimated_duration": 3600.082698083122,
    "input_throughput": 8261.709103470508,
    "output_throughput": 7346.179023632354,
    "total_throughput": 15607.888127102862,
    "itl": 112.14742812153861,
    "ttft": 1809637.5183396586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7744230263796645,
    "arrivals": 623927,
    "finished_requests": 120302,
    "scheduler_time": 255.6058231272536
}
#Debug simulation 
Total elapsed time: 149.32242628512904. Arrivals time: 0.891773484647274 Scheduler time: 148.12426309660077 Scheduler overhead time: 0.12227661302313209 Adapter cache time: 0.024961015675216913 Engine time: 0.12066490389406681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_160_slots_16_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 146.71289077261463,
    "estimated_duration": 3600.0951654068326,
    "input_throughput": 8287.407312642084,
    "output_throughput": 7374.263673665946,
    "total_throughput": 15661.67098630803,
    "itl": 112.18509747012085,
    "ttft": 1811075.0500118185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9050903197377964,
    "arrivals": 623927,
    "finished_requests": 120680,
    "scheduler_time": 254.27649838124955
}
#Debug simulation 
Total elapsed time: 146.71302709402516. Arrivals time: 0.8636289210990071 Scheduler time: 145.54810671461746 Scheduler overhead time: 0.12087115552276373 Adapter cache time: 0.024273266550153494 Engine time: 0.11896519968286157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_160_slots_16_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_160_slots_16_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 150.74813941307366,
    "estimated_duration": 3600.0951285656092,
    "input_throughput": 7678.985974744136,
    "output_throughput": 6818.934256823657,
    "total_throughput": 14497.920231567792,
    "itl": 100.82601966905894,
    "ttft": 1857921.3563728868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.089533636840062,
    "arrivals": 540089,
    "finished_requests": 111797,
    "scheduler_time": 279.8402934664799
}
#Debug simulation 
Total elapsed time: 150.7482823380269. Arrivals time: 0.9175604693591595 Scheduler time: 149.499599813018 Scheduler overhead time: 0.13187737949192524 Adapter cache time: 0.027563568204641342 Engine time: 0.13002681685611606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_160_slots_16_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_160_slots_16_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 150.43614846328273,
    "estimated_duration": 3600.0497564862867,
    "input_throughput": 7650.197042521046,
    "output_throughput": 6786.652866666585,
    "total_throughput": 14436.849909187631,
    "itl": 99.7361792152383,
    "ttft": 1864062.6431839934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1243184890272149,
    "arrivals": 540089,
    "finished_requests": 111425,
    "scheduler_time": 280.0437133134382
}
#Debug simulation 
Total elapsed time: 150.4362872550264. Arrivals time: 0.9308391101658344 Scheduler time: 149.1716362084262 Scheduler overhead time: 0.13483540946617723 Adapter cache time: 0.028039739932864904 Engine time: 0.1302606943063438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_160_slots_16_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_160_slots_16_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 150.78805296402425,
    "estimated_duration": 3600.0520420538905,
    "input_throughput": 7650.192185635001,
    "output_throughput": 6786.648558019447,
    "total_throughput": 14436.840743654448,
    "itl": 99.73623324607642,
    "ttft": 1864063.5544453354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1265691840834977,
    "arrivals": 540089,
    "finished_requests": 111425,
    "scheduler_time": 280.0437481859717
}
#Debug simulation 
Total elapsed time: 150.78825627313927. Arrivals time: 0.9260134319774806 Scheduler time: 149.5244946521707 Scheduler overhead time: 0.1364332172088325 Adapter cache time: 0.028213089797645807 Engine time: 0.13262443337589502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_160_slots_16_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_160_slots_16_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 150.16323927696794,
    "estimated_duration": 3600.0099051955817,
    "input_throughput": 7679.0222049397735,
    "output_throughput": 6818.9995712432155,
    "total_throughput": 14498.021776182988,
    "itl": 100.82723761430562,
    "ttft": 1857911.7029370798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.113489343198018,
    "arrivals": 540089,
    "finished_requests": 111793,
    "scheduler_time": 279.83234475054155
}
#Debug simulation 
Total elapsed time: 150.1634385078214. Arrivals time: 0.9158628075383604 Scheduler time: 148.91562951076776 Scheduler overhead time: 0.133144062012434 Adapter cache time: 0.026811816729605198 Engine time: 0.13002263149246573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_160_slots_16_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_160_slots_16_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 151.3707617847249,
    "estimated_duration": 3600.0661662938714,
    "input_throughput": 7650.162171422667,
    "output_throughput": 6786.621931771908,
    "total_throughput": 14436.784103194575,
    "itl": 99.73657547454559,
    "ttft": 1864069.628768479,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1406536081619612,
    "arrivals": 540089,
    "finished_requests": 111425,
    "scheduler_time": 280.04388804046863
}
#Debug simulation 
Total elapsed time: 151.37089682277292. Arrivals time: 0.930769921746105 Scheduler time: 150.10938608599827 Scheduler overhead time: 0.13313229195773602 Adapter cache time: 0.027625611051917076 Engine time: 0.12940887734293938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_160_slots_16_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_160_slots_16_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 151.16734622931108,
    "estimated_duration": 3600.0696247128276,
    "input_throughput": 7679.040374727533,
    "output_throughput": 6818.982563971446,
    "total_throughput": 14498.02293869898,
    "itl": 100.82540276658317,
    "ttft": 1857910.5941184552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0644579049851737,
    "arrivals": 540089,
    "finished_requests": 111797,
    "scheduler_time": 279.8399653840813
}
#Debug simulation 
Total elapsed time: 151.16747442120686. Arrivals time: 0.9357393491081893 Scheduler time: 149.90306633990258 Scheduler overhead time: 0.13299120916053653 Adapter cache time: 0.027803280856460333 Engine time: 0.12838375428691506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_160_slots_16_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_160_slots_16_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 150.5844371560961,
    "estimated_duration": 3600.080763060258,
    "input_throughput": 7650.1311533324115,
    "output_throughput": 6786.594414962866,
    "total_throughput": 14436.725568295278,
    "itl": 99.73692350151606,
    "ttft": 1864075.9600673285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1551152935996691,
    "arrivals": 540089,
    "finished_requests": 111425,
    "scheduler_time": 280.04402312143174
}
#Debug simulation 
Total elapsed time: 150.58456384623423. Arrivals time: 0.9221018864773214 Scheduler time: 149.3309125136584 Scheduler overhead time: 0.13504682993516326 Adapter cache time: 0.027212226297706366 Engine time: 0.12872115802019835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_160_slots_16_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_160_slots_16_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 149.98464956041425,
    "estimated_duration": 3600.118461116754,
    "input_throughput": 7729.769811898842,
    "output_throughput": 6846.454989248935,
    "total_throughput": 14576.224801147777,
    "itl": 100.33136481326518,
    "ttft": 1813567.187948396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.071170710376465,
    "arrivals": 483089,
    "finished_requests": 112775,
    "scheduler_time": 276.85303369578264
}
#Debug simulation 
Total elapsed time: 149.98479893943295. Arrivals time: 0.9119375864975154 Scheduler time: 148.75242899218574 Scheduler overhead time: 0.12912817718461156 Adapter cache time: 0.026114940643310547 Engine time: 0.12450386444106698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_160_slots_16_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_160_slots_16_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 113.54974181717262,
    "estimated_duration": 3600.069323485014,
    "input_throughput": 7729.4542131379685,
    "output_throughput": 6846.246498425962,
    "total_throughput": 14575.700711563932,
    "itl": 100.33250335455268,
    "ttft": 1813587.9356320002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1437632890045697,
    "arrivals": 483089,
    "finished_requests": 112770,
    "scheduler_time": 276.8457971248946
}
#Debug simulation 
Total elapsed time: 113.54986532311887. Arrivals time: 0.6602071076631546 Scheduler time: 112.66614408558235 Scheduler overhead time: 0.08846716908738017 Adapter cache time: 0.017956229858100414 Engine time: 0.08493767399340868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_160_slots_16_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_160_slots_16_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 148.7250703847967,
    "estimated_duration": 3600.0710452730327,
    "input_throughput": 7729.450516410464,
    "output_throughput": 6846.243224105804,
    "total_throughput": 14575.693740516268,
    "itl": 100.33253258676574,
    "ttft": 1813588.5862107256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1454965007677738,
    "arrivals": 483089,
    "finished_requests": 112770,
    "scheduler_time": 276.8457857011336
}
#Debug simulation 
Total elapsed time: 148.72521516913548. Arrivals time: 0.8177649765275419 Scheduler time: 147.60850318986923 Scheduler overhead time: 0.12173947598785162 Adapter cache time: 0.024949667509645224 Engine time: 0.11509377602487803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_160_slots_16_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_160_slots_16_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 151.33376507693902,
    "estimated_duration": 3600.021273294743,
    "input_throughput": 7729.557379679897,
    "output_throughput": 6846.337876621234,
    "total_throughput": 14575.89525630113,
    "itl": 100.33136956861495,
    "ttft": 1813568.8049897654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.096774827383927,
    "arrivals": 483089,
    "finished_requests": 112770,
    "scheduler_time": 276.844988010061
}
#Debug simulation 
Total elapsed time: 151.33391648391262. Arrivals time: 0.9256668374873698 Scheduler time: 150.07940905727446 Scheduler overhead time: 0.13276991713792086 Adapter cache time: 0.02773781167343259 Engine time: 0.128303577657789 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_160_slots_16_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_160_slots_16_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 151.95325040677562,
    "estimated_duration": 3600.08596340525,
    "input_throughput": 7729.418486907295,
    "output_throughput": 6846.214854460566,
    "total_throughput": 14575.633341367862,
    "itl": 100.33288974588108,
    "ttft": 1813594.5742895573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.159958186205482,
    "arrivals": 483089,
    "finished_requests": 112770,
    "scheduler_time": 276.8460420707674
}
#Debug simulation 
Total elapsed time: 151.9534466699697. Arrivals time: 0.9295335682108998 Scheduler time: 150.69798754760996 Scheduler overhead time: 0.13063235534355044 Adapter cache time: 0.02725848788395524 Engine time: 0.12925698375329375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_160_slots_16_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_160_slots_16_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 152.0317889363505,
    "estimated_duration": 3600.093535309342,
    "input_throughput": 7729.823330162127,
    "output_throughput": 6846.502391744689,
    "total_throughput": 14576.325721906815,
    "itl": 100.33079302460695,
    "ttft": 1813557.3382228028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0465176032157613,
    "arrivals": 483089,
    "finished_requests": 112775,
    "scheduler_time": 276.85266095689536
}
#Debug simulation 
Total elapsed time: 152.03195681842044. Arrivals time: 0.9131315811537206 Scheduler time: 150.79043449973688 Scheduler overhead time: 0.1327489772811532 Adapter cache time: 0.026893023867160082 Engine time: 0.12841868540272117 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_160_slots_16_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_160_slots_16_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 152.52711645932868,
    "estimated_duration": 3600.102138196242,
    "input_throughput": 7729.3837596346475,
    "output_throughput": 6846.184095307046,
    "total_throughput": 14575.567854941693,
    "itl": 100.33318856595558,
    "ttft": 1813601.2112906252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1754259019345084,
    "arrivals": 483089,
    "finished_requests": 112770,
    "scheduler_time": 276.84643657765264
}
#Debug simulation 
Total elapsed time: 152.52725331904367. Arrivals time: 0.8897903994657099 Scheduler time: 151.31319295987487 Scheduler overhead time: 0.132043928373605 Adapter cache time: 0.02702055126428604 Engine time: 0.12549715023487806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_160_slots_16_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_160_slots_16_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 149.76551036909223,
    "estimated_duration": 3600.0358121092254,
    "input_throughput": 7754.848134036555,
    "output_throughput": 6879.201567022805,
    "total_throughput": 14634.04970105936,
    "itl": 103.66236935994426,
    "ttft": 1789737.7478459375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.071170710376465,
    "arrivals": 473564,
    "finished_requests": 113283,
    "scheduler_time": 274.5122094148673
}
#Debug simulation 
Total elapsed time: 149.76572951395065. Arrivals time: 0.897777714766562 Scheduler time: 148.55342147499323 Scheduler overhead time: 0.1265227971598506 Adapter cache time: 0.025597209110856056 Engine time: 0.12332677328959107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_160_slots_16_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_160_slots_16_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 148.32685724180192,
    "estimated_duration": 3600.1085914965865,
    "input_throughput": 7754.691362905371,
    "output_throughput": 6879.062497863401,
    "total_throughput": 14633.753860768773,
    "itl": 103.66402156480942,
    "ttft": 1789766.8250796192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.142537503049249,
    "arrivals": 473564,
    "finished_requests": 113283,
    "scheduler_time": 274.5134219323676
}
#Debug simulation 
Total elapsed time: 148.3270084648393. Arrivals time: 0.9417905523441732 Scheduler time: 147.05279459850863 Scheduler overhead time: 0.13500555604696274 Adapter cache time: 0.028083242941647768 Engine time: 0.13011730648577213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_160_slots_16_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_160_slots_16_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 151.08380028698593,
    "estimated_duration": 3600.110548335375,
    "input_throughput": 7754.6871478456815,
    "output_throughput": 6879.058758751465,
    "total_throughput": 14633.745906597147,
    "itl": 103.66406284543358,
    "ttft": 1789767.6080568542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1444847582839492,
    "arrivals": 473564,
    "finished_requests": 113283,
    "scheduler_time": 274.51343151590595
}
#Debug simulation 
Total elapsed time: 151.0839564888738. Arrivals time: 0.9509615092538297 Scheduler time: 149.7937649646774 Scheduler overhead time: 0.13680970622226596 Adapter cache time: 0.027615965344011784 Engine time: 0.13353839423507452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_160_slots_16_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_160_slots_16_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 150.53403274109587,
    "estimated_duration": 3600.059098441704,
    "input_throughput": 7754.797973201126,
    "output_throughput": 6879.157070149144,
    "total_throughput": 14633.95504335027,
    "itl": 103.66294587537179,
    "ttft": 1789746.7843399348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0947318507917247,
    "arrivals": 473564,
    "finished_requests": 113283,
    "scheduler_time": 274.5126348769531
}
#Debug simulation 
Total elapsed time: 150.53417125111446. Arrivals time: 0.9530663564801216 Scheduler time: 149.2388933384791 Scheduler overhead time: 0.1384984916076064 Adapter cache time: 0.02899670135229826 Engine time: 0.1342936852015555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_160_slots_16_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_160_slots_16_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 151.43359202798456,
    "estimated_duration": 3600.0005945241296,
    "input_throughput": 7754.752608225682,
    "output_throughput": 6879.136363940956,
    "total_throughput": 14633.888972166638,
    "itl": 103.66388664460489,
    "ttft": 1789696.0449239705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1591979512944868,
    "arrivals": 473564,
    "finished_requests": 113281,
    "scheduler_time": 274.5049006206225
}
#Debug simulation 
Total elapsed time: 151.43372654588893. Arrivals time: 0.9395069680176675 Scheduler time: 150.15945714060217 Scheduler overhead time: 0.13455693470314145 Adapter cache time: 0.028535768389701843 Engine time: 0.13222932210192084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_160_slots_16_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_160_slots_16_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 151.84986465377733,
    "estimated_duration": 3600.0918463134512,
    "input_throughput": 7694.650631862824,
    "output_throughput": 6820.903201440706,
    "total_throughput": 14515.55383330353,
    "itl": 101.83768145023738,
    "ttft": 1785844.7085608006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0465176032157613,
    "arrivals": 473564,
    "finished_requests": 112381,
    "scheduler_time": 277.56809582446516
}
#Debug simulation 
Total elapsed time: 151.85001248307526. Arrivals time: 0.9390663132071495 Scheduler time: 150.57183012133464 Scheduler overhead time: 0.1374200787395239 Adapter cache time: 0.028059223666787148 Engine time: 0.13309272564947605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_160_slots_16_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_160_slots_16_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 151.53413602896035,
    "estimated_duration": 3600.015488451474,
    "input_throughput": 7754.720525385402,
    "output_throughput": 6879.107903686402,
    "total_throughput": 14633.828429071804,
    "itl": 103.66411918955022,
    "ttft": 1789701.8763901372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.174036898091439,
    "arrivals": 473564,
    "finished_requests": 113281,
    "scheduler_time": 274.5051556783425
}
#Debug simulation 
Total elapsed time: 151.5342838689685. Arrivals time: 0.9618353247642517 Scheduler time: 150.236099335365 Scheduler overhead time: 0.13689304934814572 Adapter cache time: 0.026676027569919825 Engine time: 0.13167118420824409 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_160_slots_16_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_160_slots_16_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 160.59837038209662,
    "estimated_duration": 3600.1256563411325,
    "input_throughput": 7710.9464085243435,
    "output_throughput": 6812.470824956129,
    "total_throughput": 14523.417233480472,
    "itl": 103.13767132420234,
    "ttft": 1803962.5270900044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.003839980009942,
    "arrivals": 468761,
    "finished_requests": 112004,
    "scheduler_time": 277.87546455131934
}
#Debug simulation 
Total elapsed time: 160.59851554781199. Arrivals time: 1.0307012717239559 Scheduler time: 159.2282626144588 Scheduler overhead time: 0.13950169878080487 Adapter cache time: 0.028293903451412916 Engine time: 0.1314249006099999 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_160_slots_16_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_160_slots_16_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 164.48504785401747,
    "estimated_duration": 3600.031263914998,
    "input_throughput": 7745.5085680773645,
    "output_throughput": 6839.04838460351,
    "total_throughput": 14584.556952680874,
    "itl": 104.20183567318813,
    "ttft": 1792083.0083943817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0732089217612575,
    "arrivals": 468761,
    "finished_requests": 112512,
    "scheduler_time": 276.62526964558225
}
#Debug simulation 
Total elapsed time: 164.4851805092767. Arrivals time: 0.9361573089845479 Scheduler time: 163.21249194350094 Scheduler overhead time: 0.13644551066681743 Adapter cache time: 0.02696263138204813 Engine time: 0.13180583901703358 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_160_slots_16_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_160_slots_16_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 164.41926867235452,
    "estimated_duration": 3600.033230096214,
    "input_throughput": 7745.50433781823,
    "output_throughput": 6839.044649413413,
    "total_throughput": 14584.548987231643,
    "itl": 104.20188477861994,
    "ttft": 1792083.6462074672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0751749025471566,
    "arrivals": 468761,
    "finished_requests": 112512,
    "scheduler_time": 276.6252698459991
}
#Debug simulation 
Total elapsed time: 164.41940223099664. Arrivals time: 0.9241861021146178 Scheduler time: 163.1533794151619 Scheduler overhead time: 0.1396726556122303 Adapter cache time: 0.027767267543822527 Engine time: 0.1322684409096837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_160_slots_16_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_160_slots_16_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 163.78801213623956,
    "estimated_duration": 3600.11040090417,
    "input_throughput": 7745.500802696703,
    "output_throughput": 6839.01915725039,
    "total_throughput": 14584.519959947093,
    "itl": 104.20044234317977,
    "ttft": 1792093.1814586695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0278548414143762,
    "arrivals": 468761,
    "finished_requests": 112516,
    "scheduler_time": 276.633224451808
}
#Debug simulation 
Total elapsed time: 163.78814426297322. Arrivals time: 0.9243756891228259 Scheduler time: 162.51874726871029 Scheduler overhead time: 0.1422437964938581 Adapter cache time: 0.02738154213875532 Engine time: 0.1338810883462429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_160_slots_16_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_160_slots_16_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 117.66978408116847,
    "estimated_duration": 3600.1248420275315,
    "input_throughput": 7739.152452365686,
    "output_throughput": 6833.898011754519,
    "total_throughput": 14573.050464120206,
    "itl": 104.16266662626995,
    "ttft": 1795473.4509489355,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1297427828237465,
    "arrivals": 468761,
    "finished_requests": 112444,
    "scheduler_time": 276.63403738346057
}
#Debug simulation 
Total elapsed time: 117.66991689382121. Arrivals time: 0.6532006799243391 Scheduler time: 116.78395546926185 Scheduler overhead time: 0.09152293996885419 Adapter cache time: 0.01875082729384303 Engine time: 0.08935699332505465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_160_slots_16_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_160_slots_16_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 119.38005192996934,
    "estimated_duration": 3600.1009150850073,
    "input_throughput": 7710.999401066653,
    "output_throughput": 6812.517642834156,
    "total_throughput": 14523.517043900809,
    "itl": 103.13716691872534,
    "ttft": 1803952.358141276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.980736496727915,
    "arrivals": 468761,
    "finished_requests": 112004,
    "scheduler_time": 277.8750272413868
}
#Debug simulation 
Total elapsed time: 119.38018183084205. Arrivals time: 0.7053699404932559 Scheduler time: 118.44197120936587 Scheduler overhead time: 0.09315376961603761 Adapter cache time: 0.017647210974246264 Engine time: 0.0889919362962246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_160_slots_16_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_160_slots_16_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 164.43384136911482,
    "estimated_duration": 3600.0100415783704,
    "input_throughput": 7738.907858097289,
    "output_throughput": 6833.673438647947,
    "total_throughput": 14572.581296745237,
    "itl": 104.16296974985069,
    "ttft": 1795464.6048486514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1443302220478693,
    "arrivals": 468761,
    "finished_requests": 112436,
    "scheduler_time": 276.6252873401628
}
#Debug simulation 
Total elapsed time: 164.4340614671819. Arrivals time: 0.9619763833470643 Scheduler time: 163.11176194390282 Scheduler overhead time: 0.14774552220478654 Adapter cache time: 0.029443827457726002 Engine time: 0.14035079814493656 
