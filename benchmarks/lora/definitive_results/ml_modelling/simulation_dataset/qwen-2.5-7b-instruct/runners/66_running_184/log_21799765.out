INFO 06-01 00:47:09 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:10 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_128_slots_128_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_128_slots_128_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 270, 270, 17280, 270, 540, 540, 540, 270, 17280, 540, 270, 17280, 540, 270, 270, 270, 270, 540, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 17280, 270, 540, 540, 270, 540, 270, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 270, 540, 17280, 17280, 540, 270, 270, 270, 17280, 540, 540, 540, 540, 17280, 540, 17280, 270, 270, 540, 270, 17280, 17280, 270, 270, 540, 540, 540, 270, 17280, 270, 17280, 540, 270, 17280, 17280, 540, 540, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 540, 17280, 17280, 540, 17280, 270, 540, 17280, 270, 17280, 540, 270, 540, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270]
Prompts retrieved: 777600 . Total input tokens: 173088458 . Total output tokens: 155300705
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.54450990119949,
    "estimated_duration": 3600.069529317909,
    "input_throughput": 6026.328609300638,
    "output_throughput": 5284.821819428785,
    "total_throughput": 11311.150428729423,
    "itl": 132.34137759495562,
    "ttft": 1725507.5086194074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037832,
    "arrivals": 259027,
    "finished_requests": 87356,
    "scheduler_time": 49.191030250329476
}
#Debug simulation 
Total elapsed time: 6.544672531075776. Arrivals time: 0.34934255946427584 Scheduler time: 6.047889169771224 Scheduler overhead time: 0.04231885261833668 Adapter cache time: 0.04117704648524523 Engine time: 0.04432161943987012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.58313298271969,
    "estimated_duration": 3600.025605822015,
    "input_throughput": 6211.468597289068,
    "output_throughput": 5485.512649705397,
    "total_throughput": 11696.981246994465,
    "itl": 156.83176169912696,
    "ttft": 1698500.6744404929,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 257078,
    "finished_requests": 90074,
    "scheduler_time": 56.705064099729626
}
#Debug simulation 
Total elapsed time: 6.583264878951013. Arrivals time: 0.32776784524321556 Scheduler time: 6.130966879427433 Scheduler overhead time: 0.0362338381819427 Adapter cache time: 0.03366858186200261 Engine time: 0.03763262229040265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.715355976019055,
    "estimated_duration": 3600.1248522896703,
    "input_throughput": 6211.4212471764395,
    "output_throughput": 5485.466146386588,
    "total_throughput": 11696.887393563027,
    "itl": 156.8320478492771,
    "ttft": 1698522.0129948265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41786563513334835,
    "arrivals": 257078,
    "finished_requests": 90078,
    "scheduler_time": 56.70660949714926
}
#Debug simulation 
Total elapsed time: 6.71557942731306. Arrivals time: 0.3707508500665426 Scheduler time: 6.217467837035656 Scheduler overhead time: 0.03659359645098448 Adapter cache time: 0.03579001035541296 Engine time: 0.0380189991556108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.866677096113563,
    "estimated_duration": 3600.112561770975,
    "input_throughput": 6109.7903531048805,
    "output_throughput": 5402.6877399722125,
    "total_throughput": 11512.478093077092,
    "itl": 129.29167443817707,
    "ttft": 1714986.3752756258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 257078,
    "finished_requests": 88604,
    "scheduler_time": 50.20649583339187
}
#Debug simulation 
Total elapsed time: 6.866751927882433. Arrivals time: 0.5658234567381442 Scheduler time: 6.155994579195976 Scheduler overhead time: 0.043346259742975235 Adapter cache time: 0.036397540010511875 Engine time: 0.04509295616298914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.664608302991837,
    "estimated_duration": 3600.111651326622,
    "input_throughput": 6211.3209716037345,
    "output_throughput": 5485.387097014885,
    "total_throughput": 11696.708068618618,
    "itl": 156.83264505033176,
    "ttft": 1698532.4596258414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 257078,
    "finished_requests": 90077,
    "scheduler_time": 56.70607058951659
}
#Debug simulation 
Total elapsed time: 6.664786641020328. Arrivals time: 0.3357982258312404 Scheduler time: 6.2048005531542 Scheduler overhead time: 0.03634086111560464 Adapter cache time: 0.0330142704769969 Engine time: 0.03790511004626751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.671734618023038,
    "estimated_duration": 3600.012914588006,
    "input_throughput": 6110.216135854438,
    "output_throughput": 5402.991172943056,
    "total_throughput": 11513.207308797495,
    "itl": 129.28717023815346,
    "ttft": 1714839.8839856177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879447,
    "arrivals": 257078,
    "finished_requests": 88606,
    "scheduler_time": 50.20360172343115
}
#Debug simulation 
Total elapsed time: 6.6719057988375425. Arrivals time: 0.36351351207122207 Scheduler time: 6.1611258909106255 Scheduler overhead time: 0.04330384777858853 Adapter cache time: 0.03887578612193465 Engine time: 0.04490040289238095 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.988454753067344,
    "estimated_duration": 3600.1457999911954,
    "input_throughput": 6211.385105585082,
    "output_throughput": 5485.434228816038,
    "total_throughput": 11696.81933440112,
    "itl": 156.82993871920235,
    "ttft": 1698512.9955152269,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 257078,
    "finished_requests": 90078,
    "scheduler_time": 56.70722570968652
}
#Debug simulation 
Total elapsed time: 6.988547402899712. Arrivals time: 0.3375484882853925 Scheduler time: 6.526414229068905 Scheduler overhead time: 0.036516806576400995 Adapter cache time: 0.03320057736709714 Engine time: 0.03803546633571386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.591300504282117,
    "estimated_duration": 3600.046040650611,
    "input_throughput": 6109.657696495734,
    "output_throughput": 5402.4820183924885,
    "total_throughput": 11512.139714888222,
    "itl": 129.2871530880067,
    "ttft": 1715045.7204741726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 257078,
    "finished_requests": 88601,
    "scheduler_time": 50.20389237007679
}
#Debug simulation 
Total elapsed time: 6.591432605404407. Arrivals time: 0.325741617474705 Scheduler time: 6.118996560573578 Scheduler overhead time: 0.0433665313757956 Adapter cache time: 0.03843768499791622 Engine time: 0.044752723537385464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.893079707864672,
    "estimated_duration": 3600.0126739200614,
    "input_throughput": 6335.602695299416,
    "output_throughput": 5596.967240134614,
    "total_throughput": 11932.56993543403,
    "itl": 153.8564980279607,
    "ttft": 1673952.7875535025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 256097,
    "finished_requests": 91798,
    "scheduler_time": 57.86047390884099
}
#Debug simulation 
Total elapsed time: 6.893187899142504. Arrivals time: 0.37705310387536883 Scheduler time: 6.38942481437698 Scheduler overhead time: 0.03739806078374386 Adapter cache time: 0.03325116541236639 Engine time: 0.03893156396225095 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.79496274003759,
    "estimated_duration": 3600.1420475778536,
    "input_throughput": 6335.389742564531,
    "output_throughput": 5597.0349874269095,
    "total_throughput": 11932.42472999144,
    "itl": 153.8585584994568,
    "ttft": 1673992.1012233433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 256097,
    "finished_requests": 91800,
    "scheduler_time": 57.86162545540988
}
#Debug simulation 
Total elapsed time: 6.795080698095262. Arrivals time: 0.36655937042087317 Scheduler time: 6.305257457308471 Scheduler overhead time: 0.03726206347346306 Adapter cache time: 0.030346836894750595 Engine time: 0.03857014188542962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.802056685090065,
    "estimated_duration": 3600.0328508573243,
    "input_throughput": 6211.908592632531,
    "output_throughput": 5493.653480214814,
    "total_throughput": 11705.562072847346,
    "itl": 127.68432738285706,
    "ttft": 1692216.477423028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 256097,
    "finished_requests": 90035,
    "scheduler_time": 51.20821416111931
}
#Debug simulation 
Total elapsed time: 6.802157714031637. Arrivals time: 0.40500543965026736 Scheduler time: 6.253968140576035 Scheduler overhead time: 0.043799768667668104 Adapter cache time: 0.033346373587846756 Engine time: 0.04570060223340988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.865363210905343,
    "estimated_duration": 3600.1077490779444,
    "input_throughput": 6335.449822534239,
    "output_throughput": 5596.918315892259,
    "total_throughput": 11932.368138426498,
    "itl": 153.85798554056817,
    "ttft": 1673986.645168813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 256097,
    "finished_requests": 91799,
    "scheduler_time": 57.86158498986841
}
#Debug simulation 
Total elapsed time: 6.865487170871347. Arrivals time: 0.36736050993204117 Scheduler time: 6.372554810717702 Scheduler overhead time: 0.03726156847551465 Adapter cache time: 0.03222872456535697 Engine time: 0.038891528733074665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.813422922976315,
    "estimated_duration": 3600.097984517401,
    "input_throughput": 6212.307302796386,
    "output_throughput": 5494.033797152722,
    "total_throughput": 11706.341099949108,
    "itl": 127.69316056116473,
    "ttft": 1692200.5542469816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794475,
    "arrivals": 256097,
    "finished_requests": 90039,
    "scheduler_time": 51.21086673331296
}
#Debug simulation 
Total elapsed time: 6.813583127222955. Arrivals time: 0.4057457819581032 Scheduler time: 6.262968459632248 Scheduler overhead time: 0.04386577708646655 Adapter cache time: 0.034830679185688496 Engine time: 0.045739137567579746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.805952892638743,
    "estimated_duration": 3600.1370637139394,
    "input_throughput": 6335.464621580399,
    "output_throughput": 5597.043291238729,
    "total_throughput": 11932.507912819126,
    "itl": 153.85446416666096,
    "ttft": 1673967.343328087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 256097,
    "finished_requests": 91801,
    "scheduler_time": 57.86287667942711
}
#Debug simulation 
Total elapsed time: 6.806093577761203. Arrivals time: 0.36578745814040303 Scheduler time: 6.314820210449398 Scheduler overhead time: 0.037092122714966536 Adapter cache time: 0.0325597757473588 Engine time: 0.03868545452132821 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_128_slots_128_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.806589525658637,
    "estimated_duration": 3600.058570758126,
    "input_throughput": 6212.375315685555,
    "output_throughput": 5494.0939463200975,
    "total_throughput": 11706.469262005652,
    "itl": 127.68617160263953,
    "ttft": 1692233.9894336157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037832,
    "arrivals": 256097,
    "finished_requests": 90039,
    "scheduler_time": 51.209473782252104
}
#Debug simulation 
Total elapsed time: 6.8067247699946165. Arrivals time: 0.4018498361110687 Scheduler time: 6.261740498244762 Scheduler overhead time: 0.04345224937424064 Adapter cache time: 0.0336242881603539 Engine time: 0.0457781869918108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.886184533126652,
    "estimated_duration": 3600.024220818421,
    "input_throughput": 6407.261058581479,
    "output_throughput": 5640.183719481742,
    "total_throughput": 12047.444778063222,
    "itl": 152.24404849176284,
    "ttft": 1658920.7513742987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 255593,
    "finished_requests": 93093,
    "scheduler_time": 58.263755423332995
}
#Debug simulation 
Total elapsed time: 6.886384422890842. Arrivals time: 0.3724688063375652 Scheduler time: 6.3890520837157965 Scheduler overhead time: 0.037724572233855724 Adapter cache time: 0.03042548382654786 Engine time: 0.03930850885808468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.900902769993991,
    "estimated_duration": 3600.160567818476,
    "input_throughput": 6426.317261182371,
    "output_throughput": 5656.871580130828,
    "total_throughput": 12083.188841313198,
    "itl": 151.7671319053455,
    "ttft": 1656181.8022037172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41786563513334835,
    "arrivals": 255593,
    "finished_requests": 93360,
    "scheduler_time": 58.46507224196486
}
#Debug simulation 
Total elapsed time: 6.90101319598034. Arrivals time: 0.37245874386280775 Scheduler time: 6.406257506925613 Scheduler overhead time: 0.03775921976193786 Adapter cache time: 0.028011877089738846 Engine time: 0.03882626863196492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.821892019826919,
    "estimated_duration": 3600.033879951613,
    "input_throughput": 6296.768518274245,
    "output_throughput": 5551.5391428117955,
    "total_throughput": 11848.30766108604,
    "itl": 126.73143511559195,
    "ttft": 1675920.1154767629,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 255593,
    "finished_requests": 91493,
    "scheduler_time": 51.81908495590878
}
#Debug simulation 
Total elapsed time: 6.822032073047012. Arrivals time: 0.38380794785916805 Scheduler time: 6.294967172201723 Scheduler overhead time: 0.04426491865888238 Adapter cache time: 0.032602079678326845 Engine time: 0.045920693315565586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.819266119971871,
    "estimated_duration": 3600.048651473316,
    "input_throughput": 6426.516761247576,
    "output_throughput": 5657.02493816727,
    "total_throughput": 12083.541699414845,
    "itl": 151.7643700478844,
    "ttft": 1656160.7030184914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 255593,
    "finished_requests": 93359,
    "scheduler_time": 58.46365622878857
}
#Debug simulation 
Total elapsed time: 6.819390062242746. Arrivals time: 0.31473806593567133 Scheduler time: 6.384812745731324 Scheduler overhead time: 0.037011431995779276 Adapter cache time: 0.026611587963998318 Engine time: 0.038864423520863056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.795576449017972,
    "estimated_duration": 3600.020939129446,
    "input_throughput": 6296.466154855591,
    "output_throughput": 5551.356599840432,
    "total_throughput": 11847.822754696024,
    "itl": 126.73021296839865,
    "ttft": 1675855.5635787772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879446,
    "arrivals": 255593,
    "finished_requests": 91489,
    "scheduler_time": 51.819325812288376
}
#Debug simulation 
Total elapsed time: 6.7957129860296845. Arrivals time: 0.33599860733374953 Scheduler time: 6.318836010526866 Scheduler overhead time: 0.04385908925905824 Adapter cache time: 0.03057361813262105 Engine time: 0.04592671059072018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.953795705921948,
    "estimated_duration": 3600.003126146655,
    "input_throughput": 6407.217491694019,
    "output_throughput": 5640.1301022572625,
    "total_throughput": 12047.347593951283,
    "itl": 152.24374280640382,
    "ttft": 1658931.71272269,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 255593,
    "finished_requests": 93092,
    "scheduler_time": 58.26347185951137
}
#Debug simulation 
Total elapsed time: 6.953922149259597. Arrivals time: 0.38941437285393476 Scheduler time: 6.438056819606572 Scheduler overhead time: 0.03783714212477207 Adapter cache time: 0.03117581130936742 Engine time: 0.0397191196680069 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.791328080929816,
    "estimated_duration": 3600.031157837565,
    "input_throughput": 6296.987999852987,
    "output_throughput": 5551.656950659588,
    "total_throughput": 11848.644950512575,
    "itl": 126.72992059879407,
    "ttft": 1675930.9734270212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 255593,
    "finished_requests": 91496,
    "scheduler_time": 51.81809222770161
}
#Debug simulation 
Total elapsed time: 6.791460705921054. Arrivals time: 0.34239510959014297 Scheduler time: 6.307764652185142 Scheduler overhead time: 0.043736405204981565 Adapter cache time: 0.03141486644744873 Engine time: 0.0457596299238503 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.13657384365797,
    "estimated_duration": 3600.1210427930964,
    "input_throughput": 6549.689779793094,
    "output_throughput": 5791.993311375554,
    "total_throughput": 12341.683091168648,
    "itl": 148.56265219254362,
    "ttft": 1631251.0405294294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 253064,
    "finished_requests": 95345,
    "scheduler_time": 59.95842315136071
}
#Debug simulation 
Total elapsed time: 7.136674372944981. Arrivals time: 0.3773230956867337 Scheduler time: 6.632156442385167 Scheduler overhead time: 0.03851114492863417 Adapter cache time: 0.030408915597945452 Engine time: 0.04050655663013458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.087774737738073,
    "estimated_duration": 3600.1576522022942,
    "input_throughput": 6549.750115963817,
    "output_throughput": 5791.934969082383,
    "total_throughput": 12341.6850850462,
    "itl": 148.56210279896098,
    "ttft": 1631266.5095809246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 253064,
    "finished_requests": 95346,
    "scheduler_time": 59.958734086318856
}
#Debug simulation 
Total elapsed time: 7.08789736777544. Arrivals time: 0.36846517212688923 Scheduler time: 6.5917521067894995 Scheduler overhead time: 0.03855601092800498 Adapter cache time: 0.030903239268809557 Engine time: 0.040368830785155296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.989731079898775,
    "estimated_duration": 3600.0960009918863,
    "input_throughput": 6412.2442828301755,
    "output_throughput": 5676.586956117135,
    "total_throughput": 12088.83123894731,
    "itl": 123.43567046336915,
    "ttft": 1652522.525202832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 253064,
    "finished_requests": 93376,
    "scheduler_time": 52.9685906690784
}
#Debug simulation 
Total elapsed time: 6.989856694824994. Arrivals time: 0.3476327694952488 Scheduler time: 6.498226710129529 Scheduler overhead time: 0.045320658944547176 Adapter cache time: 0.030465248972177505 Engine time: 0.047182006761431694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.319303682073951,
    "estimated_duration": 3600.1356492089712,
    "input_throughput": 6549.663206490836,
    "output_throughput": 5791.969812187942,
    "total_throughput": 12341.633018678778,
    "itl": 148.56246948495843,
    "ttft": 1631256.8519143655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 253064,
    "finished_requests": 95345,
    "scheduler_time": 59.95843559596688
}
#Debug simulation 
Total elapsed time: 7.319376453291625. Arrivals time: 0.41234762081876397 Scheduler time: 6.776976041961461 Scheduler overhead time: 0.038705112878233194 Adapter cache time: 0.03329154755920172 Engine time: 0.04027142142876983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.966735382098705,
    "estimated_duration": 3600.010435499804,
    "input_throughput": 6412.251968040512,
    "output_throughput": 5676.4413231938015,
    "total_throughput": 12088.693291234315,
    "itl": 123.4368465497284,
    "ttft": 1652520.4081334802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879447,
    "arrivals": 253064,
    "finished_requests": 93373,
    "scheduler_time": 52.96715149947688
}
#Debug simulation 
Total elapsed time: 6.966878081206232. Arrivals time: 0.36667873337864876 Scheduler time: 6.453131821937859 Scheduler overhead time: 0.04561431100592017 Adapter cache time: 0.03316021803766489 Engine time: 0.04734765226021409 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.119596404954791,
    "estimated_duration": 3600.077308980271,
    "input_throughput": 6549.753790337068,
    "output_throughput": 5792.0008961991625,
    "total_throughput": 12341.754686536231,
    "itl": 148.56222599115696,
    "ttft": 1631256.7716000546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 253064,
    "finished_requests": 95344,
    "scheduler_time": 59.957825801797426
}
#Debug simulation 
Total elapsed time: 7.11974758701399. Arrivals time: 0.3608400374650955 Scheduler time: 6.634468623436987 Scheduler overhead time: 0.03841873537749052 Adapter cache time: 0.02828977955505252 Engine time: 0.03993144957348704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.230202951002866,
    "estimated_duration": 3600.0205983047954,
    "input_throughput": 6412.295810437905,
    "output_throughput": 5676.571408958868,
    "total_throughput": 12088.867219396774,
    "itl": 123.43749705204351,
    "ttft": 1652595.4657668888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 253064,
    "finished_requests": 93375,
    "scheduler_time": 52.967771794957706
}
#Debug simulation 
Total elapsed time: 7.230283023789525. Arrivals time: 0.6145854885689914 Scheduler time: 6.469779542647302 Scheduler overhead time: 0.0451997397467494 Adapter cache time: 0.032539918553084135 Engine time: 0.047139763366431 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.132818627171218,
    "estimated_duration": 3600.1130973944596,
    "input_throughput": 6664.418408789549,
    "output_throughput": 5876.2373369077695,
    "total_throughput": 12540.655745697319,
    "itl": 146.1614254645715,
    "ttft": 1620785.5445621663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 252140,
    "finished_requests": 97057,
    "scheduler_time": 60.76014876686494
}
#Debug simulation 
Total elapsed time: 7.132926280144602. Arrivals time: 0.3935959986411035 Scheduler time: 6.608575396705419 Scheduler overhead time: 0.03899790160357952 Adapter cache time: 0.03235844476148486 Engine time: 0.04142213612794876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.182066208217293,
    "estimated_duration": 3600.1631263245513,
    "input_throughput": 6664.3257980630415,
    "output_throughput": 5876.155678978222,
    "total_throughput": 12540.481477041263,
    "itl": 146.16115942058676,
    "ttft": 1620818.8865265122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 252140,
    "finished_requests": 97057,
    "scheduler_time": 60.76121961640706
}
#Debug simulation 
Total elapsed time: 7.182220245245844. Arrivals time: 0.3573341751471162 Scheduler time: 6.698946750257164 Scheduler overhead time: 0.03912168089300394 Adapter cache time: 0.028182099107652903 Engine time: 0.04064142098650336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.027531240135431,
    "estimated_duration": 3600.023892009254,
    "input_throughput": 6520.640613553922,
    "output_throughput": 5749.249621909676,
    "total_throughput": 12269.890235463597,
    "itl": 121.8228245080193,
    "ttft": 1645957.248606243,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 252140,
    "finished_requests": 94969,
    "scheduler_time": 53.600963966609946
}
#Debug simulation 
Total elapsed time: 7.027636314742267. Arrivals time: 0.36072663636878133 Scheduler time: 6.522505364846438 Scheduler overhead time: 0.045769221149384975 Adapter cache time: 0.02999851666390896 Engine time: 0.04738472867757082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.447326759342104,
    "estimated_duration": 3600.1484574788938,
    "input_throughput": 6664.352951933972,
    "output_throughput": 5876.179621441076,
    "total_throughput": 12540.532573375047,
    "itl": 146.16211226998925,
    "ttft": 1620810.0449895412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 252140,
    "finished_requests": 97057,
    "scheduler_time": 60.76054388191024
}
#Debug simulation 
Total elapsed time: 7.447454810142517. Arrivals time: 0.38125374587252736 Scheduler time: 6.937702563591301 Scheduler overhead time: 0.039027998223900795 Adapter cache time: 0.030984509270638227 Engine time: 0.040549361146986485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.067504126112908,
    "estimated_duration": 3600.0155079732203,
    "input_throughput": 6520.6557994012455,
    "output_throughput": 5749.263011273107,
    "total_throughput": 12269.918810674353,
    "itl": 121.82679725433161,
    "ttft": 1645965.4930148374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 252140,
    "finished_requests": 94969,
    "scheduler_time": 53.60207181112245
}
#Debug simulation 
Total elapsed time: 7.06763447728008. Arrivals time: 0.37726213270798326 Scheduler time: 6.5446463958360255 Scheduler overhead time: 0.046206431463360786 Adapter cache time: 0.03005267260596156 Engine time: 0.04811361199244857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.136407013982534,
    "estimated_duration": 3600.041876385029,
    "input_throughput": 6664.520809433487,
    "output_throughput": 5876.35303321606,
    "total_throughput": 12540.873842649546,
    "itl": 146.1603096615463,
    "ttft": 1620754.5359998378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 252140,
    "finished_requests": 97056,
    "scheduler_time": 60.7597820006383
}
#Debug simulation 
Total elapsed time: 7.136555157136172. Arrivals time: 0.38348250603303313 Scheduler time: 6.627693812828511 Scheduler overhead time: 0.039043155033141375 Adapter cache time: 0.027868737000972033 Engine time: 0.040392110124230385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_128_slots_128_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.057044624816626,
    "estimated_duration": 3600.1396982138813,
    "input_throughput": 6520.2355929815485,
    "output_throughput": 5748.941078666556,
    "total_throughput": 12269.176671648105,
    "itl": 121.82146646643686,
    "ttft": 1645978.2995473393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 252140,
    "finished_requests": 94967,
    "scheduler_time": 53.601092872739166
}
#Debug simulation 
Total elapsed time: 7.0571845807135105. Arrivals time: 0.3688183818012476 Scheduler time: 6.542900804430246 Scheduler overhead time: 0.045705771539360285 Adapter cache time: 0.030291583854705095 Engine time: 0.04819420212879777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.605277758091688,
    "estimated_duration": 3600.091385094214,
    "input_throughput": 6741.01276997581,
    "output_throughput": 5926.062346176161,
    "total_throughput": 12667.075116151971,
    "itl": 144.68933745276695,
    "ttft": 1613220.8955663457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 251722,
    "finished_requests": 97979,
    "scheduler_time": 61.26049153042283
}
#Debug simulation 
Total elapsed time: 7.605449761264026. Arrivals time: 0.3858522274531424 Scheduler time: 7.093922967556864 Scheduler overhead time: 0.03930395795032382 Adapter cache time: 0.026708615478128195 Engine time: 0.04129670932888985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.214236308354884,
    "estimated_duration": 3600.026646506912,
    "input_throughput": 6740.753995125577,
    "output_throughput": 5925.864749001113,
    "total_throughput": 12666.618744126688,
    "itl": 144.69011064800986,
    "ttft": 1613246.8326576967,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 251722,
    "finished_requests": 97974,
    "scheduler_time": 61.258961208994265
}
#Debug simulation 
Total elapsed time: 7.214424499310553. Arrivals time: 0.38482337538152933 Scheduler time: 6.7027042200788856 Scheduler overhead time: 0.03934233170002699 Adapter cache time: 0.02815061528235674 Engine time: 0.04111392144113779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.130756495986134,
    "estimated_duration": 3600.0951917490115,
    "input_throughput": 6576.287775462403,
    "output_throughput": 5787.093643453996,
    "total_throughput": 12363.381418916399,
    "itl": 120.89888446313566,
    "ttft": 1638339.9854736393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 251722,
    "finished_requests": 95595,
    "scheduler_time": 53.93954388766487
}
#Debug simulation 
Total elapsed time: 7.130855109076947. Arrivals time: 0.3539254670031369 Scheduler time: 6.635438671801239 Scheduler overhead time: 0.04606330720707774 Adapter cache time: 0.026412174571305513 Engine time: 0.0477778147906065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.53024641983211,
    "estimated_duration": 3600.0968075953106,
    "input_throughput": 6741.002616596307,
    "output_throughput": 5926.053420282972,
    "total_throughput": 12667.05603687928,
    "itl": 144.68897199491116,
    "ttft": 1613224.3108177383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 251722,
    "finished_requests": 97979,
    "scheduler_time": 61.26046108017433
}
#Debug simulation 
Total elapsed time: 7.5303233517333865. Arrivals time: 0.392752586863935 Scheduler time: 7.009781056083739 Scheduler overhead time: 0.0395323489792645 Adapter cache time: 0.028707793448120356 Engine time: 0.04124799929559231 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.130117031745613,
    "estimated_duration": 3600.1224687780523,
    "input_throughput": 6576.373499881515,
    "output_throughput": 5787.05285186353,
    "total_throughput": 12363.426351745045,
    "itl": 120.89881376443442,
    "ttft": 1638497.865003418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879447,
    "arrivals": 251722,
    "finished_requests": 95597,
    "scheduler_time": 53.93964898838943
}
#Debug simulation 
Total elapsed time: 7.130243192892522. Arrivals time: 0.3844106080941856 Scheduler time: 6.601305553223938 Scheduler overhead time: 0.04624382173642516 Adapter cache time: 0.028766028583049774 Engine time: 0.04809746751561761 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.206351615022868,
    "estimated_duration": 3600.057330097306,
    "input_throughput": 6740.982649669099,
    "output_throughput": 5926.046183109912,
    "total_throughput": 12667.028832779011,
    "itl": 144.68870307206566,
    "ttft": 1613233.1498516966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 251722,
    "finished_requests": 97977,
    "scheduler_time": 61.2603910951585
}
#Debug simulation 
Total elapsed time: 7.2064784076064825. Arrivals time: 0.360552697442472 Scheduler time: 6.721138321328908 Scheduler overhead time: 0.03949197009205818 Adapter cache time: 0.02596490876749158 Engine time: 0.04109323164448142 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.522830504924059,
    "estimated_duration": 3600.075844166449,
    "input_throughput": 6576.3233956204895,
    "output_throughput": 5787.127244488335,
    "total_throughput": 12363.450640108826,
    "itl": 120.8975319710502,
    "ttft": 1638300.783589896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 251722,
    "finished_requests": 95596,
    "scheduler_time": 53.938125951121606
}
#Debug simulation 
Total elapsed time: 7.52290374180302. Arrivals time: 0.7325873151421547 Scheduler time: 6.648458734154701 Scheduler overhead time: 0.04602419398725033 Adapter cache time: 0.02655041730031371 Engine time: 0.04784329142421484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.352550236042589,
    "estimated_duration": 3600.1222397232586,
    "input_throughput": 6829.217277324985,
    "output_throughput": 6082.716791772976,
    "total_throughput": 12911.93406909796,
    "itl": 142.18980619057965,
    "ttft": 1586347.661749369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 250208,
    "finished_requests": 99905,
    "scheduler_time": 63.10882210515812
}
#Debug simulation 
Total elapsed time: 7.352696604095399. Arrivals time: 0.3622128935530782 Scheduler time: 6.867861442733556 Scheduler overhead time: 0.0399700989946723 Adapter cache time: 0.02253246959298849 Engine time: 0.04156708065420389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.690544871147722,
    "estimated_duration": 3600.042385146292,
    "input_throughput": 6829.242928205395,
    "output_throughput": 6082.687551221748,
    "total_throughput": 12911.930479427145,
    "itl": 142.19085916840254,
    "ttft": 1586314.4232062236,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 250208,
    "finished_requests": 99902,
    "scheduler_time": 63.1071538181716
}
#Debug simulation 
Total elapsed time: 7.6906635500490665. Arrivals time: 0.6393663338385522 Scheduler time: 6.925125950947404 Scheduler overhead time: 0.04009456094354391 Adapter cache time: 0.025516074616461992 Engine time: 0.042042513843625784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.2841444462537766,
    "estimated_duration": 3600.0630660615298,
    "input_throughput": 6639.060916826589,
    "output_throughput": 5919.910737373662,
    "total_throughput": 12558.971654200252,
    "itl": 118.77250104744985,
    "ttft": 1614033.7887873615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 250208,
    "finished_requests": 97113,
    "scheduler_time": 55.34929848264222
}
#Debug simulation 
Total elapsed time: 7.284271620213985. Arrivals time: 0.38081084517762065 Scheduler time: 6.759205236565322 Scheduler overhead time: 0.047121722251176834 Adapter cache time: 0.026091177482157946 Engine time: 0.04919587913900614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.481533451937139,
    "estimated_duration": 3600.131349465855,
    "input_throughput": 6829.19999673006,
    "output_throughput": 6082.701400116705,
    "total_throughput": 12911.901396846764,
    "itl": 142.18960046066724,
    "ttft": 1586350.9651608078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 250208,
    "finished_requests": 99905,
    "scheduler_time": 63.10889539442409
}
#Debug simulation 
Total elapsed time: 7.481667524203658. Arrivals time: 0.36927819391712546 Scheduler time: 6.988057947251946 Scheduler overhead time: 0.040267087053507566 Adapter cache time: 0.023766909260302782 Engine time: 0.0417069629766047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.669505211990327,
    "estimated_duration": 3600.0638132016848,
    "input_throughput": 6639.1031493254795,
    "output_throughput": 5919.9725632213485,
    "total_throughput": 12559.075712546828,
    "itl": 118.77633415404759,
    "ttft": 1614028.211644729,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879447,
    "arrivals": 250208,
    "finished_requests": 97114,
    "scheduler_time": 55.350329198039205
}
#Debug simulation 
Total elapsed time: 7.669573983643204. Arrivals time: 0.8000640394166112 Scheduler time: 6.7274826895445585 Scheduler overhead time: 0.046860508155077696 Adapter cache time: 0.024500052444636822 Engine time: 0.048900178633630276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.409794887062162,
    "estimated_duration": 3600.069748612162,
    "input_throughput": 6829.191020390038,
    "output_throughput": 6082.641317836056,
    "total_throughput": 12911.832338226093,
    "itl": 142.18930183476357,
    "ttft": 1586307.9055481486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 250208,
    "finished_requests": 99902,
    "scheduler_time": 63.107878751895946
}
#Debug simulation 
Total elapsed time: 7.409893414005637. Arrivals time: 0.36304989783093333 Scheduler time: 6.922773027792573 Scheduler overhead time: 0.04024206520989537 Adapter cache time: 0.02371694380417466 Engine time: 0.04161197552457452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_128_slots_128_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.269700455944985,
    "estimated_duration": 3600.0971338150302,
    "input_throughput": 6639.015035317105,
    "output_throughput": 5919.9136045019295,
    "total_throughput": 12558.928639819034,
    "itl": 118.77279195891799,
    "ttft": 1614076.4679612853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 250208,
    "finished_requests": 97115,
    "scheduler_time": 55.34952207096463
}
#Debug simulation 
Total elapsed time: 7.269817477092147. Arrivals time: 0.3804741292260587 Scheduler time: 6.744589838664979 Scheduler overhead time: 0.04711756808683276 Adapter cache time: 0.02699707355350256 Engine time: 0.0487510459497571 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.370436991099268,
    "estimated_duration": 3600.0783163793626,
    "input_throughput": 6903.67342480363,
    "output_throughput": 6127.596141348892,
    "total_throughput": 13031.269566152523,
    "itl": 140.90599699108319,
    "ttft": 1573207.7977782392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3886819434794593,
    "arrivals": 249718,
    "finished_requests": 100813,
    "scheduler_time": 63.56888369279128
}
#Debug simulation 
Total elapsed time: 7.370661241933703. Arrivals time: 0.37243774626404047 Scheduler time: 6.875019813422114 Scheduler overhead time: 0.04004622809588909 Adapter cache time: 0.022327995859086514 Engine time: 0.04205275932326913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.3787548542022705,
    "estimated_duration": 3600.0397508311316,
    "input_throughput": 6903.747380639916,
    "output_throughput": 6127.661783430893,
    "total_throughput": 13031.409164070808,
    "itl": 140.90788608770424,
    "ttft": 1573195.5590385217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4148755848384462,
    "arrivals": 249718,
    "finished_requests": 100813,
    "scheduler_time": 63.568406531556725
}
#Debug simulation 
Total elapsed time: 7.378882551100105. Arrivals time: 0.374811090528965 Scheduler time: 6.880634387489408 Scheduler overhead time: 0.040310295298695564 Adapter cache time: 0.022728837560862303 Engine time: 0.04180218046531081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.287063319701701,
    "estimated_duration": 3600.031559816083,
    "input_throughput": 6715.581960407901,
    "output_throughput": 5959.657198420806,
    "total_throughput": 12675.239158828706,
    "itl": 117.99046072953445,
    "ttft": 1603621.4268476362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4155301783233891,
    "arrivals": 249718,
    "finished_requests": 97983,
    "scheduler_time": 55.74690822800631
}
#Debug simulation 
Total elapsed time: 7.287211981602013. Arrivals time: 0.3730938066728413 Scheduler time: 6.770550655666739 Scheduler overhead time: 0.0474253436550498 Adapter cache time: 0.024781922344118357 Engine time: 0.04939733725041151 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.467777504585683,
    "estimated_duration": 3600.002785758005,
    "input_throughput": 6903.789102142846,
    "output_throughput": 6127.609980544846,
    "total_throughput": 13031.399082687692,
    "itl": 140.9072837625242,
    "ttft": 1573200.6553086045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3973059861455115,
    "arrivals": 249718,
    "finished_requests": 100812,
    "scheduler_time": 63.56776539737622
}
#Debug simulation 
Total elapsed time: 7.467909939587116. Arrivals time: 0.39532183250412345 Scheduler time: 6.946986931376159 Scheduler overhead time: 0.040752296801656485 Adapter cache time: 0.023737342562526464 Engine time: 0.042322307359427214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.228589907754213,
    "estimated_duration": 3600.0072186009193,
    "input_throughput": 6715.744033812207,
    "output_throughput": 5959.768327447465,
    "total_throughput": 12675.51236125967,
    "itl": 117.98962111613051,
    "ttft": 1603629.8444063915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42093759113922746,
    "arrivals": 249718,
    "finished_requests": 97984,
    "scheduler_time": 55.746057653462735
}
#Debug simulation 
Total elapsed time: 7.228709001094103. Arrivals time: 0.3515650616027415 Scheduler time: 6.7367074261419475 Scheduler overhead time: 0.04688173113390803 Adapter cache time: 0.022888953797519207 Engine time: 0.04886514041572809 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.4430900840088725,
    "estimated_duration": 3600.0592007853725,
    "input_throughput": 6903.710081928101,
    "output_throughput": 6127.628677658281,
    "total_throughput": 13031.338759586382,
    "itl": 140.90582642949067,
    "ttft": 1573204.4127631565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37973638745257676,
    "arrivals": 249718,
    "finished_requests": 100813,
    "scheduler_time": 63.56866925365655
}
#Debug simulation 
Total elapsed time: 7.443190224934369. Arrivals time: 0.4002583143301308 Scheduler time: 6.917629142291844 Scheduler overhead time: 0.04035272402688861 Adapter cache time: 0.024243782740086317 Engine time: 0.042064450681209564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.251397163141519,
    "estimated_duration": 3600.0786676754396,
    "input_throughput": 6715.512140630697,
    "output_throughput": 5959.599770038762,
    "total_throughput": 12675.111910669459,
    "itl": 117.99183451833372,
    "ttft": 1603654.359897999,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.426345003955066,
    "arrivals": 249718,
    "finished_requests": 97983,
    "scheduler_time": 55.747763748818095
}
#Debug simulation 
Total elapsed time: 7.251524622086436. Arrivals time: 0.3510564500465989 Scheduler time: 6.759103182703257 Scheduler overhead time: 0.04708561394363642 Adapter cache time: 0.023065081797540188 Engine time: 0.04916189843788743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.022372531238943,
    "estimated_duration": 3600.0994039883785,
    "input_throughput": 7132.375837054217,
    "output_throughput": 6301.3129512112855,
    "total_throughput": 13433.688788265503,
    "itl": 136.54262934930546,
    "ttft": 1546505.463147071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 248697,
    "finished_requests": 103788,
    "scheduler_time": 65.32499273360825
}
#Debug simulation 
Total elapsed time: 8.022488886024803. Arrivals time: 0.3920186529867351 Scheduler time: 7.508133057039231 Scheduler overhead time: 0.04164254665374756 Adapter cache time: 0.01829137047752738 Engine time: 0.04318457841873169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.50191064318642,
    "estimated_duration": 3600.1400022897274,
    "input_throughput": 7132.394569008084,
    "output_throughput": 6301.273002042088,
    "total_throughput": 13433.667571050171,
    "itl": 136.54247407986975,
    "ttft": 1546505.329223515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 248697,
    "finished_requests": 103789,
    "scheduler_time": 65.32530132129968
}
#Debug simulation 
Total elapsed time: 7.502011434175074. Arrivals time: 0.2861131043173373 Scheduler time: 7.098796335048974 Scheduler overhead time: 0.040993320755660534 Adapter cache time: 0.01417165296152234 Engine time: 0.04277306701987982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.207782784011215,
    "estimated_duration": 3600.0240080673216,
    "input_throughput": 6898.532327658738,
    "output_throughput": 6099.719043759598,
    "total_throughput": 12998.251371418337,
    "itl": 115.08310324593154,
    "ttft": 1579586.6820420853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 248697,
    "finished_requests": 100356,
    "scheduler_time": 57.107199745691986
}
#Debug simulation 
Total elapsed time: 7.207867199089378. Arrivals time: 0.27160230930894613 Scheduler time: 6.802513751666993 Scheduler overhead time: 0.04702054709196091 Adapter cache time: 0.015164224430918694 Engine time: 0.04942046198993921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.4706554231233895,
    "estimated_duration": 3600.1068230565074,
    "input_throughput": 7132.361138717513,
    "output_throughput": 6301.299965521587,
    "total_throughput": 13433.6611042391,
    "itl": 136.5419167643065,
    "ttft": 1546507.3830806746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 248697,
    "finished_requests": 103788,
    "scheduler_time": 65.32503280115775
}
#Debug simulation 
Total elapsed time: 7.470745000988245. Arrivals time: 0.2824284224770963 Scheduler time: 7.071772274561226 Scheduler overhead time: 0.04059365950524807 Adapter cache time: 0.014209134504199028 Engine time: 0.04268018016591668 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.247022422030568,
    "estimated_duration": 3600.0823528065425,
    "input_throughput": 6898.420526585813,
    "output_throughput": 6099.620188655173,
    "total_throughput": 12998.040715240984,
    "itl": 115.07984846989434,
    "ttft": 1579585.906303817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 248697,
    "finished_requests": 100356,
    "scheduler_time": 57.106500031349945
}
#Debug simulation 
Total elapsed time: 7.247131346724927. Arrivals time: 0.27110584219917655 Scheduler time: 6.841595738660544 Scheduler overhead time: 0.04718565009534359 Adapter cache time: 0.015340413432568312 Engine time: 0.04976402036845684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.431935291737318,
    "estimated_duration": 3600.0884309718963,
    "input_throughput": 7132.397576430657,
    "output_throughput": 6301.332157520297,
    "total_throughput": 13433.729733950953,
    "itl": 136.54256579620747,
    "ttft": 1546498.083270605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 248697,
    "finished_requests": 103788,
    "scheduler_time": 65.32497127450814
}
#Debug simulation 
Total elapsed time: 7.4320408697240055. Arrivals time: 0.2805382586084306 Scheduler time: 7.034653486683965 Scheduler overhead time: 0.04078424954786897 Adapter cache time: 0.014058434870094061 Engine time: 0.042866656091064215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_128_slots_128_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.34117823978886,
    "estimated_duration": 3600.040474792569,
    "input_throughput": 6898.500773503376,
    "output_throughput": 6099.691143407287,
    "total_throughput": 12998.191916910664,
    "itl": 115.08238734816025,
    "ttft": 1579588.123430398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 248697,
    "finished_requests": 100356,
    "scheduler_time": 57.107029102680706
}
#Debug simulation 
Total elapsed time: 7.341278297826648. Arrivals time: 0.2788874125108123 Scheduler time: 6.927917069289833 Scheduler overhead time: 0.04740365780889988 Adapter cache time: 0.015281636733561754 Engine time: 0.049521646462380886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.2131573259830475,
    "estimated_duration": 3600.0682699429362,
    "input_throughput": 4839.246006930904,
    "output_throughput": 4289.961701266516,
    "total_throughput": 9129.207708197418,
    "itl": 200.304892185654,
    "ttft": 1802681.241691429,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 200319,
    "finished_requests": 70409,
    "scheduler_time": 44.86858025750105
}
#Debug simulation 
Total elapsed time: 5.213238466065377. Arrivals time: 0.21710139140486717 Scheduler time: 4.903680135961622 Scheduler overhead time: 0.028269257862120867 Adapter cache time: 0.021583450958132744 Engine time: 0.029470818117260933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.170755790080875,
    "estimated_duration": 3600.211132945408,
    "input_throughput": 4839.271741750982,
    "output_throughput": 4289.9422921800615,
    "total_throughput": 9129.214033931044,
    "itl": 200.30607018833925,
    "ttft": 1802683.932830733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41786563513334835,
    "arrivals": 200319,
    "finished_requests": 70411,
    "scheduler_time": 44.86987364444473
}
#Debug simulation 
Total elapsed time: 5.170865392778069. Arrivals time: 0.2320869006216526 Scheduler time: 4.845946611836553 Scheduler overhead time: 0.028432682622224092 Adapter cache time: 0.021655825432389975 Engine time: 0.029609396122395992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.389362116809934,
    "estimated_duration": 3600.067523708466,
    "input_throughput": 4709.835548454862,
    "output_throughput": 4182.242944290749,
    "total_throughput": 8892.078492745612,
    "itl": 167.1755614069347,
    "ttft": 1828432.3433715363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 200319,
    "finished_requests": 68542,
    "scheduler_time": 39.4417219263196
}
#Debug simulation 
Total elapsed time: 5.38942376896739. Arrivals time: 0.43626730469986796 Scheduler time: 4.839723114389926 Scheduler overhead time: 0.03349393047392368 Adapter cache time: 0.029425217304378748 Engine time: 0.03501162072643638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.1886876481585205,
    "estimated_duration": 3600.1092962238567,
    "input_throughput": 4839.408630808598,
    "output_throughput": 4290.063642289942,
    "total_throughput": 9129.47227309854,
    "itl": 200.30597890559238,
    "ttft": 1802650.090591933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 200319,
    "finished_requests": 70411,
    "scheduler_time": 44.86900365690019
}
#Debug simulation 
Total elapsed time: 5.188800213392824. Arrivals time: 0.23478807043284178 Scheduler time: 4.861331732477993 Scheduler overhead time: 0.02830336708575487 Adapter cache time: 0.02163303317502141 Engine time: 0.02955924952402711 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.140218609943986,
    "estimated_duration": 3600.006691620164,
    "input_throughput": 4708.787358495434,
    "output_throughput": 4181.252227957856,
    "total_throughput": 8890.03958645329,
    "itl": 166.91094964913086,
    "ttft": 1828792.5113940819,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 200319,
    "finished_requests": 68525,
    "scheduler_time": 39.38970340107229
}
#Debug simulation 
Total elapsed time: 5.140300356782973. Arrivals time: 0.21406741440296173 Scheduler time: 4.813109739217907 Scheduler overhead time: 0.033258809708058834 Adapter cache time: 0.02948939800262451 Engine time: 0.03478927398100495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.217432206962258,
    "estimated_duration": 3600.0088395614066,
    "input_throughput": 4839.325895133773,
    "output_throughput": 4290.032521665024,
    "total_throughput": 9129.358416798797,
    "itl": 200.30490569587096,
    "ttft": 1802587.9095018886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 200319,
    "finished_requests": 70409,
    "scheduler_time": 44.86771462772509
}
#Debug simulation 
Total elapsed time: 5.217512737959623. Arrivals time: 0.21872553136199713 Scheduler time: 4.906016631051898 Scheduler overhead time: 0.02830318547785282 Adapter cache time: 0.021636563818901777 Engine time: 0.029707553796470165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.138512487988919,
    "estimated_duration": 3600.169797372611,
    "input_throughput": 4707.890725701177,
    "output_throughput": 4180.439214556947,
    "total_throughput": 8888.329940258123,
    "itl": 166.68412351081525,
    "ttft": 1828972.5843064862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037832,
    "arrivals": 200319,
    "finished_requests": 68516,
    "scheduler_time": 39.3467600423312
}
#Debug simulation 
Total elapsed time: 5.138597527053207. Arrivals time: 0.22545621125027537 Scheduler time: 4.799704506993294 Scheduler overhead time: 0.03323740465566516 Adapter cache time: 0.029759802389889956 Engine time: 0.03487231768667698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.391811806708574,
    "estimated_duration": 3600.142935538613,
    "input_throughput": 5049.941995505807,
    "output_throughput": 4465.722691533831,
    "total_throughput": 9515.664687039638,
    "itl": 192.16212649334796,
    "ttft": 1743588.9323973886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 192932,
    "finished_requests": 73738,
    "scheduler_time": 46.80943991654892
}
#Debug simulation 
Total elapsed time: 5.39189432375133. Arrivals time: 0.2396999942138791 Scheduler time: 5.054896269459277 Scheduler overhead time: 0.02942898729816079 Adapter cache time: 0.023177715949714184 Engine time: 0.030943457502871752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.394583897199482,
    "estimated_duration": 3600.13486817613,
    "input_throughput": 5049.953311668698,
    "output_throughput": 4465.732698548851,
    "total_throughput": 9515.686010217549,
    "itl": 192.1646956086507,
    "ttft": 1743590.7102844438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 192932,
    "finished_requests": 73738,
    "scheduler_time": 46.80926607763721
}
#Debug simulation 
Total elapsed time: 5.394690272863954. Arrivals time: 0.2245760909281671 Scheduler time: 5.072848154697567 Scheduler overhead time: 0.029387495946139097 Adapter cache time: 0.023470508866012096 Engine time: 0.03078109212219715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.29779381910339,
    "estimated_duration": 3600.1559587542683,
    "input_throughput": 4928.93563592732,
    "output_throughput": 4366.212791914473,
    "total_throughput": 9295.148427841792,
    "itl": 160.49467028503216,
    "ttft": 1768906.5206800138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 192932,
    "finished_requests": 71955,
    "scheduler_time": 41.360260948115155
}
#Debug simulation 
Total elapsed time: 5.297877200879157. Arrivals time: 0.2330595930106938 Scheduler time: 4.949164296500385 Scheduler overhead time: 0.034554895013570786 Adapter cache time: 0.028633500449359417 Engine time: 0.036329870112240314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.410387572366744,
    "estimated_duration": 3600.030286526744,
    "input_throughput": 5050.100013891908,
    "output_throughput": 4465.862429038363,
    "total_throughput": 9515.962442930271,
    "itl": 192.1638961364866,
    "ttft": 1743564.619840886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 192932,
    "finished_requests": 73738,
    "scheduler_time": 46.808079570740254
}
#Debug simulation 
Total elapsed time: 5.4104703003540635. Arrivals time: 0.2382932724431157 Scheduler time: 5.074810636229813 Scheduler overhead time: 0.029358008410781622 Adapter cache time: 0.023296072147786617 Engine time: 0.030961057171225548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.354884157888591,
    "estimated_duration": 3600.023075955229,
    "input_throughput": 4928.884239246657,
    "output_throughput": 4366.117846572235,
    "total_throughput": 9295.002085818893,
    "itl": 160.49664737635118,
    "ttft": 1768926.8668830602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794475,
    "arrivals": 192932,
    "finished_requests": 71951,
    "scheduler_time": 41.359281265095134
}
#Debug simulation 
Total elapsed time: 5.3549880175851285. Arrivals time: 0.24181432137265801 Scheduler time: 4.998041520360857 Scheduler overhead time: 0.03460154635831714 Adapter cache time: 0.028286687564104795 Engine time: 0.03609567368403077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.460945931728929,
    "estimated_duration": 3600.1359670724987,
    "input_throughput": 5049.951770233762,
    "output_throughput": 4465.731335439932,
    "total_throughput": 9515.683105673694,
    "itl": 192.16305981624512,
    "ttft": 1743582.81690224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 192932,
    "finished_requests": 73738,
    "scheduler_time": 46.80944278484103
}
#Debug simulation 
Total elapsed time: 5.461036033928394. Arrivals time: 0.22748464159667492 Scheduler time: 5.135697765275836 Scheduler overhead time: 0.02954997355118394 Adapter cache time: 0.023604694288223982 Engine time: 0.030857712496072054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.358617885038257,
    "estimated_duration": 3600.0697805368895,
    "input_throughput": 4929.059735434806,
    "output_throughput": 4366.237867104846,
    "total_throughput": 9295.297602539651,
    "itl": 160.49700098906004,
    "ttft": 1768907.9503350367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037832,
    "arrivals": 192932,
    "finished_requests": 71953,
    "scheduler_time": 41.359672130481506
}
#Debug simulation 
Total elapsed time: 5.358696875162423. Arrivals time: 0.23851763969287276 Scheduler time: 5.003703665919602 Scheduler overhead time: 0.034809792414307594 Adapter cache time: 0.028953720349818468 Engine time: 0.03644946403801441 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.638167690020055,
    "estimated_duration": 3600.1846506389834,
    "input_throughput": 5276.108823092908,
    "output_throughput": 4673.445568135997,
    "total_throughput": 9949.554391228905,
    "itl": 183.94915989122796,
    "ttft": 1689594.7935751474,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 188974,
    "finished_requests": 76938,
    "scheduler_time": 49.05846226351891
}
#Debug simulation 
Total elapsed time: 5.638248845934868. Arrivals time: 0.2396579529158771 Scheduler time: 5.299677967093885 Scheduler overhead time: 0.030725750606507063 Adapter cache time: 0.02158857462927699 Engine time: 0.032253303565084934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.597043517045677,
    "estimated_duration": 3600.136600586533,
    "input_throughput": 5276.178964127457,
    "output_throughput": 4673.4757223542165,
    "total_throughput": 9949.654686481674,
    "itl": 183.9501493984213,
    "ttft": 1689606.8889143812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 188974,
    "finished_requests": 76937,
    "scheduler_time": 49.057508482195175
}
#Debug simulation 
Total elapsed time: 5.597130503039807. Arrivals time: 0.22866562381386757 Scheduler time: 5.269750190898776 Scheduler overhead time: 0.030853858683258295 Adapter cache time: 0.02126113884150982 Engine time: 0.032234837766736746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.770981679204851,
    "estimated_duration": 3600.0739673874305,
    "input_throughput": 5132.41815789935,
    "output_throughput": 4546.748524691757,
    "total_throughput": 9679.166682591107,
    "itl": 153.708505364803,
    "ttft": 1720626.668671506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 188974,
    "finished_requests": 74829,
    "scheduler_time": 43.17651439251817
}
#Debug simulation 
Total elapsed time: 5.771070923190564. Arrivals time: 0.45158406207337976 Scheduler time: 5.2043648096732795 Scheduler overhead time: 0.03585195308551192 Adapter cache time: 0.02493299450725317 Engine time: 0.03752601379528642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.631510780658573,
    "estimated_duration": 3600.07158435165,
    "input_throughput": 5276.274250369072,
    "output_throughput": 4673.560123952397,
    "total_throughput": 9949.83437432147,
    "itl": 183.95129938009842,
    "ttft": 1689597.6344402784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 188974,
    "finished_requests": 76937,
    "scheduler_time": 49.05672831443744
}
#Debug simulation 
Total elapsed time: 5.63159556966275. Arrivals time: 0.22911804867908359 Scheduler time: 5.303964010439813 Scheduler overhead time: 0.030647529754787683 Adapter cache time: 0.021375967655330896 Engine time: 0.03217391250655055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.579694343265146,
    "estimated_duration": 3600.1689071288097,
    "input_throughput": 5130.893709854234,
    "output_throughput": 4545.421457197898,
    "total_throughput": 9676.315167052133,
    "itl": 153.55752797254394,
    "ttft": 1720841.6039790555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794475,
    "arrivals": 188974,
    "finished_requests": 74812,
    "scheduler_time": 43.141145060204515
}
#Debug simulation 
Total elapsed time: 5.57978033926338. Arrivals time: 0.23814950231462717 Scheduler time: 5.225447909440845 Scheduler overhead time: 0.036026534624397755 Adapter cache time: 0.024712083861231804 Engine time: 0.038532627280801535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.593416149728,
    "estimated_duration": 3600.168053523429,
    "input_throughput": 5276.133146454072,
    "output_throughput": 4673.467113162501,
    "total_throughput": 9949.600259616573,
    "itl": 183.94920383839488,
    "ttft": 1689584.8191502013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 188974,
    "finished_requests": 76938,
    "scheduler_time": 49.058488961644045
}
#Debug simulation 
Total elapsed time: 5.59349925769493. Arrivals time: 0.23116581002250314 Scheduler time: 5.264290144667029 Scheduler overhead time: 0.03066163742914796 Adapter cache time: 0.02105611376464367 Engine time: 0.03206434287130833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.508277662098408,
    "estimated_duration": 3600.0014426656135,
    "input_throughput": 5131.408499192614,
    "output_throughput": 4545.766233883158,
    "total_throughput": 9677.174733075772,
    "itl": 153.60715735406538,
    "ttft": 1720825.7491832217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 188974,
    "finished_requests": 74814,
    "scheduler_time": 43.15132331187662
}
#Debug simulation 
Total elapsed time: 5.508360974024981. Arrivals time: 0.22673328826203942 Scheduler time: 5.16631440538913 Scheduler overhead time: 0.03586740419268608 Adapter cache time: 0.024808331858366728 Engine time: 0.03773554880172014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.82789508625865,
    "estimated_duration": 3600.0098050077986,
    "input_throughput": 5506.651113123025,
    "output_throughput": 4831.843506593538,
    "total_throughput": 10338.494619716563,
    "itl": 177.00722028839775,
    "ttft": 1642376.4618250162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 187135,
    "finished_requests": 79984,
    "scheduler_time": 50.7425547435062
}
#Debug simulation 
Total elapsed time: 5.827979050111026. Arrivals time: 0.2520805262029171 Scheduler time: 5.479226558003575 Scheduler overhead time: 0.03183351131156087 Adapter cache time: 0.01645681168884039 Engine time: 0.03354791225865483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.808215918019414,
    "estimated_duration": 3600.1587825407005,
    "input_throughput": 5506.543515841688,
    "output_throughput": 4831.867995478708,
    "total_throughput": 10338.411511320395,
    "itl": 177.00894742047228,
    "ttft": 1642373.111740164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41786563513334835,
    "arrivals": 187135,
    "finished_requests": 79986,
    "scheduler_time": 50.74458316463107
}
#Debug simulation 
Total elapsed time: 5.808303980622441. Arrivals time: 0.23782819602638483 Scheduler time: 5.4740904667414725 Scheduler overhead time: 0.03185401950031519 Adapter cache time: 0.016271120868623257 Engine time: 0.03340836055576801 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.722202621866018,
    "estimated_duration": 3600.0166904114026,
    "input_throughput": 5342.191621284458,
    "output_throughput": 4691.16491736877,
    "total_throughput": 10033.356538653228,
    "itl": 149.42881528156667,
    "ttft": 1676206.5562996618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 187135,
    "finished_requests": 77571,
    "scheduler_time": 44.72105463811165
}
#Debug simulation 
Total elapsed time: 5.722311902791262. Arrivals time: 0.23115328419953585 Scheduler time: 5.37939334101975 Scheduler overhead time: 0.03705201484262943 Adapter cache time: 0.018702516797930002 Engine time: 0.038697372656315565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.8392207799479365,
    "estimated_duration": 3600.044541029358,
    "input_throughput": 5506.597980682689,
    "output_throughput": 4831.796885220301,
    "total_throughput": 10338.39486590299,
    "itl": 177.00690839799574,
    "ttft": 1642374.6790952028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 187135,
    "finished_requests": 79984,
    "scheduler_time": 50.743284676122556
}
#Debug simulation 
Total elapsed time: 5.839305971749127. Arrivals time: 0.23980677174404263 Scheduler time: 5.5028415042907 Scheduler overhead time: 0.03174944454804063 Adapter cache time: 0.016365881077945232 Engine time: 0.0335736651904881 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.7089248090051115,
    "estimated_duration": 3600.0801855098457,
    "input_throughput": 5341.528246342837,
    "output_throughput": 4690.754408185061,
    "total_throughput": 10032.282654527897,
    "itl": 149.3393858481803,
    "ttft": 1676218.9242286552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 187135,
    "finished_requests": 77559,
    "scheduler_time": 44.694768323845615
}
#Debug simulation 
Total elapsed time: 5.709005904849619. Arrivals time: 0.23649720614776015 Scheduler time: 5.360264621209353 Scheduler overhead time: 0.037343788892030716 Adapter cache time: 0.01870489912107587 Engine time: 0.03886521188542247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.822866327129304,
    "estimated_duration": 3600.149094772901,
    "input_throughput": 5506.558333593273,
    "output_throughput": 4831.880997722211,
    "total_throughput": 10338.439331315485,
    "itl": 177.00695177938118,
    "ttft": 1642358.8829871903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 187135,
    "finished_requests": 79986,
    "scheduler_time": 50.74466967129799
}
#Debug simulation 
Total elapsed time: 5.822972164023668. Arrivals time: 0.2516521820798516 Scheduler time: 5.4754114067181945 Scheduler overhead time: 0.03168128989636898 Adapter cache time: 0.016249252948909998 Engine time: 0.033166246954351664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.7294129729270935,
    "estimated_duration": 3600.0629009390395,
    "input_throughput": 5341.076955901109,
    "output_throughput": 4690.656931465083,
    "total_throughput": 10031.733887366192,
    "itl": 149.3100081625983,
    "ttft": 1676296.3458523587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 187135,
    "finished_requests": 77555,
    "scheduler_time": 44.68719751197024
}
#Debug simulation 
Total elapsed time: 5.729496710933745. Arrivals time: 0.22973945597186685 Scheduler time: 5.387663642410189 Scheduler overhead time: 0.03702488262206316 Adapter cache time: 0.01879666093736887 Engine time: 0.03898192523047328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.927231975831091,
    "estimated_duration": 3600.16384819569,
    "input_throughput": 5568.627386236193,
    "output_throughput": 4937.8233184885075,
    "total_throughput": 10506.4507047247,
    "itl": 174.07904402823863,
    "ttft": 1625925.9139200645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 186179,
    "finished_requests": 81153,
    "scheduler_time": 52.03399424953851
}
#Debug simulation 
Total elapsed time: 5.927313846070319. Arrivals time: 0.24020816152915359 Scheduler time: 5.593174069188535 Scheduler overhead time: 0.032265611458569765 Adapter cache time: 0.012632365804165602 Engine time: 0.03400104492902756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.946028536185622,
    "estimated_duration": 3600.060844535722,
    "input_throughput": 5568.7375479859265,
    "output_throughput": 4937.821266821539,
    "total_throughput": 10506.558814807466,
    "itl": 174.0792086723198,
    "ttft": 1625948.6041842783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 186179,
    "finished_requests": 81151,
    "scheduler_time": 52.032295164215974
}
#Debug simulation 
Total elapsed time: 5.946123272180557. Arrivals time: 0.2452858411706984 Scheduler time: 5.607143214438111 Scheduler overhead time: 0.032119243405759335 Adapter cache time: 0.012639131862670183 Engine time: 0.03389491094276309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.785683383233845,
    "estimated_duration": 3600.0590002712547,
    "input_throughput": 5376.94743295637,
    "output_throughput": 4773.821206459416,
    "total_throughput": 10150.768639415786,
    "itl": 146.50090714464594,
    "ttft": 1664428.852091728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 186179,
    "finished_requests": 78348,
    "scheduler_time": 45.51338664706736
}
#Debug simulation 
Total elapsed time: 5.785767475143075. Arrivals time: 0.2314815199933946 Scheduler time: 5.445193908177316 Scheduler overhead time: 0.037336685694754124 Adapter cache time: 0.014766997192054987 Engine time: 0.039371642749756575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.94879759196192,
    "estimated_duration": 3600.1780801301898,
    "input_throughput": 5568.605372786177,
    "output_throughput": 4937.803798682411,
    "total_throughput": 10506.40917146859,
    "itl": 174.07876174977378,
    "ttft": 1625934.0170097323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 186179,
    "finished_requests": 81153,
    "scheduler_time": 52.03385532700962
}
#Debug simulation 
Total elapsed time: 5.948905074968934. Arrivals time: 0.2421024781651795 Scheduler time: 5.613023884128779 Scheduler overhead time: 0.032273979391902685 Adapter cache time: 0.012413549236953259 Engine time: 0.03394477302208543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.801998197101057,
    "estimated_duration": 3600.0817525104035,
    "input_throughput": 5378.194533082078,
    "output_throughput": 4775.1115618451,
    "total_throughput": 10153.306094927178,
    "itl": 146.67563920869478,
    "ttft": 1664215.5731156284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 186179,
    "finished_requests": 78370,
    "scheduler_time": 45.56478813838677
}
#Debug simulation 
Total elapsed time: 5.802079462911934. Arrivals time: 0.23266740329563618 Scheduler time: 5.460028405301273 Scheduler overhead time: 0.03763709170743823 Adapter cache time: 0.014712383970618248 Engine time: 0.03944977791979909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.91571797337383,
    "estimated_duration": 3600.08141128103,
    "input_throughput": 5568.7060123638485,
    "output_throughput": 4937.842778859403,
    "total_throughput": 10506.548791223251,
    "itl": 174.07690576928707,
    "ttft": 1625938.6230428056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 186179,
    "finished_requests": 81152,
    "scheduler_time": 52.03298770127987
}
#Debug simulation 
Total elapsed time: 5.91582773020491. Arrivals time: 0.24141111923381686 Scheduler time: 5.580789929255843 Scheduler overhead time: 0.032200580928474665 Adapter cache time: 0.012608121614903212 Engine time: 0.033763653598725796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.772778952028602,
    "estimated_duration": 3600.010923327569,
    "input_throughput": 5377.279795058714,
    "output_throughput": 4774.116902988116,
    "total_throughput": 10151.396698046829,
    "itl": 146.54465194172482,
    "ttft": 1664392.2203029406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 186179,
    "finished_requests": 78352,
    "scheduler_time": 45.52586014245014
}
#Debug simulation 
Total elapsed time: 5.77285871328786. Arrivals time: 0.23601694079115987 Scheduler time: 5.427258652634919 Scheduler overhead time: 0.03796292096376419 Adapter cache time: 0.014672875870019197 Engine time: 0.03947891667485237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.008886817377061,
    "estimated_duration": 3600.1244102946516,
    "input_throughput": 5706.721673631969,
    "output_throughput": 4990.509480345858,
    "total_throughput": 10697.231153977827,
    "itl": 171.15967502238655,
    "ttft": 1609692.0566829608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3886819434794593,
    "arrivals": 185762,
    "finished_requests": 82418,
    "scheduler_time": 52.43192958201181
}
#Debug simulation 
Total elapsed time: 6.008970175404102. Arrivals time: 0.24517729552462697 Scheduler time: 5.67152124363929 Scheduler overhead time: 0.03287399932742119 Adapter cache time: 0.009645193815231323 Engine time: 0.03442934062331915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.973734629806131,
    "estimated_duration": 3600.027856694871,
    "input_throughput": 5706.790563243497,
    "output_throughput": 4990.5524943616465,
    "total_throughput": 10697.343057605143,
    "itl": 171.15898631880754,
    "ttft": 1609667.866303255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4144669895200058,
    "arrivals": 185762,
    "finished_requests": 82417,
    "scheduler_time": 52.43008930300626
}
#Debug simulation 
Total elapsed time: 5.973839264828712. Arrivals time: 0.24391073919832706 Scheduler time: 5.637549845967442 Scheduler overhead time: 0.032813765574246645 Adapter cache time: 0.009649020154029131 Engine time: 0.03454211167991161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.820001824293286,
    "estimated_duration": 3600.0787550596365,
    "input_throughput": 5499.6291323276955,
    "output_throughput": 4816.770459709775,
    "total_throughput": 10316.399592037471,
    "itl": 144.8512815805967,
    "ttft": 1649435.6279318403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41519293082878095,
    "arrivals": 185762,
    "finished_requests": 79455,
    "scheduler_time": 45.83997309377246
}
#Debug simulation 
Total elapsed time: 5.820081994403154. Arrivals time: 0.23251428361982107 Scheduler time: 5.480439004488289 Scheduler overhead time: 0.03812726680189371 Adapter cache time: 0.011400436516851187 Engine time: 0.039801038801670074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.008645923808217,
    "estimated_duration": 3600.143142952709,
    "input_throughput": 5706.691979794393,
    "output_throughput": 4990.4835131818,
    "total_throughput": 10697.175492976192,
    "itl": 171.15912494235258,
    "ttft": 1609698.1742390024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3973059861455115,
    "arrivals": 185762,
    "finished_requests": 82418,
    "scheduler_time": 52.4319976939502
}
#Debug simulation 
Total elapsed time: 6.008726519998163. Arrivals time: 0.24138743756338954 Scheduler time: 5.6748257372528315 Scheduler overhead time: 0.0329422140493989 Adapter cache time: 0.009664817713201046 Engine time: 0.03466820623725653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.812668506987393,
    "estimated_duration": 3600.0662376189794,
    "input_throughput": 5499.2940944036445,
    "output_throughput": 4816.783318817173,
    "total_throughput": 10316.077413220817,
    "itl": 144.8074420053459,
    "ttft": 1649352.0102308579,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42047458985820446,
    "arrivals": 185762,
    "finished_requests": 79451,
    "scheduler_time": 45.82506085689082
}
#Debug simulation 
Total elapsed time: 5.812777539715171. Arrivals time: 0.23468851391226053 Scheduler time: 5.471334836445749 Scheduler overhead time: 0.03786188596859574 Adapter cache time: 0.011417871806770563 Engine time: 0.0395950498059392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.007143042981625,
    "estimated_duration": 3600.0773308682033,
    "input_throughput": 5706.796302357578,
    "output_throughput": 4990.57474292286,
    "total_throughput": 10697.371045280437,
    "itl": 171.159030731889,
    "ttft": 1609672.054451145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37973638745257676,
    "arrivals": 185762,
    "finished_requests": 82418,
    "scheduler_time": 52.43130439015572
}
#Debug simulation 
Total elapsed time: 6.0072319260798395. Arrivals time: 0.24551876727491617 Scheduler time: 5.669168166350573 Scheduler overhead time: 0.032783391419798136 Adapter cache time: 0.009716690052300692 Engine time: 0.03470693435519934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.845464397221804,
    "estimated_duration": 3600.1412204186704,
    "input_throughput": 5499.378993165599,
    "output_throughput": 4816.684662715369,
    "total_throughput": 10316.06365588097,
    "itl": 144.80056490021943,
    "ttft": 1649376.3674071957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42588200267404297,
    "arrivals": 185762,
    "finished_requests": 79453,
    "scheduler_time": 45.824625272574494
}
#Debug simulation 
Total elapsed time: 5.845543850213289. Arrivals time: 0.23288938077166677 Scheduler time: 5.505884959362447 Scheduler overhead time: 0.03786052856594324 Adapter cache time: 0.01138839591294527 Engine time: 0.03975174110382795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_128_slots_128_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_128_slots_128_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.480058212764561,
    "estimated_duration": 3600.149017996557,
    "input_throughput": 5085.642263271867,
    "output_throughput": 4481.132564056388,
    "total_throughput": 9566.774827328254,
    "itl": 190.74917429896848,
    "ttft": 1581644.70315529,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 146610,
    "finished_requests": 74045,
    "scheduler_time": 48.183814019733035
}
#Debug simulation 
Total elapsed time: 5.48015669407323. Arrivals time: 0.23759242286905646 Scheduler time: 5.131945881061256 Scheduler overhead time: 0.029821500182151794 Adapter cache time: 0.035469673573970795 Engine time: 0.03136628260836005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_128_slots_128_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_128_slots_128_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.451671316754073,
    "estimated_duration": 3600.1466033083652,
    "input_throughput": 5085.507902143563,
    "output_throughput": 4481.078905279845,
    "total_throughput": 9566.586807423408,
    "itl": 190.75292622066016,
    "ttft": 1581675.7917091106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 146610,
    "finished_requests": 74044,
    "scheduler_time": 48.18303203138977
}
#Debug simulation 
Total elapsed time: 5.451751438900828. Arrivals time: 0.23231102852150798 Scheduler time: 5.1092034499160945 Scheduler overhead time: 0.029672191943973303 Adapter cache time: 0.035186325665563345 Engine time: 0.03139250120148063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_128_slots_128_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_128_slots_128_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.457254009321332,
    "estimated_duration": 3600.1028451240018,
    "input_throughput": 5010.5640799766325,
    "output_throughput": 4425.339409838791,
    "total_throughput": 9435.903489815422,
    "itl": 157.85492615549168,
    "ttft": 1599142.098989378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 146610,
    "finished_requests": 72974,
    "scheduler_time": 43.38177912705389
}
#Debug simulation 
Total elapsed time: 5.457351907156408. Arrivals time: 0.23355083260685205 Scheduler time: 5.0873141791671515 Scheduler overhead time: 0.03525453945621848 Adapter cache time: 0.047675295267254114 Engine time: 0.03703666804358363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_128_slots_128_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_128_slots_128_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.457742327824235,
    "estimated_duration": 3600.1977099215806,
    "input_throughput": 5085.573758780822,
    "output_throughput": 4481.072513195783,
    "total_throughput": 9566.646271976606,
    "itl": 190.74792745662165,
    "ttft": 1581715.5935195012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 146610,
    "finished_requests": 74046,
    "scheduler_time": 48.184565259366934
}
#Debug simulation 
Total elapsed time: 5.457849883940071. Arrivals time: 0.2353085707873106 Scheduler time: 5.111474048346281 Scheduler overhead time: 0.029845256824046373 Adapter cache time: 0.03532408317551017 Engine time: 0.03193738032132387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_128_slots_128_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_128_slots_128_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.4518006863072515,
    "estimated_duration": 3600.057736283005,
    "input_throughput": 5010.283256907763,
    "output_throughput": 4424.859868066223,
    "total_throughput": 9435.143124973985,
    "itl": 157.7629599481102,
    "ttft": 1599164.8070834377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879448,
    "arrivals": 146610,
    "finished_requests": 72965,
    "scheduler_time": 43.363727727720764
}
#Debug simulation 
Total elapsed time: 5.451888103969395. Arrivals time: 0.2330398834310472 Scheduler time: 5.08403882291168 Scheduler overhead time: 0.035153301898390055 Adapter cache time: 0.04610839020460844 Engine time: 0.03696296224370599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_128_slots_128_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_128_slots_128_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.47507821302861,
    "estimated_duration": 3600.1266573197977,
    "input_throughput": 5085.6738506057545,
    "output_throughput": 4481.160396731824,
    "total_throughput": 9566.83424733758,
    "itl": 190.7514679190901,
    "ttft": 1581645.1562197048,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 146610,
    "finished_requests": 74045,
    "scheduler_time": 48.1831833634586
}
#Debug simulation 
Total elapsed time: 5.475160267204046. Arrivals time: 0.23389971116557717 Scheduler time: 5.130567824002355 Scheduler overhead time: 0.02988381776958704 Adapter cache time: 0.03552248887717724 Engine time: 0.03138385433703661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_128_slots_128_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_128_slots_128_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.436984322965145,
    "estimated_duration": 3600.102454176732,
    "input_throughput": 5010.092693077161,
    "output_throughput": 4424.799350229269,
    "total_throughput": 9434.89204330643,
    "itl": 157.70181130590404,
    "ttft": 1599244.9699148808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378326,
    "arrivals": 146610,
    "finished_requests": 72963,
    "scheduler_time": 43.35270214444464
}
#Debug simulation 
Total elapsed time: 5.437085773795843. Arrivals time: 0.2337240483611822 Scheduler time: 5.068448909558356 Scheduler overhead time: 0.035211579874157906 Adapter cache time: 0.04614504845812917 Engine time: 0.037027329206466675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_128_slots_128_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_128_slots_128_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.644852331839502,
    "estimated_duration": 3600.0228311947562,
    "input_throughput": 5334.676167491516,
    "output_throughput": 4680.488094129102,
    "total_throughput": 10015.164261620619,
    "itl": 182.17629485441907,
    "ttft": 1506232.5045384571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 142827,
    "finished_requests": 77209,
    "scheduler_time": 50.681021319035246
}
#Debug simulation 
Total elapsed time: 5.644933027680963. Arrivals time: 0.22575421538203955 Scheduler time: 5.308605771046132 Scheduler overhead time: 0.030875220894813538 Adapter cache time: 0.03279565321281552 Engine time: 0.032415458001196384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_128_slots_128_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_128_slots_128_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.6865025367587805,
    "estimated_duration": 3600.1675836058307,
    "input_throughput": 5334.6588885076635,
    "output_throughput": 4680.41239989257,
    "total_throughput": 10015.071288400233,
    "itl": 182.1766529543705,
    "ttft": 1506220.0292880386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 142827,
    "finished_requests": 77212,
    "scheduler_time": 50.68283997209388
}
#Debug simulation 
Total elapsed time: 5.686582524795085. Arrivals time: 0.2264526621438563 Scheduler time: 5.348126353230327 Scheduler overhead time: 0.031140089500695467 Adapter cache time: 0.03352197911590338 Engine time: 0.032876286655664444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_128_slots_128_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_128_slots_128_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.6686844122596085,
    "estimated_duration": 3600.116226203586,
    "input_throughput": 5244.807615533974,
    "output_throughput": 4614.946283976017,
    "total_throughput": 9859.753899509991,
    "itl": 150.85495757897166,
    "ttft": 1526567.7984833291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 142827,
    "finished_requests": 75976,
    "scheduler_time": 45.65524374135195
}
#Debug simulation 
Total elapsed time: 5.668763211928308. Arrivals time: 0.2249023918993771 Scheduler time: 5.31039121048525 Scheduler overhead time: 0.036565516609698534 Adapter cache time: 0.04119126591831446 Engine time: 0.038524734787642956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_128_slots_128_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_128_slots_128_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.634546088986099,
    "estimated_duration": 3600.0991624063918,
    "input_throughput": 5334.7602756482065,
    "output_throughput": 4680.501352839648,
    "total_throughput": 10015.261628487855,
    "itl": 182.17699698969156,
    "ttft": 1506196.3583464597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 142827,
    "finished_requests": 77212,
    "scheduler_time": 50.681859747325475
}
#Debug simulation 
Total elapsed time: 5.634632095228881. Arrivals time: 0.2230099057778716 Scheduler time: 5.301214722450823 Scheduler overhead time: 0.0308001646772027 Adapter cache time: 0.03258649306371808 Engine time: 0.0325582274235785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_128_slots_128_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_128_slots_128_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.628789972048253,
    "estimated_duration": 3600.1536439663296,
    "input_throughput": 5244.660052674293,
    "output_throughput": 4614.843321435585,
    "total_throughput": 9859.503374109878,
    "itl": 150.85550195921934,
    "ttft": 1526757.2735856152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794475,
    "arrivals": 142827,
    "finished_requests": 75975,
    "scheduler_time": 45.65510114263282
}
#Debug simulation 
Total elapsed time: 5.6288678222335875. Arrivals time: 0.23487180145457387 Scheduler time: 5.260285963304341 Scheduler overhead time: 0.03671643929556012 Adapter cache time: 0.041332992259413004 Engine time: 0.03832631558179855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_128_slots_128_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_128_slots_128_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.6479518339037895,
    "estimated_duration": 3600.009898765577,
    "input_throughput": 5334.6953314170805,
    "output_throughput": 4680.504907994204,
    "total_throughput": 10015.200239411284,
    "itl": 182.17708649139584,
    "ttft": 1506229.7628816592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 142827,
    "finished_requests": 77209,
    "scheduler_time": 50.68089444911226
}
#Debug simulation 
Total elapsed time: 5.648051609750837. Arrivals time: 0.2230074512772262 Scheduler time: 5.31463827053085 Scheduler overhead time: 0.030992668587714434 Adapter cache time: 0.03237091051414609 Engine time: 0.03258512681350112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_128_slots_128_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_128_slots_128_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.651799018960446,
    "estimated_duration": 3600.0225518230136,
    "input_throughput": 5244.824922121283,
    "output_throughput": 4615.002478689137,
    "total_throughput": 9859.82740081042,
    "itl": 150.85705762867477,
    "ttft": 1526749.0435237673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037832,
    "arrivals": 142827,
    "finished_requests": 75974,
    "scheduler_time": 45.654522838068154
}
#Debug simulation 
Total elapsed time: 5.651878214906901. Arrivals time: 0.22096903063356876 Scheduler time: 5.297812005970627 Scheduler overhead time: 0.03663767222315073 Adapter cache time: 0.041058225091546774 Engine time: 0.038222418166697025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.8534772540442646,
    "estimated_duration": 3600.0376634525783,
    "input_throughput": 5473.326904336582,
    "output_throughput": 4865.283821281532,
    "total_throughput": 10338.610725618115,
    "itl": 176.92455042592488,
    "ttft": 1452813.5795052312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 140936,
    "finished_requests": 79995,
    "scheduler_time": 53.086567964262485
}
#Debug simulation 
Total elapsed time: 5.853581163100898. Arrivals time: 0.2387277283705771 Scheduler time: 5.505666977725923 Scheduler overhead time: 0.03177188662812114 Adapter cache time: 0.028385287150740623 Engine time: 0.0340717313811183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.804600905161351,
    "estimated_duration": 3600.1145572667765,
    "input_throughput": 5473.468881768088,
    "output_throughput": 4865.499061590555,
    "total_throughput": 10338.967943358644,
    "itl": 176.92500736814117,
    "ttft": 1452825.5151668072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41786563513334846,
    "arrivals": 140936,
    "finished_requests": 79998,
    "scheduler_time": 53.0870086315394
}
#Debug simulation 
Total elapsed time: 5.804703781846911. Arrivals time: 0.21842980151996017 Scheduler time: 5.4781294097192585 Scheduler overhead time: 0.03161744819954038 Adapter cache time: 0.028371901251375675 Engine time: 0.03333571879193187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.683444369118661,
    "estimated_duration": 3600.0929351755917,
    "input_throughput": 5367.448382011705,
    "output_throughput": 4778.374144711061,
    "total_throughput": 10145.822526722766,
    "itl": 146.86288487231482,
    "ttft": 1480059.8845899985,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 140936,
    "finished_requests": 78430,
    "scheduler_time": 47.6423300413979
}
#Debug simulation 
Total elapsed time: 5.683523301966488. Arrivals time: 0.21355033246800303 Scheduler time: 5.341409225948155 Scheduler overhead time: 0.037597183138132095 Adapter cache time: 0.03523267572745681 Engine time: 0.03841724433004856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.8165951343253255,
    "estimated_duration": 3600.0393034359026,
    "input_throughput": 5473.324410984677,
    "output_throughput": 4865.281604921192,
    "total_throughput": 10338.60601590587,
    "itl": 176.9246406470579,
    "ttft": 1452814.7657199318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 140936,
    "finished_requests": 79995,
    "scheduler_time": 53.08656190582305
}
#Debug simulation 
Total elapsed time: 5.81669280026108. Arrivals time: 0.21946382569149137 Scheduler time: 5.489840611349791 Scheduler overhead time: 0.03165756165981293 Adapter cache time: 0.028129198122769594 Engine time: 0.03273630794137716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.727922384161502,
    "estimated_duration": 3600.0692537990376,
    "input_throughput": 5367.4836892675685,
    "output_throughput": 4778.4055770167915,
    "total_throughput": 10145.88926628436,
    "itl": 146.86098984957488,
    "ttft": 1480090.529081166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 140936,
    "finished_requests": 78430,
    "scheduler_time": 47.641505632125835
}
#Debug simulation 
Total elapsed time: 5.7280235714279115. Arrivals time: 0.21527931792661548 Scheduler time: 5.384079372975975 Scheduler overhead time: 0.037309217266738415 Adapter cache time: 0.035118944477289915 Engine time: 0.038670025300234556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.806513859890401,
    "estimated_duration": 3600.0257894368265,
    "input_throughput": 5473.344957087777,
    "output_throughput": 4865.299868515666,
    "total_throughput": 10338.644825603444,
    "itl": 176.92489710862014,
    "ttft": 1452806.9802755052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 140936,
    "finished_requests": 79995,
    "scheduler_time": 53.08650786585065
}
#Debug simulation 
Total elapsed time: 5.8065906199626625. Arrivals time: 0.21714706672355533 Scheduler time: 5.482979689724743 Scheduler overhead time: 0.03158116387203336 Adapter cache time: 0.027453266084194183 Engine time: 0.032682356890290976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.730460230726749,
    "estimated_duration": 3600.0195257913974,
    "input_throughput": 5367.54255402327,
    "output_throughput": 4778.430193713564,
    "total_throughput": 10145.972747736834,
    "itl": 146.86572250701474,
    "ttft": 1480063.5630227497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037832,
    "arrivals": 140936,
    "finished_requests": 78429,
    "scheduler_time": 47.640800609648416
}
#Debug simulation 
Total elapsed time: 5.7305502677336335. Arrivals time: 0.2165355971083045 Scheduler time: 5.384508287999779 Scheduler overhead time: 0.037330000661313534 Adapter cache time: 0.035702376160770655 Engine time: 0.03886992810294032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.953656958881766,
    "estimated_duration": 3600.1780974745884,
    "input_throughput": 5620.633049846731,
    "output_throughput": 4982.694887395531,
    "total_throughput": 10603.327937242262,
    "itl": 172.33074242130775,
    "ttft": 1416305.739154661,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 139992,
    "finished_requests": 82019,
    "scheduler_time": 54.51277310707686
}
#Debug simulation 
Total elapsed time: 5.953763646073639. Arrivals time: 0.23351771710440516 Scheduler time: 5.616149557754397 Scheduler overhead time: 0.03228799067437649 Adapter cache time: 0.02323866682127118 Engine time: 0.033524513710290194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.967439035885036,
    "estimated_duration": 3600.127482633025,
    "input_throughput": 5619.93154342484,
    "output_throughput": 4982.542170112499,
    "total_throughput": 10602.473713537338,
    "itl": 172.332222348677,
    "ttft": 1416379.038015334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 139992,
    "finished_requests": 82014,
    "scheduler_time": 54.5116605304964
}
#Debug simulation 
Total elapsed time: 5.967519728001207. Arrivals time: 0.22434165747836232 Scheduler time: 5.638519164174795 Scheduler overhead time: 0.0323106124997139 Adapter cache time: 0.023373113479465246 Engine time: 0.03374709561467171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.866149565204978,
    "estimated_duration": 3600.037252951584,
    "input_throughput": 5491.453729761133,
    "output_throughput": 4875.637324477448,
    "total_throughput": 10367.09105423858,
    "itl": 143.64296049272497,
    "ttft": 1446756.8397037038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 139992,
    "finished_requests": 80157,
    "scheduler_time": 48.780365881506405
}
#Debug simulation 
Total elapsed time: 5.866229643113911. Arrivals time: 0.2191295293159783 Scheduler time: 5.520990111865103 Scheduler overhead time: 0.03808588348329067 Adapter cache time: 0.0307350791990757 Engine time: 0.0393486600369215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.926493596285582,
    "estimated_duration": 3600.1110948558685,
    "input_throughput": 5619.957125464766,
    "output_throughput": 4982.564850743348,
    "total_throughput": 10602.521976208114,
    "itl": 172.3337955845127,
    "ttft": 1416374.6715327662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 139992,
    "finished_requests": 82014,
    "scheduler_time": 54.51153035916302
}
#Debug simulation 
Total elapsed time: 5.926588061265647. Arrivals time: 0.21978194499388337 Scheduler time: 5.602972399443388 Scheduler overhead time: 0.032325576059520245 Adapter cache time: 0.02289392799139023 Engine time: 0.033339489717036486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.849451477173716,
    "estimated_duration": 3600.121708737423,
    "input_throughput": 5491.618228355283,
    "output_throughput": 4875.851268416183,
    "total_throughput": 10367.469496771466,
    "itl": 143.64007282332366,
    "ttft": 1446711.2478888223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879447,
    "arrivals": 139992,
    "finished_requests": 80161,
    "scheduler_time": 48.77913095875501
}
#Debug simulation 
Total elapsed time: 5.849528925959021. Arrivals time: 0.2192021319642663 Scheduler time: 5.504073034506291 Scheduler overhead time: 0.038119541481137276 Adapter cache time: 0.030950938817113638 Engine time: 0.03932126658037305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.9113644734025,
    "estimated_duration": 3600.135420412601,
    "input_throughput": 5620.301082363468,
    "output_throughput": 4982.540628414527,
    "total_throughput": 10602.841710777995,
    "itl": 172.3293935913544,
    "ttft": 1416352.5228004656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 139992,
    "finished_requests": 82016,
    "scheduler_time": 54.51205692442729
}
#Debug simulation 
Total elapsed time: 5.911451864056289. Arrivals time: 0.2319536772556603 Scheduler time: 5.575841567013413 Scheduler overhead time: 0.032251755241304636 Adapter cache time: 0.02277881745249033 Engine time: 0.0333920051343739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_128_slots_128_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.817364108748734,
    "estimated_duration": 3600.0601315682543,
    "input_throughput": 5491.471330338023,
    "output_throughput": 4875.733004035577,
    "total_throughput": 10367.2043343736,
    "itl": 143.63689512396203,
    "ttft": 1446730.4556542598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037832,
    "arrivals": 139992,
    "finished_requests": 80158,
    "scheduler_time": 48.77791877491617
}
#Debug simulation 
Total elapsed time: 5.817454774864018. Arrivals time: 0.22773679625242949 Scheduler time: 5.464035851415247 Scheduler overhead time: 0.03845524648204446 Adapter cache time: 0.030092892702668905 Engine time: 0.03927248623222113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.213854586239904,
    "estimated_duration": 3600.1178661641475,
    "input_throughput": 5724.120088867224,
    "output_throughput": 5070.724259217254,
    "total_throughput": 10794.844348084478,
    "itl": 169.2560797641669,
    "ttft": 1388053.7817188508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 139506,
    "finished_requests": 83398,
    "scheduler_time": 55.62918359781179
}
#Debug simulation 
Total elapsed time: 6.21399109903723. Arrivals time: 0.3490518080070615 Scheduler time: 5.755316991358995 Scheduler overhead time: 0.03376681776717305 Adapter cache time: 0.02579683717340231 Engine time: 0.03452224936336279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.1706680208444595,
    "estimated_duration": 3600.029896392146,
    "input_throughput": 5724.125519249662,
    "output_throughput": 5070.7192788299935,
    "total_throughput": 10794.844798079655,
    "itl": 169.2578539068332,
    "ttft": 1388055.9967321067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 139506,
    "finished_requests": 83396,
    "scheduler_time": 55.62705326609739
}
#Debug simulation 
Total elapsed time: 6.170768844895065. Arrivals time: 0.31520755495876074 Scheduler time: 5.750351736787707 Scheduler overhead time: 0.03336044028401375 Adapter cache time: 0.02214842615649104 Engine time: 0.034190922044217587 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.031519983895123,
    "estimated_duration": 3600.151299782336,
    "input_throughput": 5576.47924441781,
    "output_throughput": 4950.3622253535605,
    "total_throughput": 10526.84146977137,
    "itl": 141.39783632631446,
    "ttft": 1422914.0948321524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 139506,
    "finished_requests": 81323,
    "scheduler_time": 49.680274222894504
}
#Debug simulation 
Total elapsed time: 6.0316190789453685. Arrivals time: 0.3112864992581308 Scheduler time: 5.594270913861692 Scheduler overhead time: 0.038991112262010574 Adapter cache time: 0.028760389890521765 Engine time: 0.040055034682154655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.123803548049182,
    "estimated_duration": 3600.1629329020725,
    "input_throughput": 5724.048434493601,
    "output_throughput": 5070.660784034176,
    "total_throughput": 10794.709218527776,
    "itl": 169.2563536975079,
    "ttft": 1388072.4803698834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 139506,
    "finished_requests": 83398,
    "scheduler_time": 55.629866359415495
}
#Debug simulation 
Total elapsed time: 6.123921359423548. Arrivals time: 0.32571284426376224 Scheduler time: 5.691046551801264 Scheduler overhead time: 0.03376904642209411 Adapter cache time: 0.023212465457618237 Engine time: 0.034459616523236036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.061412647832185,
    "estimated_duration": 3600.008346180414,
    "input_throughput": 5576.536238117354,
    "output_throughput": 4950.403800843544,
    "total_throughput": 10526.940038960896,
    "itl": 141.40311013503052,
    "ttft": 1422870.6881014144,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879446,
    "arrivals": 139506,
    "finished_requests": 81322,
    "scheduler_time": 49.681347766479114
}
#Debug simulation 
Total elapsed time: 6.061536537948996. Arrivals time: 0.3185928934253752 Scheduler time: 5.615692878141999 Scheduler overhead time: 0.0394794587045908 Adapter cache time: 0.029172078240662813 Engine time: 0.04029236361384392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.222122814971954,
    "estimated_duration": 3600.045626104335,
    "input_throughput": 5724.169396791631,
    "output_throughput": 5070.697678838515,
    "total_throughput": 10794.867075630145,
    "itl": 169.25450996005492,
    "ttft": 1388065.873233852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 139506,
    "finished_requests": 83397,
    "scheduler_time": 55.62847503078099
}
#Debug simulation 
Total elapsed time: 6.222243315074593. Arrivals time: 0.32679450511932373 Scheduler time: 5.7881297641433775 Scheduler overhead time: 0.03376892860978842 Adapter cache time: 0.023589818738400936 Engine time: 0.03442592220380902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.090143979061395,
    "estimated_duration": 3600.07867569019,
    "input_throughput": 5576.3589655860105,
    "output_throughput": 4950.261815204186,
    "total_throughput": 10526.620780790196,
    "itl": 141.40632400891576,
    "ttft": 1422926.2421513433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 139506,
    "finished_requests": 81320,
    "scheduler_time": 49.68086124792137
}
#Debug simulation 
Total elapsed time: 6.090241431258619. Arrivals time: 0.323138817679137 Scheduler time: 5.63869759067893 Scheduler overhead time: 0.039810985792428255 Adapter cache time: 0.029647118411958218 Engine time: 0.040578076150268316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_128_slots_128_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_128_slots_128_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.039001993834972,
    "estimated_duration": 3600.0660508737583,
    "input_throughput": 5487.219323435885,
    "output_throughput": 4840.406468589835,
    "total_throughput": 10327.62579202572,
    "itl": 176.75121000097815,
    "ttft": 1428603.7506699585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 135164,
    "finished_requests": 79494,
    "scheduler_time": 53.147554995601304
}
#Debug simulation 
Total elapsed time: 6.039136657025665. Arrivals time: 0.33892877493053675 Scheduler time: 5.578077574726194 Scheduler overhead time: 0.032676602713763714 Adapter cache time: 0.041068649385124445 Engine time: 0.033303340431302786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_128_slots_128_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_128_slots_128_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.004680758342147,
    "estimated_duration": 3600.009628685296,
    "input_throughput": 5487.217545919195,
    "output_throughput": 4840.370109333195,
    "total_throughput": 10327.58765525239,
    "itl": 176.7526448716158,
    "ttft": 1428616.6409868354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 135164,
    "finished_requests": 79493,
    "scheduler_time": 53.145688899184165
}
#Debug simulation 
Total elapsed time: 6.004783839918673. Arrivals time: 0.3411749452352524 Scheduler time: 5.541830962989479 Scheduler overhead time: 0.03267155680805445 Adapter cache time: 0.040532343089580536 Engine time: 0.033588082529604435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_128_slots_128_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_128_slots_128_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.992879858240485,
    "estimated_duration": 3600.1073546206776,
    "input_throughput": 5437.306744443596,
    "output_throughput": 4802.377345167155,
    "total_throughput": 10239.68408961075,
    "itl": 145.00679641982867,
    "ttft": 1442267.00125335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 135164,
    "finished_requests": 78769,
    "scheduler_time": 48.33138406119414
}
#Debug simulation 
Total elapsed time: 5.993098269216716. Arrivals time: 0.318584484513849 Scheduler time: 5.529636109713465 Scheduler overhead time: 0.03896881500259042 Adapter cache time: 0.04801274789497256 Engine time: 0.03988252440467477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_128_slots_128_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_128_slots_128_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.976037612184882,
    "estimated_duration": 3600.136383821165,
    "input_throughput": 5487.112124078155,
    "output_throughput": 4840.311905490749,
    "total_throughput": 10327.424029568905,
    "itl": 176.7528914287755,
    "ttft": 1428628.305348619,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 135164,
    "finished_requests": 79494,
    "scheduler_time": 53.14767758338616
}
#Debug simulation 
Total elapsed time: 5.976144880987704. Arrivals time: 0.32134512066841125 Scheduler time: 5.532969722524285 Scheduler overhead time: 0.0326809617690742 Adapter cache time: 0.040607301983982325 Engine time: 0.033457144163548946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_128_slots_128_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_128_slots_128_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.97110960772261,
    "estimated_duration": 3600.116479460164,
    "input_throughput": 5437.345461371093,
    "output_throughput": 4802.38628351061,
    "total_throughput": 10239.731744881703,
    "itl": 145.00677969654976,
    "ttft": 1442279.273748417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879446,
    "arrivals": 135164,
    "finished_requests": 78769,
    "scheduler_time": 48.32987643056728
}
#Debug simulation 
Total elapsed time: 5.9712361306883395. Arrivals time: 0.31240633502602577 Scheduler time: 5.512914293911308 Scheduler overhead time: 0.03994174627587199 Adapter cache time: 0.048013805877417326 Engine time: 0.03994106315076351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_128_slots_128_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_128_slots_128_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.941035215742886,
    "estimated_duration": 3600.192392831884,
    "input_throughput": 5487.138698290804,
    "output_throughput": 4840.237714710855,
    "total_throughput": 10327.376413001659,
    "itl": 176.74807922762668,
    "ttft": 1428639.7545324452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 135164,
    "finished_requests": 79496,
    "scheduler_time": 53.148984787915026
}
#Debug simulation 
Total elapsed time: 5.941139642614871. Arrivals time: 0.30094909109175205 Scheduler time: 5.520825802814215 Scheduler overhead time: 0.03225372172892094 Adapter cache time: 0.03898042952641845 Engine time: 0.0330927656032145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_128_slots_128_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_128_slots_128_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.961959275882691,
    "estimated_duration": 3600.0456178115915,
    "input_throughput": 5437.452209813765,
    "output_throughput": 4802.446923022524,
    "total_throughput": 10239.89913283629,
    "itl": 145.00291499803092,
    "ttft": 1442244.1146110515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 135164,
    "finished_requests": 78768,
    "scheduler_time": 48.32987568711035
}
#Debug simulation 
Total elapsed time: 5.9620732381008565. Arrivals time: 0.29138220753520727 Scheduler time: 5.527103011496365 Scheduler overhead time: 0.03927074884995818 Adapter cache time: 0.04663792531937361 Engine time: 0.039582967292517424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.134514223318547,
    "estimated_duration": 3600.004128246934,
    "input_throughput": 5747.5525757454725,
    "output_throughput": 5055.156703073566,
    "total_throughput": 10802.709278819038,
    "itl": 168.97003676458894,
    "ttft": 1360462.8633896995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 133259,
    "finished_requests": 83182,
    "scheduler_time": 55.83218819931076
}
#Debug simulation 
Total elapsed time: 6.134611031040549. Arrivals time: 0.3000737736001611 Scheduler time: 5.71626598155126 Scheduler overhead time: 0.033485652413219213 Adapter cache time: 0.034911917988210917 Engine time: 0.03437460865825415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.155767942313105,
    "estimated_duration": 3600.133070068025,
    "input_throughput": 5747.508382963473,
    "output_throughput": 5055.273137349925,
    "total_throughput": 10802.781520313398,
    "itl": 168.97162037133646,
    "ttft": 1360453.5568485954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 133259,
    "finished_requests": 83185,
    "scheduler_time": 55.83364796182271
}
#Debug simulation 
Total elapsed time: 6.155922594945878. Arrivals time: 0.301783409435302 Scheduler time: 5.7347540822811425 Scheduler overhead time: 0.033479786943644285 Adapter cache time: 0.03573976969346404 Engine time: 0.034536061342805624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.101848392747343,
    "estimated_duration": 3600.120795187363,
    "input_throughput": 5673.417688457593,
    "output_throughput": 4996.561510948921,
    "total_throughput": 10669.979199406513,
    "itl": 139.35112669908133,
    "ttft": 1379342.7769496492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 133259,
    "finished_requests": 82118,
    "scheduler_time": 50.64304020090123
}
#Debug simulation 
Total elapsed time: 6.1019772458821535. Arrivals time: 0.29321574280038476 Scheduler time: 5.667357771191746 Scheduler overhead time: 0.03988338075578213 Adapter cache time: 0.042014604434370995 Engine time: 0.04091119719669223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.181014129891992,
    "estimated_duration": 3600.0891452341402,
    "input_throughput": 5747.417123654114,
    "output_throughput": 5055.106211492548,
    "total_throughput": 10802.523335146661,
    "itl": 168.97171144424775,
    "ttft": 1360469.945350038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4002960364404137,
    "arrivals": 133259,
    "finished_requests": 83183,
    "scheduler_time": 55.83316370889377
}
#Debug simulation 
Total elapsed time: 6.181125108618289. Arrivals time: 0.2983721769414842 Scheduler time: 5.7631304720416665 Scheduler overhead time: 0.03379536932334304 Adapter cache time: 0.035797178745269775 Engine time: 0.03440626850351691 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.131705874111503,
    "estimated_duration": 3600.1162574559235,
    "input_throughput": 5673.328731454184,
    "output_throughput": 4996.518365968416,
    "total_throughput": 10669.8470974226,
    "itl": 139.34974864635453,
    "ttft": 1379360.2961080496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 133259,
    "finished_requests": 82118,
    "scheduler_time": 50.64262346270017
}
#Debug simulation 
Total elapsed time: 6.131806991063058. Arrivals time: 0.2955703204497695 Scheduler time: 5.693259978666902 Scheduler overhead time: 0.039929548278450966 Adapter cache time: 0.04181524785235524 Engine time: 0.04261391097679734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.160880314651877,
    "estimated_duration": 3600.1685943555394,
    "input_throughput": 5747.577497465529,
    "output_throughput": 5055.297140399043,
    "total_throughput": 10802.874637864572,
    "itl": 168.9714506316693,
    "ttft": 1360400.8997748946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 133259,
    "finished_requests": 83187,
    "scheduler_time": 55.83434105231996
}
#Debug simulation 
Total elapsed time: 6.161044302862138. Arrivals time: 0.30026177084073424 Scheduler time: 5.7420253357850015 Scheduler overhead time: 0.03358573419973254 Adapter cache time: 0.03499756706878543 Engine time: 0.034455787390470505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.083847508765757,
    "estimated_duration": 3600.007580733585,
    "input_throughput": 5673.303053389168,
    "output_throughput": 4996.262256851919,
    "total_throughput": 10669.565310241087,
    "itl": 139.3498726979121,
    "ttft": 1379280.09380254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037832,
    "arrivals": 133259,
    "finished_requests": 82113,
    "scheduler_time": 50.63986877026459
}
#Debug simulation 
Total elapsed time: 6.083933848887682. Arrivals time: 0.29393806774169207 Scheduler time: 5.648023589514196 Scheduler overhead time: 0.039964073337614536 Adapter cache time: 0.04264154518023133 Engine time: 0.04093792103230953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.399496098048985,
    "estimated_duration": 3600.067647975329,
    "input_throughput": 5883.50138695648,
    "output_throughput": 5183.288433619978,
    "total_throughput": 11066.789820576458,
    "itl": 164.93250723855158,
    "ttft": 1318331.8767576073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 132319,
    "finished_requests": 85453,
    "scheduler_time": 57.614024205973465
}
#Debug simulation 
Total elapsed time: 6.399582660757005. Arrivals time: 0.39090735698118806 Scheduler time: 5.891812303569168 Scheduler overhead time: 0.03456530114635825 Adapter cache time: 0.0308597544208169 Engine time: 0.03552888985723257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.333999239839613,
    "estimated_duration": 3600.010727376196,
    "input_throughput": 5883.499690412631,
    "output_throughput": 5183.125110740852,
    "total_throughput": 11066.624801153483,
    "itl": 164.93297921995338,
    "ttft": 1318362.492557806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 132319,
    "finished_requests": 85451,
    "scheduler_time": 57.61214102600065
}
#Debug simulation 
Total elapsed time: 6.334104300942272. Arrivals time: 0.32660125521942973 Scheduler time: 5.890471255406737 Scheduler overhead time: 0.03448132844641805 Adapter cache time: 0.030993307009339333 Engine time: 0.03549497667700052 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.3286707028746605,
    "estimated_duration": 3600.019682446065,
    "input_throughput": 5788.823072722803,
    "output_throughput": 5108.104850000499,
    "total_throughput": 10896.927922723302,
    "itl": 136.66287707210438,
    "ttft": 1342708.2685266752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 132319,
    "finished_requests": 84083,
    "scheduler_time": 52.208113372712376
}
#Debug simulation 
Total elapsed time: 6.328770752064884. Arrivals time: 0.31655898597091436 Scheduler time: 5.87284255027771 Scheduler overhead time: 0.040546403266489506 Adapter cache time: 0.03798359399661422 Engine time: 0.04196918196976185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.364723498001695,
    "estimated_duration": 3600.1263573521724,
    "input_throughput": 5883.405441240747,
    "output_throughput": 5183.2039066884945,
    "total_throughput": 11066.609347929241,
    "itl": 164.93272003458492,
    "ttft": 1318391.9821056258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 132319,
    "finished_requests": 85453,
    "scheduler_time": 57.61425005934901
}
#Debug simulation 
Total elapsed time: 6.364822105970234. Arrivals time: 0.3248308305628598 Scheduler time: 5.9221555525437 Scheduler overhead time: 0.034571582451462746 Adapter cache time: 0.03193930210545659 Engine time: 0.035364840645343065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.305282685905695,
    "estimated_duration": 3600.1024683213977,
    "input_throughput": 5788.76884293714,
    "output_throughput": 5108.012108492657,
    "total_throughput": 10896.780951429797,
    "itl": 136.66496657278188,
    "ttft": 1342672.7333272202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879447,
    "arrivals": 132319,
    "finished_requests": 84084,
    "scheduler_time": 52.2106156982219
}
#Debug simulation 
Total elapsed time: 6.305400623008609. Arrivals time: 0.3205752200447023 Scheduler time: 5.8450546227395535 Scheduler overhead time: 0.0408154521137476 Adapter cache time: 0.03818924492225051 Engine time: 0.04166866093873978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.364166984334588,
    "estimated_duration": 3600.0406794416563,
    "input_throughput": 5883.450740141355,
    "output_throughput": 5183.081987532969,
    "total_throughput": 11066.532727674325,
    "itl": 164.93225191042157,
    "ttft": 1318378.1645471954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 132319,
    "finished_requests": 85451,
    "scheduler_time": 57.613559761342884
}
#Debug simulation 
Total elapsed time: 6.3642780049704015. Arrivals time: 0.3674317942932248 Scheduler time: 5.877161898650229 Scheduler overhead time: 0.034636808559298515 Adapter cache time: 0.03373260237276554 Engine time: 0.03538909601047635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_128_slots_128_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.329774698242545,
    "estimated_duration": 3600.109925617152,
    "input_throughput": 5788.772129347536,
    "output_throughput": 5108.067636809047,
    "total_throughput": 10896.839766156583,
    "itl": 136.67017474588857,
    "ttft": 1342702.5100006382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 132319,
    "finished_requests": 84085,
    "scheduler_time": 52.21134984283157
}
#Debug simulation 
Total elapsed time: 6.330002050381154. Arrivals time: 0.3243225459009409 Scheduler time: 5.86529033863917 Scheduler overhead time: 0.04089050926268101 Adapter cache time: 0.038521507289260626 Engine time: 0.04194699414074421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.444668342359364,
    "estimated_duration": 3600.039914307124,
    "input_throughput": 5982.33144982922,
    "output_throughput": 5256.349776789071,
    "total_throughput": 11238.681226618291,
    "itl": 162.3651163333565,
    "ttft": 1294184.5107550735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 131875,
    "finished_requests": 86839,
    "scheduler_time": 58.59774623661879
}
#Debug simulation 
Total elapsed time: 6.444770284928381. Arrivals time: 0.33906765282154083 Scheduler time: 5.987534963525832 Scheduler overhead time: 0.0349751403555274 Adapter cache time: 0.03107704920694232 Engine time: 0.03585481597110629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.469216392375529,
    "estimated_duration": 3600.1470002687142,
    "input_throughput": 5982.619875908507,
    "output_throughput": 5256.429250968787,
    "total_throughput": 11239.049126877293,
    "itl": 162.36446933571364,
    "ttft": 1294131.3731971623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 131875,
    "finished_requests": 86844,
    "scheduler_time": 58.59886964794234
}
#Debug simulation 
Total elapsed time: 6.469349036924541. Arrivals time: 0.3598549887537956 Scheduler time: 5.989695942495018 Scheduler overhead time: 0.035369052551686764 Adapter cache time: 0.031641047447919846 Engine time: 0.03652547160163522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.346880893222988,
    "estimated_duration": 3600.0001936964727,
    "input_throughput": 5872.94440623094,
    "output_throughput": 5168.564721907568,
    "total_throughput": 11041.509128138508,
    "itl": 135.33805988739124,
    "ttft": 1320826.0136590097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 131875,
    "finished_requests": 85262,
    "scheduler_time": 53.06428372722647
}
#Debug simulation 
Total elapsed time: 6.346978513058275. Arrivals time: 0.35110585298389196 Scheduler time: 5.857385546900332 Scheduler overhead time: 0.041367955040186644 Adapter cache time: 0.03567419806495309 Engine time: 0.042392530012875795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.443474233150482,
    "estimated_duration": 3600.0733537230076,
    "input_throughput": 5982.50226699606,
    "output_throughput": 5256.302064076208,
    "total_throughput": 11238.804331072268,
    "itl": 162.36305396974768,
    "ttft": 1294190.2171335223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 131875,
    "finished_requests": 86841,
    "scheduler_time": 58.5979393570032
}
#Debug simulation 
Total elapsed time: 6.443598584271967. Arrivals time: 0.35857557132840157 Scheduler time: 5.966002951841801 Scheduler overhead time: 0.035339313093572855 Adapter cache time: 0.03121041227132082 Engine time: 0.036146857775747776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.375314890872687,
    "estimated_duration": 3600.0168671216807,
    "input_throughput": 5872.619707169115,
    "output_throughput": 5168.308562641831,
    "total_throughput": 11040.928269810945,
    "itl": 135.33729884455403,
    "ttft": 1320774.8019492791,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879445,
    "arrivals": 131875,
    "finished_requests": 85259,
    "scheduler_time": 53.062748520540595
}
#Debug simulation 
Total elapsed time: 6.375429036095738. Arrivals time: 0.3430321994237602 Scheduler time: 5.893129021860659 Scheduler overhead time: 0.04152711480855942 Adapter cache time: 0.036037336103618145 Engine time: 0.04252033634111285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.479750245809555,
    "estimated_duration": 3600.1800626655645,
    "input_throughput": 5982.564934280839,
    "output_throughput": 5256.380978341616,
    "total_throughput": 11238.945912622456,
    "itl": 162.363199068112,
    "ttft": 1294145.444742301,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 131875,
    "finished_requests": 86844,
    "scheduler_time": 58.59945984452403
}
#Debug simulation 
Total elapsed time: 6.479968751780689. Arrivals time: 0.3491582376882434 Scheduler time: 6.012320663779974 Scheduler overhead time: 0.03527436684817076 Adapter cache time: 0.030597071163356304 Engine time: 0.03627661755308509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.393053628038615,
    "estimated_duration": 3600.1166053315105,
    "input_throughput": 5872.782556178652,
    "output_throughput": 5168.372594500069,
    "total_throughput": 11041.155150678722,
    "itl": 135.3417917005095,
    "ttft": 1320876.6169457417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 131875,
    "finished_requests": 85262,
    "scheduler_time": 53.064652015277524
}
#Debug simulation 
Total elapsed time: 6.393163546919823. Arrivals time: 0.33647485403344035 Scheduler time: 5.918646382167935 Scheduler overhead time: 0.04124778276309371 Adapter cache time: 0.034883761778473854 Engine time: 0.04263828415423632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.519229602068663,
    "estimated_duration": 3600.041240621388,
    "input_throughput": 6022.923780800196,
    "output_throughput": 5329.802832116479,
    "total_throughput": 11352.726612916676,
    "itl": 160.7650307091887,
    "ttft": 1257542.7252504374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 129404,
    "finished_requests": 87632,
    "scheduler_time": 60.04737141735435
}
#Debug simulation 
Total elapsed time: 6.51932412898168. Arrivals time: 0.343380028847605 Scheduler time: 6.049633136950433 Scheduler overhead time: 0.035666839219629765 Adapter cache time: 0.03717731684446335 Engine time: 0.036719927098602057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.484362538903952,
    "estimated_duration": 3600.1618003657104,
    "input_throughput": 6022.940135023195,
    "output_throughput": 5329.7912882834435,
    "total_throughput": 11352.731423306637,
    "itl": 160.76568018127446,
    "ttft": 1257566.6389765325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 129404,
    "finished_requests": 87635,
    "scheduler_time": 60.049291904637
}
#Debug simulation 
Total elapsed time: 6.484492307063192. Arrivals time: 0.3393856515176594 Scheduler time: 6.02158790640533 Scheduler overhead time: 0.035531812347471714 Adapter cache time: 0.03518205229192972 Engine time: 0.03637306485325098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.4944629007950425,
    "estimated_duration": 3600.0167101502934,
    "input_throughput": 5945.928511844703,
    "output_throughput": 5265.111116444981,
    "total_throughput": 11211.039628289685,
    "itl": 132.5969011643482,
    "ttft": 1278413.6756668286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 129404,
    "finished_requests": 86490,
    "scheduler_time": 54.81862639691585
}
#Debug simulation 
Total elapsed time: 6.494590894784778. Arrivals time: 0.3371158605441451 Scheduler time: 6.012139214668423 Scheduler overhead time: 0.04208270041272044 Adapter cache time: 0.03983139619231224 Engine time: 0.04373347153887153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.483129750005901,
    "estimated_duration": 3600.10069193986,
    "input_throughput": 6022.824319482176,
    "output_throughput": 5329.714816854497,
    "total_throughput": 11352.539136336674,
    "itl": 160.76605662562088,
    "ttft": 1257591.1035131307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 129404,
    "finished_requests": 87632,
    "scheduler_time": 60.04789106079498
}
#Debug simulation 
Total elapsed time: 6.483230142854154. Arrivals time: 0.3376312516629696 Scheduler time: 6.020183761138469 Scheduler overhead time: 0.03544865967705846 Adapter cache time: 0.0367660541087389 Engine time: 0.036760243121534586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.455433398019522,
    "estimated_duration": 3600.1396410376597,
    "input_throughput": 5945.871864523068,
    "output_throughput": 5265.079938548339,
    "total_throughput": 11210.951803071408,
    "itl": 132.5952157999787,
    "ttft": 1278445.3706278473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42398110998794464,
    "arrivals": 129404,
    "finished_requests": 86492,
    "scheduler_time": 54.81819527276472
}
#Debug simulation 
Total elapsed time: 6.4555275570601225. Arrivals time: 0.3308344576507807 Scheduler time: 5.979414229746908 Scheduler overhead time: 0.042004129849374294 Adapter cache time: 0.040120824705809355 Engine time: 0.04361027153208852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.461192382033914,
    "estimated_duration": 3600.1538993864847,
    "input_throughput": 6022.953353103925,
    "output_throughput": 5329.802985164028,
    "total_throughput": 11352.756338267953,
    "itl": 160.7631896268822,
    "ttft": 1257548.7199815384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 129404,
    "finished_requests": 87635,
    "scheduler_time": 60.04985573975999
}
#Debug simulation 
Total elapsed time: 6.461299302987754. Arrivals time: 0.3362574130296707 Scheduler time: 6.0005291714333 Scheduler overhead time: 0.03548271721228957 Adapter cache time: 0.03579637361690402 Engine time: 0.03686445951461792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_128_slots_128_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.47309982823208,
    "estimated_duration": 3600.1423012192936,
    "input_throughput": 5945.787751987009,
    "output_throughput": 5264.930220558366,
    "total_throughput": 11210.717972545375,
    "itl": 132.58901967670303,
    "ttft": 1278405.9179908533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4293885228037831,
    "arrivals": 129404,
    "finished_requests": 86491,
    "scheduler_time": 54.81784502186068
}
#Debug simulation 
Total elapsed time: 6.473216424230486. Arrivals time: 0.3345018485561013 Scheduler time: 5.993782839272171 Scheduler overhead time: 0.04224546765908599 Adapter cache time: 0.03943837061524391 Engine time: 0.04357366403564811 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.508660677820444,
    "estimated_duration": 3600.0617611037237,
    "input_throughput": 6245.9264568578465,
    "output_throughput": 5486.316155294131,
    "total_throughput": 11732.242612151978,
    "itl": 155.331126980331,
    "ttft": 1206384.693207994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 128419,
    "finished_requests": 90258,
    "scheduler_time": 62.48407603961342
}
#Debug simulation 
Total elapsed time: 6.508730806875974. Arrivals time: 0.23329123994335532 Scheduler time: 6.155879120808095 Scheduler overhead time: 0.03577411128208041 Adapter cache time: 0.029941083397716284 Engine time: 0.037082800175994635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.492434344720095,
    "estimated_duration": 3600.1472188332173,
    "input_throughput": 6246.000964173828,
    "output_throughput": 5486.485357229791,
    "total_throughput": 11732.486321403618,
    "itl": 155.3320669501709,
    "ttft": 1206335.6679480968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 128419,
    "finished_requests": 90262,
    "scheduler_time": 62.48579629443362
}
#Debug simulation 
Total elapsed time: 6.4925073138438165. Arrivals time: 0.24510741839185357 Scheduler time: 6.127893168479204 Scheduler overhead time: 0.03578409552574158 Adapter cache time: 0.029900339897722006 Engine time: 0.037039848044514656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.442391117103398,
    "estimated_duration": 3600.136074625902,
    "input_throughput": 6141.959509765946,
    "output_throughput": 5402.592734504097,
    "total_throughput": 11544.552244270044,
    "itl": 128.4416679379567,
    "ttft": 1231928.3780828493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41857369717210635,
    "arrivals": 128419,
    "finished_requests": 88750,
    "scheduler_time": 56.812495695687375
}
#Debug simulation 
Total elapsed time: 6.442462889011949. Arrivals time: 0.23234012350440025 Scheduler time: 6.070179264061153 Scheduler overhead time: 0.04245119448751211 Adapter cache time: 0.033647509291768074 Engine time: 0.043920597061514854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.499265537131578,
    "estimated_duration": 3600.103542077768,
    "input_throughput": 6245.931189807503,
    "output_throughput": 5486.408312745504,
    "total_throughput": 11732.339502553008,
    "itl": 155.33181175021096,
    "ttft": 1206350.596440954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40029603644041367,
    "arrivals": 128419,
    "finished_requests": 90260,
    "scheduler_time": 62.48410318245855
}
#Debug simulation 
Total elapsed time: 6.499334423337132. Arrivals time: 0.2338117640465498 Scheduler time: 6.146124367136508 Scheduler overhead time: 0.03587403800338507 Adapter cache time: 0.029770642518997192 Engine time: 0.03702743956819177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.483814540784806,
    "estimated_duration": 3600.0707820724133,
    "input_throughput": 6142.03173729578,
    "output_throughput": 5402.804882853763,
    "total_throughput": 11544.836620149543,
    "itl": 128.43942607921406,
    "ttft": 1231838.9159084703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4239811099879448,
    "arrivals": 128419,
    "finished_requests": 88750,
    "scheduler_time": 56.81146006704146
}
#Debug simulation 
Total elapsed time: 6.483885582070798. Arrivals time: 0.2542974906973541 Scheduler time: 6.08865003567189 Scheduler overhead time: 0.04249246045947075 Adapter cache time: 0.034073940012604 Engine time: 0.04428537515923381 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.535231751855463,
    "estimated_duration": 3600.0269519747385,
    "input_throughput": 6245.986849533393,
    "output_throughput": 5486.369203198841,
    "total_throughput": 11732.356052732235,
    "itl": 155.33090013862474,
    "ttft": 1206375.5638536005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 128419,
    "finished_requests": 90258,
    "scheduler_time": 62.48398623450667
}
#Debug simulation 
Total elapsed time: 6.535304455086589. Arrivals time: 0.24549380131065845 Scheduler time: 6.170121656730771 Scheduler overhead time: 0.03581067454069853 Adapter cache time: 0.029992399271577597 Engine time: 0.03705780254676938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_128_slots_128_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.448039819020778,
    "estimated_duration": 3600.120137826536,
    "input_throughput": 6142.043085637,
    "output_throughput": 5402.532208756292,
    "total_throughput": 11544.575294393291,
    "itl": 128.44459006629867,
    "ttft": 1231930.3442146997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378326,
    "arrivals": 128419,
    "finished_requests": 88751,
    "scheduler_time": 56.812887530271894
}
#Debug simulation 
Total elapsed time: 6.44811664102599. Arrivals time: 0.2360273115336895 Scheduler time: 6.072109862696379 Scheduler overhead time: 0.04249606607481837 Adapter cache time: 0.03351149708032608 Engine time: 0.0440010572783649 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_128_slots_128_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.596245032735169,
    "estimated_duration": 3600.1311454861607,
    "input_throughput": 6329.965514887547,
    "output_throughput": 5562.9420681272195,
    "total_throughput": 11892.907583014767,
    "itl": 153.292791077387,
    "ttft": 1186171.8671990496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 127927,
    "finished_requests": 91657,
    "scheduler_time": 63.48453398528356
}
#Debug simulation 
Total elapsed time: 6.596314199734479. Arrivals time: 0.23798760818317533 Scheduler time: 6.240214471705258 Scheduler overhead time: 0.036291065625846386 Adapter cache time: 0.02704957313835621 Engine time: 0.03777599101886153 
