INFO 06-01 00:47:09 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:09 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 135, 33, 33, 33, 17280, 33, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 33, 17280, 135, 17280, 17280, 17280, 135, 17280, 33, 135, 135, 135, 17280, 33, 33, 17280, 33, 17280, 33, 33, 135, 33, 135, 17280, 33, 33, 135, 33, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 33, 17280, 17280, 17280, 135, 135, 33, 33, 33, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 33, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 135, 33, 33, 135, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 17280, 135, 17280, 33, 33, 33, 135, 135, 135, 17280, 17280, 135, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 942024 . Total input tokens: 209985148 . Total output tokens: 188245389
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.810647982172668,
    "estimated_duration": 3600.046054153623,
    "input_throughput": 6326.361567992353,
    "output_throughput": 5621.283365708977,
    "total_throughput": 11947.644933701331,
    "itl": 153.6556612775095,
    "ttft": 1759581.3550662247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 831,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.792148890905103,
    "arrivals": 314142,
    "finished_requests": 92539,
    "scheduler_time": 139.06296871333174
}
#Debug simulation 
Total elapsed time: 6.810762007255107. Arrivals time: 0.2911055041477084 Scheduler time: 6.411972706206143 Scheduler overhead time: 0.037035224959254265 Adapter cache time: 0.015033061150461435 Engine time: 0.03819633275270462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 66, 33, 33, 33, 17280, 33, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 17280, 17280, 66, 66, 66, 17280, 17280, 33, 17280, 66, 17280, 17280, 17280, 66, 17280, 33, 66, 66, 66, 17280, 33, 33, 17280, 33, 17280, 33, 33, 66, 33, 66, 17280, 33, 33, 66, 33, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 33, 17280, 17280, 17280, 66, 66, 33, 33, 33, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 33, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 66, 33, 33, 66, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 17280, 66, 17280, 33, 33, 33, 66, 66, 66, 17280, 17280, 66, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 66, 17280, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 938367 . Total input tokens: 209161351 . Total output tokens: 187500351
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.89641915820539,
    "estimated_duration": 3600.0371709374863,
    "input_throughput": 6489.597437660926,
    "output_throughput": 5736.349104035015,
    "total_throughput": 12225.946541695941,
    "itl": 150.10882088439467,
    "ttft": 1743228.4529241207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6526633817237084,
    "arrivals": 312976,
    "finished_requests": 94513,
    "scheduler_time": 141.92108162942367
}
#Debug simulation 
Total elapsed time: 6.896548682823777. Arrivals time: 0.29533691238611937 Scheduler time: 6.495800211559981 Scheduler overhead time: 0.037250516936182976 Adapter cache time: 0.012169311288744211 Engine time: 0.038405075669288635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 66, 33, 33, 33, 17280, 33, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 17280, 17280, 66, 66, 66, 17280, 17280, 33, 17280, 66, 17280, 17280, 17280, 66, 17280, 33, 66, 66, 66, 17280, 33, 33, 17280, 33, 17280, 33, 33, 66, 33, 66, 17280, 33, 33, 66, 33, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 33, 17280, 17280, 17280, 66, 66, 33, 33, 33, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 33, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 66, 33, 33, 66, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 17280, 66, 17280, 33, 33, 33, 66, 66, 66, 17280, 17280, 66, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 66, 17280, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 938367 . Total input tokens: 209161351 . Total output tokens: 187500351
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.894293340854347,
    "estimated_duration": 3600.0070352114767,
    "input_throughput": 6489.421762651538,
    "output_throughput": 5736.11406811791,
    "total_throughput": 12225.535830769448,
    "itl": 150.1117070439469,
    "ttft": 1743260.0105656348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7633558551594672,
    "arrivals": 312976,
    "finished_requests": 94509,
    "scheduler_time": 141.91682477715668
}
#Debug simulation 
Total elapsed time: 6.894376050680876. Arrivals time: 0.28261030092835426 Scheduler time: 6.506019125226885 Scheduler overhead time: 0.03727872250601649 Adapter cache time: 0.012218873482197523 Engine time: 0.03872230462729931 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 66, 33, 33, 33, 17280, 33, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 17280, 17280, 66, 66, 66, 17280, 17280, 33, 17280, 66, 17280, 17280, 17280, 66, 17280, 33, 66, 66, 66, 17280, 33, 33, 17280, 33, 17280, 33, 33, 66, 33, 66, 17280, 33, 33, 66, 33, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 33, 17280, 17280, 17280, 66, 66, 33, 33, 33, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 33, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 66, 33, 33, 66, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 17280, 66, 17280, 33, 33, 33, 66, 66, 66, 17280, 17280, 66, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 66, 17280, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 938367 . Total input tokens: 209161351 . Total output tokens: 187500351
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.917356918100268,
    "estimated_duration": 3600.0101002047327,
    "input_throughput": 6489.416237657612,
    "output_throughput": 5736.109184478574,
    "total_throughput": 12225.525422136187,
    "itl": 150.1117817224909,
    "ttft": 1743261.3084445565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7662582663446773,
    "arrivals": 312976,
    "finished_requests": 94509,
    "scheduler_time": 141.91683716590052
}
#Debug simulation 
Total elapsed time: 6.917438342235982. Arrivals time: 0.28170693945139647 Scheduler time: 6.530051674228162 Scheduler overhead time: 0.037314462941139936 Adapter cache time: 0.01233332185074687 Engine time: 0.038513720501214266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 66, 33, 33, 33, 17280, 33, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 17280, 17280, 66, 66, 66, 17280, 17280, 33, 17280, 66, 17280, 17280, 17280, 66, 17280, 33, 66, 66, 66, 17280, 33, 33, 17280, 33, 17280, 33, 33, 66, 33, 66, 17280, 33, 33, 66, 33, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 33, 17280, 17280, 17280, 66, 66, 33, 33, 33, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 33, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 66, 33, 33, 66, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 17280, 66, 17280, 33, 33, 33, 66, 66, 66, 17280, 17280, 66, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 66, 17280, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 938367 . Total input tokens: 209161351 . Total output tokens: 187500351
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.94615312712267,
    "estimated_duration": 3600.1010689135046,
    "input_throughput": 6489.4822541887115,
    "output_throughput": 5736.247289921893,
    "total_throughput": 12225.729544110603,
    "itl": 150.11024283568452,
    "ttft": 1743241.4906214676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6910344837955125,
    "arrivals": 312976,
    "finished_requests": 94513,
    "scheduler_time": 141.9223865572569
}
#Debug simulation 
Total elapsed time: 6.946233737748116. Arrivals time: 0.29558105347678065 Scheduler time: 6.544781521428376 Scheduler overhead time: 0.037478381767868996 Adapter cache time: 0.012168626300990582 Engine time: 0.03861828334629536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 66, 33, 33, 33, 17280, 33, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 17280, 17280, 66, 66, 66, 17280, 17280, 33, 17280, 66, 17280, 17280, 17280, 66, 17280, 33, 66, 66, 66, 17280, 33, 33, 17280, 33, 17280, 33, 33, 66, 33, 66, 17280, 33, 33, 66, 33, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 33, 17280, 17280, 17280, 66, 66, 33, 33, 33, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 33, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 66, 33, 33, 66, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 17280, 66, 17280, 33, 33, 33, 66, 66, 66, 17280, 17280, 66, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 66, 17280, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 938367 . Total input tokens: 209161351 . Total output tokens: 187500351
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.902606495190412,
    "estimated_duration": 3600.0271329041843,
    "input_throughput": 6489.385534478911,
    "output_throughput": 5736.082045398742,
    "total_throughput": 12225.467579877652,
    "itl": 150.11220188842066,
    "ttft": 1743268.9725956442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7885166865401008,
    "arrivals": 312976,
    "finished_requests": 94509,
    "scheduler_time": 141.91685724616795
}
#Debug simulation 
Total elapsed time: 6.902719371020794. Arrivals time: 0.2773389667272568 Scheduler time: 6.519691185094416 Scheduler overhead time: 0.037441795226186514 Adapter cache time: 0.012152797542512417 Engine time: 0.03838846040889621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 66, 33, 33, 33, 17280, 33, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 17280, 17280, 66, 66, 66, 17280, 17280, 33, 17280, 66, 17280, 17280, 17280, 66, 17280, 33, 66, 66, 66, 17280, 33, 33, 17280, 33, 17280, 33, 33, 66, 33, 66, 17280, 33, 33, 66, 33, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 33, 17280, 17280, 17280, 66, 66, 33, 33, 33, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 33, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 66, 33, 33, 66, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 17280, 66, 17280, 33, 33, 33, 66, 66, 66, 17280, 17280, 66, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 66, 17280, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 938367 . Total input tokens: 209161351 . Total output tokens: 187500351
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.935384391341358,
    "estimated_duration": 3600.106151509501,
    "input_throughput": 6489.473092398715,
    "output_throughput": 5736.239191541933,
    "total_throughput": 12225.712283940647,
    "itl": 150.10652613935883,
    "ttft": 1743225.4133395713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6146271592471544,
    "arrivals": 312976,
    "finished_requests": 94513,
    "scheduler_time": 141.92593717618647
}
#Debug simulation 
Total elapsed time: 6.935475922189653. Arrivals time: 0.2805243469774723 Scheduler time: 6.548772277310491 Scheduler overhead time: 0.0375756467692554 Adapter cache time: 0.012251547537744045 Engine time: 0.038630958180874586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 66, 33, 33, 33, 17280, 33, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 17280, 17280, 66, 66, 66, 17280, 17280, 33, 17280, 66, 17280, 17280, 17280, 66, 17280, 33, 66, 66, 66, 17280, 33, 33, 17280, 33, 17280, 33, 33, 66, 33, 66, 17280, 33, 33, 66, 33, 33, 33, 33, 33, 17280, 66, 66, 17280, 66, 33, 17280, 17280, 17280, 66, 66, 33, 33, 33, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 33, 33, 17280, 66, 17280, 17280, 33, 17280, 33, 66, 33, 33, 66, 66, 17280, 17280, 17280, 33, 17280, 66, 66, 33, 33, 33, 33, 17280, 66, 17280, 66, 17280, 17280, 17280, 33, 66, 66, 17280, 66, 17280, 33, 33, 33, 66, 66, 66, 17280, 17280, 66, 17280, 33, 66, 66, 33, 17280, 17280, 17280, 17280, 17280, 17280, 66, 66, 66, 66, 66, 17280, 33, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 938367 . Total input tokens: 209161351 . Total output tokens: 187500351
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.9254169780761,
    "estimated_duration": 3600.055043958454,
    "input_throughput": 6489.335222583782,
    "output_throughput": 5736.037573829471,
    "total_throughput": 12225.372796413252,
    "itl": 150.11310346665874,
    "ttft": 1743279.5442179928,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8120326445996726,
    "arrivals": 312976,
    "finished_requests": 94509,
    "scheduler_time": 141.91704069640238
}
#Debug simulation 
Total elapsed time: 6.925499759148806. Arrivals time: 0.2651187679730356 Scheduler time: 6.554797441698611 Scheduler overhead time: 0.03730261279270053 Adapter cache time: 0.012143993750214577 Engine time: 0.0384670696221292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [53 53 54]
Adapter prompts. [1080, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 4320, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 1080, 8640, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 752760 . Total input tokens: 167901726 . Total output tokens: 150505799
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 74.11773832095787,
    "estimated_duration": 3600.002166110147,
    "input_throughput": 6304.860650827047,
    "output_throughput": 5574.34664593088,
    "total_throughput": 11879.207296757928,
    "itl": 153.5213808547558,
    "ttft": 1652478.3909820158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8263316908618424,
    "arrivals": 250580,
    "finished_requests": 91792,
    "scheduler_time": 136.8828187272388
}
#Debug simulation 
Total elapsed time: 74.11789773683995. Arrivals time: 0.39932795939967036 Scheduler time: 73.56817623972893 Scheduler overhead time: 0.05886490270495415 Adapter cache time: 0.0132437190040946 Engine time: 0.056946159806102514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [53 53 54]
Adapter prompts. [1080, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 4320, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 1080, 8640, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 752760 . Total input tokens: 167901726 . Total output tokens: 150505799
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 82.63700854871422,
    "estimated_duration": 3600.17261033729,
    "input_throughput": 6305.899037955595,
    "output_throughput": 5577.20536575019,
    "total_throughput": 11883.104403705785,
    "itl": 153.26124529207414,
    "ttft": 1643643.569674686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8876580281695394,
    "arrivals": 250580,
    "finished_requests": 91856,
    "scheduler_time": 137.1340894276795
}
#Debug simulation 
Total elapsed time: 82.63716848287731. Arrivals time: 0.42851156322285533 Scheduler time: 82.0526908361353 Scheduler overhead time: 0.0612750924192369 Adapter cache time: 0.013647683430463076 Engine time: 0.05911879101768136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [53 53 54]
Adapter prompts. [1080, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 4320, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 1080, 8640, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 752760 . Total input tokens: 167901726 . Total output tokens: 150505799
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 83.2280950108543,
    "estimated_duration": 3600.1751858535376,
    "input_throughput": 6305.89452680139,
    "output_throughput": 5577.201375893504,
    "total_throughput": 11883.095902694893,
    "itl": 153.26131109311757,
    "ttft": 1643644.891232442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8892161708697721,
    "arrivals": 250580,
    "finished_requests": 91856,
    "scheduler_time": 137.13413213568782
}
#Debug simulation 
Total elapsed time: 83.22825632663444. Arrivals time: 0.4232785487547517 Scheduler time: 82.64731697971001 Scheduler overhead time: 0.06214373325929046 Adapter cache time: 0.013791820034384727 Engine time: 0.05963045917451382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [53 53 54]
Adapter prompts. [1080, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 4320, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 1080, 8640, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 752760 . Total input tokens: 167901726 . Total output tokens: 150505799
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 74.90654064435512,
    "estimated_duration": 3600.062737481674,
    "input_throughput": 6304.094304721968,
    "output_throughput": 5573.958417746087,
    "total_throughput": 11878.052722468055,
    "itl": 153.53625650637002,
    "ttft": 1652402.965022519,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8449043489201019,
    "arrivals": 250580,
    "finished_requests": 91791,
    "scheduler_time": 136.88589207084885
}
#Debug simulation 
Total elapsed time: 74.90668883314356. Arrivals time: 0.41951871244236827 Scheduler time: 74.33370175585151 Scheduler overhead time: 0.06076659308746457 Adapter cache time: 0.01351359486579895 Engine time: 0.057574419770389795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [53 53 54]
Adapter prompts. [1080, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 4320, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 1080, 8640, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 752760 . Total input tokens: 167901726 . Total output tokens: 150505799
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 83.23910102667287,
    "estimated_duration": 3600.046262101723,
    "input_throughput": 6300.601533591926,
    "output_throughput": 5575.15085605666,
    "total_throughput": 11875.752389648585,
    "itl": 153.36125221466696,
    "ttft": 1642975.5342135234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.910087555609648,
    "arrivals": 250580,
    "finished_requests": 91800,
    "scheduler_time": 137.04990289606465
}
#Debug simulation 
Total elapsed time: 83.23925966769457. Arrivals time: 0.4035385111346841 Scheduler time: 82.6793173160404 Scheduler overhead time: 0.06189744872972369 Adapter cache time: 0.013419043738394976 Engine time: 0.05910711642354727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [53 53 54]
Adapter prompts. [1080, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 4320, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 1080, 8640, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 752760 . Total input tokens: 167901726 . Total output tokens: 150505799
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 76.72167776990682,
    "estimated_duration": 3600.0252062666214,
    "input_throughput": 6287.041813096611,
    "output_throughput": 5578.512051816076,
    "total_throughput": 11865.553864912687,
    "itl": 153.4686307326158,
    "ttft": 1652514.4812183778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0674479552800757,
    "arrivals": 250580,
    "finished_requests": 91844,
    "scheduler_time": 137.0574063233755
}
#Debug simulation 
Total elapsed time: 76.72183651011437. Arrivals time: 0.4151943624019623 Scheduler time: 76.15635798918083 Scheduler overhead time: 0.058506501372903585 Adapter cache time: 0.01368328370153904 Engine time: 0.05679311929270625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [53 53 54]
Adapter prompts. [1080, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 1080, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 4320, 8640, 1080, 1080, 8640, 1080, 8640, 1080, 1080, 4320, 1080, 4320, 8640, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 4320, 4320, 8640, 4320, 1080, 8640, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 1080, 8640, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 8640, 8640, 1080, 8640, 4320, 4320, 1080, 1080, 1080, 1080, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 1080, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 752760 . Total input tokens: 167901726 . Total output tokens: 150505799
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 73.14104208396748,
    "estimated_duration": 3600.1100711461377,
    "input_throughput": 6287.82663658761,
    "output_throughput": 5573.798190456899,
    "total_throughput": 11861.624827044508,
    "itl": 153.4646872274948,
    "ttft": 1656557.7876162066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.222467688508335,
    "arrivals": 250580,
    "finished_requests": 91799,
    "scheduler_time": 136.99706540260996
}
#Debug simulation 
Total elapsed time: 73.14119457593188. Arrivals time: 0.38562369998544455 Scheduler time: 72.60387180373073 Scheduler overhead time: 0.0586148789152503 Adapter cache time: 0.014491587411612272 Engine time: 0.05676269484683871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 4320, 540, 540, 540, 8640, 540, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 540, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 540, 4320, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 540, 540, 4320, 540, 4320, 8640, 540, 540, 4320, 540, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 540, 8640, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 540, 4320, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 8640, 4320, 8640, 540, 540, 540, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 724140 . Total input tokens: 161448183 . Total output tokens: 144817153
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 70.33700193976983,
    "estimated_duration": 3600.044765192246,
    "input_throughput": 6299.1294495164475,
    "output_throughput": 5563.742482777265,
    "total_throughput": 11862.871932293712,
    "itl": 153.49507628007638,
    "ttft": 1649429.8637233202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 359,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0987151000718607,
    "arrivals": 240851,
    "finished_requests": 91586,
    "scheduler_time": 136.42637458106225
}
#Debug simulation 
Total elapsed time: 70.33714994089678. Arrivals time: 0.3729605609551072 Scheduler time: 69.82133953087032 Scheduler overhead time: 0.05412136297672987 Adapter cache time: 0.013829616364091635 Engine time: 0.05358686810359359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 4320, 540, 540, 540, 8640, 540, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 540, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 540, 4320, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 540, 540, 4320, 540, 4320, 8640, 540, 540, 4320, 540, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 540, 8640, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 540, 4320, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 8640, 4320, 8640, 540, 540, 540, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 724140 . Total input tokens: 161448183 . Total output tokens: 144817153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 70.20462761679664,
    "estimated_duration": 3600.112284270252,
    "input_throughput": 6303.62422282061,
    "output_throughput": 5566.670819563692,
    "total_throughput": 11870.295042384303,
    "itl": 153.4899169910096,
    "ttft": 1649391.3391721572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1754280562931727,
    "arrivals": 240851,
    "finished_requests": 91606,
    "scheduler_time": 136.43865756406413
}
#Debug simulation 
Total elapsed time: 70.20477415807545. Arrivals time: 0.36570604564622045 Scheduler time: 69.69567711045966 Scheduler overhead time: 0.054344333242625 Adapter cache time: 0.013950520195066929 Engine time: 0.054476505145430565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 4320, 540, 540, 540, 8640, 540, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 540, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 540, 4320, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 540, 540, 4320, 540, 4320, 8640, 540, 540, 4320, 540, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 540, 8640, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 540, 4320, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 8640, 4320, 8640, 540, 540, 540, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 724140 . Total input tokens: 161448183 . Total output tokens: 144817153
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 69.4797334508039,
    "estimated_duration": 3600.0261459950766,
    "input_throughput": 6302.165062117587,
    "output_throughput": 5565.812910078626,
    "total_throughput": 11867.977972196213,
    "itl": 153.48842178837444,
    "ttft": 1649595.2765489125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2131284104287692,
    "arrivals": 240851,
    "finished_requests": 91603,
    "scheduler_time": 136.42652824038169
}
#Debug simulation 
Total elapsed time: 69.47988024586812. Arrivals time: 0.37152044102549553 Scheduler time: 68.96379952318966 Scheduler overhead time: 0.05497677903622389 Adapter cache time: 0.013981427997350693 Engine time: 0.05418526194989681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 4320, 540, 540, 540, 8640, 540, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 540, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 540, 4320, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 540, 540, 4320, 540, 4320, 8640, 540, 540, 4320, 540, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 540, 8640, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 540, 4320, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 8640, 4320, 8640, 540, 540, 540, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 724140 . Total input tokens: 161448183 . Total output tokens: 144817153
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 69.69620211608708,
    "estimated_duration": 3600.1034874544684,
    "input_throughput": 6300.53249275848,
    "output_throughput": 5563.053970473757,
    "total_throughput": 11863.586463232237,
    "itl": 153.4707252113082,
    "ttft": 1649536.1774452177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 370,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1586188098741712,
    "arrivals": 240851,
    "finished_requests": 91564,
    "scheduler_time": 136.4304951381344
}
#Debug simulation 
Total elapsed time: 69.69634902989492. Arrivals time: 0.37475405680015683 Scheduler time: 69.17708140704781 Scheduler overhead time: 0.05536106554791331 Adapter cache time: 0.01378830661997199 Engine time: 0.05414755269885063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 4320, 540, 540, 540, 8640, 540, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 540, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 540, 4320, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 540, 540, 4320, 540, 4320, 8640, 540, 540, 4320, 540, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 540, 8640, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 540, 4320, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 8640, 4320, 8640, 540, 540, 540, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 724140 . Total input tokens: 161448183 . Total output tokens: 144817153
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 69.72917791921645,
    "estimated_duration": 3600.0673481885015,
    "input_throughput": 6302.621258298734,
    "output_throughput": 5566.29336672919,
    "total_throughput": 11868.914625027925,
    "itl": 153.50343778687764,
    "ttft": 1649640.386920445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 369,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2188825267367118,
    "arrivals": 240851,
    "finished_requests": 91593,
    "scheduler_time": 136.42772549427934
}
#Debug simulation 
Total elapsed time: 69.7293251962401. Arrivals time: 0.37541030254215 Scheduler time: 69.20978265581653 Scheduler overhead time: 0.05474545760080218 Adapter cache time: 0.013576822821050882 Engine time: 0.054956977255642414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 4320, 540, 540, 540, 8640, 540, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 540, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 540, 4320, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 540, 540, 4320, 540, 4320, 8640, 540, 540, 4320, 540, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 540, 8640, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 540, 4320, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 8640, 4320, 8640, 540, 540, 540, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 724140 . Total input tokens: 161448183 . Total output tokens: 144817153
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 70.24457233725116,
    "estimated_duration": 3600.017023095229,
    "input_throughput": 6299.177991248109,
    "output_throughput": 5563.78535754223,
    "total_throughput": 11862.963348790337,
    "itl": 153.49464636031632,
    "ttft": 1649421.1819586835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 359,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0734280558698799,
    "arrivals": 240851,
    "finished_requests": 91586,
    "scheduler_time": 136.42603565649156
}
#Debug simulation 
Total elapsed time: 70.24471647990867. Arrivals time: 0.3757823961786926 Scheduler time: 69.72547003626823 Scheduler overhead time: 0.05506429076194763 Adapter cache time: 0.01367965992540121 Engine time: 0.053585955407470465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 4320, 540, 540, 540, 8640, 540, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 540, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 540, 4320, 4320, 4320, 8640, 540, 540, 8640, 540, 8640, 540, 540, 4320, 540, 4320, 8640, 540, 540, 4320, 540, 540, 540, 540, 540, 8640, 4320, 4320, 8640, 4320, 540, 8640, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 540, 540, 8640, 4320, 8640, 8640, 540, 8640, 540, 4320, 540, 540, 4320, 4320, 8640, 8640, 8640, 540, 8640, 4320, 4320, 540, 540, 540, 540, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 540, 4320, 4320, 8640, 4320, 8640, 540, 540, 540, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 540, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 540, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 724140 . Total input tokens: 161448183 . Total output tokens: 144817153
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 70.03344209399074,
    "estimated_duration": 3600.1316359719394,
    "input_throughput": 6298.223035358124,
    "output_throughput": 5562.199670677842,
    "total_throughput": 11860.422706035966,
    "itl": 153.4865033599778,
    "ttft": 1648992.4094118322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2136026431247624,
    "arrivals": 240851,
    "finished_requests": 91570,
    "scheduler_time": 136.42618251723815
}
#Debug simulation 
Total elapsed time: 70.03359232284129. Arrivals time: 0.3765977518633008 Scheduler time: 69.51230781339109 Scheduler overhead time: 0.05508887628093362 Adapter cache time: 0.013892931398004293 Engine time: 0.05430451640859246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 66.8672089879401,
    "estimated_duration": 3600.109875714815,
    "input_throughput": 6303.273728692607,
    "output_throughput": 5564.568774730066,
    "total_throughput": 11867.842503422673,
    "itl": 153.40528624660269,
    "ttft": 1624181.1931276137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 491,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5026994822709983,
    "arrivals": 236027,
    "finished_requests": 91755,
    "scheduler_time": 136.29769666969912
}
#Debug simulation 
Total elapsed time: 66.86736737191677. Arrivals time: 0.3844378418289125 Scheduler time: 66.33581950934604 Scheduler overhead time: 0.055662971921265125 Adapter cache time: 0.014903598930686712 Engine time: 0.054830731358379126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 85.66271331813186,
    "estimated_duration": 3600.102028049121,
    "input_throughput": 6326.519865977874,
    "output_throughput": 5564.18869352296,
    "total_throughput": 11890.708559500836,
    "itl": 153.074100203712,
    "ttft": 1601381.4872691971,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3044277604133863,
    "arrivals": 236027,
    "finished_requests": 91975,
    "scheduler_time": 136.37619736149225
}
#Debug simulation 
Total elapsed time: 85.66287237033248. Arrivals time: 0.38749911868944764 Scheduler time: 85.121427051723 Scheduler overhead time: 0.05947250546887517 Adapter cache time: 0.014992585871368647 Engine time: 0.05791178345680237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 85.75207943236455,
    "estimated_duration": 3600.1050713966606,
    "input_throughput": 6326.51451785656,
    "output_throughput": 5564.183989838031,
    "total_throughput": 11890.698507694591,
    "itl": 153.0742175833494,
    "ttft": 1601382.61700215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.307460911944516,
    "arrivals": 236027,
    "finished_requests": 91975,
    "scheduler_time": 136.3762075574916
}
#Debug simulation 
Total elapsed time: 85.75224769022316. Arrivals time: 0.3847995107062161 Scheduler time: 85.21448349021375 Scheduler overhead time: 0.05864971224218607 Adapter cache time: 0.015064523089677095 Engine time: 0.05746850650757551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 69.43017953494564,
    "estimated_duration": 3600.0993625906062,
    "input_throughput": 6312.891315212418,
    "output_throughput": 5566.223034905378,
    "total_throughput": 11879.114350117796,
    "itl": 153.318524294956,
    "ttft": 1622741.3189186829,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5674764018808434,
    "arrivals": 236027,
    "finished_requests": 91940,
    "scheduler_time": 136.3147335365054
}
#Debug simulation 
Total elapsed time: 69.43034222209826. Arrivals time: 0.3718552836216986 Scheduler time: 68.91154450085014 Scheduler overhead time: 0.056080985348671675 Adapter cache time: 0.015442927367985249 Engine time: 0.054449133574962616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 85.59817308094352,
    "estimated_duration": 3600.120280020185,
    "input_throughput": 6326.487791644645,
    "output_throughput": 5564.160484073517,
    "total_throughput": 11890.648275718162,
    "itl": 153.07438163005088,
    "ttft": 1601388.3692789946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.322551366314297,
    "arrivals": 236027,
    "finished_requests": 91975,
    "scheduler_time": 136.37637309254023
}
#Debug simulation 
Total elapsed time: 85.59833288192749. Arrivals time: 0.3832010608166456 Scheduler time: 85.06201037112623 Scheduler overhead time: 0.05876593478024006 Adapter cache time: 0.014442805666476488 Engine time: 0.057584129739552736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 68.2389007890597,
    "estimated_duration": 3600.0798028047666,
    "input_throughput": 6310.283728238782,
    "output_throughput": 5567.440750725756,
    "total_throughput": 11877.724478964537,
    "itl": 153.30059550771315,
    "ttft": 1622514.9703092806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 497,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4860549965663654,
    "arrivals": 236027,
    "finished_requests": 91947,
    "scheduler_time": 136.31434938666146
}
#Debug simulation 
Total elapsed time: 68.23905370710418. Arrivals time: 0.3771280664950609 Scheduler time: 67.7148156138137 Scheduler overhead time: 0.05643429001793265 Adapter cache time: 0.015354432631283998 Engine time: 0.05446689063683152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 85.60609383415431,
    "estimated_duration": 3600.1374305085947,
    "input_throughput": 6326.457653251975,
    "output_throughput": 5564.133977288226,
    "total_throughput": 11890.5916305402,
    "itl": 153.0742614449274,
    "ttft": 1601410.826760141,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3399053888395447,
    "arrivals": 236027,
    "finished_requests": 91975,
    "scheduler_time": 136.37657230565716
}
#Debug simulation 
Total elapsed time: 85.60623998800293. Arrivals time: 0.390901573933661 Scheduler time: 85.06172186834738 Scheduler overhead time: 0.05958724953234196 Adapter cache time: 0.014783956110477448 Engine time: 0.057464457117021084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 81.30348127894104,
    "estimated_duration": 3600.0252111704563,
    "input_throughput": 6283.254331056678,
    "output_throughput": 5566.011020652055,
    "total_throughput": 11849.265351708733,
    "itl": 153.62145167139374,
    "ttft": 1604029.048691957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 503,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5394253351981926,
    "arrivals": 233648,
    "finished_requests": 91312,
    "scheduler_time": 136.18921936761348
}
#Debug simulation 
Total elapsed time: 81.30364438099787. Arrivals time: 0.39401617320254445 Scheduler time: 80.75643628975376 Scheduler overhead time: 0.05855670617893338 Adapter cache time: 0.016309216152876616 Engine time: 0.0564618450589478 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 81.50836763065308,
    "estimated_duration": 3600.134833741574,
    "input_throughput": 6283.201336792972,
    "output_throughput": 5564.948515880538,
    "total_throughput": 11848.14985267351,
    "itl": 153.66651678578577,
    "ttft": 1603426.2724911333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 466,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5147162470012017,
    "arrivals": 233648,
    "finished_requests": 91319,
    "scheduler_time": 136.15330915414725
}
#Debug simulation 
Total elapsed time: 81.50853030569851. Arrivals time: 0.38031703140586615 Scheduler time: 80.97524458356202 Scheduler overhead time: 0.058805654756724834 Adapter cache time: 0.015545844566076994 Engine time: 0.0569648495875299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 81.72215104475617,
    "estimated_duration": 3600.138750637615,
    "input_throughput": 6283.194500765767,
    "output_throughput": 5564.942461301445,
    "total_throughput": 11848.136962067212,
    "itl": 153.66667646346727,
    "ttft": 1603427.8051717465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 466,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5184422894008547,
    "arrivals": 233648,
    "finished_requests": 91319,
    "scheduler_time": 136.1533346369387
}
#Debug simulation 
Total elapsed time: 81.72231294680387. Arrivals time: 0.39262488298118114 Scheduler time: 81.17641588672996 Scheduler overhead time: 0.058450919575989246 Adapter cache time: 0.01568187167868018 Engine time: 0.05703264055773616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 80.88881725212559,
    "estimated_duration": 3600.068997951256,
    "input_throughput": 6283.177909332467,
    "output_throughput": 5565.943322587204,
    "total_throughput": 11849.121231919671,
    "itl": 153.62506589937564,
    "ttft": 1604045.9893411759,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5756293621286683,
    "arrivals": 233648,
    "finished_requests": 91312,
    "scheduler_time": 136.18994770741182
}
#Debug simulation 
Total elapsed time: 80.88897862099111. Arrivals time: 0.38198593677952886 Scheduler time: 80.35442694090307 Scheduler overhead time: 0.05805951403453946 Adapter cache time: 0.015962533187121153 Engine time: 0.057088025845587254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 82.12683437298983,
    "estimated_duration": 3600.042373058022,
    "input_throughput": 6281.996614609155,
    "output_throughput": 5564.80283396851,
    "total_throughput": 11846.799448577663,
    "itl": 153.6898382933427,
    "ttft": 1603492.9764023013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 468,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5422606109827803,
    "arrivals": 233648,
    "finished_requests": 91271,
    "scheduler_time": 136.1445654277273
}
#Debug simulation 
Total elapsed time: 82.12700179032981. Arrivals time: 0.387645429931581 Scheduler time: 81.586524348706 Scheduler overhead time: 0.05839046882465482 Adapter cache time: 0.015611032955348492 Engine time: 0.05706530110910535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 81.8123236997053,
    "estimated_duration": 3600.1553593180865,
    "input_throughput": 6283.059685592152,
    "output_throughput": 5565.810360971589,
    "total_throughput": 11848.870046563741,
    "itl": 153.61966330965788,
    "ttft": 1604085.031331074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 503,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5039952983357778,
    "arrivals": 233648,
    "finished_requests": 91313,
    "scheduler_time": 136.1954016589864
}
#Debug simulation 
Total elapsed time: 81.81248204503208. Arrivals time: 0.3818911835551262 Scheduler time: 81.27795530389994 Scheduler overhead time: 0.058113761246204376 Adapter cache time: 0.015723712742328644 Engine time: 0.056966055650264025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 81.57396539207548,
    "estimated_duration": 3600.062479831174,
    "input_throughput": 6281.961528917842,
    "output_throughput": 5564.771753888971,
    "total_throughput": 11846.733282806812,
    "itl": 153.69048434332032,
    "ttft": 1603500.7756321353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 467,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5588346828147766,
    "arrivals": 233648,
    "finished_requests": 91271,
    "scheduler_time": 136.14468905027556
}
#Debug simulation 
Total elapsed time: 81.57411914225668. Arrivals time: 0.3839013804681599 Scheduler time: 81.03751901118085 Scheduler overhead time: 0.05839356081560254 Adapter cache time: 0.01549851754680276 Engine time: 0.05721917748451233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 83.24835002282634,
    "estimated_duration": 3600.058021451761,
    "input_throughput": 6343.902199329046,
    "output_throughput": 5622.696600827891,
    "total_throughput": 11966.598800156937,
    "itl": 152.31801020459184,
    "ttft": 1606920.2824936246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1874692446459136,
    "arrivals": 232444,
    "finished_requests": 92362,
    "scheduler_time": 137.48108593882688
}
#Debug simulation 
Total elapsed time: 83.24850072292611. Arrivals time: 0.38384261215105653 Scheduler time: 82.71274817641824 Scheduler overhead time: 0.05866174167022109 Adapter cache time: 0.014641790185123682 Engine time: 0.05663121119141579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 82.9544279850088,
    "estimated_duration": 3600.020471542394,
    "input_throughput": 6343.833092208865,
    "output_throughput": 5622.643026618032,
    "total_throughput": 11966.476118826897,
    "itl": 152.3221006612646,
    "ttft": 1606828.0751605637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2618797487136957,
    "arrivals": 232444,
    "finished_requests": 92358,
    "scheduler_time": 137.47626022760633
}
#Debug simulation 
Total elapsed time: 82.954580783844. Arrivals time: 0.3917570193298161 Scheduler time: 82.41109640477225 Scheduler overhead time: 0.05837577115744352 Adapter cache time: 0.014662888366729021 Engine time: 0.05710070626810193 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 82.91072770673782,
    "estimated_duration": 3600.021684199648,
    "input_throughput": 6343.830955306398,
    "output_throughput": 5622.641132646425,
    "total_throughput": 11966.472087952823,
    "itl": 152.32190976125742,
    "ttft": 1606828.841395937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2648599394597184,
    "arrivals": 232444,
    "finished_requests": 92358,
    "scheduler_time": 137.4762539949397
}
#Debug simulation 
Total elapsed time: 82.91088696196675. Arrivals time: 0.38889103569090366 Scheduler time: 82.36933467676863 Scheduler overhead time: 0.05905995983630419 Adapter cache time: 0.014547282364219427 Engine time: 0.05695522390305996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 83.00979631999508,
    "estimated_duration": 3600.156079021808,
    "input_throughput": 6343.729410255287,
    "output_throughput": 5622.543455254841,
    "total_throughput": 11966.272865510127,
    "itl": 152.32075076573526,
    "ttft": 1606948.5378645319,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2108053339086473,
    "arrivals": 232444,
    "finished_requests": 92362,
    "scheduler_time": 137.4816345891308
}
#Debug simulation 
Total elapsed time: 83.00994590902701. Arrivals time: 0.3818342280574143 Scheduler time: 82.47678148420528 Scheduler overhead time: 0.05845559947192669 Adapter cache time: 0.014439891092479229 Engine time: 0.057317836210131645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 83.21406235080212,
    "estimated_duration": 3600.135188943079,
    "input_throughput": 6343.743721108661,
    "output_throughput": 5622.5596922493905,
    "total_throughput": 11966.303413358051,
    "itl": 152.32195392518847,
    "ttft": 1606957.6816390622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.281042164042597,
    "arrivals": 232444,
    "finished_requests": 92361,
    "scheduler_time": 137.48016265248984
}
#Debug simulation 
Total elapsed time: 83.21421505697072. Arrivals time: 0.381781539414078 Scheduler time: 82.68016697373241 Scheduler overhead time: 0.05894251586869359 Adapter cache time: 0.014334050938487053 Engine time: 0.056940171867609024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 83.14842157065868,
    "estimated_duration": 3600.03910870059,
    "input_throughput": 6343.913027168014,
    "output_throughput": 5622.709750868847,
    "total_throughput": 11966.62277803686,
    "itl": 152.31717279075076,
    "ttft": 1606931.2601319125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1601395144220399,
    "arrivals": 232444,
    "finished_requests": 92361,
    "scheduler_time": 137.48014350722755
}
#Debug simulation 
Total elapsed time: 83.14857917884365. Arrivals time: 0.38837282871827483 Scheduler time: 82.60781716136262 Scheduler overhead time: 0.05867871781811118 Adapter cache time: 0.015109865460544825 Engine time: 0.05694265104830265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 82.95041671209037,
    "estimated_duration": 3600.1498083482593,
    "input_throughput": 6343.717960580695,
    "output_throughput": 5622.53686028887,
    "total_throughput": 11966.254820869566,
    "itl": 152.32203574567382,
    "ttft": 1606962.4233644719,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2966356335580373,
    "arrivals": 232444,
    "finished_requests": 92361,
    "scheduler_time": 137.4803748492292
}
#Debug simulation 
Total elapsed time: 82.95058343326673. Arrivals time: 0.386456330306828 Scheduler time: 82.41311740782112 Scheduler overhead time: 0.05799172865226865 Adapter cache time: 0.01447072671726346 Engine time: 0.05678631504997611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 86.4341163909994,
    "estimated_duration": 3600.076441995553,
    "input_throughput": 6488.220563186823,
    "output_throughput": 5696.106549513464,
    "total_throughput": 12184.327112700286,
    "itl": 149.6419182731927,
    "ttft": 1586708.1159038038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.746759009519591,
    "arrivals": 231825,
    "finished_requests": 93857,
    "scheduler_time": 139.5761602741872
}
#Debug simulation 
Total elapsed time: 86.43427022313699. Arrivals time: 0.38814612105488777 Scheduler time: 85.89275284018368 Scheduler overhead time: 0.059867884032428265 Adapter cache time: 0.013355677481740713 Engine time: 0.0579331056214869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 86.00636154366657,
    "estimated_duration": 3600.1356597292106,
    "input_throughput": 6482.235450470586,
    "output_throughput": 5691.437194769816,
    "total_throughput": 12173.672645240402,
    "itl": 149.8036682836669,
    "ttft": 1587332.3088512898,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7995719544752534,
    "arrivals": 231825,
    "finished_requests": 93777,
    "scheduler_time": 139.42900223668363
}
#Debug simulation 
Total elapsed time: 86.0065262238495. Arrivals time: 0.39090696536004543 Scheduler time: 85.4618124156259 Scheduler overhead time: 0.06005059741437435 Adapter cache time: 0.013404740020632744 Engine time: 0.058338049333542585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 85.79116177465767,
    "estimated_duration": 3600.1369492838407,
    "input_throughput": 6482.233128559821,
    "output_throughput": 5691.435156119817,
    "total_throughput": 12173.668284679638,
    "itl": 149.80355347467324,
    "ttft": 1587332.4927515215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8009707070514598,
    "arrivals": 231825,
    "finished_requests": 93777,
    "scheduler_time": 139.42903800183566
}
#Debug simulation 
Total elapsed time: 85.79132304573432. Arrivals time: 0.39511299785226583 Scheduler time: 85.24294232716784 Scheduler overhead time: 0.05971352057531476 Adapter cache time: 0.013063490390777588 Engine time: 0.0583786191418767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 86.56998207513243,
    "estimated_duration": 3600.0952285860694,
    "input_throughput": 6488.240315013534,
    "output_throughput": 5696.191266599917,
    "total_throughput": 12184.431581613451,
    "itl": 149.64298947972478,
    "ttft": 1586697.0349152673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7634856833866818,
    "arrivals": 231825,
    "finished_requests": 93858,
    "scheduler_time": 139.57691547855677
}
#Debug simulation 
Total elapsed time: 86.57014582213014. Arrivals time: 0.3916236823424697 Scheduler time: 86.02483776258305 Scheduler overhead time: 0.05948964925482869 Adapter cache time: 0.013672798871994019 Engine time: 0.058386605232954025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 86.2344443066977,
    "estimated_duration": 3600.1143255134484,
    "input_throughput": 6488.15228851619,
    "output_throughput": 5696.046610151852,
    "total_throughput": 12184.198898668043,
    "itl": 149.64503078538033,
    "ttft": 1586719.488061199,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8081132449023462,
    "arrivals": 231825,
    "finished_requests": 93857,
    "scheduler_time": 139.57595334084147
}
#Debug simulation 
Total elapsed time: 86.23460407694802. Arrivals time: 0.3903180155903101 Scheduler time: 85.6900803935714 Scheduler overhead time: 0.06018117582425475 Adapter cache time: 0.013516500126570463 Engine time: 0.05828946828842163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 85.80421641934663,
    "estimated_duration": 3600.0605587492996,
    "input_throughput": 6488.249188817773,
    "output_throughput": 5696.131680385997,
    "total_throughput": 12184.38086920377,
    "itl": 149.6423697345092,
    "ttft": 1586701.3504308043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7295722719561318,
    "arrivals": 231825,
    "finished_requests": 93857,
    "scheduler_time": 139.57586612553072
}
#Debug simulation 
Total elapsed time: 85.80437428504229. Arrivals time: 0.38989879190921783 Scheduler time: 85.26251398026943 Scheduler overhead time: 0.058733349200338125 Adapter cache time: 0.013429603539407253 Engine time: 0.057749069295823574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 86.66237243404612,
    "estimated_duration": 3600.123857770075,
    "input_throughput": 6488.1351094592765,
    "output_throughput": 5696.031528399059,
    "total_throughput": 12184.166637858336,
    "itl": 149.64483458664841,
    "ttft": 1586724.0416826643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8185508091747796,
    "arrivals": 231825,
    "finished_requests": 93857,
    "scheduler_time": 139.5762045558756
}
#Debug simulation 
Total elapsed time: 86.66252875607461. Arrivals time: 0.3909877436235547 Scheduler time: 86.11919484147802 Scheduler overhead time: 0.059108864050358534 Adapter cache time: 0.013449969235807657 Engine time: 0.05807623919099569 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_160_slots_64_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_160_slots_64_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 31.712422382086515,
    "estimated_duration": 3600.1549460818774,
    "input_throughput": 6307.723789698117,
    "output_throughput": 5581.423938952601,
    "total_throughput": 11889.147728650718,
    "itl": 153.89749492437664,
    "ttft": 1473888.8984511197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8110292521421787,
    "arrivals": 183717,
    "finished_requests": 91673,
    "scheduler_time": 132.7996184301843
}
#Debug simulation 
Total elapsed time: 31.71255512908101. Arrivals time: 0.35816834261640906 Scheduler time: 31.214200952090323 Scheduler overhead time: 0.054522760678082705 Adapter cache time: 0.012104797177016735 Engine time: 0.05340714659541845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_160_slots_64_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_160_slots_64_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 31.60188267286867,
    "estimated_duration": 3600.011778419901,
    "input_throughput": 6308.659081656757,
    "output_throughput": 5581.827293026202,
    "total_throughput": 11890.48637468296,
    "itl": 153.89681294400572,
    "ttft": 1474061.4748971022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8634589135577011,
    "arrivals": 183717,
    "finished_requests": 91680,
    "scheduler_time": 132.79107045960606
}
#Debug simulation 
Total elapsed time: 31.601988991256803. Arrivals time: 0.358139181509614 Scheduler time: 31.10284766741097 Scheduler overhead time: 0.05480878707021475 Adapter cache time: 0.012480431701987982 Engine time: 0.05332684051245451 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_160_slots_64_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_160_slots_64_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 31.7650309628807,
    "estimated_duration": 3600.0127574700277,
    "input_throughput": 6308.657365970205,
    "output_throughput": 5581.825775006938,
    "total_throughput": 11890.483140977143,
    "itl": 153.89677156052264,
    "ttft": 1474061.4816743499,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8652135589718861,
    "arrivals": 183717,
    "finished_requests": 91680,
    "scheduler_time": 132.7910673888353
}
#Debug simulation 
Total elapsed time: 31.765131387859583. Arrivals time: 0.35841843020170927 Scheduler time: 31.26698523806408 Scheduler overhead time: 0.053224705159664154 Adapter cache time: 0.0124741205945611 Engine time: 0.05360695533454418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_160_slots_64_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_160_slots_64_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 31.845345233101398,
    "estimated_duration": 3600.1277031644413,
    "input_throughput": 6308.455941725973,
    "output_throughput": 5581.647557206708,
    "total_throughput": 11890.103498932682,
    "itl": 153.89654921619868,
    "ttft": 1474079.982224886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8283197161718295,
    "arrivals": 183717,
    "finished_requests": 91680,
    "scheduler_time": 132.79651210611928
}
#Debug simulation 
Total elapsed time: 31.845477015245706. Arrivals time: 0.3517737863585353 Scheduler time: 31.352805253118277 Scheduler overhead time: 0.05411567538976669 Adapter cache time: 0.012670943513512611 Engine time: 0.053294521290808916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_160_slots_64_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_160_slots_64_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 31.833250197116286,
    "estimated_duration": 3600.0279909846213,
    "input_throughput": 6308.630670893308,
    "output_throughput": 5581.802155517141,
    "total_throughput": 11890.432826410448,
    "itl": 153.8963561460113,
    "ttft": 1474067.5425597432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.876028384603564,
    "arrivals": 183717,
    "finished_requests": 91680,
    "scheduler_time": 132.79146057048686
}
#Debug simulation 
Total elapsed time: 31.833412890788168. Arrivals time: 0.36599360359832644 Scheduler time: 31.325472735334188 Scheduler overhead time: 0.054999754298478365 Adapter cache time: 0.012714154087007046 Engine time: 0.05392166692763567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_160_slots_64_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_160_slots_64_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 31.986054059118032,
    "estimated_duration": 3600.1403019839963,
    "input_throughput": 6308.470252529885,
    "output_throughput": 5581.6680224729,
    "total_throughput": 11890.138275002786,
    "itl": 153.8931838980416,
    "ttft": 1474081.875909445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7923633281490776,
    "arrivals": 183717,
    "finished_requests": 91681,
    "scheduler_time": 132.79784805412288
}
#Debug simulation 
Total elapsed time: 31.98621160397306. Arrivals time: 0.35968811297789216 Scheduler time: 31.484630100429058 Scheduler overhead time: 0.05400882102549076 Adapter cache time: 0.01226318022236228 Engine time: 0.054493334610015154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_160_slots_64_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_160_slots_64_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 31.79851462272927,
    "estimated_duration": 3600.04495672897,
    "input_throughput": 6308.600940538148,
    "output_throughput": 5581.775850448867,
    "total_throughput": 11890.376790987015,
    "itl": 153.8963403326459,
    "ttft": 1474072.5392978697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8870947178080718,
    "arrivals": 183717,
    "finished_requests": 91680,
    "scheduler_time": 132.79183796623602
}
#Debug simulation 
Total elapsed time: 31.798654484562576. Arrivals time: 0.36377085093408823 Scheduler time: 31.294342822395265 Scheduler overhead time: 0.05461709853261709 Adapter cache time: 0.01247653178870678 Engine time: 0.05339792463928461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_160_slots_64_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_160_slots_64_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 26.936068340670317,
    "estimated_duration": 3600.0647717674906,
    "input_throughput": 6283.764719292399,
    "output_throughput": 5582.35511694231,
    "total_throughput": 11866.11983623471,
    "itl": 154.5292961874558,
    "ttft": 1452807.182799499,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8079687643982459,
    "arrivals": 179046,
    "finished_requests": 91582,
    "scheduler_time": 131.9557746570125
}
#Debug simulation 
Total elapsed time: 26.936152485664934. Arrivals time: 0.343182822689414 Scheduler time: 26.458243418950588 Scheduler overhead time: 0.05182938976213336 Adapter cache time: 0.011730176862329245 Engine time: 0.051128615625202656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_160_slots_64_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_160_slots_64_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 27.27322308300063,
    "estimated_duration": 3600.0110727734072,
    "input_throughput": 6283.585673133238,
    "output_throughput": 5582.199219325814,
    "total_throughput": 11865.784892459053,
    "itl": 154.53063728843435,
    "ttft": 1453050.7019024787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.872299181376588,
    "arrivals": 179046,
    "finished_requests": 91574,
    "scheduler_time": 131.95049123178666
}
#Debug simulation 
Total elapsed time: 27.27334042219445. Arrivals time: 0.341924159321934 Scheduler time: 26.794104584958404 Scheduler overhead time: 0.05334883602336049 Adapter cache time: 0.011641759891062975 Engine time: 0.05231564724817872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_160_slots_64_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_160_slots_64_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 27.201877106912434,
    "estimated_duration": 3600.012601474782,
    "input_throughput": 6283.5830048853395,
    "output_throughput": 5582.196848913106,
    "total_throughput": 11865.779853798445,
    "itl": 154.5306894261135,
    "ttft": 1453051.472703611,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8736613291315778,
    "arrivals": 179046,
    "finished_requests": 91574,
    "scheduler_time": 131.9505185474638
}
#Debug simulation 
Total elapsed time: 27.202022439800203. Arrivals time: 0.3473106436431408 Scheduler time: 26.716226952616125 Scheduler overhead time: 0.05372669268399477 Adapter cache time: 0.012206692714244127 Engine time: 0.052378639578819275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_160_slots_64_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_160_slots_64_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 27.386630414053798,
    "estimated_duration": 3600.092539657225,
    "input_throughput": 6283.805416333528,
    "output_throughput": 5582.3473365252685,
    "total_throughput": 11866.152752858798,
    "itl": 154.52954635722153,
    "ttft": 1452797.417889886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8257382611953678,
    "arrivals": 179046,
    "finished_requests": 91583,
    "scheduler_time": 131.9564418013938
}
#Debug simulation 
Total elapsed time: 27.38673495873809. Arrivals time: 0.3544536652043462 Scheduler time: 26.8930541574955 Scheduler overhead time: 0.05524830659851432 Adapter cache time: 0.011998436879366636 Engine time: 0.051697914488613605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_160_slots_64_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_160_slots_64_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 27.223272545728832,
    "estimated_duration": 3600.0315188619666,
    "input_throughput": 6283.5499860153695,
    "output_throughput": 5582.167515675722,
    "total_throughput": 11865.717501691091,
    "itl": 154.5310886600325,
    "ttft": 1453057.837073697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8849791699089152,
    "arrivals": 179046,
    "finished_requests": 91574,
    "scheduler_time": 131.9509610199996
}
#Debug simulation 
Total elapsed time: 27.223389953840524. Arrivals time: 0.34103649808093905 Scheduler time: 26.74604205787182 Scheduler overhead time: 0.052856691647320986 Adapter cache time: 0.011995918583124876 Engine time: 0.051526573952287436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_160_slots_64_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_160_slots_64_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 26.70989551814273,
    "estimated_duration": 3600.0012704863,
    "input_throughput": 6283.602782435763,
    "output_throughput": 5582.214418853627,
    "total_throughput": 11865.81720128939,
    "itl": 154.52784795218633,
    "ttft": 1453043.265234319,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.798343428738882,
    "arrivals": 179046,
    "finished_requests": 91574,
    "scheduler_time": 131.9517862040824
}
#Debug simulation 
Total elapsed time: 26.710024104919285. Arrivals time: 0.3410646189004183 Scheduler time: 26.234319753479213 Scheduler overhead time: 0.05143936164677143 Adapter cache time: 0.011625804472714663 Engine time: 0.05158943962305784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_160_slots_64_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_160_slots_64_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 27.24485524185002,
    "estimated_duration": 3600.077374998129,
    "input_throughput": 6283.469949034569,
    "output_throughput": 5582.096412583478,
    "total_throughput": 11865.566361618046,
    "itl": 154.53150358759103,
    "ttft": 1453075.120447434,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8964227644726678,
    "arrivals": 179046,
    "finished_requests": 91574,
    "scheduler_time": 131.95250361362804
}
#Debug simulation 
Total elapsed time: 27.244952981825918. Arrivals time: 0.33785995515063405 Scheduler time: 26.769140601623803 Scheduler overhead time: 0.05376778868958354 Adapter cache time: 0.011848721653223038 Engine time: 0.05179600138217211 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 21.34845190588385,
    "estimated_duration": 3600.001404215432,
    "input_throughput": 6307.827261792116,
    "output_throughput": 5580.59476767853,
    "total_throughput": 11888.422029470645,
    "itl": 154.12224748559726,
    "ttft": 1444666.1615380947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9059043722040939,
    "arrivals": 176720,
    "finished_requests": 91580,
    "scheduler_time": 131.79751273794497
}
#Debug simulation 
Total elapsed time: 21.348581321071833. Arrivals time: 0.31812422489747405 Scheduler time: 20.906726024579257 Scheduler overhead time: 0.046463594771921635 Adapter cache time: 0.011219983454793692 Engine time: 0.046905791852623224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 21.410024710930884,
    "estimated_duration": 3600.0965097461667,
    "input_throughput": 6308.387827525567,
    "output_throughput": 5580.980661381392,
    "total_throughput": 11889.36848890696,
    "itl": 154.12405466989506,
    "ttft": 1444673.2994712167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9663653556606786,
    "arrivals": 176720,
    "finished_requests": 91586,
    "scheduler_time": 131.80039954344454
}
#Debug simulation 
Total elapsed time: 21.41013104096055. Arrivals time: 0.3365375669673085 Scheduler time: 20.948082586750388 Scheduler overhead time: 0.048324914649128914 Adapter cache time: 0.011146461125463247 Engine time: 0.04677246883511543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 21.34044180205092,
    "estimated_duration": 3600.098292796802,
    "input_throughput": 6308.384703117841,
    "output_throughput": 5580.977897242664,
    "total_throughput": 11889.362600360504,
    "itl": 154.12403034736573,
    "ttft": 1444674.181549749,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9679938306473246,
    "arrivals": 176720,
    "finished_requests": 91586,
    "scheduler_time": 131.80041984908198
}
#Debug simulation 
Total elapsed time: 21.34052957734093. Arrivals time: 0.32327246153727174 Scheduler time: 20.892085257917643 Scheduler overhead time: 0.04741486953571439 Adapter cache time: 0.01090873870998621 Engine time: 0.047553880140185356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 21.39298186916858,
    "estimated_duration": 3600.0364746116047,
    "input_throughput": 6307.62831436308,
    "output_throughput": 5580.809011710794,
    "total_throughput": 11888.437326073874,
    "itl": 154.12283210727688,
    "ttft": 1444661.1284475406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9263230144535224,
    "arrivals": 176720,
    "finished_requests": 91582,
    "scheduler_time": 131.79836095347704
}
#Debug simulation 
Total elapsed time: 21.393111017066985. Arrivals time: 0.3179773255251348 Scheduler time: 20.950786537490785 Scheduler overhead time: 0.04748008493334055 Adapter cache time: 0.011083825957030058 Engine time: 0.04660388547927141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 21.336682568769902,
    "estimated_duration": 3600.119976535948,
    "input_throughput": 6308.361984605995,
    "output_throughput": 5580.947338130851,
    "total_throughput": 11889.309322736846,
    "itl": 154.12417443888614,
    "ttft": 1444665.1052037773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9803177017159811,
    "arrivals": 176720,
    "finished_requests": 91587,
    "scheduler_time": 131.80099908713987
}
#Debug simulation 
Total elapsed time: 21.336785259656608. Arrivals time: 0.3239868567325175 Scheduler time: 20.88712612213567 Scheduler overhead time: 0.04748133476823568 Adapter cache time: 0.011066032107919455 Engine time: 0.04777993028983474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 21.356806709896773,
    "estimated_duration": 3600.0570321305872,
    "input_throughput": 6308.53783629064,
    "output_throughput": 5581.218247564464,
    "total_throughput": 11889.756083855103,
    "itl": 154.12063201244965,
    "ttft": 1444619.6706806086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8850548872910452,
    "arrivals": 176720,
    "finished_requests": 91588,
    "scheduler_time": 131.80079238801545
}
#Debug simulation 
Total elapsed time: 21.3568975799717. Arrivals time: 0.32332332292571664 Scheduler time: 20.90778420260176 Scheduler overhead time: 0.04773971205577254 Adapter cache time: 0.011359119322150946 Engine time: 0.04757788544520736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 21.624418797902763,
    "estimated_duration": 3600.136755771642,
    "input_throughput": 6308.332583086062,
    "output_throughput": 5580.921326888186,
    "total_throughput": 11889.253909974248,
    "itl": 154.12446645454244,
    "ttft": 1444671.590271397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9930188341438825,
    "arrivals": 176720,
    "finished_requests": 91587,
    "scheduler_time": 131.80133164931974
}
#Debug simulation 
Total elapsed time: 21.624545821920037. Arrivals time: 0.32347045419737697 Scheduler time: 21.175604244694114 Scheduler overhead time: 0.04761565336957574 Adapter cache time: 0.011185031849890947 Engine time: 0.04717533476650715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 21.067277046386153,
    "estimated_duration": 3600.0176207074446,
    "input_throughput": 6309.840504485124,
    "output_throughput": 5580.4232413874015,
    "total_throughput": 11890.263745872526,
    "itl": 153.91288124779692,
    "ttft": 1439936.695625414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8538760805572372,
    "arrivals": 175562,
    "finished_requests": 91914,
    "scheduler_time": 131.83206826955
}
#Debug simulation 
Total elapsed time: 21.06740499427542. Arrivals time: 0.32298178458586335 Scheduler time: 20.618336980231106 Scheduler overhead time: 0.04885672498494387 Adapter cache time: 0.010868187062442303 Engine time: 0.04647965217009187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 21.108345536980778,
    "estimated_duration": 3600.0066137541457,
    "input_throughput": 6309.859796705169,
    "output_throughput": 5580.440303427725,
    "total_throughput": 11890.300100132894,
    "itl": 153.9150522761192,
    "ttft": 1439932.4233128761,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9110399521444971,
    "arrivals": 175562,
    "finished_requests": 91914,
    "scheduler_time": 131.83027879941358
}
#Debug simulation 
Total elapsed time: 21.108426512684673. Arrivals time: 0.32096965750679374 Scheduler time: 20.661419607698917 Scheduler overhead time: 0.048740163911134005 Adapter cache time: 0.011001547798514366 Engine time: 0.04740932630375028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 21.309095489792526,
    "estimated_duration": 3600.013385490946,
    "input_throughput": 6309.847927663249,
    "output_throughput": 5580.4298064464865,
    "total_throughput": 11890.277734109735,
    "itl": 153.91519814602844,
    "ttft": 1439934.424291014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9125442877784418,
    "arrivals": 175562,
    "finished_requests": 91914,
    "scheduler_time": 131.83048844819183
}
#Debug simulation 
Total elapsed time: 21.309226237703115. Arrivals time: 0.3287001638673246 Scheduler time: 20.855340919923037 Scheduler overhead time: 0.04795889323577285 Adapter cache time: 0.010874259285628796 Engine time: 0.04696969501674175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 21.125173475127667,
    "estimated_duration": 3600.095670653194,
    "input_throughput": 6310.052586985365,
    "output_throughput": 5580.549751433361,
    "total_throughput": 11890.602338418727,
    "itl": 153.9144180173441,
    "ttft": 1439923.941770149,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8730405875295429,
    "arrivals": 175562,
    "finished_requests": 91916,
    "scheduler_time": 131.83469042919737
}
#Debug simulation 
Total elapsed time: 21.12528897402808. Arrivals time: 0.3245278368704021 Scheduler time: 20.672129792161286 Scheduler overhead time: 0.04946004645898938 Adapter cache time: 0.011449266225099564 Engine time: 0.04827964073047042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 21.16797816287726,
    "estimated_duration": 3600.0354441658505,
    "input_throughput": 6309.809265020534,
    "output_throughput": 5580.395613203437,
    "total_throughput": 11890.204878223973,
    "itl": 153.91526495725506,
    "ttft": 1439956.2097572181,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9242393899150239,
    "arrivals": 175562,
    "finished_requests": 91914,
    "scheduler_time": 131.83106337747287
}
#Debug simulation 
Total elapsed time: 21.168100209906697. Arrivals time: 0.3284584190696478 Scheduler time: 20.71314340736717 Scheduler overhead time: 0.048303638119250536 Adapter cache time: 0.01161364559084177 Engine time: 0.04733915254473686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 20.90172199299559,
    "estimated_duration": 3600.145927990359,
    "input_throughput": 6309.964499878138,
    "output_throughput": 5580.471848043878,
    "total_throughput": 11890.436347922016,
    "itl": 153.9123266491565,
    "ttft": 1439949.4101227361,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8342240322777081,
    "arrivals": 175562,
    "finished_requests": 91916,
    "scheduler_time": 131.83749603920165
}
#Debug simulation 
Total elapsed time: 20.901804351247847. Arrivals time: 0.32338834134861827 Scheduler time: 20.45397400483489 Scheduler overhead time: 0.047674971632659435 Adapter cache time: 0.011167491786181927 Engine time: 0.04620296414941549 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 21.213824714999646,
    "estimated_duration": 3600.059251837565,
    "input_throughput": 6309.767537410778,
    "output_throughput": 5580.358709303389,
    "total_throughput": 11890.126246714166,
    "itl": 153.91563033334813,
    "ttft": 1439960.765543696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9361859996244359,
    "arrivals": 175562,
    "finished_requests": 91914,
    "scheduler_time": 131.83169428549687
}
#Debug simulation 
Total elapsed time: 21.213954816106707. Arrivals time: 0.3247513654641807 Scheduler time: 20.763419885654002 Scheduler overhead time: 0.047274854965507984 Adapter cache time: 0.011465709656476974 Engine time: 0.047945485915988684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 18.15552121028304,
    "estimated_duration": 3600.130623869721,
    "input_throughput": 6336.577303264406,
    "output_throughput": 5581.788023680107,
    "total_throughput": 11918.365326944513,
    "itl": 153.57297066193476,
    "ttft": 1435889.8104334807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8814204702526319,
    "arrivals": 174997,
    "finished_requests": 91709,
    "scheduler_time": 131.60685874186063
}
#Debug simulation 
Total elapsed time: 18.15561454836279. Arrivals time: 0.3158997525461018 Scheduler time: 17.719972271937877 Scheduler overhead time: 0.0453316573984921 Adapter cache time: 0.010665650013834238 Engine time: 0.0449836989864707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 18.336173983756453,
    "estimated_duration": 3600.0754107437597,
    "input_throughput": 6336.364213906082,
    "output_throughput": 5581.715021866318,
    "total_throughput": 11918.0792357724,
    "itl": 153.5755894978634,
    "ttft": 1435884.5614861988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9399933813908186,
    "arrivals": 174997,
    "finished_requests": 91704,
    "scheduler_time": 131.60321838501025
}
#Debug simulation 
Total elapsed time: 18.336308151949197. Arrivals time: 0.31525546265766025 Scheduler time: 17.90077868849039 Scheduler overhead time: 0.04501400841400027 Adapter cache time: 0.010839378461241722 Engine time: 0.04552761884406209 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 18.281246423721313,
    "estimated_duration": 3600.0759343192326,
    "input_throughput": 6336.363292379718,
    "output_throughput": 5581.714210092029,
    "total_throughput": 11918.077502471746,
    "itl": 153.57535635844204,
    "ttft": 1435885.2307647797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9416221948899377,
    "arrivals": 174997,
    "finished_requests": 91704,
    "scheduler_time": 131.60322705044914
}
#Debug simulation 
Total elapsed time: 18.281330728903413. Arrivals time: 0.31174958404153585 Scheduler time: 17.849420935846865 Scheduler overhead time: 0.04586789570748806 Adapter cache time: 0.010880788788199425 Engine time: 0.044635605067014694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 18.361951588187367,
    "estimated_duration": 3600.11549879317,
    "input_throughput": 6337.385844328646,
    "output_throughput": 5582.205906098506,
    "total_throughput": 11919.591750427151,
    "itl": 153.5873679651023,
    "ttft": 1435704.4276969584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9053926623892069,
    "arrivals": 174997,
    "finished_requests": 91711,
    "scheduler_time": 131.60420118035535
}
#Debug simulation 
Total elapsed time: 18.362075608223677. Arrivals time: 0.3089971183799207 Scheduler time: 17.933369434904307 Scheduler overhead time: 0.04511158587411046 Adapter cache time: 0.010509630665183067 Engine time: 0.04517152300104499 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 18.258766491897404,
    "estimated_duration": 3600.02864154688,
    "input_throughput": 6335.239596927247,
    "output_throughput": 5579.245889379807,
    "total_throughput": 11914.485486307054,
    "itl": 153.55276080798038,
    "ttft": 1436524.8550180355,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9600788297131707,
    "arrivals": 174997,
    "finished_requests": 91666,
    "scheduler_time": 131.59942954431995
}
#Debug simulation 
Total elapsed time: 18.258847197983414. Arrivals time: 0.31018859799951315 Scheduler time: 17.828049731440842 Scheduler overhead time: 0.04552274430170655 Adapter cache time: 0.010587943717837334 Engine time: 0.04568014433607459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 18.08919736277312,
    "estimated_duration": 3600.158667868497,
    "input_throughput": 6335.291331341152,
    "output_throughput": 5579.334649684306,
    "total_throughput": 11914.625981025458,
    "itl": 153.55055682874124,
    "ttft": 1436557.4809320234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8671145855216321,
    "arrivals": 174997,
    "finished_requests": 91670,
    "scheduler_time": 131.60674646475096
}
#Debug simulation 
Total elapsed time: 18.089326781686395. Arrivals time: 0.31041238084435463 Scheduler time: 17.659159153699875 Scheduler overhead time: 0.04560408368706703 Adapter cache time: 0.010614401660859585 Engine time: 0.04470022162422538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 18.186365916393697,
    "estimated_duration": 3600.0479338819127,
    "input_throughput": 6335.205646944619,
    "output_throughput": 5579.215990699871,
    "total_throughput": 11914.42163764449,
    "itl": 153.5530186073906,
    "ttft": 1436530.0677665032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9729057159274868,
    "arrivals": 174997,
    "finished_requests": 91666,
    "scheduler_time": 131.59984168806307
}
#Debug simulation 
Total elapsed time: 18.186473197303712. Arrivals time: 0.3172508766874671 Scheduler time: 17.748744470067322 Scheduler overhead time: 0.045112396124750376 Adapter cache time: 0.01063384860754013 Engine time: 0.04604903422296047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_160_slots_64_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_160_slots_64_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 15.734763213898987,
    "estimated_duration": 3600.1391222725015,
    "input_throughput": 6304.103321894381,
    "output_throughput": 5583.754770929771,
    "total_throughput": 11887.858092824154,
    "itl": 154.09195389624847,
    "ttft": 1421451.7063448317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0528077839128678,
    "arrivals": 169522,
    "finished_requests": 91550,
    "scheduler_time": 130.9347507682002
}
#Debug simulation 
Total elapsed time: 15.734899600036442. Arrivals time: 0.3006490054540336 Scheduler time: 15.318425964564085 Scheduler overhead time: 0.04344443278387189 Adapter cache time: 0.010971148498356342 Engine time: 0.0428148852661252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_160_slots_64_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_160_slots_64_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 15.704633108805865,
    "estimated_duration": 3600.1498733640033,
    "input_throughput": 6304.0219986156435,
    "output_throughput": 5583.691987027319,
    "total_throughput": 11887.713985642962,
    "itl": 154.09572458795682,
    "ttft": 1421471.5288497824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1213284387323135,
    "arrivals": 169522,
    "finished_requests": 91549,
    "scheduler_time": 130.93340800242302
}
#Debug simulation 
Total elapsed time: 15.704709994606674. Arrivals time: 0.30244295625016093 Scheduler time: 15.285436195321381 Scheduler overhead time: 0.04370192904025316 Adapter cache time: 0.01100451173260808 Engine time: 0.04348816582933068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_160_slots_64_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_160_slots_64_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 15.689043416175991,
    "estimated_duration": 3600.156932459823,
    "input_throughput": 6304.009637850218,
    "output_throughput": 5583.681038666593,
    "total_throughput": 11887.690676516811,
    "itl": 154.09604015047282,
    "ttft": 1421493.1882692347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1235256652347805,
    "arrivals": 169522,
    "finished_requests": 91549,
    "scheduler_time": 130.93365841602085
}
#Debug simulation 
Total elapsed time: 15.689165708143264. Arrivals time: 0.29411064367741346 Scheduler time: 15.280353003647178 Scheduler overhead time: 0.04261689633131027 Adapter cache time: 0.010934367775917053 Engine time: 0.042674779426306486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_160_slots_64_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_160_slots_64_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 15.770127770956606,
    "estimated_duration": 3600.1662544525266,
    "input_throughput": 6304.055811847862,
    "output_throughput": 5583.712689695475,
    "total_throughput": 11887.768501543336,
    "itl": 154.09220938409612,
    "ttft": 1421463.3116607051,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0747485724301102,
    "arrivals": 169522,
    "finished_requests": 91550,
    "scheduler_time": 130.93533486215333
}
#Debug simulation 
Total elapsed time: 15.770208683330566. Arrivals time: 0.29785696510225534 Scheduler time: 15.35524163255468 Scheduler overhead time: 0.04368354054167867 Adapter cache time: 0.011156557593494654 Engine time: 0.04365504812449217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_160_slots_64_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_160_slots_64_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 15.956543193664402,
    "estimated_duration": 3600.0069523475026,
    "input_throughput": 6304.240047425681,
    "output_throughput": 5584.086993746315,
    "total_throughput": 11888.327041171995,
    "itl": 154.09888466494772,
    "ttft": 1421367.8206576342,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1378615968860732,
    "arrivals": 169522,
    "finished_requests": 91548,
    "scheduler_time": 130.92734023146684
}
#Debug simulation 
Total elapsed time: 15.956669692881405. Arrivals time: 0.30602624733000994 Scheduler time: 15.534595174714923 Scheduler overhead time: 0.04324350832030177 Adapter cache time: 0.011069545987993479 Engine time: 0.04315470438450575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_160_slots_64_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_160_slots_64_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 15.577834502793849,
    "estimated_duration": 3600.1001385174027,
    "input_throughput": 6304.171586001091,
    "output_throughput": 5583.81523472804,
    "total_throughput": 11887.986820729131,
    "itl": 154.09167841251212,
    "ttft": 1421439.6005693562,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0285773014463488,
    "arrivals": 169522,
    "finished_requests": 91550,
    "scheduler_time": 130.93390903092475
}
#Debug simulation 
Total elapsed time: 15.57790950499475. Arrivals time: 0.2943305866792798 Scheduler time: 15.168407950550318 Scheduler overhead time: 0.04313791776075959 Adapter cache time: 0.01092173159122467 Engine time: 0.042781137861311436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_160_slots_64_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_160_slots_64_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 15.649331558030099,
    "estimated_duration": 3600.0366662331103,
    "input_throughput": 6304.202457956418,
    "output_throughput": 5583.89534988651,
    "total_throughput": 11888.097807842927,
    "itl": 154.09803740157852,
    "ttft": 1421443.5536626854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1520717747509517,
    "arrivals": 169522,
    "finished_requests": 91547,
    "scheduler_time": 130.92835040263805
}
#Debug simulation 
Total elapsed time: 15.649416414089501. Arrivals time: 0.29864645190536976 Scheduler time: 15.234616129659116 Scheduler overhead time: 0.04378879303112626 Adapter cache time: 0.010849250480532646 Engine time: 0.04297129902988672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 12.901172189041972,
    "estimated_duration": 3600.124698491943,
    "input_throughput": 6378.726272903681,
    "output_throughput": 5574.158030805474,
    "total_throughput": 11952.884303709156,
    "itl": 152.87523022029336,
    "ttft": 1401750.5720684028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1629853426944508,
    "arrivals": 167239,
    "finished_requests": 92155,
    "scheduler_time": 130.66914881732728
}
#Debug simulation 
Total elapsed time: 12.901309535373002. Arrivals time: 0.28678913693875074 Scheduler time: 12.501804342959076 Scheduler overhead time: 0.040972917806357145 Adapter cache time: 0.011350405402481556 Engine time: 0.04210362350568175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.972226451151073,
    "estimated_duration": 3600.1201988446524,
    "input_throughput": 6378.655081396883,
    "output_throughput": 5574.094722292888,
    "total_throughput": 11952.749803689772,
    "itl": 152.87823110288133,
    "ttft": 1401775.6792221616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2379593463544785,
    "arrivals": 167239,
    "finished_requests": 92154,
    "scheduler_time": 130.66722176074273
}
#Debug simulation 
Total elapsed time: 12.972310008015484. Arrivals time: 0.28890418633818626 Scheduler time: 12.570057998877019 Scheduler overhead time: 0.041792189702391624 Adapter cache time: 0.011337007861584425 Engine time: 0.04173173801973462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.921764458995312,
    "estimated_duration": 3600.1222769690658,
    "input_throughput": 6378.6513994000425,
    "output_throughput": 5574.091504718197,
    "total_throughput": 11952.74290411824,
    "itl": 152.87813603712877,
    "ttft": 1401775.4934652606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2405117886699806,
    "arrivals": 167239,
    "finished_requests": 92154,
    "scheduler_time": 130.66721749044285
}
#Debug simulation 
Total elapsed time: 12.921865964308381. Arrivals time: 0.28579056775197387 Scheduler time: 12.524377793073654 Scheduler overhead time: 0.04093493986874819 Adapter cache time: 0.011261752806603909 Engine time: 0.04131271503865719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 13.023337297141552,
    "estimated_duration": 3600.025878531849,
    "input_throughput": 6378.67748033102,
    "output_throughput": 5574.002986940604,
    "total_throughput": 11952.680467271624,
    "itl": 152.87682474694682,
    "ttft": 1401786.2297356382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1877021221863109,
    "arrivals": 167239,
    "finished_requests": 92151,
    "scheduler_time": 130.66466674145767
}
#Debug simulation 
Total elapsed time: 13.023415298201144. Arrivals time: 0.28261654637753963 Scheduler time: 12.627728421241045 Scheduler overhead time: 0.04188455315306783 Adapter cache time: 0.011131293140351772 Engine time: 0.04152879770845175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 12.997670760378242,
    "estimated_duration": 3600.1428749667143,
    "input_throughput": 6378.614904335516,
    "output_throughput": 5574.059612893984,
    "total_throughput": 11952.674517229501,
    "itl": 152.8782775903616,
    "ttft": 1401783.6484708504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2559795043990067,
    "arrivals": 167239,
    "finished_requests": 92154,
    "scheduler_time": 130.6676936506977
}
#Debug simulation 
Total elapsed time: 12.997795143164694. Arrivals time: 0.28346406761556864 Scheduler time: 12.601486500352621 Scheduler overhead time: 0.04143354808911681 Adapter cache time: 0.011325071100145578 Engine time: 0.04176854761317372 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 12.964662628248334,
    "estimated_duration": 3600.091789360878,
    "input_throughput": 6378.784582066676,
    "output_throughput": 5574.208985255512,
    "total_throughput": 11952.993567322188,
    "itl": 152.87458359516816,
    "ttft": 1401738.5389021412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1362191120628233,
    "arrivals": 167239,
    "finished_requests": 92155,
    "scheduler_time": 130.66850193534182
}
#Debug simulation 
Total elapsed time: 12.964756765868515. Arrivals time: 0.2924503437243402 Scheduler time: 12.559660496655852 Scheduler overhead time: 0.041267354506999254 Adapter cache time: 0.011276873294264078 Engine time: 0.04177260911092162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.978895538952202,
    "estimated_duration": 3600.02563262615,
    "input_throughput": 6378.676527158124,
    "output_throughput": 5573.952812486495,
    "total_throughput": 11952.629339644618,
    "itl": 152.87878196034293,
    "ttft": 1401833.770224002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.271824481487277,
    "arrivals": 167239,
    "finished_requests": 92150,
    "scheduler_time": 130.66263931798122
}
#Debug simulation 
Total elapsed time: 12.978970095980912. Arrivals time: 0.2822991297580302 Scheduler time: 12.584471084643155 Scheduler overhead time: 0.041283730417490005 Adapter cache time: 0.011270517017692327 Engine time: 0.041304209269583225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 11.814587022177875,
    "estimated_duration": 3600.166340759253,
    "input_throughput": 6272.620724307281,
    "output_throughput": 5582.265678246868,
    "total_throughput": 11854.88640255415,
    "itl": 154.47961817236268,
    "ttft": 1402761.3355355621,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2088926588534437,
    "arrivals": 166072,
    "finished_requests": 91574,
    "scheduler_time": 130.2999496105503
}
#Debug simulation 
Total elapsed time: 11.814683999866247. Arrivals time: 0.2807027813978493 Scheduler time: 11.423180550336838 Scheduler overhead time: 0.04045609897002578 Adapter cache time: 0.011167412158101797 Engine time: 0.0409658788703382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.582211641594768,
    "estimated_duration": 3600.0472898269873,
    "input_throughput": 6272.038443440875,
    "output_throughput": 5582.485279232496,
    "total_throughput": 11854.523722673372,
    "itl": 154.48767982511973,
    "ttft": 1402807.7048829228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2881218399177357,
    "arrivals": 166072,
    "finished_requests": 91570,
    "scheduler_time": 130.2922530861129
}
#Debug simulation 
Total elapsed time: 11.582292631734163. Arrivals time: 0.27907039504498243 Scheduler time: 11.19343595020473 Scheduler overhead time: 0.039848266169428825 Adapter cache time: 0.011177075561136007 Engine time: 0.0406457232311368 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.61475367192179,
    "estimated_duration": 3600.0497432855122,
    "input_throughput": 6272.034169003775,
    "output_throughput": 5582.481474730594,
    "total_throughput": 11854.515643734368,
    "itl": 154.48776472271751,
    "ttft": 1402808.732795659,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2905487888306453,
    "arrivals": 166072,
    "finished_requests": 91570,
    "scheduler_time": 130.29227959571122
}
#Debug simulation 
Total elapsed time: 11.614833632018417. Arrivals time: 0.27988366642966866 Scheduler time: 11.224580515641719 Scheduler overhead time: 0.04041292518377304 Adapter cache time: 0.011239242274314165 Engine time: 0.04059936897829175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 11.75374817661941,
    "estimated_duration": 3600.028284160451,
    "input_throughput": 6272.675161847018,
    "output_throughput": 5582.1939200920815,
    "total_throughput": 11854.8690819391,
    "itl": 154.47970239924274,
    "ttft": 1402789.704007655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2358216391573662,
    "arrivals": 166072,
    "finished_requests": 91571,
    "scheduler_time": 130.29385712953356
}
#Debug simulation 
Total elapsed time: 11.753839532844722. Arrivals time: 0.2862763046287 Scheduler time: 11.357031593099236 Scheduler overhead time: 0.040569729171693325 Adapter cache time: 0.011299361009150743 Engine time: 0.040508235804736614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 11.59549731388688,
    "estimated_duration": 3600.175491637612,
    "input_throughput": 6272.42006742516,
    "output_throughput": 5581.981224714932,
    "total_throughput": 11854.401292140092,
    "itl": 154.48328470201926,
    "ttft": 1402836.0250731907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3066452734917455,
    "arrivals": 166072,
    "finished_requests": 91572,
    "scheduler_time": 130.29800127069078
}
#Debug simulation 
Total elapsed time: 11.595598708838224. Arrivals time: 0.2820368940010667 Scheduler time: 11.202581818215549 Scheduler overhead time: 0.04027544939890504 Adapter cache time: 0.011275325901806355 Engine time: 0.04123349720612168 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 11.553796832915395,
    "estimated_duration": 3600.0800305414814,
    "input_throughput": 6272.708331043254,
    "output_throughput": 5582.227292035318,
    "total_throughput": 11854.935623078572,
    "itl": 154.47763133653845,
    "ttft": 1402776.4087858004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1810698664863544,
    "arrivals": 166072,
    "finished_requests": 91573,
    "scheduler_time": 130.29725704817972
}
#Debug simulation 
Total elapsed time: 11.553874857258052. Arrivals time: 0.2755526313558221 Scheduler time: 11.168980923015624 Scheduler overhead time: 0.03957914700731635 Adapter cache time: 0.011137157678604126 Engine time: 0.04056616732850671 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.84077098686248,
    "estimated_duration": 3600.0174522087236,
    "input_throughput": 6272.340148280708,
    "output_throughput": 5581.912106473567,
    "total_throughput": 11854.252254754274,
    "itl": 154.48228144881028,
    "ttft": 1402835.2192258588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.323496280871334,
    "arrivals": 166072,
    "finished_requests": 91568,
    "scheduler_time": 130.29134771727922
}
#Debug simulation 
Total elapsed time: 11.840870357118547. Arrivals time: 0.28308917907997966 Scheduler time: 11.446774106938392 Scheduler overhead time: 0.04082591878250241 Adapter cache time: 0.01127141760662198 Engine time: 0.04078338900581002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 11.056782000232488,
    "estimated_duration": 3600.1722407043712,
    "input_throughput": 6341.3735437094865,
    "output_throughput": 5579.628600233268,
    "total_throughput": 11921.002143942755,
    "itl": 153.50008048086997,
    "ttft": 1403546.5071056788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1538038794626522,
    "arrivals": 165445,
    "finished_requests": 91667,
    "scheduler_time": 130.46395194575584
}
#Debug simulation 
Total elapsed time: 11.056871956214309. Arrivals time: 0.2808724041096866 Scheduler time: 10.666944873984903 Scheduler overhead time: 0.03974715434014797 Adapter cache time: 0.011062209494411945 Engine time: 0.04014306282624602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.132672785781324,
    "estimated_duration": 3600.0271089425923,
    "input_throughput": 6341.383636609733,
    "output_throughput": 5579.733260925374,
    "total_throughput": 11921.116897535108,
    "itl": 153.50452947971493,
    "ttft": 1403592.2889257856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2298063861066533,
    "arrivals": 165445,
    "finished_requests": 91662,
    "scheduler_time": 130.45622363464557
}
#Debug simulation 
Total elapsed time: 11.132773711811751. Arrivals time: 0.28784988168627024 Scheduler time: 10.734757795929909 Scheduler overhead time: 0.04011485166847706 Adapter cache time: 0.011018878780305386 Engine time: 0.040771568194031715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.058541296981275,
    "estimated_duration": 3600.0318555459667,
    "input_throughput": 6341.37527556345,
    "output_throughput": 5579.725904106939,
    "total_throughput": 11921.101179670388,
    "itl": 153.50466784913743,
    "ttft": 1403594.9300691932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2320557271130452,
    "arrivals": 165445,
    "finished_requests": 91662,
    "scheduler_time": 130.45638080169428
}
#Debug simulation 
Total elapsed time: 11.058642911259085. Arrivals time: 0.27665039617568254 Scheduler time: 10.671967696398497 Scheduler overhead time: 0.04007569141685963 Adapter cache time: 0.011131459381431341 Engine time: 0.04074702551588416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 11.021015302278101,
    "estimated_duration": 3600.03297515042,
    "input_throughput": 6341.478302444192,
    "output_throughput": 5579.735835380979,
    "total_throughput": 11921.21413782517,
    "itl": 153.50150188254653,
    "ttft": 1403588.9404064913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1811835432122473,
    "arrivals": 165445,
    "finished_requests": 91663,
    "scheduler_time": 130.45773397002966
}
#Debug simulation 
Total elapsed time: 11.021143839228898. Arrivals time: 0.2795703704468906 Scheduler time: 10.632209434174001 Scheduler overhead time: 0.04023946914821863 Adapter cache time: 0.010969811119139194 Engine time: 0.04009163659065962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 11.074890670366585,
    "estimated_duration": 3600.055490794285,
    "input_throughput": 6341.438641259135,
    "output_throughput": 5579.700938323072,
    "total_throughput": 11921.139579582206,
    "itl": 153.5054047312121,
    "ttft": 1403582.6649927406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2470204276964119,
    "arrivals": 165445,
    "finished_requests": 91663,
    "scheduler_time": 130.45693329024803
}
#Debug simulation 
Total elapsed time: 11.074978093151003. Arrivals time: 0.2784594912081957 Scheduler time: 10.687226331792772 Scheduler overhead time: 0.03961445763707161 Adapter cache time: 0.011136381886899471 Engine time: 0.0404935940168798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 11.056041592732072,
    "estimated_duration": 3600.1355897013027,
    "input_throughput": 6341.492544144479,
    "output_throughput": 5579.741790132592,
    "total_throughput": 11921.234334277071,
    "itl": 153.4998398501814,
    "ttft": 1403515.3802585853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1272489611781171,
    "arrivals": 165445,
    "finished_requests": 91668,
    "scheduler_time": 130.46309823316255
}
#Debug simulation 
Total elapsed time: 11.056118560023606. Arrivals time: 0.2751525631174445 Scheduler time: 10.67125640809536 Scheduler overhead time: 0.040180716663599014 Adapter cache time: 0.010863935574889183 Engine time: 0.04044498270377517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.029452717863023,
    "estimated_duration": 3600.078450468581,
    "input_throughput": 6341.3981984277425,
    "output_throughput": 5579.665353510692,
    "total_throughput": 11921.063551938434,
    "itl": 153.5051470803115,
    "ttft": 1403590.1882782064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2636199275031712,
    "arrivals": 165445,
    "finished_requests": 91663,
    "scheduler_time": 130.45742533705896
}
#Debug simulation 
Total elapsed time: 11.029544436838478. Arrivals time: 0.27958631003275514 Scheduler time: 10.640558496583253 Scheduler overhead time: 0.04000228829681873 Adapter cache time: 0.01105790538713336 Engine time: 0.04028434446081519 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 10.102054308634251,
    "estimated_duration": 3600.0724657180576,
    "input_throughput": 6315.065659509,
    "output_throughput": 5581.627923145727,
    "total_throughput": 11896.693582654729,
    "itl": 154.0120121543073,
    "ttft": 1388444.275509591,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3588565583061538,
    "arrivals": 162486,
    "finished_requests": 91549,
    "scheduler_time": 129.94444705442038
}
#Debug simulation 
Total elapsed time: 10.102129950653762. Arrivals time: 0.26930846367031336 Scheduler time: 9.72416828526184 Scheduler overhead time: 0.03943798132240772 Adapter cache time: 0.011809054762125015 Engine time: 0.03953777952119708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.096500772982836,
    "estimated_duration": 3600.0863792716254,
    "input_throughput": 6314.952088621732,
    "output_throughput": 5581.498853942894,
    "total_throughput": 11896.450942564627,
    "itl": 154.01449957109497,
    "ttft": 1388493.437390896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4439021136262509,
    "arrivals": 162486,
    "finished_requests": 91549,
    "scheduler_time": 129.9429807463369
}
#Debug simulation 
Total elapsed time: 10.096594832837582. Arrivals time: 0.27476135175675154 Scheduler time: 9.712432271335274 Scheduler overhead time: 0.03936917707324028 Adapter cache time: 0.01177146378904581 Engine time: 0.04019844392314553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.106740911025554,
    "estimated_duration": 3600.08967074132,
    "input_throughput": 6314.946315022927,
    "output_throughput": 5581.493750921579,
    "total_throughput": 11896.440065944505,
    "itl": 154.01458282572455,
    "ttft": 1388495.0112783504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4467551184073175,
    "arrivals": 162486,
    "finished_requests": 91549,
    "scheduler_time": 129.9430464133611
}
#Debug simulation 
Total elapsed time: 10.10681596910581. Arrivals time: 0.2716828966513276 Scheduler time: 9.725444909650832 Scheduler overhead time: 0.03979337727651 Adapter cache time: 0.01181799080222845 Engine time: 0.040071604773402214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 10.116834907792509,
    "estimated_duration": 3600.114605407069,
    "input_throughput": 6314.991741055799,
    "output_throughput": 5581.562589652037,
    "total_throughput": 11896.554330707835,
    "itl": 154.01246365137186,
    "ttft": 1388458.4906523256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3892802240210556,
    "arrivals": 162486,
    "finished_requests": 91549,
    "scheduler_time": 129.94533587090103
}
#Debug simulation 
Total elapsed time: 10.116931149736047. Arrivals time: 0.2801150414161384 Scheduler time: 9.727823099121451 Scheduler overhead time: 0.039338136557489634 Adapter cache time: 0.011849742382764816 Engine time: 0.0397065719589591 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 10.084063956979662,
    "estimated_duration": 3600.1429635316886,
    "input_throughput": 6314.94227598606,
    "output_throughput": 5581.519179529419,
    "total_throughput": 11896.461455515479,
    "itl": 154.01656624047112,
    "ttft": 1388480.7499968254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 443,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.464612156078224,
    "arrivals": 162486,
    "finished_requests": 91550,
    "scheduler_time": 129.94460801708698
}
#Debug simulation 
Total elapsed time: 10.08416648209095. Arrivals time: 0.2708335891366005 Scheduler time: 9.704230683855712 Scheduler overhead time: 0.03947186470031738 Adapter cache time: 0.011782178189605474 Engine time: 0.03990945080295205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 10.085232246201485,
    "estimated_duration": 3600.032932079992,
    "input_throughput": 6315.135285961001,
    "output_throughput": 5581.689773151639,
    "total_throughput": 11896.82505911264,
    "itl": 154.01068157307822,
    "ttft": 1388468.9340354863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3275823309365558,
    "arrivals": 162486,
    "finished_requests": 91550,
    "scheduler_time": 129.9435965787161
}
#Debug simulation 
Total elapsed time: 10.085307618137449. Arrivals time: 0.26822786405682564 Scheduler time: 9.708653614390641 Scheduler overhead time: 0.03938120091333985 Adapter cache time: 0.01148549234494567 Engine time: 0.03964684857055545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.150397150311619,
    "estimated_duration": 3600.1691150860947,
    "input_throughput": 6314.8964043752485,
    "output_throughput": 5581.478635488896,
    "total_throughput": 11896.375039864144,
    "itl": 154.0170291602961,
    "ttft": 1388487.6780085142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4869817441701891,
    "arrivals": 162486,
    "finished_requests": 91550,
    "scheduler_time": 129.9450721628705
}
#Debug simulation 
Total elapsed time: 10.15051172208041. Arrivals time: 0.28534352220594883 Scheduler time: 9.756301070097834 Scheduler overhead time: 0.03953686123713851 Adapter cache time: 0.011724488344043493 Engine time: 0.03958367323502898 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 8.923655474092811,
    "estimated_duration": 3600.1224246018724,
    "input_throughput": 6364.6874460203835,
    "output_throughput": 5575.576225638991,
    "total_throughput": 11940.263671659373,
    "itl": 152.94937721082408,
    "ttft": 1380976.348365276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 475,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4537316783680725,
    "arrivals": 161281,
    "finished_requests": 92070,
    "scheduler_time": 129.87736629107727
}
#Debug simulation 
Total elapsed time: 8.923735056072474. Arrivals time: 0.2660267264582217 Scheduler time: 8.548487429041415 Scheduler overhead time: 0.039129726123064756 Adapter cache time: 0.012325172312557697 Engine time: 0.039834339171648026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.952133807819337,
    "estimated_duration": 3600.0753558610463,
    "input_throughput": 6365.676196941399,
    "output_throughput": 5575.395794791447,
    "total_throughput": 11941.071991732846,
    "itl": 152.94480430295107,
    "ttft": 1381077.792544669,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 476,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5515628703637097,
    "arrivals": 161281,
    "finished_requests": 92072,
    "scheduler_time": 129.87635686832098
}
#Debug simulation 
Total elapsed time: 8.95220664376393. Arrivals time: 0.2681531682610512 Scheduler time: 8.575161480810493 Scheduler overhead time: 0.0393129144795239 Adapter cache time: 0.011920238845050335 Engine time: 0.039712028577923775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.991850352846086,
    "estimated_duration": 3600.0781308772193,
    "input_throughput": 6365.671290143892,
    "output_throughput": 5575.3914971587465,
    "total_throughput": 11941.062787302639,
    "itl": 152.94491363219512,
    "ttft": 1381078.7993977226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 476,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.554610685296366,
    "arrivals": 161281,
    "finished_requests": 92072,
    "scheduler_time": 129.87636581202148
}
#Debug simulation 
Total elapsed time: 8.991969518829137. Arrivals time: 0.2735882452689111 Scheduler time: 8.60915796039626 Scheduler overhead time: 0.03933534258976579 Adapter cache time: 0.011823337990790606 Engine time: 0.04008913692086935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 8.97548444988206,
    "estimated_duration": 3600.153897977677,
    "input_throughput": 6364.6318044546215,
    "output_throughput": 5575.527482665538,
    "total_throughput": 11940.15928712016,
    "itl": 152.949691105308,
    "ttft": 1380986.367368108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 475,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.483197569118342,
    "arrivals": 161281,
    "finished_requests": 92070,
    "scheduler_time": 129.87775137834976
}
#Debug simulation 
Total elapsed time: 8.975586962886155. Arrivals time: 0.26610643323510885 Scheduler time: 8.60047585843131 Scheduler overhead time: 0.039635611698031425 Adapter cache time: 0.011761864181607962 Engine time: 0.03957462264224887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 8.963263714686036,
    "estimated_duration": 3600.130938117204,
    "input_throughput": 6365.81374231503,
    "output_throughput": 5575.53193065183,
    "total_throughput": 11941.345672966861,
    "itl": 152.94714043902002,
    "ttft": 1381048.1795461155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 476,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5747312911227387,
    "arrivals": 161281,
    "finished_requests": 92074,
    "scheduler_time": 129.87793120635564
}
#Debug simulation 
Total elapsed time: 8.96333895996213. Arrivals time: 0.27209291281178594 Scheduler time: 8.581994947977364 Scheduler overhead time: 0.03949803998693824 Adapter cache time: 0.01193755678832531 Engine time: 0.03979550767689943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 8.975780522916466,
    "estimated_duration": 3600.0260568926424,
    "input_throughput": 6366.0483668225415,
    "output_throughput": 5575.753531441342,
    "total_throughput": 11941.801898263884,
    "itl": 152.94051195413823,
    "ttft": 1380978.2068931658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 476,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.423263940373422,
    "arrivals": 161281,
    "finished_requests": 92075,
    "scheduler_time": 129.8776560961362
}
#Debug simulation 
Total elapsed time: 8.97588088363409. Arrivals time: 0.2679876806214452 Scheduler time: 8.59825758356601 Scheduler overhead time: 0.03980788495391607 Adapter cache time: 0.011898762080818415 Engine time: 0.039830608293414116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.986294709146023,
    "estimated_duration": 3600.149845385055,
    "input_throughput": 6365.78031033284,
    "output_throughput": 5575.502649071854,
    "total_throughput": 11941.282959404694,
    "itl": 152.94723921976964,
    "ttft": 1381055.5353384565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 476,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5940973742306226,
    "arrivals": 161281,
    "finished_requests": 92074,
    "scheduler_time": 129.8781920608666
}
#Debug simulation 
Total elapsed time: 8.986371369101107. Arrivals time: 0.26558809261769056 Scheduler time: 8.611736497376114 Scheduler overhead time: 0.03913528146222234 Adapter cache time: 0.01191816246137023 Engine time: 0.039942783303558826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 8.316959240939468,
    "estimated_duration": 3600.0632984870485,
    "input_throughput": 6279.955135650328,
    "output_throughput": 5584.252090358717,
    "total_throughput": 11864.207226009044,
    "itl": 154.47371962559737,
    "ttft": 1380028.9781679397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6312399675161784,
    "arrivals": 160680,
    "finished_requests": 91611,
    "scheduler_time": 129.43349309492012
}
#Debug simulation 
Total elapsed time: 8.317042621783912. Arrivals time: 0.26703775534406304 Scheduler time: 7.941161340102553 Scheduler overhead time: 0.03914593253284693 Adapter cache time: 0.012495713774114847 Engine time: 0.03938247635960579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.502467706799507,
    "estimated_duration": 3600.030421464047,
    "input_throughput": 6279.716656062646,
    "output_throughput": 5583.8628140883775,
    "total_throughput": 11863.579470151022,
    "itl": 154.47520390303686,
    "ttft": 1380166.4387589118,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7375223592738671,
    "arrivals": 160680,
    "finished_requests": 91604,
    "scheduler_time": 129.42960295379447
}
#Debug simulation 
Total elapsed time: 8.50256193196401. Arrivals time: 0.47878728713840246 Scheduler time: 7.914700045250356 Scheduler overhead time: 0.03910907544195652 Adapter cache time: 0.012604591902345419 Engine time: 0.03942783456295729 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.325993247330189,
    "estimated_duration": 3600.0331524437606,
    "input_throughput": 6279.711892279071,
    "output_throughput": 5583.858578178478,
    "total_throughput": 11863.57047045755,
    "itl": 154.47525181755233,
    "ttft": 1380166.3502784497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7409066644683586,
    "arrivals": 160680,
    "finished_requests": 91604,
    "scheduler_time": 129.42954986265082
}
#Debug simulation 
Total elapsed time: 8.326068808324635. Arrivals time: 0.26329041039571166 Scheduler time: 7.953490471467376 Scheduler overhead time: 0.03933798149228096 Adapter cache time: 0.012531313113868237 Engine time: 0.03949247905984521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 8.340188491158187,
    "estimated_duration": 3600.115101604097,
    "input_throughput": 6279.864771525356,
    "output_throughput": 5584.171736909869,
    "total_throughput": 11864.036508435225,
    "itl": 154.4746123875672,
    "ttft": 1380065.4019193626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6676525598205552,
    "arrivals": 160680,
    "finished_requests": 91611,
    "scheduler_time": 129.4345845030539
}
#Debug simulation 
Total elapsed time: 8.34026334900409. Arrivals time: 0.2672235583886504 Scheduler time: 7.96414604736492 Scheduler overhead time: 0.03923424473032355 Adapter cache time: 0.012565900105983019 Engine time: 0.03929907223209739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 8.326800510287285,
    "estimated_duration": 3600.0743118840132,
    "input_throughput": 6279.640096698192,
    "output_throughput": 5583.794738248072,
    "total_throughput": 11863.434834946263,
    "itl": 154.4761513162066,
    "ttft": 1380173.4173521707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7624105619452937,
    "arrivals": 160680,
    "finished_requests": 91604,
    "scheduler_time": 129.43063824301197
}
#Debug simulation 
Total elapsed time: 8.326945826411247. Arrivals time: 0.2645625928416848 Scheduler time: 7.952985807321966 Scheduler overhead time: 0.03938083816319704 Adapter cache time: 0.012546131387352943 Engine time: 0.03952998388558626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 8.283435149118304,
    "estimated_duration": 3600.007438913998,
    "input_throughput": 6279.965078855518,
    "output_throughput": 5584.285683049739,
    "total_throughput": 11864.250761905258,
    "itl": 154.47154640993062,
    "ttft": 1380013.0479420368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.59369680718284,
    "arrivals": 160680,
    "finished_requests": 91610,
    "scheduler_time": 129.43216288467954
}
#Debug simulation 
Total elapsed time: 8.283512711990625. Arrivals time: 0.2655756724998355 Scheduler time: 7.909290231298655 Scheduler overhead time: 0.03887169202789664 Adapter cache time: 0.012541843578219414 Engine time: 0.039368520956486464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.340647137258202,
    "estimated_duration": 3600.0021377037015,
    "input_throughput": 6279.714604397457,
    "output_throughput": 5583.848073163141,
    "total_throughput": 11863.562677560598,
    "itl": 154.478728580503,
    "ttft": 1380170.0012256312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7851719972863769,
    "arrivals": 160680,
    "finished_requests": 91602,
    "scheduler_time": 129.42725085175334
}
#Debug simulation 
Total elapsed time: 8.340729760937393. Arrivals time: 0.26561506977304816 Scheduler time: 7.965785164851695 Scheduler overhead time: 0.03932324657216668 Adapter cache time: 0.012472294736653566 Engine time: 0.03960635932162404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.813840611837804,
    "estimated_duration": 3600.0026044195865,
    "input_throughput": 6281.266844707112,
    "output_throughput": 5586.354014108386,
    "total_throughput": 11867.620858815497,
    "itl": 154.35855594585675,
    "ttft": 1373719.0097230135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.79344581794462,
    "arrivals": 158932,
    "finished_requests": 91455,
    "scheduler_time": 129.27920182528825
}
#Debug simulation 
Total elapsed time: 7.8139374940656126. Arrivals time: 0.27670610370114446 Scheduler time: 7.428817915730178 Scheduler overhead time: 0.03835656773298979 Adapter cache time: 0.012955773156136274 Engine time: 0.03934893663972616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.820469264872372,
    "estimated_duration": 3600.0084832434977,
    "input_throughput": 6281.222976348897,
    "output_throughput": 5586.272113970393,
    "total_throughput": 11867.49509031929,
    "itl": 154.36315565670105,
    "ttft": 1373765.1254167657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9098872657306536,
    "arrivals": 158932,
    "finished_requests": 91453,
    "scheduler_time": 129.27641968275069
}
#Debug simulation 
Total elapsed time: 7.820573765784502. Arrivals time: 0.2585601154714823 Scheduler time: 7.453175309114158 Scheduler overhead time: 0.038850368931889534 Adapter cache time: 0.013221617322415113 Engine time: 0.038977937307208776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.847763222176582,
    "estimated_duration": 3600.012451553196,
    "input_throughput": 6281.21605252894,
    "output_throughput": 5586.265956197855,
    "total_throughput": 11867.482008726796,
    "itl": 154.36331601417876,
    "ttft": 1373766.6126623987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9136795782670497,
    "arrivals": 158932,
    "finished_requests": 91453,
    "scheduler_time": 129.27646379183852
}
#Debug simulation 
Total elapsed time: 7.847835781984031. Arrivals time: 0.2602255414240062 Scheduler time: 7.478714091237634 Scheduler overhead time: 0.03887288458645344 Adapter cache time: 0.012974969111382961 Engine time: 0.03910960117354989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 7.845829629804939,
    "estimated_duration": 3600.0612743772895,
    "input_throughput": 6281.164479321633,
    "output_throughput": 5586.2629736708095,
    "total_throughput": 11867.427452992442,
    "itl": 154.35997507640386,
    "ttft": 1373737.0322371125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.834705727137615,
    "arrivals": 158932,
    "finished_requests": 91455,
    "scheduler_time": 129.28046340815786
}
#Debug simulation 
Total elapsed time: 7.845921133644879. Arrivals time: 0.2646646127104759 Scheduler time: 7.472595592960715 Scheduler overhead time: 0.0386775117367506 Adapter cache time: 0.013130424078553915 Engine time: 0.03904277039691806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 8.009054656140506,
    "estimated_duration": 3600.0381238415666,
    "input_throughput": 6281.171260450559,
    "output_throughput": 5586.226119889014,
    "total_throughput": 11867.397380339573,
    "itl": 154.36403087317277,
    "ttft": 1373777.4548504872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9368182749673766,
    "arrivals": 158932,
    "finished_requests": 91453,
    "scheduler_time": 129.27692460300372
}
#Debug simulation 
Total elapsed time: 8.009141169954091. Arrivals time: 0.26341244392096996 Scheduler time: 7.636888217646629 Scheduler overhead time: 0.03863485436886549 Adapter cache time: 0.013042906299233437 Engine time: 0.03940334590151906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.838157108053565,
    "estimated_duration": 3600.0927742924873,
    "input_throughput": 6281.535898597576,
    "output_throughput": 5586.559641911606,
    "total_throughput": 11868.095540509183,
    "itl": 154.3575913688305,
    "ttft": 1373640.1427407025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7521694728126496,
    "arrivals": 158932,
    "finished_requests": 91462,
    "scheduler_time": 129.28391135085423
}
#Debug simulation 
Total elapsed time: 7.838234161958098. Arrivals time: 0.27552421437576413 Scheduler time: 7.451688440516591 Scheduler overhead time: 0.03858265280723572 Adapter cache time: 0.015413319692015648 Engine time: 0.039244774263352156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.788662274833769,
    "estimated_duration": 3600.0703941974307,
    "input_throughput": 6281.139956720499,
    "output_throughput": 5586.176601550396,
    "total_throughput": 11867.316558270895,
    "itl": 154.36395334168847,
    "ttft": 1373772.1211774612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9622205398231705,
    "arrivals": 158932,
    "finished_requests": 91454,
    "scheduler_time": 129.27751419549227
}
#Debug simulation 
Total elapsed time: 7.788758102804422. Arrivals time: 0.2646955819800496 Scheduler time: 7.41607134277001 Scheduler overhead time: 0.03844563942402601 Adapter cache time: 0.013013182673603296 Engine time: 0.03893593791872263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.384735002648085,
    "estimated_duration": 3600.013844515881,
    "input_throughput": 6292.724966741462,
    "output_throughput": 5584.68713408619,
    "total_throughput": 11877.412100827652,
    "itl": 154.15399941209895,
    "ttft": 1371642.7467192637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 650,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.989317033556323,
    "arrivals": 158306,
    "finished_requests": 91485,
    "scheduler_time": 129.10317330530523
}
#Debug simulation 
Total elapsed time: 7.384807592723519. Arrivals time: 0.2565575633198023 Scheduler time: 7.02038854919374 Scheduler overhead time: 0.03800824470818043 Adapter cache time: 0.013494012411683798 Engine time: 0.03877323353663087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.341987297870219,
    "estimated_duration": 3600.0919426641726,
    "input_throughput": 6292.518458080169,
    "output_throughput": 5584.439597707076,
    "total_throughput": 11876.958055787245,
    "itl": 154.1608569943962,
    "ttft": 1371702.9283764656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 650,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1212716552079716,
    "arrivals": 158306,
    "finished_requests": 91483,
    "scheduler_time": 129.10273284467078
}
#Debug simulation 
Total elapsed time: 7.342116117011756. Arrivals time: 0.2559865494258702 Scheduler time: 6.978575159329921 Scheduler overhead time: 0.03792003635317087 Adapter cache time: 0.013403264805674553 Engine time: 0.038660726975649595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.34579102601856,
    "estimated_duration": 3600.0964135395216,
    "input_throughput": 6292.510643548994,
    "output_throughput": 5584.43266252244,
    "total_throughput": 11876.943306071435,
    "itl": 154.1610261313824,
    "ttft": 1371705.0824488834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 650,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.124989911820747,
    "arrivals": 158306,
    "finished_requests": 91483,
    "scheduler_time": 129.10282683976504
}
#Debug simulation 
Total elapsed time: 7.345871206838638. Arrivals time: 0.2567248255945742 Scheduler time: 6.981880791950971 Scheduler overhead time: 0.03795703873038292 Adapter cache time: 0.013321008067578077 Engine time: 0.038387677166610956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 7.365276239812374,
    "estimated_duration": 3600.142851234694,
    "input_throughput": 6292.807517965658,
    "output_throughput": 5584.593398316052,
    "total_throughput": 11877.40091628171,
    "itl": 154.1563108775475,
    "ttft": 1371708.4788779195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 650,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0362838289723597,
    "arrivals": 158306,
    "finished_requests": 91487,
    "scheduler_time": 129.1070842049471
}
#Debug simulation 
Total elapsed time: 7.3653616877272725. Arrivals time: 0.2602775488048792 Scheduler time: 6.997561512514949 Scheduler overhead time: 0.03802929911762476 Adapter cache time: 0.013387384358793497 Engine time: 0.03851830447092652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 7.306920426897705,
    "estimated_duration": 3600.059883929821,
    "input_throughput": 6292.088668056605,
    "output_throughput": 5584.425717400625,
    "total_throughput": 11876.514385457229,
    "itl": 154.16874759387613,
    "ttft": 1371727.363497185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1447224142029917,
    "arrivals": 158306,
    "finished_requests": 91477,
    "scheduler_time": 129.09968858940755
}
#Debug simulation 
Total elapsed time: 7.30700115300715. Arrivals time: 0.2459480850957334 Scheduler time: 6.954187600873411 Scheduler overhead time: 0.03821906726807356 Adapter cache time: 0.013205468188971281 Engine time: 0.038016599137336016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.357166964095086,
    "estimated_duration": 3600.126807018307,
    "input_throughput": 6293.064720896315,
    "output_throughput": 5584.638007973857,
    "total_throughput": 11877.702728870172,
    "itl": 154.15041939404566,
    "ttft": 1371645.2200464963,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 650,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.943532691686382,
    "arrivals": 158306,
    "finished_requests": 91490,
    "scheduler_time": 129.10881520022747
}
#Debug simulation 
Total elapsed time: 7.357258533127606. Arrivals time: 0.2439419045113027 Scheduler time: 7.007471509277821 Scheduler overhead time: 0.03750610910356045 Adapter cache time: 0.013169268146157265 Engine time: 0.03762997454032302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.361700445879251,
    "estimated_duration": 3600.163839967749,
    "input_throughput": 6292.420013918849,
    "output_throughput": 5584.346405795826,
    "total_throughput": 11876.766419714675,
    "itl": 154.1628914703274,
    "ttft": 1371729.1800298225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 650,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.179692808911205,
    "arrivals": 158306,
    "finished_requests": 91484,
    "scheduler_time": 129.1040853711646
}
#Debug simulation 
Total elapsed time: 7.361787877045572. Arrivals time: 0.24314087955281138 Scheduler time: 7.012590869795531 Scheduler overhead time: 0.03765967208892107 Adapter cache time: 0.013260246720165014 Engine time: 0.03767198324203491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.837822430301458,
    "estimated_duration": 3600.148463965211,
    "input_throughput": 6337.954178386487,
    "output_throughput": 5588.261484594823,
    "total_throughput": 11926.215662981309,
    "itl": 153.23493707846674,
    "ttft": 1355453.9273348777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7421970185638065,
    "arrivals": 157137,
    "finished_requests": 91932,
    "scheduler_time": 128.83103464721313
}
#Debug simulation 
Total elapsed time: 6.837897597346455. Arrivals time: 0.24027198646217585 Scheduler time: 6.490613812115043 Scheduler overhead time: 0.037069670390337706 Adapter cache time: 0.015171108301728964 Engine time: 0.037371129263192415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.806608977727592,
    "estimated_duration": 3600.0584820732847,
    "input_throughput": 6337.965372956828,
    "output_throughput": 5588.249218777682,
    "total_throughput": 11926.21459173451,
    "itl": 153.24176163909067,
    "ttft": 1355425.6598228891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9262852318887664,
    "arrivals": 157137,
    "finished_requests": 91930,
    "scheduler_time": 128.8223451931883
}
#Debug simulation 
Total elapsed time: 6.806683859787881. Arrivals time: 0.24131087306886911 Scheduler time: 6.4576815334148705 Scheduler overhead time: 0.037267149426043034 Adapter cache time: 0.015297071542590857 Engine time: 0.037684868555516005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.838318171910942,
    "estimated_duration": 3600.063266539747,
    "input_throughput": 6337.956949831866,
    "output_throughput": 5588.2417920218195,
    "total_throughput": 11926.198741853685,
    "itl": 153.241955183295,
    "ttft": 1355427.5710353225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9310276226885383,
    "arrivals": 157137,
    "finished_requests": 91930,
    "scheduler_time": 128.8223938020335
}
#Debug simulation 
Total elapsed time: 6.8384010987356305. Arrivals time: 0.24292410863563418 Scheduler time: 6.487849731463939 Scheduler overhead time: 0.03702893340960145 Adapter cache time: 0.015276025049388409 Engine time: 0.03796999854966998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.820783147122711,
    "estimated_duration": 3600.0525143629834,
    "input_throughput": 6338.122821532644,
    "output_throughput": 5588.360147451871,
    "total_throughput": 11926.482968984514,
    "itl": 153.23705094447376,
    "ttft": 1355384.9402372532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8098355661332364,
    "arrivals": 157137,
    "finished_requests": 91931,
    "scheduler_time": 128.8251402564952
}
#Debug simulation 
Total elapsed time: 6.820858798921108. Arrivals time: 0.24045512592419982 Scheduler time: 6.472696786280721 Scheduler overhead time: 0.03724891459569335 Adapter cache time: 0.01543422695249319 Engine time: 0.037629119120538235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.847058357205242,
    "estimated_duration": 3600.099066948191,
    "input_throughput": 6337.893923386403,
    "output_throughput": 5588.186220956991,
    "total_throughput": 11926.080144343394,
    "itl": 153.24305978463826,
    "ttft": 1355441.2503292684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9668674518167957,
    "arrivals": 157137,
    "finished_requests": 91930,
    "scheduler_time": 128.82294687618185
}
#Debug simulation 
Total elapsed time: 6.847127777058631. Arrivals time: 0.24216639902442694 Scheduler time: 6.4974639168940485 Scheduler overhead time: 0.03713215980678797 Adapter cache time: 0.015273967757821083 Engine time: 0.03771994495764375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.818328856024891,
    "estimated_duration": 3600.0026507903854,
    "input_throughput": 6338.210610758959,
    "output_throughput": 5588.437551728742,
    "total_throughput": 11926.6481624877,
    "itl": 153.23072318288982,
    "ttft": 1355394.6843198787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.679085064232291,
    "arrivals": 157137,
    "finished_requests": 91931,
    "scheduler_time": 128.82726804582046
}
#Debug simulation 
Total elapsed time: 6.8184008579701185. Arrivals time: 0.25238431990146637 Scheduler time: 6.4590277024544775 Scheduler overhead time: 0.03708021203055978 Adapter cache time: 0.01522627891972661 Engine time: 0.03738451236858964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_160_slots_64_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.822905878070742,
    "estimated_duration": 3600.1445003072795,
    "input_throughput": 6337.8139399828315,
    "output_throughput": 5588.115698767893,
    "total_throughput": 11925.929638750724,
    "itl": 153.24422660278552,
    "ttft": 1355456.613790204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 896,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0071086634695825,
    "arrivals": 157137,
    "finished_requests": 91930,
    "scheduler_time": 128.8235591494242
}
#Debug simulation 
Total elapsed time: 6.82297381805256. Arrivals time: 0.23996000597253442 Scheduler time: 6.475525145884603 Scheduler overhead time: 0.03718892438337207 Adapter cache time: 0.015258012805134058 Engine time: 0.03761009406298399 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_160_slots_64_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_160_slots_64_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 70.68127979198471,
    "estimated_duration": 3600.092929962798,
    "input_throughput": 6225.48040731593,
    "output_throughput": 5469.627141042573,
    "total_throughput": 11695.107548358505,
    "itl": 139.50063921916902,
    "ttft": 731160.1773919053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 301,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9212068109237577,
    "arrivals": 106283,
    "finished_requests": 89819,
    "scheduler_time": 98.2174903244651
}
#Debug simulation 
Total elapsed time: 70.68143069883808. Arrivals time: 0.2989389356225729 Scheduler time: 70.22261185385287 Scheduler overhead time: 0.0627774060703814 Adapter cache time: 0.013341886922717094 Engine time: 0.06043224409222603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_160_slots_64_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_160_slots_64_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 70.53949074586853,
    "estimated_duration": 3600.0547037558763,
    "input_throughput": 6223.492653215889,
    "output_throughput": 5468.236351925981,
    "total_throughput": 11691.72900514187,
    "itl": 139.5246090610722,
    "ttft": 731396.1607866576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 301,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9813156071351896,
    "arrivals": 106283,
    "finished_requests": 89795,
    "scheduler_time": 98.18490127186763
}
#Debug simulation 
Total elapsed time: 70.53963176114485. Arrivals time: 0.29926564591005445 Scheduler time: 70.08287030877545 Scheduler overhead time: 0.061889398377388716 Adapter cache time: 0.013118238188326359 Engine time: 0.05911830998957157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_160_slots_64_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_160_slots_64_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 70.81433076877147,
    "estimated_duration": 3600.070393002524,
    "input_throughput": 6223.465530993103,
    "output_throughput": 5468.2125211395,
    "total_throughput": 11691.678052132602,
    "itl": 139.52481497094857,
    "ttft": 731475.4330310455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 301,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9832114248909107,
    "arrivals": 106283,
    "finished_requests": 89795,
    "scheduler_time": 98.18530430370822
}
#Debug simulation 
Total elapsed time: 70.81447815801948. Arrivals time: 0.30007631704211235 Scheduler time: 70.35673496313393 Scheduler overhead time: 0.06128189340233803 Adapter cache time: 0.01324651949107647 Engine time: 0.060228096321225166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_160_slots_64_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_160_slots_64_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 70.75234955176711,
    "estimated_duration": 3600.0938188441423,
    "input_throughput": 6225.531646615762,
    "output_throughput": 5469.572736391089,
    "total_throughput": 11695.10438300685,
    "itl": 139.49861840924873,
    "ttft": 731163.4793310241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 301,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9420904565649141,
    "arrivals": 106283,
    "finished_requests": 89819,
    "scheduler_time": 98.21785944431487
}
#Debug simulation 
Total elapsed time: 70.7524905949831. Arrivals time: 0.30157661670818925 Scheduler time: 70.29094701772556 Scheduler overhead time: 0.06257378309965134 Adapter cache time: 0.01327167497947812 Engine time: 0.06119244731962681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_160_slots_64_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_160_slots_64_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 70.57882988080382,
    "estimated_duration": 3600.08596274137,
    "input_throughput": 6223.47583693228,
    "output_throughput": 5468.191094250889,
    "total_throughput": 11691.666931183168,
    "itl": 139.52380737201747,
    "ttft": 731492.1701055553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 301,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9952837883867378,
    "arrivals": 106283,
    "finished_requests": 89796,
    "scheduler_time": 98.1860139213214
}
#Debug simulation 
Total elapsed time: 70.5789536270313. Arrivals time: 0.3013017694465816 Scheduler time: 70.11842746753246 Scheduler overhead time: 0.06189617374911904 Adapter cache time: 0.013478965032845736 Engine time: 0.0604636101052165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_160_slots_64_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_160_slots_64_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 71.17342296428978,
    "estimated_duration": 3600.104289055879,
    "input_throughput": 6225.460764604014,
    "output_throughput": 5469.609883208126,
    "total_throughput": 11695.07064781214,
    "itl": 139.49750789859849,
    "ttft": 731163.474131134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 301,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9000051387655561,
    "arrivals": 106283,
    "finished_requests": 89819,
    "scheduler_time": 98.21823132094232
}
#Debug simulation 
Total elapsed time: 71.17353979498148. Arrivals time: 0.30024966783821583 Scheduler time: 70.71555857174098 Scheduler overhead time: 0.061307195108383894 Adapter cache time: 0.013230039272457361 Engine time: 0.05957159586250782 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_160_slots_64_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_160_slots_64_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 71.36413101991639,
    "estimated_duration": 3600.0896253366395,
    "input_throughput": 6223.873662007733,
    "output_throughput": 5468.490801297507,
    "total_throughput": 11692.36446330524,
    "itl": 139.51982707271975,
    "ttft": 731459.9864491615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 301,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0082364283874687,
    "arrivals": 106283,
    "finished_requests": 89800,
    "scheduler_time": 98.19620656108572
}
#Debug simulation 
Total elapsed time: 71.36424296209589. Arrivals time: 0.30403574695810676 Scheduler time: 70.9005990405567 Scheduler overhead time: 0.062011575791984797 Adapter cache time: 0.013206866104155779 Engine time: 0.06119472999125719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_160_slots_64_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_160_slots_64_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 54.01279552001506,
    "estimated_duration": 3600.0908950191124,
    "input_throughput": 6199.043482728933,
    "output_throughput": 5484.556800306893,
    "total_throughput": 11683.600283035825,
    "itl": 139.87462496856938,
    "ttft": 538490.3572606042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7896058379346494,
    "arrivals": 101554,
    "finished_requests": 89921,
    "scheduler_time": 92.22807507848266
}
#Debug simulation 
Total elapsed time: 54.012905094306916. Arrivals time: 0.280336775816977 Scheduler time: 53.58128412393853 Scheduler overhead time: 0.05899914354085922 Adapter cache time: 0.012226252350956202 Engine time: 0.057255000341683626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_160_slots_64_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_160_slots_64_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 54.10290165198967,
    "estimated_duration": 3600.1321989771723,
    "input_throughput": 6198.922363556658,
    "output_throughput": 5484.49332099796,
    "total_throughput": 11683.415684554617,
    "itl": 139.8777013092806,
    "ttft": 538563.6787311658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8417113708565049,
    "arrivals": 101554,
    "finished_requests": 89920,
    "scheduler_time": 92.23117073681976
}
#Debug simulation 
Total elapsed time: 54.103007117286325. Arrivals time: 0.28206514613702893 Scheduler time: 53.669434619136155 Scheduler overhead time: 0.058813237585127354 Adapter cache time: 0.012282233219593763 Engine time: 0.05788269219920039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_160_slots_64_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_160_slots_64_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 53.937614941969514,
    "estimated_duration": 3600.138488756387,
    "input_throughput": 6198.9115334585495,
    "output_throughput": 5484.48373907432,
    "total_throughput": 11683.395272532869,
    "itl": 139.8778407588482,
    "ttft": 538564.7650712836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8432344320416492,
    "arrivals": 101554,
    "finished_requests": 89920,
    "scheduler_time": 92.23144325138344
}
#Debug simulation 
Total elapsed time: 53.93772384384647. Arrivals time: 0.28535643592476845 Scheduler time: 53.50007462501526 Scheduler overhead time: 0.05963901011273265 Adapter cache time: 0.012409855145961046 Engine time: 0.05804179096594453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_160_slots_64_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_160_slots_64_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 54.096357583999634,
    "estimated_duration": 3600.123788152254,
    "input_throughput": 6198.936845850531,
    "output_throughput": 5484.506134199895,
    "total_throughput": 11683.442980050428,
    "itl": 139.8744100367704,
    "ttft": 538562.571996518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8073893641075142,
    "arrivals": 101554,
    "finished_requests": 89920,
    "scheduler_time": 92.23106704495936
}
#Debug simulation 
Total elapsed time: 54.09646594012156. Arrivals time: 0.2868891516700387 Scheduler time: 53.65569951757789 Scheduler overhead time: 0.06031639315187931 Adapter cache time: 0.012133958749473095 Engine time: 0.05877150967717171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_160_slots_64_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_160_slots_64_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 54.18075299123302,
    "estimated_duration": 3600.0967540753877,
    "input_throughput": 6199.033393959909,
    "output_throughput": 5484.54787434486,
    "total_throughput": 11683.58126830477,
    "itl": 139.87946472779262,
    "ttft": 538521.8058828631,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8537977501004973,
    "arrivals": 101554,
    "finished_requests": 89921,
    "scheduler_time": 92.22801718977658
}
#Debug simulation 
Total elapsed time: 54.18086580513045. Arrivals time: 0.28427555970847607 Scheduler time: 53.744430406950414 Scheduler overhead time: 0.05879403231665492 Adapter cache time: 0.012358511798083782 Engine time: 0.058611382730305195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_160_slots_64_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_160_slots_64_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 54.40635792771354,
    "estimated_duration": 3600.1089124587184,
    "input_throughput": 6198.962459932496,
    "output_throughput": 5484.528796245525,
    "total_throughput": 11683.49125617802,
    "itl": 139.87212560072066,
    "ttft": 538559.7717938514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7714329760847624,
    "arrivals": 101554,
    "finished_requests": 89920,
    "scheduler_time": 92.23063829426455
}
#Debug simulation 
Total elapsed time: 54.40646552294493. Arrivals time: 0.2825702764093876 Scheduler time: 53.971367792226374 Scheduler overhead time: 0.05950534110888839 Adapter cache time: 0.012333182152360678 Engine time: 0.05781279969960451 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_160_slots_64_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_160_slots_64_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 54.044900918379426,
    "estimated_duration": 3600.115848733827,
    "input_throughput": 6199.000514899821,
    "output_throughput": 5484.518784845312,
    "total_throughput": 11683.519299745134,
    "itl": 139.87932485536632,
    "ttft": 538527.797133461,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8648640833050051,
    "arrivals": 101554,
    "finished_requests": 89921,
    "scheduler_time": 92.22883005302933
}
#Debug simulation 
Total elapsed time: 54.04500722931698. Arrivals time: 0.28473764937371016 Scheduler time: 53.60818621004 Scheduler overhead time: 0.05873054685071111 Adapter cache time: 0.012374806217849255 Engine time: 0.05858747661113739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 42.82262369804084,
    "estimated_duration": 3600.0423334796883,
    "input_throughput": 6179.599276683205,
    "output_throughput": 5474.190905125141,
    "total_throughput": 11653.790181808346,
    "itl": 137.87581959247055,
    "ttft": 507809.136355484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 342,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.046686808425002,
    "arrivals": 99226,
    "finished_requests": 89639,
    "scheduler_time": 89.4218212691693
}
#Debug simulation 
Total elapsed time: 42.82270928192884. Arrivals time: 0.2665568422526121 Scheduler time: 42.409872291143984 Scheduler overhead time: 0.05651481635868549 Adapter cache time: 0.012418106198310852 Engine time: 0.05555149866268039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 44.71944316243753,
    "estimated_duration": 3600.0785950441577,
    "input_throughput": 6197.626638127973,
    "output_throughput": 5481.700879299259,
    "total_throughput": 11679.327517427231,
    "itl": 138.25335138244924,
    "ttft": 457054.81476352393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8922824597382031,
    "arrivals": 99226,
    "finished_requests": 89817,
    "scheduler_time": 89.33731356341409
}
#Debug simulation 
Total elapsed time: 44.719538405071944. Arrivals time: 0.2688941680826247 Scheduler time: 44.30425812769681 Scheduler overhead time: 0.05704539828002453 Adapter cache time: 0.011500427965074778 Engine time: 0.05589363258332014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 44.782865658868104,
    "estimated_duration": 3600.1367926429884,
    "input_throughput": 6197.595337376008,
    "output_throughput": 5481.686707107583,
    "total_throughput": 11679.28204448359,
    "itl": 138.25646867191358,
    "ttft": 457015.4480554988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.893608679696922,
    "arrivals": 99226,
    "finished_requests": 89818,
    "scheduler_time": 89.33879002376577
}
#Debug simulation 
Total elapsed time: 44.782973256893456. Arrivals time: 0.2740426901727915 Scheduler time: 44.361959943547845 Scheduler overhead time: 0.05651587434113026 Adapter cache time: 0.012325013056397438 Engine time: 0.05578891281038523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 44.85945082874969,
    "estimated_duration": 3600.058358336466,
    "input_throughput": 6197.661476329517,
    "output_throughput": 5481.731693127065,
    "total_throughput": 11679.393169456584,
    "itl": 138.2510300215983,
    "ttft": 457018.61355869,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8530573091679279,
    "arrivals": 99226,
    "finished_requests": 89817,
    "scheduler_time": 89.33661075966417
}
#Debug simulation 
Total elapsed time: 44.85956421587616. Arrivals time: 0.27013158379122615 Scheduler time: 44.44262130558491 Scheduler overhead time: 0.05699244420975447 Adapter cache time: 0.012041326146572828 Engine time: 0.0556315453723073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 44.70694881910458,
    "estimated_duration": 3600.0415558432364,
    "input_throughput": 6197.540959988726,
    "output_throughput": 5481.603390930223,
    "total_throughput": 11679.144350918948,
    "itl": 138.25707088684345,
    "ttft": 457049.2276143679,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9056810431927487,
    "arrivals": 99226,
    "finished_requests": 89816,
    "scheduler_time": 89.33563542804633
}
#Debug simulation 
Total elapsed time: 44.70705803018063. Arrivals time: 0.26870214380323887 Scheduler time: 44.291085006669164 Scheduler overhead time: 0.0578699940815568 Adapter cache time: 0.012100419960916042 Engine time: 0.0551855843514204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 44.81303073000163,
    "estimated_duration": 3600.04046809638,
    "input_throughput": 6197.692275331018,
    "output_throughput": 5481.758934347531,
    "total_throughput": 11679.45120967855,
    "itl": 138.2485195853309,
    "ttft": 457014.53159938706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8162837305082951,
    "arrivals": 99226,
    "finished_requests": 89817,
    "scheduler_time": 89.3359810831635
}
#Debug simulation 
Total elapsed time: 44.81313319411129. Arrivals time: 0.27067364752292633 Scheduler time: 44.394992616493255 Scheduler overhead time: 0.0574519825167954 Adapter cache time: 0.012048288248479366 Engine time: 0.056058808229863644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_160_slots_64_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 44.852296642027795,
    "estimated_duration": 3600.0543987762603,
    "input_throughput": 6197.439407466746,
    "output_throughput": 5481.556613896723,
    "total_throughput": 11678.99602136347,
    "itl": 138.25610011908597,
    "ttft": 457115.2121642122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9169988839700863,
    "arrivals": 99226,
    "finished_requests": 89815,
    "scheduler_time": 89.33816159370647
}
#Debug simulation 
Total elapsed time: 44.85239356197417. Arrivals time: 0.2718214890919626 Scheduler time: 44.432400124147534 Scheduler overhead time: 0.05720778414979577 Adapter cache time: 0.012101275380700827 Engine time: 0.05674801906570792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 46.04138053394854,
    "estimated_duration": 3600.0410100982176,
    "input_throughput": 6120.584442841903,
    "output_throughput": 5433.084219078486,
    "total_throughput": 11553.668661920388,
    "itl": 134.19301034764678,
    "ttft": 410501.9753554769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.697791205616667,
    "arrivals": 98086,
    "finished_requests": 89130,
    "scheduler_time": 87.67963468715814
}
#Debug simulation 
Total elapsed time: 46.041481987107545. Arrivals time: 0.2697090576402843 Scheduler time: 45.62236869800836 Scheduler overhead time: 0.058223925065249205 Adapter cache time: 0.011765428818762302 Engine time: 0.057043665554374456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 35.23992947489023,
    "estimated_duration": 3600.1409964158433,
    "input_throughput": 6142.8319118659165,
    "output_throughput": 5441.311331834645,
    "total_throughput": 11584.143243700562,
    "itl": 134.2317839529967,
    "ttft": 475055.5432875227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 422,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3770251042489021,
    "arrivals": 98086,
    "finished_requests": 89248,
    "scheduler_time": 87.62608927238767
}
#Debug simulation 
Total elapsed time: 35.24004984879866. Arrivals time: 0.25910171680152416 Scheduler time: 34.836145704612136 Scheduler overhead time: 0.05515060946345329 Adapter cache time: 0.013105651829391718 Engine time: 0.054479677230119705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 35.28442765213549,
    "estimated_duration": 3600.1421501171117,
    "input_throughput": 6142.829943334488,
    "output_throughput": 5441.309588112447,
    "total_throughput": 11584.139531446936,
    "itl": 134.2317975358508,
    "ttft": 475055.6045826465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 422,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.379468747638174,
    "arrivals": 98086,
    "finished_requests": 89248,
    "scheduler_time": 87.62606390478523
}
#Debug simulation 
Total elapsed time: 35.28453710395843. Arrivals time: 0.25821084436029196 Scheduler time: 34.88360057026148 Scheduler overhead time: 0.0533989230170846 Adapter cache time: 0.013792043551802635 Engine time: 0.053723365534096956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 46.126794000156224,
    "estimated_duration": 3600.0651387642197,
    "input_throughput": 6120.543698707532,
    "output_throughput": 5433.250856875969,
    "total_throughput": 11553.7945555835,
    "itl": 134.19309544588492,
    "ttft": 410531.0885840733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7123761161207242,
    "arrivals": 98086,
    "finished_requests": 89131,
    "scheduler_time": 87.6801192007545
}
#Debug simulation 
Total elapsed time: 46.12689696904272. Arrivals time: 0.26360672572627664 Scheduler time: 45.714668398723006 Scheduler overhead time: 0.05788169149309397 Adapter cache time: 0.011522712651640177 Engine time: 0.05682772817090154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 35.24437752785161,
    "estimated_duration": 3600.0982426973083,
    "input_throughput": 6141.809336690114,
    "output_throughput": 5440.256815137897,
    "total_throughput": 11582.066151828012,
    "itl": 134.2235211897661,
    "ttft": 475680.46499408333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.381182188503449,
    "arrivals": 98086,
    "finished_requests": 89230,
    "scheduler_time": 87.62269042794462
}
#Debug simulation 
Total elapsed time: 35.24448652705178. Arrivals time: 0.2557056322693825 Scheduler time: 34.84585906425491 Scheduler overhead time: 0.0539932013489306 Adapter cache time: 0.013152940664440393 Engine time: 0.05374430399388075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 45.94670558581129,
    "estimated_duration": 3600.042716895028,
    "input_throughput": 6120.581541044667,
    "output_throughput": 5433.081643228268,
    "total_throughput": 11553.663184272935,
    "itl": 134.18899460072836,
    "ttft": 410500.42599056097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.681731467237697,
    "arrivals": 98086,
    "finished_requests": 89130,
    "scheduler_time": 87.67944344351534
}
#Debug simulation 
Total elapsed time: 45.946816376876086. Arrivals time: 0.26717651169747114 Scheduler time: 45.52987805288285 Scheduler overhead time: 0.05783621221780777 Adapter cache time: 0.011369160376489162 Engine time: 0.05756174447014928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_160_slots_64_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 46.77963710390031,
    "estimated_duration": 3600.0568694693716,
    "input_throughput": 6120.557479762186,
    "output_throughput": 5433.060284651263,
    "total_throughput": 11553.61776441345,
    "itl": 134.1998215581579,
    "ttft": 410535.48076884844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7647614935040503,
    "arrivals": 98086,
    "finished_requests": 89130,
    "scheduler_time": 87.67970121254292
}
#Debug simulation 
Total elapsed time: 46.779740575701. Arrivals time: 0.2693338873796165 Scheduler time: 46.360337641555816 Scheduler overhead time: 0.058149993885308504 Adapter cache time: 0.01185264578089118 Engine time: 0.057659830898046494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_160_slots_64_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [53 53 54]
Adapter prompts. [33, 4320, 33, 33, 1080, 33, 33, 33, 4320, 33, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 33, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 33, 1080, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 33, 33, 1080, 33, 1080, 4320, 33, 33, 1080, 33, 33, 33, 33, 33, 4320, 1080, 1080, 4320, 1080, 33, 4320, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 33, 33, 4320, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 33, 1080, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 33, 1080, 1080, 4320, 1080, 4320, 33, 33, 33, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 33, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 292269 . Total input tokens: 65130866 . Total output tokens: 58502807
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 40.17886448092759,
    "estimated_duration": 3600.056880359689,
    "input_throughput": 6212.998222896204,
    "output_throughput": 5434.518578507922,
    "total_throughput": 11647.516801404126,
    "itl": 133.8233986854452,
    "ttft": 377345.59027671587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7681824237271202,
    "arrivals": 97521,
    "finished_requests": 89689,
    "scheduler_time": 86.3077947358195
}
#Debug simulation 
Total elapsed time: 40.17894957680255. Arrivals time: 0.26478230813518167 Scheduler time: 39.76816097274423 Scheduler overhead time: 0.056302829179912806 Adapter cache time: 0.011883934959769249 Engine time: 0.05579867493361235 
