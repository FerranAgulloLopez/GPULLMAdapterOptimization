INFO 06-01 00:46:59 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:46:59 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 21.360633762087673,
    "estimated_duration": 3600.0343318636283,
    "input_throughput": 7757.38685401456,
    "output_throughput": 6847.769695362405,
    "total_throughput": 14605.156549376967,
    "itl": 125.67084708583798,
    "ttft": 1539944.7837356303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30310992879560233,
    "arrivals": 284045,
    "finished_requests": 112550,
    "scheduler_time": 151.77034263831297
}
#Debug simulation 
Total elapsed time: 21.360827820841223. Arrivals time: 0.4276747927069664 Scheduler time: 20.790427128318697 Scheduler overhead time: 0.054444610606878996 Adapter cache time: 0.00975348986685276 Engine time: 0.05570437805727124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 21.621601501014084,
    "estimated_duration": 3600.0874225259836,
    "input_throughput": 7757.521060531532,
    "output_throughput": 6847.957870618089,
    "total_throughput": 14605.478931149622,
    "itl": 125.67093500876223,
    "ttft": 1539924.5730018334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3222182929888367,
    "arrivals": 284045,
    "finished_requests": 112554,
    "scheduler_time": 151.7721417917816
}
#Debug simulation 
Total elapsed time: 21.62171993497759. Arrivals time: 0.40717396698892117 Scheduler time: 21.072173778433353 Scheduler overhead time: 0.05468461290001869 Adapter cache time: 0.009863552637398243 Engine time: 0.05481302784755826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 21.51728641288355,
    "estimated_duration": 3600.011168412268,
    "input_throughput": 7757.401767260815,
    "output_throughput": 6847.664589571887,
    "total_throughput": 14605.066356832702,
    "itl": 125.67043026982572,
    "ttft": 1539952.3297870357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29003487860551136,
    "arrivals": 284045,
    "finished_requests": 112549,
    "scheduler_time": 151.76976492642223
}
#Debug simulation 
Total elapsed time: 21.51744741713628. Arrivals time: 0.4017376648262143 Scheduler time: 20.97384486766532 Scheduler overhead time: 0.05449987016618252 Adapter cache time: 0.009723152965307236 Engine time: 0.055065570399165154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 21.392086095176637,
    "estimated_duration": 3600.0927801957764,
    "input_throughput": 7757.509515763441,
    "output_throughput": 6847.947679464898,
    "total_throughput": 14605.45719522834,
    "itl": 125.67101842107573,
    "ttft": 1539924.778040822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.326242414154112,
    "arrivals": 284045,
    "finished_requests": 112554,
    "scheduler_time": 151.7722507847382
}
#Debug simulation 
Total elapsed time: 21.392236828338355. Arrivals time: 0.41329985251650214 Scheduler time: 20.838150972034782 Scheduler overhead time: 0.05352055048570037 Adapter cache time: 0.009619082324206829 Engine time: 0.055310937110334635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 18.04087590891868,
    "estimated_duration": 3600.0194014679237,
    "input_throughput": 7704.95347572008,
    "output_throughput": 6852.717235340658,
    "total_throughput": 14557.670711060739,
    "itl": 126.1837910726106,
    "ttft": 1542660.1799606704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2785043846978803,
    "arrivals": 283764,
    "finished_requests": 112163,
    "scheduler_time": 151.62702671234658
}
#Debug simulation 
Total elapsed time: 18.04097917629406. Arrivals time: 0.3853476014919579 Scheduler time: 17.520907352678478 Scheduler overhead time: 0.05133274896070361 Adapter cache time: 0.009267606306821108 Engine time: 0.05208043800666928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 17.846652128268033,
    "estimated_duration": 3600.1316907532346,
    "input_throughput": 7705.2473028274535,
    "output_throughput": 6852.819318628375,
    "total_throughput": 14558.066621455828,
    "itl": 126.18407427562312,
    "ttft": 1542864.5856877752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2962017006240785,
    "arrivals": 283764,
    "finished_requests": 112164,
    "scheduler_time": 151.63134500989406
}
#Debug simulation 
Total elapsed time: 17.84682040521875. Arrivals time: 0.3917280691675842 Scheduler time: 17.322035901714116 Scheduler overhead time: 0.05105794034898281 Adapter cache time: 0.009065752848982811 Engine time: 0.05096205323934555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 17.971244148910046,
    "estimated_duration": 3600.133292264322,
    "input_throughput": 7705.243875165757,
    "output_throughput": 6852.816270167325,
    "total_throughput": 14558.060145333082,
    "itl": 126.1840260759903,
    "ttft": 1542865.2119195722,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2968578174151482,
    "arrivals": 283764,
    "finished_requests": 112164,
    "scheduler_time": 151.6313855639414
}
#Debug simulation 
Total elapsed time: 17.971350335981697. Arrivals time: 0.3872897671535611 Scheduler time: 17.449311644304544 Scheduler overhead time: 0.05148873431608081 Adapter cache time: 0.009340841323137283 Engine time: 0.05177172692492604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 20.64739108318463,
    "estimated_duration": 3600.028904870674,
    "input_throughput": 7701.32705392708,
    "output_throughput": 6853.543305338083,
    "total_throughput": 14554.870359265164,
    "itl": 126.23001374334619,
    "ttft": 1540240.4179140741,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 76,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23705011005513382,
    "arrivals": 283764,
    "finished_requests": 112100,
    "scheduler_time": 151.62189060437123
}
#Debug simulation 
Total elapsed time: 20.64754730416462. Arrivals time: 0.39299872424453497 Scheduler time: 20.117070441134274 Scheduler overhead time: 0.05265842890366912 Adapter cache time: 0.009274887852370739 Engine time: 0.05335372081026435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 17.895905188750476,
    "estimated_duration": 3600.1371524158963,
    "input_throughput": 7705.23561342238,
    "output_throughput": 6852.808922416838,
    "total_throughput": 14558.044535839217,
    "itl": 126.18398318836942,
    "ttft": 1542865.7386058916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30100769236683833,
    "arrivals": 283764,
    "finished_requests": 112164,
    "scheduler_time": 151.63142175035324
}
#Debug simulation 
Total elapsed time: 17.896017271094024. Arrivals time: 0.3638396682217717 Scheduler time: 17.397430441342294 Scheduler overhead time: 0.052358381915837526 Adapter cache time: 0.009037334471940994 Engine time: 0.051494843792170286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 17.991228207945824,
    "estimated_duration": 3600.0115178293277,
    "input_throughput": 7704.970348740708,
    "output_throughput": 6852.7322420554465,
    "total_throughput": 14557.702590796154,
    "itl": 126.18379308784733,
    "ttft": 1542656.611060617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2720945768360983,
    "arrivals": 283764,
    "finished_requests": 112163,
    "scheduler_time": 151.62686386364038
}
#Debug simulation 
Total elapsed time: 17.991424161940813. Arrivals time: 0.38059489196166396 Scheduler time: 17.475889131892473 Scheduler overhead time: 0.05162681918591261 Adapter cache time: 0.009200537577271461 Engine time: 0.052117845974862576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 17.942613664083183,
    "estimated_duration": 3600.010021745129,
    "input_throughput": 7705.094661529196,
    "output_throughput": 6852.449535138125,
    "total_throughput": 14557.54419666732,
    "itl": 126.18307549060926,
    "ttft": 1542902.4949447578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30427729081362453,
    "arrivals": 283764,
    "finished_requests": 112156,
    "scheduler_time": 151.62581313087367
}
#Debug simulation 
Total elapsed time: 17.9427312691696. Arrivals time: 0.3913786141201854 Scheduler time: 17.41512051364407 Scheduler overhead time: 0.05223341565579176 Adapter cache time: 0.009224476758390665 Engine time: 0.05274440906941891 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_64_slots_32_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 9.605860170908272,
    "estimated_duration": 3600.005448883924,
    "input_throughput": 7714.313323778373,
    "output_throughput": 6849.783798978659,
    "total_throughput": 14564.097122757034,
    "itl": 126.0747453321077,
    "ttft": 1498516.7122920733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1844087569019808,
    "arrivals": 264567,
    "finished_requests": 112497,
    "scheduler_time": 151.12666991463084
}
#Debug simulation 
Total elapsed time: 9.60595406871289. Arrivals time: 0.3103615092113614 Scheduler time: 9.168502869550139 Scheduler overhead time: 0.047054659109562635 Adapter cache time: 0.010962638538330793 Engine time: 0.04765544878318906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_64_slots_32_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 9.700012943707407,
    "estimated_duration": 3600.1309272045223,
    "input_throughput": 7714.089448842507,
    "output_throughput": 6849.6219994832145,
    "total_throughput": 14563.711448325723,
    "itl": 126.0779709215235,
    "ttft": 1498595.373273996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.265835818832281,
    "arrivals": 264567,
    "finished_requests": 112498,
    "scheduler_time": 151.12916031771988
}
#Debug simulation 
Total elapsed time: 9.700122457928956. Arrivals time: 0.34037367766723037 Scheduler time: 9.230920718982816 Scheduler overhead time: 0.04877114109694958 Adapter cache time: 0.01103097340092063 Engine time: 0.04771615471690893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_64_slots_32_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 9.632341347169131,
    "estimated_duration": 3600.132653865892,
    "input_throughput": 7714.085749084323,
    "output_throughput": 6849.618714332683,
    "total_throughput": 14563.704463417005,
    "itl": 126.07801831873242,
    "ttft": 1498596.2241789617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2675496280193401,
    "arrivals": 264567,
    "finished_requests": 112498,
    "scheduler_time": 151.12917316988413
}
#Debug simulation 
Total elapsed time: 9.632463527843356. Arrivals time: 0.34581117099151015 Scheduler time: 9.159131787251681 Scheduler overhead time: 0.04748745309188962 Adapter cache time: 0.010913427453488111 Engine time: 0.0476955845952034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_64_slots_32_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 9.659661774057895,
    "estimated_duration": 3600.0318845791617,
    "input_throughput": 7714.256676158982,
    "output_throughput": 6849.733499758331,
    "total_throughput": 14563.990175917314,
    "itl": 126.07519505070674,
    "ttft": 1498529.689556806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2074066882953043,
    "arrivals": 264567,
    "finished_requests": 112497,
    "scheduler_time": 151.12697810074118
}
#Debug simulation 
Total elapsed time: 9.659756722860038. Arrivals time: 0.3497107899747789 Scheduler time: 9.182247899007052 Scheduler overhead time: 0.04767954349517822 Adapter cache time: 0.010978370904922485 Engine time: 0.04773320257663727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_64_slots_32_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 9.668711917940527,
    "estimated_duration": 3600.010141119424,
    "input_throughput": 7713.836881405268,
    "output_throughput": 6849.4418163868195,
    "total_throughput": 14563.278697792088,
    "itl": 126.08006356304999,
    "ttft": 1498558.236148933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2821516531333372,
    "arrivals": 264567,
    "finished_requests": 112490,
    "scheduler_time": 151.1232569508214
}
#Debug simulation 
Total elapsed time: 9.668822451960295. Arrivals time: 0.36427515046671033 Scheduler time: 9.177371398545802 Scheduler overhead time: 0.047235348261892796 Adapter cache time: 0.010903508868068457 Engine time: 0.04770613927394152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_64_slots_32_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 9.633559973910451,
    "estimated_duration": 3600.0985454024785,
    "input_throughput": 7714.509380713376,
    "output_throughput": 6850.639416939284,
    "total_throughput": 14565.14879765266,
    "itl": 126.07876751939214,
    "ttft": 1498586.904637249,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.163129564716942,
    "arrivals": 264567,
    "finished_requests": 112504,
    "scheduler_time": 151.13123154449204
}
#Debug simulation 
Total elapsed time: 9.633672215975821. Arrivals time: 0.30702626798301935 Scheduler time: 9.199110373854637 Scheduler overhead time: 0.04739582072943449 Adapter cache time: 0.010994563344866037 Engine time: 0.04778967797756195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_64_slots_32_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 9.676703474950045,
    "estimated_duration": 3600.0282361539507,
    "input_throughput": 7714.041162540598,
    "output_throughput": 6849.408499735314,
    "total_throughput": 14563.449662275912,
    "itl": 126.07908187577269,
    "ttft": 1498587.471310214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2974936150759482,
    "arrivals": 264567,
    "finished_requests": 112492,
    "scheduler_time": 151.12352831399073
}
#Debug simulation 
Total elapsed time: 9.676803200971335. Arrivals time: 0.36334890965372324 Scheduler time: 9.185863277874887 Scheduler overhead time: 0.04750079661607742 Adapter cache time: 0.010999693535268307 Engine time: 0.047641645185649395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 9.181698998436332,
    "estimated_duration": 3600.122442830093,
    "input_throughput": 7730.426240203701,
    "output_throughput": 6845.559113991254,
    "total_throughput": 14575.985354194956,
    "itl": 125.86691061830018,
    "ttft": 1497219.1024617427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.312949242147161,
    "arrivals": 262719,
    "finished_requests": 112378,
    "scheduler_time": 151.04790817598635
}
#Debug simulation 
Total elapsed time: 9.181792600080371. Arrivals time: 0.31284055346623063 Scheduler time: 8.741365790367126 Scheduler overhead time: 0.04741519829258323 Adapter cache time: 0.011330917477607727 Engine time: 0.047396163921803236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 9.12216773815453,
    "estimated_duration": 3600.0969213706785,
    "input_throughput": 7730.384933471216,
    "output_throughput": 6845.33765013672,
    "total_throughput": 14575.722583607936,
    "itl": 125.86947814893298,
    "ttft": 1497256.2240500879,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4028586001345018,
    "arrivals": 262719,
    "finished_requests": 112375,
    "scheduler_time": 151.04335300907056
}
#Debug simulation 
Total elapsed time: 9.122296450193971. Arrivals time: 0.33968743635341525 Scheduler time: 8.655130438506603 Scheduler overhead time: 0.04720988916233182 Adapter cache time: 0.01135381730273366 Engine time: 0.04748832434415817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 9.141433813143522,
    "estimated_duration": 3600.0991361322076,
    "input_throughput": 7730.380177780189,
    "output_throughput": 6845.333438921998,
    "total_throughput": 14575.713616702187,
    "itl": 125.869541838774,
    "ttft": 1497257.255614687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4048203495144926,
    "arrivals": 262719,
    "finished_requests": 112375,
    "scheduler_time": 151.0433830777501
}
#Debug simulation 
Total elapsed time: 9.141529709100723. Arrivals time: 0.3341217623092234 Scheduler time: 8.680450749583542 Scheduler overhead time: 0.04679331462830305 Adapter cache time: 0.01129327341914177 Engine time: 0.04757305467501283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 9.101496340241283,
    "estimated_duration": 3600.011058957868,
    "input_throughput": 7730.446808142902,
    "output_throughput": 6845.478693372095,
    "total_throughput": 14575.925501514997,
    "itl": 125.86716876555131,
    "ttft": 1497245.0378355964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3383005398209178,
    "arrivals": 262719,
    "finished_requests": 112374,
    "scheduler_time": 151.04210492558448
}
#Debug simulation 
Total elapsed time: 9.101604843046516. Arrivals time: 0.32472042879089713 Scheduler time: 8.64982123253867 Scheduler overhead time: 0.04715128801763058 Adapter cache time: 0.011243584100157022 Engine time: 0.04731396213173866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 9.13558669667691,
    "estimated_duration": 3600.1210131613902,
    "input_throughput": 7730.333202205723,
    "output_throughput": 6845.2918415537815,
    "total_throughput": 14575.625043759504,
    "itl": 125.8699347720187,
    "ttft": 1497265.8075577638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4246894477680367,
    "arrivals": 262719,
    "finished_requests": 112375,
    "scheduler_time": 151.04356547290476
}
#Debug simulation 
Total elapsed time: 9.135768713895231. Arrivals time: 0.32568785920739174 Scheduler time: 8.682570262346417 Scheduler overhead time: 0.04708616295829415 Adapter cache time: 0.011367316823452711 Engine time: 0.04763395292684436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 9.135391850955784,
    "estimated_duration": 3600.0687537315443,
    "input_throughput": 7730.448195233352,
    "output_throughput": 6845.430653082937,
    "total_throughput": 14575.878848316288,
    "itl": 125.86507082687164,
    "ttft": 1497228.0551084832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2827315765130247,
    "arrivals": 262719,
    "finished_requests": 112376,
    "scheduler_time": 151.04666929896675
}
#Debug simulation 
Total elapsed time: 9.135489353910089. Arrivals time: 0.3244785265997052 Scheduler time: 8.683380406349897 Scheduler overhead time: 0.04710537241771817 Adapter cache time: 0.011328320018947124 Engine time: 0.04773293389007449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 9.138558355160058,
    "estimated_duration": 3600.012748720255,
    "input_throughput": 7730.297069056995,
    "output_throughput": 6845.262425462296,
    "total_throughput": 14575.55949451929,
    "itl": 125.87135977127554,
    "ttft": 1497259.6842603744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 430,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4448354815691715,
    "arrivals": 262719,
    "finished_requests": 112372,
    "scheduler_time": 151.0381426157945
}
#Debug simulation 
Total elapsed time: 9.138682013843209. Arrivals time: 0.3231676369905472 Scheduler time: 8.687848117668182 Scheduler overhead time: 0.04734520986676216 Adapter cache time: 0.011257535312324762 Engine time: 0.04767819307744503 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.825394780840725,
    "estimated_duration": 3600.0248709322545,
    "input_throughput": 7679.3461131952945,
    "output_throughput": 6852.4726590602,
    "total_throughput": 14531.818772255494,
    "itl": 126.48736062995569,
    "ttft": 1502832.4371364652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3710985092818853,
    "arrivals": 261754,
    "finished_requests": 111849,
    "scheduler_time": 150.92396827714134
}
#Debug simulation 
Total elapsed time: 8.825493908021599. Arrivals time: 0.34231111453846097 Scheduler time: 8.356455990113318 Scheduler overhead time: 0.04675388289615512 Adapter cache time: 0.011299328412860632 Engine time: 0.04732548212632537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 9.144026641268283,
    "estimated_duration": 3600.0111718541275,
    "input_throughput": 7678.769226086202,
    "output_throughput": 6851.991791818672,
    "total_throughput": 14530.761017904873,
    "itl": 126.48962304149669,
    "ttft": 1502875.7790812368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4527423814451381,
    "arrivals": 261754,
    "finished_requests": 111836,
    "scheduler_time": 150.9202846952176
}
#Debug simulation 
Total elapsed time: 9.144097808282822. Arrivals time: 0.5703345984220505 Scheduler time: 8.44700735528022 Scheduler overhead time: 0.0469550509005785 Adapter cache time: 0.011297088582068682 Engine time: 0.04716556938365102 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.897128459066153,
    "estimated_duration": 3600.014567210673,
    "input_throughput": 7678.761983849021,
    "output_throughput": 6851.985329357272,
    "total_throughput": 14530.747313206293,
    "itl": 126.48972561698429,
    "ttft": 1502877.106963392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4552028885670092,
    "arrivals": 261754,
    "finished_requests": 111836,
    "scheduler_time": 150.9203245042119
}
#Debug simulation 
Total elapsed time: 8.897227096371353. Arrivals time: 0.3341957372613251 Scheduler time: 8.436230568680912 Scheduler overhead time: 0.04667056445032358 Adapter cache time: 0.011415075976401567 Engine time: 0.0474393661133945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.829724760260433,
    "estimated_duration": 3600.0729458375563,
    "input_throughput": 7679.185509828121,
    "output_throughput": 6852.521704740243,
    "total_throughput": 14531.707214568363,
    "itl": 126.48789339706165,
    "ttft": 1502772.431058965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3808296053996263,
    "arrivals": 261754,
    "finished_requests": 111845,
    "scheduler_time": 150.92581981103427
}
#Debug simulation 
Total elapsed time: 8.829812806099653. Arrivals time: 0.2984230602160096 Scheduler time: 8.404901320580393 Scheduler overhead time: 0.04669109079986811 Adapter cache time: 0.01128115737810731 Engine time: 0.04731775587424636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.894390845205635,
    "estimated_duration": 3600.0518190209355,
    "input_throughput": 7678.824469676124,
    "output_throughput": 6851.986371326327,
    "total_throughput": 14530.81084100245,
    "itl": 126.490780961767,
    "ttft": 1502871.639121473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4773355549760199,
    "arrivals": 261754,
    "finished_requests": 111837,
    "scheduler_time": 150.92114443564492
}
#Debug simulation 
Total elapsed time: 8.89450732106343. Arrivals time: 0.34085547644644976 Scheduler time: 8.426369631197304 Scheduler overhead time: 0.04720946680754423 Adapter cache time: 0.011321004014462233 Engine time: 0.04749515978619456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.811401061248034,
    "estimated_duration": 3600.1280102356964,
    "input_throughput": 7679.15610817129,
    "output_throughput": 6852.442171461816,
    "total_throughput": 14531.598279633105,
    "itl": 126.48660506818871,
    "ttft": 1502887.0362871226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.339542532116164,
    "arrivals": 261754,
    "finished_requests": 111851,
    "scheduler_time": 150.92970621167694
}
#Debug simulation 
Total elapsed time: 8.811514105182141. Arrivals time: 0.3184008952230215 Scheduler time: 8.366464104037732 Scheduler overhead time: 0.04666416300460696 Adapter cache time: 0.011270406190305948 Engine time: 0.047362387645989656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.864410493057221,
    "estimated_duration": 3600.0691021188954,
    "input_throughput": 7678.787605418311,
    "output_throughput": 6851.953476526722,
    "total_throughput": 14530.741081945032,
    "itl": 126.49125057837595,
    "ttft": 1502879.1772849178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4928032707050445,
    "arrivals": 261754,
    "finished_requests": 111837,
    "scheduler_time": 150.9213326579436
}
#Debug simulation 
Total elapsed time: 8.864507621154189. Arrivals time: 0.33379244431853294 Scheduler time: 8.403821101412177 Scheduler overhead time: 0.04681668384000659 Adapter cache time: 0.011219818145036697 Engine time: 0.047578874975442886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.621343026868999,
    "estimated_duration": 3600.0238912180876,
    "input_throughput": 7778.414767832335,
    "output_throughput": 6841.214043073142,
    "total_throughput": 14619.628810905479,
    "itl": 125.46206599949747,
    "ttft": 1491987.6051160078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3037677789153623,
    "arrivals": 261307,
    "finished_requests": 112736,
    "scheduler_time": 151.07249131395676
}
#Debug simulation 
Total elapsed time: 8.621439496986568. Arrivals time: 0.3200573087669909 Scheduler time: 8.174482367001474 Scheduler overhead time: 0.0469013387337327 Adapter cache time: 0.011052062269300222 Engine time: 0.04753149673342705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.652647740207613,
    "estimated_duration": 3600.118148625612,
    "input_throughput": 7778.211115290836,
    "output_throughput": 6841.034928090412,
    "total_throughput": 14619.246043381248,
    "itl": 125.46460348973586,
    "ttft": 1492027.340295829,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3922540679760338,
    "arrivals": 261307,
    "finished_requests": 112736,
    "scheduler_time": 151.07319707362183
}
#Debug simulation 
Total elapsed time: 8.652770980261266. Arrivals time: 0.3203232381492853 Scheduler time: 8.205465908162296 Scheduler overhead time: 0.046936292201280594 Adapter cache time: 0.011087058577686548 Engine time: 0.04757622396573424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.641890397295356,
    "estimated_duration": 3600.12024430128,
    "input_throughput": 7778.206587495466,
    "output_throughput": 6841.03094583719,
    "total_throughput": 14619.237533332656,
    "itl": 125.46466453409586,
    "ttft": 1492028.24886772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3943408029899083,
    "arrivals": 261307,
    "finished_requests": 112736,
    "scheduler_time": 151.07320601425783
}
#Debug simulation 
Total elapsed time: 8.641983826179057. Arrivals time: 0.31712833186611533 Scheduler time: 8.198499942664057 Scheduler overhead time: 0.0466981683857739 Adapter cache time: 0.011089996434748173 Engine time: 0.04716823436319828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.633961054962128,
    "estimated_duration": 3600.0417628138557,
    "input_throughput": 7778.376153645721,
    "output_throughput": 6841.180081408252,
    "total_throughput": 14619.556235053973,
    "itl": 125.46245355352809,
    "ttft": 1491995.3705638868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3187069106567613,
    "arrivals": 261307,
    "finished_requests": 112736,
    "scheduler_time": 151.0727004780358
}
#Debug simulation 
Total elapsed time: 8.63412117678672. Arrivals time: 0.3259749161079526 Scheduler time: 8.1814503967762 Scheduler overhead time: 0.046773675829172134 Adapter cache time: 0.011049954686313868 Engine time: 0.04740522615611553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.598214915022254,
    "estimated_duration": 3600.0023433511865,
    "input_throughput": 7778.139659190889,
    "output_throughput": 6840.853047077471,
    "total_throughput": 14618.99270626836,
    "itl": 125.46445232973657,
    "ttft": 1492075.4985695092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4169764845445785,
    "arrivals": 261307,
    "finished_requests": 112730,
    "scheduler_time": 151.06720317457228
}
#Debug simulation 
Total elapsed time: 8.59834121586755. Arrivals time: 0.3184013497084379 Scheduler time: 8.153107894118875 Scheduler overhead time: 0.04676357842981815 Adapter cache time: 0.011133592575788498 Engine time: 0.04752394184470177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.628360787872225,
    "estimated_duration": 3600.1330160667944,
    "input_throughput": 7778.299267006987,
    "output_throughput": 6841.031398030726,
    "total_throughput": 14619.330665037714,
    "itl": 125.46146058709725,
    "ttft": 1492043.4729240001,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2737614256283185,
    "arrivals": 261307,
    "finished_requests": 112738,
    "scheduler_time": 151.07831674909025
}
#Debug simulation 
Total elapsed time: 8.628457651939243. Arrivals time: 0.33621574006974697 Scheduler time: 8.165652567986399 Scheduler overhead time: 0.04688434023410082 Adapter cache time: 0.010989515110850334 Engine time: 0.04736357135698199 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.585890160873532,
    "estimated_duration": 3600.0191417947412,
    "input_throughput": 7778.103364761643,
    "output_throughput": 6840.821126223927,
    "total_throughput": 14618.924490985572,
    "itl": 125.46492337402351,
    "ttft": 1492082.2627601484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4308094010502106,
    "arrivals": 261307,
    "finished_requests": 112730,
    "scheduler_time": 151.0673979707105
}
#Debug simulation 
Total elapsed time: 8.585987579077482. Arrivals time: 0.32126343064010143 Scheduler time: 8.138791285455227 Scheduler overhead time: 0.0466473326086998 Adapter cache time: 0.01094070728868246 Engine time: 0.04712921101599932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.532076525036246,
    "estimated_duration": 3600.025780976002,
    "input_throughput": 7675.655031700508,
    "output_throughput": 6855.00757533728,
    "total_throughput": 14530.662607037788,
    "itl": 126.5357121509483,
    "ttft": 1499790.971434377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1844087569019808,
    "arrivals": 261056,
    "finished_requests": 111888,
    "scheduler_time": 150.9669702431339
}
#Debug simulation 
Total elapsed time: 8.532203265931457. Arrivals time: 0.32910954067483544 Scheduler time: 8.07813499448821 Scheduler overhead time: 0.04598433943465352 Adapter cache time: 0.010970314498990774 Engine time: 0.04687218787148595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.515059899073094,
    "estimated_duration": 3600.011547497083,
    "input_throughput": 7675.613157188738,
    "output_throughput": 6854.76745683133,
    "total_throughput": 14530.380614020069,
    "itl": 126.53863895303196,
    "ttft": 1499826.7333667874,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2633842469216383,
    "arrivals": 261056,
    "finished_requests": 111886,
    "scheduler_time": 150.96338202428373
}
#Debug simulation 
Total elapsed time: 8.515170468948781. Arrivals time: 0.31816365336999297 Scheduler time: 8.072450228966773 Scheduler overhead time: 0.046154861338436604 Adapter cache time: 0.010633138008415699 Engine time: 0.04674716992303729 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.484484755899757,
    "estimated_duration": 3600.0137072745033,
    "input_throughput": 7675.608552312942,
    "output_throughput": 6854.7633444103285,
    "total_throughput": 14530.37189672327,
    "itl": 126.53870199151956,
    "ttft": 1499827.7606562425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.265526143051691,
    "arrivals": 261056,
    "finished_requests": 111886,
    "scheduler_time": 150.96339990555575
}
#Debug simulation 
Total elapsed time: 8.484585478901863. Arrivals time: 0.3208065442740917 Scheduler time: 8.039253754075617 Scheduler overhead time: 0.04614759748801589 Adapter cache time: 0.010722620878368616 Engine time: 0.04668493801727891 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.478184972889721,
    "estimated_duration": 3600.063829098289,
    "input_throughput": 7675.840571671583,
    "output_throughput": 6855.060679905024,
    "total_throughput": 14530.901251576608,
    "itl": 126.53672952771342,
    "ttft": 1499814.877753603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1914714708761294,
    "arrivals": 261056,
    "finished_requests": 111890,
    "scheduler_time": 150.9684766789776
}
#Debug simulation 
Total elapsed time: 8.478284119162709. Arrivals time: 0.31708236411213875 Scheduler time: 8.037195782642812 Scheduler overhead time: 0.04569512652233243 Adapter cache time: 0.010634006932377815 Engine time: 0.04661819105967879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.426770834252238,
    "estimated_duration": 3600.034794291556,
    "input_throughput": 7675.563592834026,
    "output_throughput": 6854.723192989635,
    "total_throughput": 14530.286785823662,
    "itl": 126.53941302371113,
    "ttft": 1499836.5085856284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.287658809460703,
    "arrivals": 261056,
    "finished_requests": 111886,
    "scheduler_time": 150.9634645484539
}
#Debug simulation 
Total elapsed time: 8.42686519306153. Arrivals time: 0.31910837395116687 Scheduler time: 7.984269474167377 Scheduler overhead time: 0.04559395322576165 Adapter cache time: 0.010559008922427893 Engine time: 0.046300568617880344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.476851352956146,
    "estimated_duration": 3600.137468352271,
    "input_throughput": 7675.7007872389595,
    "output_throughput": 6854.921018139636,
    "total_throughput": 14530.621805378596,
    "itl": 126.53484858260663,
    "ttft": 1499852.3717437123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1571494641271378,
    "arrivals": 261056,
    "finished_requests": 111891,
    "scheduler_time": 150.97288288797466
}
#Debug simulation 
Total elapsed time: 8.476947910152376. Arrivals time: 0.3177038808353245 Scheduler time: 8.035458880476654 Scheduler overhead time: 0.045666614547371864 Adapter cache time: 0.010610956232994795 Engine time: 0.04657400865107775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.470518840942532,
    "estimated_duration": 3600.0485748020305,
    "input_throughput": 7675.534211790329,
    "output_throughput": 6854.696954014577,
    "total_throughput": 14530.231165804906,
    "itl": 126.53964934165815,
    "ttft": 1499842.5630781667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.29822212751955,
    "arrivals": 261056,
    "finished_requests": 111886,
    "scheduler_time": 150.96364389921968
}
#Debug simulation 
Total elapsed time: 8.470620058942586. Arrivals time: 0.30837074760347605 Scheduler time: 8.038391562644392 Scheduler overhead time: 0.04571916861459613 Adapter cache time: 0.010573208332061768 Engine time: 0.046619482804089785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_64_slots_32_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.52318708691746,
    "estimated_duration": 3600.0905267823236,
    "input_throughput": 7709.210308332369,
    "output_throughput": 6849.59525782808,
    "total_throughput": 14558.80556616045,
    "itl": 126.22248044021231,
    "ttft": 1490687.6289757907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8026272811764186,
    "arrivals": 258928,
    "finished_requests": 112085,
    "scheduler_time": 150.83437365914233
}
#Debug simulation 
Total elapsed time: 8.523311602883041. Arrivals time: 0.320378172211349 Scheduler time: 8.076845662668347 Scheduler overhead time: 0.04610980348661542 Adapter cache time: 0.01205896446481347 Engine time: 0.046693716663867235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_64_slots_32_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.523241315968335,
    "estimated_duration": 3600.1309100235653,
    "input_throughput": 7709.042446964334,
    "output_throughput": 6849.435094525102,
    "total_throughput": 14558.477541489436,
    "itl": 126.22743536341524,
    "ttft": 1490688.5942786532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9262121323472896,
    "arrivals": 258928,
    "finished_requests": 112085,
    "scheduler_time": 150.83147986058682
}
#Debug simulation 
Total elapsed time: 8.523354770150036. Arrivals time: 0.30735996132716537 Scheduler time: 8.08967766072601 Scheduler overhead time: 0.046311075799167156 Adapter cache time: 0.012067843228578568 Engine time: 0.04677788866683841 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_64_slots_32_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.536260040011257,
    "estimated_duration": 3600.1336007394216,
    "input_throughput": 7709.036685277394,
    "output_throughput": 6849.429975302967,
    "total_throughput": 14558.466660580361,
    "itl": 126.22752212072211,
    "ttft": 1490689.9853019167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9288805897161485,
    "arrivals": 258928,
    "finished_requests": 112085,
    "scheduler_time": 150.83150211904518
}
#Debug simulation 
Total elapsed time: 8.536397521849722. Arrivals time: 0.32024839660152793 Scheduler time: 8.08973297663033 Scheduler overhead time: 0.045976121444255114 Adapter cache time: 0.012188737746328115 Engine time: 0.046923627611249685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_64_slots_32_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.492943986319005,
    "estimated_duration": 3600.0333507038436,
    "input_throughput": 7709.251080847319,
    "output_throughput": 6849.592933681839,
    "total_throughput": 14558.844014529157,
    "itl": 126.22416697517866,
    "ttft": 1490659.4185735707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8355039716535129,
    "arrivals": 258928,
    "finished_requests": 112084,
    "scheduler_time": 150.83080830097202
}
#Debug simulation 
Total elapsed time: 8.493047083262354. Arrivals time: 0.2911082827486098 Scheduler time: 8.076349938753992 Scheduler overhead time: 0.04598115850239992 Adapter cache time: 0.012134097982198 Engine time: 0.046464412938803434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_64_slots_32_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.49027533037588,
    "estimated_duration": 3600.022566809287,
    "input_throughput": 7708.966398118192,
    "output_throughput": 6849.447619394959,
    "total_throughput": 14558.41401751315,
    "itl": 126.22822855510924,
    "ttft": 1490694.9651049362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.956797930300238,
    "arrivals": 258928,
    "finished_requests": 112080,
    "scheduler_time": 150.8256502105021
}
#Debug simulation 
Total elapsed time: 8.490375326015055. Arrivals time: 0.3310642000287771 Scheduler time: 8.033417758066207 Scheduler overhead time: 0.04598166700452566 Adapter cache time: 0.012116062454879284 Engine time: 0.046702305786311626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_64_slots_32_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.557276692707092,
    "estimated_duration": 3600.041667304367,
    "input_throughput": 7709.314937118904,
    "output_throughput": 6849.688219987811,
    "total_throughput": 14559.003157106716,
    "itl": 126.22110195611178,
    "ttft": 1490667.6045122964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7611396236973558,
    "arrivals": 258928,
    "finished_requests": 112085,
    "scheduler_time": 150.83382067056277
}
#Debug simulation 
Total elapsed time: 8.557370183989406. Arrivals time: 0.319072088226676 Scheduler time: 8.111801366321743 Scheduler overhead time: 0.04624592699110508 Adapter cache time: 0.012087163515388966 Engine time: 0.046998030971735716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_64_slots_32_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.545173638965935,
    "estimated_duration": 3600.047656323261,
    "input_throughput": 7708.912672657134,
    "output_throughput": 6849.399884106939,
    "total_throughput": 14558.312556764073,
    "itl": 126.22893172393051,
    "ttft": 1490705.5632326468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.979685119427735,
    "arrivals": 258928,
    "finished_requests": 112080,
    "scheduler_time": 150.82586517769684
}
#Debug simulation 
Total elapsed time: 8.545262330211699. Arrivals time: 0.3092921716161072 Scheduler time: 8.110235576052219 Scheduler overhead time: 0.045988386031240225 Adapter cache time: 0.012046991381794214 Engine time: 0.04669314296916127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.324847920797765,
    "estimated_duration": 3600.1034180651122,
    "input_throughput": 7752.328963649574,
    "output_throughput": 6843.29118890729,
    "total_throughput": 14595.620152556863,
    "itl": 125.71103410693362,
    "ttft": 1489819.9406426747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.068889714898577,
    "arrivals": 257941,
    "finished_requests": 112268,
    "scheduler_time": 150.9344757178361
}
#Debug simulation 
Total elapsed time: 8.324951890856028. Arrivals time: 0.3233782071620226 Scheduler time: 7.874714822042733 Scheduler overhead time: 0.04602112201973796 Adapter cache time: 0.012663084547966719 Engine time: 0.04707831609994173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.32264576293528,
    "estimated_duration": 3600.115021442782,
    "input_throughput": 7752.142871483964,
    "output_throughput": 6843.133303592853,
    "total_throughput": 14595.276175076817,
    "itl": 125.71589510384385,
    "ttft": 1489841.7792913767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.210862227110199,
    "arrivals": 257941,
    "finished_requests": 112266,
    "scheduler_time": 150.92949275077177
}
#Debug simulation 
Total elapsed time: 8.322765089571476. Arrivals time: 0.30668533174321055 Scheduler time: 7.889303490519524 Scheduler overhead time: 0.0460105249658227 Adapter cache time: 0.012643972877413034 Engine time: 0.04697834420949221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.304680234752595,
    "estimated_duration": 3600.126963866374,
    "input_throughput": 7752.11715589814,
    "output_throughput": 6843.110603394381,
    "total_throughput": 14595.22775929252,
    "itl": 125.71629902537786,
    "ttft": 1489843.2770417759,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2139015792310275,
    "arrivals": 257941,
    "finished_requests": 112266,
    "scheduler_time": 150.92987101322856
}
#Debug simulation 
Total elapsed time: 8.304790703114122. Arrivals time: 0.32067847019061446 Scheduler time: 7.858105673454702 Scheduler overhead time: 0.04582483181729913 Adapter cache time: 0.012532617431133986 Engine time: 0.0465757017955184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.293394675944,
    "estimated_duration": 3600.0024951727305,
    "input_throughput": 7752.385182349972,
    "output_throughput": 6843.347201296299,
    "total_throughput": 14595.732383646271,
    "itl": 125.71233319287137,
    "ttft": 1489795.2477821268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0976813239021967,
    "arrivals": 257941,
    "finished_requests": 112266,
    "scheduler_time": 150.9290065308765
}
#Debug simulation 
Total elapsed time: 8.293485580012202. Arrivals time: 0.309277324937284 Scheduler time: 7.857838722411543 Scheduler overhead time: 0.04595000483095646 Adapter cache time: 0.012739176396280527 Engine time: 0.046598758548498154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.547254025004804,
    "estimated_duration": 3600.0183020501822,
    "input_throughput": 7751.728646520364,
    "output_throughput": 6842.971044333486,
    "total_throughput": 14594.699690853851,
    "itl": 125.71761982108396,
    "ttft": 1489844.908334421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2487353780679418,
    "arrivals": 257941,
    "finished_requests": 112259,
    "scheduler_time": 150.92381650623537
}
#Debug simulation 
Total elapsed time: 8.547324398998171. Arrivals time: 0.3265123078599572 Scheduler time: 8.093862398527563 Scheduler overhead time: 0.046212018467485905 Adapter cache time: 0.012643709778785706 Engine time: 0.04697240423411131 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.341021686792374,
    "estimated_duration": 3600.049402574923,
    "input_throughput": 7752.401114284199,
    "output_throughput": 6843.319422888747,
    "total_throughput": 14595.720537172947,
    "itl": 125.70931821169951,
    "ttft": 1489814.7165013673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.021273999353836,
    "arrivals": 257941,
    "finished_requests": 112267,
    "scheduler_time": 150.93397815705086
}
#Debug simulation 
Total elapsed time: 8.341109848115593. Arrivals time: 0.3110362538136542 Scheduler time: 7.903658843133599 Scheduler overhead time: 0.04581158189103007 Adapter cache time: 0.012675976380705833 Engine time: 0.0469089113175869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.320104653947055,
    "estimated_duration": 3600.047069483421,
    "input_throughput": 7751.666703625724,
    "output_throughput": 6842.916363183803,
    "total_throughput": 14594.583066809528,
    "itl": 125.71843943819225,
    "ttft": 1489855.830848884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.272251336127523,
    "arrivals": 257941,
    "finished_requests": 112259,
    "scheduler_time": 150.9241555425165
}
#Debug simulation 
Total elapsed time: 8.320193717256188. Arrivals time: 0.30711694713681936 Scheduler time: 7.886385947931558 Scheduler overhead time: 0.04630317213013768 Adapter cache time: 0.01261807419359684 Engine time: 0.046706202905625105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.137105925008655,
    "estimated_duration": 3600.02356885991,
    "input_throughput": 7732.975206275188,
    "output_throughput": 6851.369033623071,
    "total_throughput": 14584.344239898259,
    "itl": 125.93560554470696,
    "ttft": 1488842.8048355323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 739,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2617004427663474,
    "arrivals": 257455,
    "finished_requests": 112285,
    "scheduler_time": 150.8838025620837
}
#Debug simulation 
Total elapsed time: 8.137210271321237. Arrivals time: 0.3185406303964555 Scheduler time: 7.692566303536296 Scheduler overhead time: 0.04561261925846338 Adapter cache time: 0.013003846630454063 Engine time: 0.04653639905154705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.18048857524991,
    "estimated_duration": 3600.0727263923072,
    "input_throughput": 7732.607954255655,
    "output_throughput": 6851.019097249286,
    "total_throughput": 14583.627051504942,
    "itl": 125.94091461887655,
    "ttft": 1488904.0797010586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 739,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4172135897004132,
    "arrivals": 257455,
    "finished_requests": 112282,
    "scheduler_time": 150.8796712454051
}
#Debug simulation 
Total elapsed time: 8.180610151030123. Arrivals time: 0.3073193719610572 Scheduler time: 7.7471541771665215 Scheduler overhead time: 0.04555209260433912 Adapter cache time: 0.013078242540359497 Engine time: 0.0465764906257391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.186107771005481,
    "estimated_duration": 3600.0762528216046,
    "input_throughput": 7732.600379834083,
    "output_throughput": 6851.012386381858,
    "total_throughput": 14583.612766215942,
    "itl": 125.94101954278474,
    "ttft": 1488906.031172754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 739,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4204821564629624,
    "arrivals": 257455,
    "finished_requests": 112282,
    "scheduler_time": 150.8797135143275
}
#Debug simulation 
Total elapsed time: 8.186200119089335. Arrivals time: 0.31722167041152716 Scheduler time: 7.7429847763851285 Scheduler overhead time: 0.045511951204389334 Adapter cache time: 0.013034012634307146 Engine time: 0.04660883452743292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.181144061964005,
    "estimated_duration": 3600.043239689698,
    "input_throughput": 7732.932952882961,
    "output_throughput": 6851.331597374364,
    "total_throughput": 14584.264550257325,
    "itl": 125.93614046789345,
    "ttft": 1488850.7384996177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 739,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.279108372067541,
    "arrivals": 257455,
    "finished_requests": 112285,
    "scheduler_time": 150.88396186059526
}
#Debug simulation 
Total elapsed time: 8.181234066840261. Arrivals time: 0.31011324701830745 Scheduler time: 7.744736546650529 Scheduler overhead time: 0.045638168696314096 Adapter cache time: 0.013110684230923653 Engine time: 0.046650626230984926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.196802013088018,
    "estimated_duration": 3600.121880348607,
    "input_throughput": 7732.502377754054,
    "output_throughput": 6850.925557445771,
    "total_throughput": 14583.427935199825,
    "itl": 125.94249853055634,
    "ttft": 1488924.5011723929,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 739,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.462986936271197,
    "arrivals": 257455,
    "finished_requests": 112282,
    "scheduler_time": 150.8799703891113
}
#Debug simulation 
Total elapsed time: 8.196892539039254. Arrivals time: 0.3231085021980107 Scheduler time: 7.746996177826077 Scheduler overhead time: 0.04572929907590151 Adapter cache time: 0.013157600071281195 Engine time: 0.04676778521388769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.191806348972023,
    "estimated_duration": 3600.1065489696152,
    "input_throughput": 7732.986682843582,
    "output_throughput": 6851.317499772193,
    "total_throughput": 14584.304182615775,
    "itl": 125.93383071883771,
    "ttft": 1488844.4924278765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 738,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2066571176377643,
    "arrivals": 257455,
    "finished_requests": 112287,
    "scheduler_time": 150.88956109811255
}
#Debug simulation 
Total elapsed time: 8.19191082008183. Arrivals time: 0.30907141510397196 Scheduler time: 7.757045658305287 Scheduler overhead time: 0.04542198311537504 Adapter cache time: 0.012975305318832397 Engine time: 0.04656135989353061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.184555699117482,
    "estimated_duration": 3600.0060262579723,
    "input_throughput": 7732.267056490002,
    "output_throughput": 6850.887142996316,
    "total_throughput": 14583.154199486318,
    "itl": 125.94323545166064,
    "ttft": 1488899.0724056787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 739,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.484365079961729,
    "arrivals": 257455,
    "finished_requests": 112277,
    "scheduler_time": 150.87402904853786
}
#Debug simulation 
Total elapsed time: 8.184670289047062. Arrivals time: 0.31707796221598983 Scheduler time: 7.74184634629637 Scheduler overhead time: 0.04537643399089575 Adapter cache time: 0.013007145840674639 Engine time: 0.046552108600735664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.125543031841516,
    "estimated_duration": 3600.0144423445317,
    "input_throughput": 7701.3021597613515,
    "output_throughput": 6853.394728031146,
    "total_throughput": 14554.696887792497,
    "itl": 126.2311384931538,
    "ttft": 1487956.4312585422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 744,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2770028814860117,
    "arrivals": 257175,
    "finished_requests": 111996,
    "scheduler_time": 150.80684242410283
}
#Debug simulation 
Total elapsed time: 8.125654958654195. Arrivals time: 0.33146931091323495 Scheduler time: 7.668709673918784 Scheduler overhead time: 0.045319534838199615 Adapter cache time: 0.012950361706316471 Engine time: 0.04635390779003501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.178914099931717,
    "estimated_duration": 3600.0924823074524,
    "input_throughput": 7701.070774223596,
    "output_throughput": 6853.231443706272,
    "total_throughput": 14554.302217929868,
    "itl": 126.23752319575834,
    "ttft": 1488018.9712515892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 749,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4516086411522773,
    "arrivals": 257175,
    "finished_requests": 111995,
    "scheduler_time": 150.80340930207254
}
#Debug simulation 
Total elapsed time: 8.179006584919989. Arrivals time: 0.31751173781231046 Scheduler time: 7.735762741882354 Scheduler overhead time: 0.045469971373677254 Adapter cache time: 0.013055313378572464 Engine time: 0.046416649129241705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.153528651222587,
    "estimated_duration": 3600.095953304894,
    "input_throughput": 7701.063349311232,
    "output_throughput": 6853.22483622994,
    "total_throughput": 14554.288185541172,
    "itl": 126.23761032806671,
    "ttft": 1488020.6483509499,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 749,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.454627067390823,
    "arrivals": 257175,
    "finished_requests": 111995,
    "scheduler_time": 150.80344946911578
}
#Debug simulation 
Total elapsed time: 8.153618708252907. Arrivals time: 0.30503234593197703 Scheduler time: 7.723274589050561 Scheduler overhead time: 0.04535538284108043 Adapter cache time: 0.012974797748029232 Engine time: 0.04622181924059987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.185746501199901,
    "estimated_duration": 3600.0333708935686,
    "input_throughput": 7701.261667226822,
    "output_throughput": 6853.358693693457,
    "total_throughput": 14554.620360920278,
    "itl": 126.23145285025156,
    "ttft": 1487959.9958654093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 744,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2891554797207663,
    "arrivals": 257175,
    "finished_requests": 111996,
    "scheduler_time": 150.8070876693548
}
#Debug simulation 
Total elapsed time: 8.185837497934699. Arrivals time: 0.31926826061680913 Scheduler time: 7.7408048724755645 Scheduler overhead time: 0.045455632731318474 Adapter cache time: 0.013056749012321234 Engine time: 0.046462742146104574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.145355272106826,
    "estimated_duration": 3600.0054770265133,
    "input_throughput": 7700.968839330413,
    "output_throughput": 6853.0467960225005,
    "total_throughput": 14554.015635352915,
    "itl": 126.23851309544753,
    "ttft": 1488062.7524292981,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 749,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5000241842866004,
    "arrivals": 257175,
    "finished_requests": 111989,
    "scheduler_time": 150.7976448618879
}
#Debug simulation 
Total elapsed time: 8.14544489281252. Arrivals time: 0.31965621188282967 Scheduler time: 7.7007423234172165 Scheduler overhead time: 0.04520308878272772 Adapter cache time: 0.012914169114083052 Engine time: 0.046157036907970905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.218371362891048,
    "estimated_duration": 3600.1006871611926,
    "input_throughput": 7701.449878576294,
    "output_throughput": 6853.9694147990895,
    "total_throughput": 14555.419293375384,
    "itl": 126.23068007172422,
    "ttft": 1487922.7694486827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 744,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2245974194071767,
    "arrivals": 257175,
    "finished_requests": 112005,
    "scheduler_time": 150.81272560248524
}
#Debug simulation 
Total elapsed time: 8.218473076820374. Arrivals time: 0.34750358667224646 Scheduler time: 7.745097992476076 Scheduler overhead time: 0.04541992023587227 Adapter cache time: 0.012999146245419979 Engine time: 0.04649875359609723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.100550925824791,
    "estimated_duration": 3600.024635270438,
    "input_throughput": 7700.927857099893,
    "output_throughput": 6853.010326177028,
    "total_throughput": 14553.938183276921,
    "itl": 126.23911922118374,
    "ttft": 1488069.3211229064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 749,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5198932825401554,
    "arrivals": 257175,
    "finished_requests": 111989,
    "scheduler_time": 150.79772370258422
}
#Debug simulation 
Total elapsed time: 8.10066699795425. Arrivals time: 0.32928888965398073 Scheduler time: 7.64617427252233 Scheduler overhead time: 0.04509469075128436 Adapter cache time: 0.012970440089702606 Engine time: 0.0463387556374073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.156807884573936,
    "estimated_duration": 3600.1202559279272,
    "input_throughput": 7816.026965673483,
    "output_throughput": 6903.834381386169,
    "total_throughput": 14719.861347059652,
    "itl": 124.79373691853424,
    "ttft": 1473745.2663314743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8034067734424637,
    "arrivals": 256011,
    "finished_requests": 113169,
    "scheduler_time": 151.95984604352952
}
#Debug simulation 
Total elapsed time: 8.156949944794178. Arrivals time: 0.3167167226783931 Scheduler time: 7.7138742776587605 Scheduler overhead time: 0.04501621471717954 Adapter cache time: 0.014276368543505669 Engine time: 0.046349541284143925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.11845690291375,
    "estimated_duration": 3600.0454292967665,
    "input_throughput": 7815.36859813906,
    "output_throughput": 6903.567882157204,
    "total_throughput": 14718.936480296263,
    "itl": 124.79956780537614,
    "ttft": 1473812.231036939,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9967097160662606,
    "arrivals": 256011,
    "finished_requests": 113161,
    "scheduler_time": 151.94895580667503
}
#Debug simulation 
Total elapsed time: 8.118545437697321. Arrivals time: 0.30624528089538217 Scheduler time: 7.685991292819381 Scheduler overhead time: 0.0451214867644012 Adapter cache time: 0.014226282481104136 Engine time: 0.04627687577158213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.132823787163943,
    "estimated_duration": 3600.05102854938,
    "input_throughput": 7815.356442693845,
    "output_throughput": 6903.55714485926,
    "total_throughput": 14718.913587553105,
    "itl": 124.79979948901679,
    "ttft": 1473814.0889157334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.000666434522691,
    "arrivals": 256011,
    "finished_requests": 113161,
    "scheduler_time": 151.94897701337266
}
#Debug simulation 
Total elapsed time: 8.132918541319668. Arrivals time: 0.3146471311338246 Scheduler time: 7.692141785286367 Scheduler overhead time: 0.0449067335575819 Adapter cache time: 0.01424988592043519 Engine time: 0.0462529631331563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.130765788257122,
    "estimated_duration": 3600.027644523169,
    "input_throughput": 7815.75414922315,
    "output_throughput": 6903.876429341137,
    "total_throughput": 14719.630578564287,
    "itl": 124.79509558351671,
    "ttft": 1473732.1633454734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8443036622879685,
    "arrivals": 256011,
    "finished_requests": 113165,
    "scheduler_time": 151.954166617127
}
#Debug simulation 
Total elapsed time: 8.130851634312421. Arrivals time: 0.30704124830663204 Scheduler time: 7.697373910341412 Scheduler overhead time: 0.0449549718759954 Adapter cache time: 0.014262745156884193 Engine time: 0.046411935705691576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.172013880684972,
    "estimated_duration": 3600.098354700427,
    "input_throughput": 7815.253703628671,
    "output_throughput": 6903.466392119749,
    "total_throughput": 14718.72009574842,
    "itl": 124.80139022570111,
    "ttft": 1473833.7055102738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0475725968554674,
    "arrivals": 256011,
    "finished_requests": 113161,
    "scheduler_time": 151.94915336287988
}
#Debug simulation 
Total elapsed time: 8.17210335796699. Arrivals time: 0.30923628248274326 Scheduler time: 7.73671638360247 Scheduler overhead time: 0.044817954301834106 Adapter cache time: 0.014168170280754566 Engine time: 0.0463066678494215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.201873867306858,
    "estimated_duration": 3600.036695709984,
    "input_throughput": 7816.208382967779,
    "output_throughput": 6903.994625837633,
    "total_throughput": 14720.203008805413,
    "itl": 124.79182700840417,
    "ttft": 1473714.6717538026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7388860701303326,
    "arrivals": 256011,
    "finished_requests": 113169,
    "scheduler_time": 151.9582749631453
}
#Debug simulation 
Total elapsed time: 8.201976413372904. Arrivals time: 0.32952554523944855 Scheduler time: 7.746007352601737 Scheduler overhead time: 0.04508127877488732 Adapter cache time: 0.014238507021218538 Engine time: 0.04633918311446905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.13773331232369,
    "estimated_duration": 3600.137857627169,
    "input_throughput": 7815.167949858474,
    "output_throughput": 6903.390643040702,
    "total_throughput": 14718.558592899177,
    "itl": 124.80173282106914,
    "ttft": 1473840.0077530858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.080017073750528,
    "arrivals": 256011,
    "finished_requests": 113161,
    "scheduler_time": 151.9497632299393
}
#Debug simulation 
Total elapsed time: 8.137846416328102. Arrivals time: 0.2990634338930249 Scheduler time: 7.712222612462938 Scheduler overhead time: 0.045054920483380556 Adapter cache time: 0.014236778486520052 Engine time: 0.04645002447068691 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.18333769729361,
    "estimated_duration": 3600.10755554243,
    "input_throughput": 7838.492479635977,
    "output_throughput": 6970.968120484039,
    "total_throughput": 14809.460600120017,
    "itl": 124.10379364362157,
    "ttft": 1461772.953645007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2127326388634216,
    "arrivals": 255515,
    "finished_requests": 114135,
    "scheduler_time": 153.2027285712203
}
#Debug simulation 
Total elapsed time: 8.183448001276702. Arrivals time: 0.3084055199287832 Scheduler time: 7.750101742800325 Scheduler overhead time: 0.044722958002239466 Adapter cache time: 0.013139310292899609 Engine time: 0.04636413464322686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.378604452125728,
    "estimated_duration": 3600.1245385878356,
    "input_throughput": 7838.105514835521,
    "output_throughput": 6970.475529672498,
    "total_throughput": 14808.581044508019,
    "itl": 124.10844667603523,
    "ttft": 1461845.7134746986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.363243855205371,
    "arrivals": 255515,
    "finished_requests": 114129,
    "scheduler_time": 153.19760046420745
}
#Debug simulation 
Total elapsed time: 8.378672882914543. Arrivals time: 0.3026166879571974 Scheduler time: 7.951025002636015 Scheduler overhead time: 0.04473027866333723 Adapter cache time: 0.013183702249079943 Engine time: 0.046132474672049284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.19631711114198,
    "estimated_duration": 3600.024111278183,
    "input_throughput": 7838.198891946128,
    "output_throughput": 6970.484703528956,
    "total_throughput": 14808.683595475084,
    "itl": 124.1099495888706,
    "ttft": 1461818.5039765835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3667271424643688,
    "arrivals": 255515,
    "finished_requests": 114125,
    "scheduler_time": 153.19217297320708
}
#Debug simulation 
Total elapsed time: 8.196410671342164. Arrivals time: 0.315273093059659 Scheduler time: 7.756255621556193 Scheduler overhead time: 0.04458017088472843 Adapter cache time: 0.013108997605741024 Engine time: 0.04649021988734603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.20932124229148,
    "estimated_duration": 3600.123787478837,
    "input_throughput": 7838.457138098029,
    "output_throughput": 6970.936690367213,
    "total_throughput": 14809.393828465241,
    "itl": 124.1041348303339,
    "ttft": 1461780.5796465783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.233310543941309,
    "arrivals": 255515,
    "finished_requests": 114135,
    "scheduler_time": 153.20303719987427
}
#Debug simulation 
Total elapsed time: 8.209410325158387. Arrivals time: 0.32492034137248993 Scheduler time: 7.7594212125986814 Scheduler overhead time: 0.044685674365609884 Adapter cache time: 0.013194283936172724 Engine time: 0.04637313913553953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.248474622145295,
    "estimated_duration": 3600.057614698793,
    "input_throughput": 7838.125946870687,
    "output_throughput": 6970.419833711339,
    "total_throughput": 14808.545780582026,
    "itl": 124.1111086094743,
    "ttft": 1461831.150656397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4067168465443,
    "arrivals": 255515,
    "finished_requests": 114125,
    "scheduler_time": 153.19206100301972
}
#Debug simulation 
Total elapsed time: 8.248566548340023. Arrivals time: 0.34542515128850937 Scheduler time: 7.778131993487477 Scheduler overhead time: 0.04481705371290445 Adapter cache time: 0.013152219820767641 Engine time: 0.04629286751151085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.250564114190638,
    "estimated_duration": 3600.04807344667,
    "input_throughput": 7838.621991784365,
    "output_throughput": 6971.083298888555,
    "total_throughput": 14809.705290672919,
    "itl": 124.10201297589695,
    "ttft": 1461749.821648783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1618063632142333,
    "arrivals": 255515,
    "finished_requests": 114135,
    "scheduler_time": 153.20245478498666
}
#Debug simulation 
Total elapsed time: 8.250654363073409. Arrivals time: 0.3280441425740719 Scheduler time: 7.79715362098068 Scheduler overhead time: 0.044829089660197496 Adapter cache time: 0.013182223308831453 Engine time: 0.04663910297676921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.249984151683748,
    "estimated_duration": 3600.0882869817096,
    "input_throughput": 7838.059167059355,
    "output_throughput": 6970.360446642982,
    "total_throughput": 14808.419613702337,
    "itl": 124.11203625966773,
    "ttft": 1461845.8668696806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 723,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.428723759166904,
    "arrivals": 255515,
    "finished_requests": 114125,
    "scheduler_time": 153.19248983430757
}
#Debug simulation 
Total elapsed time: 8.250122318975627. Arrivals time: 0.33129091933369637 Scheduler time: 7.793515203520656 Scheduler overhead time: 0.044711433816701174 Adapter cache time: 0.013227635528892279 Engine time: 0.04666575137525797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.248083096928895,
    "estimated_duration": 3600.0354415890292,
    "input_throughput": 7892.071192348948,
    "output_throughput": 7007.431845966768,
    "total_throughput": 14899.503038315715,
    "itl": 123.28612312878514,
    "ttft": 1452438.9746961556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.83629264635968,
    "arrivals": 255262,
    "finished_requests": 114718,
    "scheduler_time": 153.8982365900167
}
#Debug simulation 
Total elapsed time: 8.24819917185232. Arrivals time: 0.3139815288595855 Scheduler time: 7.809511595405638 Scheduler overhead time: 0.04490923788398504 Adapter cache time: 0.012474329676479101 Engine time: 0.04648440610617399 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.24536960804835,
    "estimated_duration": 3600.043126765353,
    "input_throughput": 7891.8499027891785,
    "output_throughput": 7007.317165854676,
    "total_throughput": 14899.167068643854,
    "itl": 123.2897223038625,
    "ttft": 1452421.517676882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9586940902727719,
    "arrivals": 255262,
    "finished_requests": 114715,
    "scheduler_time": 153.89325631050897
}
#Debug simulation 
Total elapsed time: 8.24548582173884. Arrivals time: 0.3112277779728174 Scheduler time: 7.809177606832236 Scheduler overhead time: 0.04519047122448683 Adapter cache time: 0.012443591374903917 Engine time: 0.04647078085690737 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.24143082369119,
    "estimated_duration": 3600.045715031803,
    "input_throughput": 7891.844228913914,
    "output_throughput": 7007.312127917561,
    "total_throughput": 14899.156356831476,
    "itl": 123.28982551244093,
    "ttft": 1452422.448375966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9620220495574299,
    "arrivals": 255262,
    "finished_requests": 114715,
    "scheduler_time": 153.89326180363247
}
#Debug simulation 
Total elapsed time: 8.24152335897088. Arrivals time: 0.3268680605106056 Scheduler time: 7.79027563566342 Scheduler overhead time: 0.044680500868707895 Adapter cache time: 0.012430769857019186 Engine time: 0.04650875926017761 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.221482158172876,
    "estimated_duration": 3600.067226803822,
    "input_throughput": 7892.001512767371,
    "output_throughput": 7007.369976920349,
    "total_throughput": 14899.371489687719,
    "itl": 123.28576880838133,
    "ttft": 1452447.0428352691,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8430616151541288,
    "arrivals": 255262,
    "finished_requests": 114718,
    "scheduler_time": 153.89890138819047
}
#Debug simulation 
Total elapsed time: 8.221571697853506. Arrivals time: 0.31116332672536373 Scheduler time: 7.785374261904508 Scheduler overhead time: 0.04495987668633461 Adapter cache time: 0.012418004218488932 Engine time: 0.0468444568105042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.26251029688865,
    "estimated_duration": 3600.085854912419,
    "input_throughput": 7891.7562372109505,
    "output_throughput": 7007.233998482989,
    "total_throughput": 14898.99023569394,
    "itl": 123.29095669579922,
    "ttft": 1452436.510443641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9976103711128228,
    "arrivals": 255262,
    "finished_requests": 114715,
    "scheduler_time": 153.89344042053892
}
#Debug simulation 
Total elapsed time: 8.262596583925188. Arrivals time: 0.3261136496439576 Scheduler time: 7.8117105648852885 Scheduler overhead time: 0.04504854138940573 Adapter cache time: 0.012452403083443642 Engine time: 0.04641158366575837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.26612968975678,
    "estimated_duration": 3600.1202964754884,
    "input_throughput": 7892.517377215775,
    "output_throughput": 7007.61472462418,
    "total_throughput": 14900.132101839954,
    "itl": 123.28554564504701,
    "ttft": 1452389.800073797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 601,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7970202272361806,
    "arrivals": 255262,
    "finished_requests": 114725,
    "scheduler_time": 153.9039764691239
}
#Debug simulation 
Total elapsed time: 8.26622186275199. Arrivals time: 0.3190492386929691 Scheduler time: 7.821424518711865 Scheduler overhead time: 0.04499154072254896 Adapter cache time: 0.012444605585187674 Engine time: 0.04745207307860255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.233897703234106,
    "estimated_duration": 3600.103580219663,
    "input_throughput": 7891.71738171669,
    "output_throughput": 7007.199497982438,
    "total_throughput": 14898.916879699127,
    "itl": 123.29148640297726,
    "ttft": 1452444.58474657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0127008254826015,
    "arrivals": 255262,
    "finished_requests": 114715,
    "scheduler_time": 153.89367877824918
}
#Debug simulation 
Total elapsed time: 8.234010220970958. Arrivals time: 0.3226431831717491 Scheduler time: 7.7868665577843785 Scheduler overhead time: 0.04472510423511267 Adapter cache time: 0.012488191481679678 Engine time: 0.04651712579652667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.281464635860175,
    "estimated_duration": 3600.060997044155,
    "input_throughput": 7975.7393065214865,
    "output_throughput": 7091.102073259369,
    "total_throughput": 15066.841379780855,
    "itl": 121.96601266296358,
    "ttft": 1443063.3439064303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5792116758693198,
    "arrivals": 254534,
    "finished_requests": 116011,
    "scheduler_time": 155.77402853991322
}
#Debug simulation 
Total elapsed time: 8.281555006746203. Arrivals time: 0.31348928436636925 Scheduler time: 7.843273140490055 Scheduler overhead time: 0.04512013355270028 Adapter cache time: 0.011823156848549843 Engine time: 0.04694285709410906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.368744092993438,
    "estimated_duration": 3600.106285284697,
    "input_throughput": 7975.447868681303,
    "output_throughput": 7090.821763886136,
    "total_throughput": 15066.269632567439,
    "itl": 121.97057120896687,
    "ttft": 1443091.3287593331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6875086948974125,
    "arrivals": 254534,
    "finished_requests": 116010,
    "scheduler_time": 155.77077162906178
}
#Debug simulation 
Total elapsed time: 8.368844856973737. Arrivals time: 0.32313161762431264 Scheduler time: 7.920539505779743 Scheduler overhead time: 0.044917435850948095 Adapter cache time: 0.01182724442332983 Engine time: 0.047494460828602314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.314297956880182,
    "estimated_duration": 3600.108632736782,
    "input_throughput": 7975.442668287748,
    "output_throughput": 7090.817140313341,
    "total_throughput": 15066.25980860109,
    "itl": 121.97064335877346,
    "ttft": 1443092.3890288402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.689841339029382,
    "arrivals": 254534,
    "finished_requests": 116010,
    "scheduler_time": 155.77078643699014
}
#Debug simulation 
Total elapsed time: 8.314385000616312. Arrivals time: 0.31211486645042896 Scheduler time: 7.877430101856589 Scheduler overhead time: 0.04512983048334718 Adapter cache time: 0.011767845135182142 Engine time: 0.04704653983935714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.348333242349327,
    "estimated_duration": 3600.0204110749755,
    "input_throughput": 7975.346726277781,
    "output_throughput": 7090.763130528364,
    "total_throughput": 15066.109856806146,
    "itl": 121.96737286770973,
    "ttft": 1443099.1736110642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 517,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.605510918956707,
    "arrivals": 254534,
    "finished_requests": 116007,
    "scheduler_time": 155.77001890179884
}
#Debug simulation 
Total elapsed time: 8.348423625342548. Arrivals time: 0.3263056534342468 Scheduler time: 7.897188010625541 Scheduler overhead time: 0.04499053955078125 Adapter cache time: 0.011886204592883587 Engine time: 0.04707574378699064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.353496908210218,
    "estimated_duration": 3600.004591363917,
    "input_throughput": 7975.255939638237,
    "output_throughput": 7090.709845547408,
    "total_throughput": 15065.965785185645,
    "itl": 121.97103908378342,
    "ttft": 1443135.9456199768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7159981266036655,
    "arrivals": 254534,
    "finished_requests": 116006,
    "scheduler_time": 155.76493805789872
}
#Debug simulation 
Total elapsed time: 8.353600313421339. Arrivals time: 0.32281389413401484 Scheduler time: 7.905736467335373 Scheduler overhead time: 0.04520273022353649 Adapter cache time: 0.01175245177000761 Engine time: 0.04712170036509633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.358336249366403,
    "estimated_duration": 3600.0257640614313,
    "input_throughput": 7975.817364042075,
    "output_throughput": 7091.171472950708,
    "total_throughput": 15066.988836992783,
    "itl": 121.96499229744785,
    "ttft": 1443050.3890638114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5428659521695047,
    "arrivals": 254534,
    "finished_requests": 116011,
    "scheduler_time": 155.7737837208995
}
#Debug simulation 
Total elapsed time: 8.358461830299348. Arrivals time: 0.3233245350420475 Scheduler time: 7.910275094676763 Scheduler overhead time: 0.04505477426573634 Adapter cache time: 0.011906624305993319 Engine time: 0.04691507155075669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.359080257359892,
    "estimated_duration": 3600.027916630588,
    "input_throughput": 7975.2042664357305,
    "output_throughput": 7090.663903487551,
    "total_throughput": 15065.86816992328,
    "itl": 121.97178940063554,
    "ttft": 1443147.0850000533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7343581794202303,
    "arrivals": 254534,
    "finished_requests": 116006,
    "scheduler_time": 155.76509110047138
}
#Debug simulation 
Total elapsed time: 8.359168143942952. Arrivals time: 0.3176122843287885 Scheduler time: 7.916431789752096 Scheduler overhead time: 0.04516759188845754 Adapter cache time: 0.01181544503197074 Engine time: 0.04702368751168251 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.403927007690072,
    "estimated_duration": 3600.113888116718,
    "input_throughput": 8064.036278359753,
    "output_throughput": 7165.2594339159095,
    "total_throughput": 15229.295712275663,
    "itl": 120.66049642644164,
    "ttft": 1432715.9485349488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1109570510475921,
    "arrivals": 254308,
    "finished_requests": 117266,
    "scheduler_time": 157.3044058885744
}
#Debug simulation 
Total elapsed time: 8.40402055485174. Arrivals time: 0.3288478716276586 Scheduler time: 7.9502790328115225 Scheduler overhead time: 0.04560556402429938 Adapter cache time: 0.010810447856783867 Engine time: 0.047321769408881664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.409248256124556,
    "estimated_duration": 3600.0618213614375,
    "input_throughput": 8063.66624810399,
    "output_throughput": 7164.985291904049,
    "total_throughput": 15228.651540008039,
    "itl": 120.66284816956987,
    "ttft": 1432728.4440519516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1842683241120597,
    "arrivals": 254308,
    "finished_requests": 117261,
    "scheduler_time": 157.29891769363772
}
#Debug simulation 
Total elapsed time: 8.409338505007327. Arrivals time: 0.31661388324573636 Scheduler time: 7.967764258850366 Scheduler overhead time: 0.0456914440728724 Adapter cache time: 0.01075183181092143 Engine time: 0.047407083213329315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.405373414047062,
    "estimated_duration": 3600.0640834723695,
    "input_throughput": 8063.6611812754145,
    "output_throughput": 7164.980789764314,
    "total_throughput": 15228.64197103973,
    "itl": 120.66291459062901,
    "ttft": 1432729.4812999398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1864112357795304,
    "arrivals": 254308,
    "finished_requests": 117261,
    "scheduler_time": 157.29893440434674
}
#Debug simulation 
Total elapsed time: 8.405483895447105. Arrivals time: 0.33266657032072544 Scheduler time: 7.947818353306502 Scheduler overhead time: 0.0457405224442482 Adapter cache time: 0.010838657151907682 Engine time: 0.04717347305268049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.38135610613972,
    "estimated_duration": 3600.128454673329,
    "input_throughput": 8064.003650290382,
    "output_throughput": 7165.230442406721,
    "total_throughput": 15229.234092697103,
    "itl": 120.66081576191345,
    "ttft": 1432721.6592896201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1205274544353592,
    "arrivals": 254308,
    "finished_requests": 117266,
    "scheduler_time": 157.30467136042324
}
#Debug simulation 
Total elapsed time: 8.381440300028771. Arrivals time: 0.31297947419807315 Scheduler time: 7.943680393509567 Scheduler overhead time: 0.04557507997378707 Adapter cache time: 0.010809413157403469 Engine time: 0.047130406368523836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.334976120851934,
    "estimated_duration": 3600.083050576484,
    "input_throughput": 8063.618697727391,
    "output_throughput": 7164.943040930549,
    "total_throughput": 15228.56173865794,
    "itl": 120.66320088795665,
    "ttft": 1432737.5066036966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2060288264602466,
    "arrivals": 254308,
    "finished_requests": 117261,
    "scheduler_time": 157.29900747252418
}
#Debug simulation 
Total elapsed time: 8.335071614943445. Arrivals time: 0.3258079709485173 Scheduler time: 7.885129095520824 Scheduler overhead time: 0.0452598100528121 Adapter cache time: 0.010773409623652697 Engine time: 0.04708217643201351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.383787984028459,
    "estimated_duration": 3600.0901066940423,
    "input_throughput": 8064.0895476528885,
    "output_throughput": 7165.306766082086,
    "total_throughput": 15229.396313734975,
    "itl": 120.65975509814277,
    "ttft": 1432705.066132757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0853882570494882,
    "arrivals": 254308,
    "finished_requests": 117266,
    "scheduler_time": 157.3042905402821
}
#Debug simulation 
Total elapsed time: 8.38387686619535. Arrivals time: 0.31677537504583597 Scheduler time: 7.941526602488011 Scheduler overhead time: 0.045661237090826035 Adapter cache time: 0.010843675583600998 Engine time: 0.04784698644652963 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.39857308799401,
    "estimated_duration": 3600.015495592207,
    "input_throughput": 8063.6455691768215,
    "output_throughput": 7165.076381360628,
    "total_throughput": 15228.72195053745,
    "itl": 120.66551540566016,
    "ttft": 1432689.8765584514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.216843652091924,
    "arrivals": 254308,
    "finished_requests": 117259,
    "scheduler_time": 157.29521228789173
}
#Debug simulation 
Total elapsed time: 8.398674672935158. Arrivals time: 0.32834534626454115 Scheduler time: 7.944859517738223 Scheduler overhead time: 0.04565627221018076 Adapter cache time: 0.010921190027147532 Engine time: 0.04746552975848317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.453497082926333,
    "estimated_duration": 3600.0566219697703,
    "input_throughput": 8140.527240920181,
    "output_throughput": 7225.379967987187,
    "total_throughput": 15365.907208907367,
    "itl": 119.59063386062256,
    "ttft": 1418699.8140320606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8140897398861114,
    "arrivals": 253785,
    "finished_requests": 118238,
    "scheduler_time": 158.5928823398336
}
#Debug simulation 
Total elapsed time: 8.453614538069814. Arrivals time: 0.336383449845016 Scheduler time: 7.991984186228365 Scheduler overhead time: 0.045888838823884726 Adapter cache time: 0.010061385110020638 Engine time: 0.04800617741420865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.499440143816173,
    "estimated_duration": 3600.010208546074,
    "input_throughput": 8140.61191560783,
    "output_throughput": 7225.289233417212,
    "total_throughput": 15365.901149025043,
    "itl": 119.59206051287994,
    "ttft": 1418712.2787679296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8676747498079246,
    "arrivals": 253785,
    "finished_requests": 118236,
    "scheduler_time": 158.58873547047645
}
#Debug simulation 
Total elapsed time: 8.499559719115496. Arrivals time: 0.3289041481912136 Scheduler time: 8.045724491588771 Scheduler overhead time: 0.0456743435934186 Adapter cache time: 0.010065491311252117 Engine time: 0.047941709868609905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.468922079075128,
    "estimated_duration": 3600.011420941028,
    "input_throughput": 8140.609174050747,
    "output_throughput": 7225.286800118207,
    "total_throughput": 15365.895974168954,
    "itl": 119.59198555508091,
    "ttft": 1418713.2104713575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8692688203044279,
    "arrivals": 253785,
    "finished_requests": 118236,
    "scheduler_time": 158.58879676556347
}
#Debug simulation 
Total elapsed time: 8.469036484137177. Arrivals time: 0.3161674467846751 Scheduler time: 8.027820288669318 Scheduler overhead time: 0.045862738974392414 Adapter cache time: 0.01006270619109273 Engine time: 0.047761695459485054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.495616623200476,
    "estimated_duration": 3600.0762425009943,
    "input_throughput": 8140.482874785091,
    "output_throughput": 7225.340589434146,
    "total_throughput": 15365.823464219236,
    "itl": 119.59091914611258,
    "ttft": 1418722.84546957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8280410039192088,
    "arrivals": 253785,
    "finished_requests": 118238,
    "scheduler_time": 158.5931288786494
}
#Debug simulation 
Total elapsed time: 8.49573193024844. Arrivals time: 0.32865099469199777 Scheduler time: 8.041990287601948 Scheduler overhead time: 0.04584689252078533 Adapter cache time: 0.01007138891145587 Engine time: 0.04782340256497264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.44039614778012,
    "estimated_duration": 3600.0257007080254,
    "input_throughput": 8140.576883725098,
    "output_throughput": 7225.258140486146,
    "total_throughput": 15365.835024211243,
    "itl": 119.5923418372966,
    "ttft": 1418719.8147469417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8814669375866695,
    "arrivals": 253785,
    "finished_requests": 118236,
    "scheduler_time": 158.58897687421756
}
#Debug simulation 
Total elapsed time: 8.440534455701709. Arrivals time: 0.3173674470745027 Scheduler time: 7.9979488593526185 Scheduler overhead time: 0.04605772718787193 Adapter cache time: 0.01002077478915453 Engine time: 0.04776375414803624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.45674654096365,
    "estimated_duration": 3600.0137181224713,
    "input_throughput": 8140.623979423127,
    "output_throughput": 7225.41080025826,
    "total_throughput": 15366.034779681388,
    "itl": 119.58936616012785,
    "ttft": 1418701.1809653833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7953533784439798,
    "arrivals": 253785,
    "finished_requests": 118237,
    "scheduler_time": 158.59204636409228
}
#Debug simulation 
Total elapsed time: 8.456835998222232. Arrivals time: 0.35258904146030545 Scheduler time: 7.978297028224915 Scheduler overhead time: 0.04645920777693391 Adapter cache time: 0.010090739466249943 Engine time: 0.04814196331426501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_64_slots_32_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.480217266827822,
    "estimated_duration": 3600.0368879089474,
    "input_throughput": 8140.551586687302,
    "output_throughput": 7225.235687823285,
    "total_throughput": 15365.787274510587,
    "itl": 119.59274861883839,
    "ttft": 1418720.742562093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8915272404998584,
    "arrivals": 253785,
    "finished_requests": 118236,
    "scheduler_time": 158.5895369072208
}
#Debug simulation 
Total elapsed time: 8.480309202801436. Arrivals time: 0.31999650644138455 Scheduler time: 8.033360181376338 Scheduler overhead time: 0.04583360394462943 Adapter cache time: 0.010139557532966137 Engine time: 0.04949982604011893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 85.40892894612625,
    "estimated_duration": 3600.067211053581,
    "input_throughput": 7729.825130641381,
    "output_throughput": 6814.562218359336,
    "total_throughput": 14544.387349000717,
    "itl": 124.34177614691184,
    "ttft": 1366600.709279927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3152302376250733,
    "arrivals": 216813,
    "finished_requests": 112240,
    "scheduler_time": 149.26022329113732
}
#Debug simulation 
Total elapsed time: 85.40907633211464. Arrivals time: 0.4660073844715953 Scheduler time: 84.7802585917525 Scheduler overhead time: 0.06348221190273762 Adapter cache time: 0.011229646392166615 Engine time: 0.06359396409243345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 92.11839588312432,
    "estimated_duration": 3600.0289636258835,
    "input_throughput": 7722.043428228636,
    "output_throughput": 6813.156573967196,
    "total_throughput": 14535.200002195832,
    "itl": 124.3961945943342,
    "ttft": 1356647.2680355697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3161849789856933,
    "arrivals": 216813,
    "finished_requests": 112259,
    "scheduler_time": 149.21879921005046
}
#Debug simulation 
Total elapsed time: 92.11854912387207. Arrivals time: 0.46606774162501097 Scheduler time: 91.48258032277226 Scheduler overhead time: 0.06740599311888218 Adapter cache time: 0.011396960821002722 Engine time: 0.0664423294365406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 90.9298252412118,
    "estimated_duration": 3600.0424398139535,
    "input_throughput": 7726.978907903535,
    "output_throughput": 6814.617719136621,
    "total_throughput": 14541.596627040157,
    "itl": 124.35246562621161,
    "ttft": 1358124.768351756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 98,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3198486868292097,
    "arrivals": 216813,
    "finished_requests": 112283,
    "scheduler_time": 149.24049056102808
}
#Debug simulation 
Total elapsed time: 90.92998162843287. Arrivals time: 0.4600126729346812 Scheduler time: 90.29987247288227 Scheduler overhead time: 0.06699907686561346 Adapter cache time: 0.011455072555691004 Engine time: 0.06623735697939992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 92.12272849911824,
    "estimated_duration": 3600.0141533268047,
    "input_throughput": 7722.075196373927,
    "output_throughput": 6813.184602992148,
    "total_throughput": 14535.259799366075,
    "itl": 124.39641727864534,
    "ttft": 1356639.9371469757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.302701333477162,
    "arrivals": 216813,
    "finished_requests": 112259,
    "scheduler_time": 149.21848008812017
}
#Debug simulation 
Total elapsed time: 92.12287698918954. Arrivals time: 0.46351956902071834 Scheduler time: 91.48703216155991 Scheduler overhead time: 0.0686434330418706 Adapter cache time: 0.011766347102820873 Engine time: 0.06701243855059147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 90.75618160003796,
    "estimated_duration": 3600.0460410979426,
    "input_throughput": 7726.97117826755,
    "output_throughput": 6814.610902175559,
    "total_throughput": 14541.58208044311,
    "itl": 124.35231030541993,
    "ttft": 1358126.2567815746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 98,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32399856178089986,
    "arrivals": 216813,
    "finished_requests": 112283,
    "scheduler_time": 149.24050912852854
}
#Debug simulation 
Total elapsed time: 90.75633577024564. Arrivals time: 0.45139329554513097 Scheduler time: 90.13417032314464 Scheduler overhead time: 0.06780724227428436 Adapter cache time: 0.011640103999525309 Engine time: 0.06633982388302684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 85.67534017702565,
    "estimated_duration": 3600.057081158305,
    "input_throughput": 7729.846880940699,
    "output_throughput": 6814.581393277974,
    "total_throughput": 14544.428274218672,
    "itl": 124.34094398358741,
    "ttft": 1366598.0855378774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30797518037492444,
    "arrivals": 216813,
    "finished_requests": 112240,
    "scheduler_time": 149.25963457553854
}
#Debug simulation 
Total elapsed time: 85.67549399891868. Arrivals time: 0.4444424007087946 Scheduler time: 85.06775662396103 Scheduler overhead time: 0.06420065928250551 Adapter cache time: 0.011374619323760271 Engine time: 0.06314786849543452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 90.90652022603899,
    "estimated_duration": 3600.0506239340357,
    "input_throughput": 7726.961341894093,
    "output_throughput": 6814.602227229547,
    "total_throughput": 14541.563569123638,
    "itl": 124.35227810640932,
    "ttft": 1358128.5920728757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 98,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32789692915976026,
    "arrivals": 216813,
    "finished_requests": 112283,
    "scheduler_time": 149.24062807209125
}
#Debug simulation 
Total elapsed time: 90.90668840985745. Arrivals time: 0.46561728650704026 Scheduler time: 90.27120505599305 Scheduler overhead time: 0.06721159489825368 Adapter cache time: 0.011316121090203524 Engine time: 0.06590242916718125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 125.10416897386312,
    "estimated_duration": 3600.1159221963353,
    "input_throughput": 7704.103034293186,
    "output_throughput": 6838.304524644526,
    "total_throughput": 14542.407558937712,
    "itl": 125.14812917388363,
    "ttft": 1262586.5893402507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22341560530709081,
    "arrivals": 194341,
    "finished_requests": 112058,
    "scheduler_time": 146.80629534164387
}
#Debug simulation 
Total elapsed time: 125.10431415075436. Arrivals time: 0.4866163833066821 Scheduler time: 124.43201788142323 Scheduler overhead time: 0.07412292109802365 Adapter cache time: 0.012576247565448284 Engine time: 0.072511394508183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 122.57200018828735,
    "estimated_duration": 3600.038766071779,
    "input_throughput": 7700.542633392094,
    "output_throughput": 6839.2663523637975,
    "total_throughput": 14539.808985755892,
    "itl": 125.2010192530888,
    "ttft": 1268839.339380347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2387034374498762,
    "arrivals": 194341,
    "finished_requests": 112041,
    "scheduler_time": 146.79091546730825
}
#Debug simulation 
Total elapsed time: 122.57215929124504. Arrivals time: 0.46814494067803025 Scheduler time: 121.92168812640011 Scheduler overhead time: 0.07284282753244042 Adapter cache time: 0.012498390395194292 Engine time: 0.07081118039786816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 122.26438791584224,
    "estimated_duration": 3600.0414704648865,
    "input_throughput": 7700.536848654725,
    "output_throughput": 6839.261214627208,
    "total_throughput": 14539.798063281933,
    "itl": 125.20112796279575,
    "ttft": 1268838.4576486754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23903925068676463,
    "arrivals": 194341,
    "finished_requests": 112041,
    "scheduler_time": 146.7911833701306
}
#Debug simulation 
Total elapsed time: 122.26454690704122. Arrivals time: 0.4714703490026295 Scheduler time: 121.61244238354266 Scheduler overhead time: 0.07188646728172898 Adapter cache time: 0.012347877491265535 Engine time: 0.07100376207381487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 124.54037669394165,
    "estimated_duration": 3600.120817777022,
    "input_throughput": 7704.092557961993,
    "output_throughput": 6838.29522565895,
    "total_throughput": 14542.387783620943,
    "itl": 125.14879138884491,
    "ttft": 1262583.713115901,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22807995917042725,
    "arrivals": 194341,
    "finished_requests": 112058,
    "scheduler_time": 146.806652262179
}
#Debug simulation 
Total elapsed time: 124.54058441659436. Arrivals time: 0.47241772012785077 Scheduler time: 123.88458938663825 Scheduler overhead time: 0.0728124724701047 Adapter cache time: 0.012242989614605904 Engine time: 0.07155360421165824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 122.29945644829422,
    "estimated_duration": 3600.0463246169184,
    "input_throughput": 7700.526465572615,
    "output_throughput": 6839.251992853173,
    "total_throughput": 14539.778458425788,
    "itl": 125.20109614258118,
    "ttft": 1268840.3097831227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24218309534713608,
    "arrivals": 194341,
    "finished_requests": 112041,
    "scheduler_time": 146.79130775711783
}
#Debug simulation 
Total elapsed time: 122.29961115028709. Arrivals time: 0.47052935929968953 Scheduler time: 121.6479413495399 Scheduler overhead time: 0.07195970602333546 Adapter cache time: 0.012116728816181421 Engine time: 0.07149239862337708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 124.33830730011687,
    "estimated_duration": 3600.1112155085784,
    "input_throughput": 7704.113106428534,
    "output_throughput": 6838.3134648583855,
    "total_throughput": 14542.42657128692,
    "itl": 125.14835494415492,
    "ttft": 1262584.4459653157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21827367152785904,
    "arrivals": 194341,
    "finished_requests": 112058,
    "scheduler_time": 146.8061416504347
}
#Debug simulation 
Total elapsed time: 124.33845692500472. Arrivals time: 0.48468856886029243 Scheduler time: 123.67001768946648 Scheduler overhead time: 0.07350019505247474 Adapter cache time: 0.012815079186111689 Engine time: 0.07135093118995428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 122.38540277024731,
    "estimated_duration": 3600.047902584821,
    "input_throughput": 7700.523090288751,
    "output_throughput": 6839.248995081917,
    "total_throughput": 14539.772085370669,
    "itl": 125.20087108795732,
    "ttft": 1268840.3797144047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 73,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2453269400075075,
    "arrivals": 194341,
    "finished_requests": 112041,
    "scheduler_time": 146.7914769065344
}
#Debug simulation 
Total elapsed time: 122.38556022988632. Arrivals time: 0.474983389955014 Scheduler time: 121.73007500264794 Scheduler overhead time: 0.07190665090456605 Adapter cache time: 0.012530054897069931 Engine time: 0.07013614010065794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 120.76495038019493,
    "estimated_duration": 3600.0068338328633,
    "input_throughput": 7732.737821045043,
    "output_throughput": 6841.320624321747,
    "total_throughput": 14574.05844536679,
    "itl": 124.85138891741188,
    "ttft": 1261160.9402890175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2356575562828218,
    "arrivals": 190487,
    "finished_requests": 112254,
    "scheduler_time": 146.29741755257524
}
#Debug simulation 
Total elapsed time: 120.76509893592447. Arrivals time: 0.4775563022121787 Scheduler time: 120.10486756358296 Scheduler overhead time: 0.07241496304050088 Adapter cache time: 0.012747606728225946 Engine time: 0.07126470282673836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 120.85587847512215,
    "estimated_duration": 3600.125964279923,
    "input_throughput": 7727.787659664458,
    "output_throughput": 6837.863798169569,
    "total_throughput": 14565.651457834028,
    "itl": 124.89023247201706,
    "ttft": 1261354.9998275307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.317002169622574,
    "arrivals": 190487,
    "finished_requests": 112197,
    "scheduler_time": 146.2465658599131
}
#Debug simulation 
Total elapsed time: 120.85602862294763. Arrivals time: 0.47075815685093403 Scheduler time: 120.20128878299147 Scheduler overhead time: 0.0735017885453999 Adapter cache time: 0.012770385947078466 Engine time: 0.07135573960840702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 121.1376262488775,
    "estimated_duration": 3600.125216351677,
    "input_throughput": 7727.789265117137,
    "output_throughput": 6837.865218740014,
    "total_throughput": 14565.654483857152,
    "itl": 124.89015183408907,
    "ttft": 1261353.9363687541,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31747966296970875,
    "arrivals": 190487,
    "finished_requests": 112197,
    "scheduler_time": 146.246471488622
}
#Debug simulation 
Total elapsed time: 121.1377785038203. Arrivals time: 0.46858142875134945 Scheduler time: 120.4863433088176 Scheduler overhead time: 0.07287604315206409 Adapter cache time: 0.012582625728100538 Engine time: 0.07088327314704657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 120.73560465080664,
    "estimated_duration": 3600.0134558585364,
    "input_throughput": 7732.723597101438,
    "output_throughput": 6841.30804009078,
    "total_throughput": 14574.031637192218,
    "itl": 124.85098310386972,
    "ttft": 1261162.129532111,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24085735098691669,
    "arrivals": 190487,
    "finished_requests": 112254,
    "scheduler_time": 146.29771125121474
}
#Debug simulation 
Total elapsed time: 120.73575254390016. Arrivals time: 0.4598689442500472 Scheduler time: 120.0927546042949 Scheduler overhead time: 0.07263286272063851 Adapter cache time: 0.012777617666870356 Engine time: 0.07103370549157262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 120.07747409772128,
    "estimated_duration": 3600.134474519839,
    "input_throughput": 7732.50254873128,
    "output_throughput": 6841.096679669119,
    "total_throughput": 14573.599228400399,
    "itl": 124.85153370545213,
    "ttft": 1261178.4419476027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25515741951763604,
    "arrivals": 190487,
    "finished_requests": 112255,
    "scheduler_time": 146.30195027987918
}
#Debug simulation 
Total elapsed time: 120.07761907204986. Arrivals time: 0.4613075368106365 Scheduler time: 119.43526001879945 Scheduler overhead time: 0.0718337781727314 Adapter cache time: 0.012649302836507559 Engine time: 0.07021995401009917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 121.03605393413454,
    "estimated_duration": 3600.1375185591414,
    "input_throughput": 7732.496010636125,
    "output_throughput": 6841.09089528809,
    "total_throughput": 14573.586905924216,
    "itl": 124.85149320027354,
    "ttft": 1261180.1361379148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23023387270746776,
    "arrivals": 190487,
    "finished_requests": 112255,
    "scheduler_time": 146.3029357399088
}
#Debug simulation 
Total elapsed time: 121.03620851505548. Arrivals time: 0.45001829136162996 Scheduler time: 120.40278568305075 Scheduler overhead time: 0.07352911867201328 Adapter cache time: 0.012431168463081121 Engine time: 0.07088551111519337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 120.85667906003073,
    "estimated_duration": 3600.1378068638937,
    "input_throughput": 7732.495391405566,
    "output_throughput": 6841.090347442668,
    "total_throughput": 14573.585738848235,
    "itl": 124.85157171906187,
    "ttft": 1261180.3212225663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25842701796442236,
    "arrivals": 190487,
    "finished_requests": 112255,
    "scheduler_time": 146.30202772524245
}
#Debug simulation 
Total elapsed time: 120.85682793008164. Arrivals time: 0.466952761169523 Scheduler time: 120.20675033843145 Scheduler overhead time: 0.07254579290747643 Adapter cache time: 0.01276886835694313 Engine time: 0.07131650717929006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 105.68519226787612,
    "estimated_duration": 3600.0123264301137,
    "input_throughput": 7813.21102527787,
    "output_throughput": 6861.080396492219,
    "total_throughput": 14674.291421770089,
    "itl": 123.78330663902733,
    "ttft": 1245578.9522159256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2723834092100148,
    "arrivals": 188561,
    "finished_requests": 113046,
    "scheduler_time": 146.69701016391932
}
#Debug simulation 
Total elapsed time: 105.68534672865644. Arrivals time: 0.44322019116953015 Scheduler time: 105.06614598399028 Scheduler overhead time: 0.06926634768024087 Adapter cache time: 0.013237170409411192 Engine time: 0.0676805037073791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.80637328978628,
    "estimated_duration": 3600.002299906008,
    "input_throughput": 7813.232786194159,
    "output_throughput": 6861.099505587785,
    "total_throughput": 14674.332291781944,
    "itl": 123.78332635978578,
    "ttft": 1245557.9679084993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28981300471583377,
    "arrivals": 188561,
    "finished_requests": 113046,
    "scheduler_time": 146.6954888936224
}
#Debug simulation 
Total elapsed time: 105.80651784688234. Arrivals time: 0.44844043301418424 Scheduler time: 105.17890565562993 Scheduler overhead time: 0.07019185973331332 Adapter cache time: 0.01330639235675335 Engine time: 0.06963054416701198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.72076807403937,
    "estimated_duration": 3600.0025768070586,
    "input_throughput": 7813.232185224488,
    "output_throughput": 6861.098977853257,
    "total_throughput": 14674.331163077744,
    "itl": 123.78329332849523,
    "ttft": 1245558.022928961,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2904335322231056,
    "arrivals": 188561,
    "finished_requests": 113046,
    "scheduler_time": 146.69548458225475
}
#Debug simulation 
Total elapsed time: 105.72095113294199. Arrivals time: 0.4483884065411985 Scheduler time: 105.09563232492656 Scheduler overhead time: 0.0699756434187293 Adapter cache time: 0.012669548392295837 Engine time: 0.068432183470577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 105.84647823637351,
    "estimated_duration": 3600.0205696987055,
    "input_throughput": 7813.193134714247,
    "output_throughput": 6861.064686101836,
    "total_throughput": 14674.257820816083,
    "itl": 123.78327925234048,
    "ttft": 1245581.485148624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27796374048106387,
    "arrivals": 188561,
    "finished_requests": 113046,
    "scheduler_time": 146.6971698879406
}
#Debug simulation 
Total elapsed time: 105.84662569221109. Arrivals time: 0.44814318092539907 Scheduler time: 105.22118660900742 Scheduler overhead time: 0.06983259366825223 Adapter cache time: 0.012804961763322353 Engine time: 0.06904095085337758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 105.85446081869304,
    "estimated_duration": 3600.0064541927663,
    "input_throughput": 7813.223769985461,
    "output_throughput": 6861.091588108973,
    "total_throughput": 14674.315358094434,
    "itl": 123.78328598643314,
    "ttft": 1245559.7851897818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2940803920291364,
    "arrivals": 188561,
    "finished_requests": 113046,
    "scheduler_time": 146.69561343627578
}
#Debug simulation 
Total elapsed time: 105.85461288597435. Arrivals time: 0.6440767948515713 Scheduler time: 105.03254390694201 Scheduler overhead time: 0.06986606772989035 Adapter cache time: 0.013203574810177088 Engine time: 0.06934497458860278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 106.14070942485705,
    "estimated_duration": 3600.0074473708287,
    "input_throughput": 7813.221614455909,
    "output_throughput": 6861.089695255764,
    "total_throughput": 14674.311309711673,
    "itl": 123.78333253318009,
    "ttft": 1245576.7877733507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2661144762462939,
    "arrivals": 188561,
    "finished_requests": 113046,
    "scheduler_time": 146.69694722306087
}
#Debug simulation 
Total elapsed time: 106.14085866091773. Arrivals time: 0.4606231930665672 Scheduler time: 105.50225296057761 Scheduler overhead time: 0.07069839583709836 Adapter cache time: 0.013320144731551409 Engine time: 0.06833750940859318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.61336658522487,
    "estimated_duration": 3600.0114017465776,
    "input_throughput": 7813.213032145847,
    "output_throughput": 6861.0821587999935,
    "total_throughput": 14674.29519094584,
    "itl": 123.78351367813215,
    "ttft": 1245560.9458122796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29772725183516713,
    "arrivals": 188561,
    "finished_requests": 113046,
    "scheduler_time": 146.6957945131098
}
#Debug simulation 
Total elapsed time: 105.6135169621557. Arrivals time: 0.4579437389038503 Scheduler time: 104.97887099254876 Scheduler overhead time: 0.06966518936678767 Adapter cache time: 0.012873959727585316 Engine time: 0.06896203849464655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 116.60851645190269,
    "estimated_duration": 3600.116624887248,
    "input_throughput": 7819.0350294225855,
    "output_throughput": 6913.784633512962,
    "total_throughput": 14732.819662935546,
    "itl": 123.3689895198993,
    "ttft": 1240297.6613587488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2968673111614768,
    "arrivals": 187655,
    "finished_requests": 113847,
    "scheduler_time": 147.55672166869428
}
#Debug simulation 
Total elapsed time: 116.60867075296119. Arrivals time: 0.4689049473963678 Scheduler time: 115.95891517912969 Scheduler overhead time: 0.07095115631818771 Adapter cache time: 0.013333371840417385 Engine time: 0.07057676929980516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 116.89045147132128,
    "estimated_duration": 3600.135859054633,
    "input_throughput": 7819.120472690144,
    "output_throughput": 6913.901578852131,
    "total_throughput": 14733.022051542275,
    "itl": 123.36956830644206,
    "ttft": 1240265.8287733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31659357430413365,
    "arrivals": 187655,
    "finished_requests": 113849,
    "scheduler_time": 147.55760694805195
}
#Debug simulation 
Total elapsed time: 116.89060669625178. Arrivals time: 0.45632196962833405 Scheduler time: 116.25243952870369 Scheduler overhead time: 0.07156710233539343 Adapter cache time: 0.013665237464010715 Engine time: 0.07024593139067292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 117.06853663595393,
    "estimated_duration": 3600.0000590776554,
    "input_throughput": 7819.165982795002,
    "output_throughput": 6913.993219871525,
    "total_throughput": 14733.159202666528,
    "itl": 123.36919280419293,
    "ttft": 1240263.6308251065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31714241547510064,
    "arrivals": 187655,
    "finished_requests": 113846,
    "scheduler_time": 147.5516584139577
}
#Debug simulation 
Total elapsed time: 117.06868270179257. Arrivals time: 0.4597219037823379 Scheduler time: 116.42622249992564 Scheduler overhead time: 0.07306490279734135 Adapter cache time: 0.013447482138872147 Engine time: 0.0705425450578332 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 116.83421910693869,
    "estimated_duration": 3600.1292834409605,
    "input_throughput": 7819.007536611325,
    "output_throughput": 6913.760323687605,
    "total_throughput": 14732.76786029893,
    "itl": 123.36925089511874,
    "ttft": 1240302.452688899,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3035185241140426,
    "arrivals": 187655,
    "finished_requests": 113847,
    "scheduler_time": 147.55707375788066
}
#Debug simulation 
Total elapsed time: 116.83435859298334. Arrivals time: 0.459244123660028 Scheduler time: 116.19387787766755 Scheduler overhead time: 0.07187078846618533 Adapter cache time: 0.013770760502666235 Engine time: 0.06972127500921488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 117.62349515175447,
    "estimated_duration": 3600.0068867654445,
    "input_throughput": 7819.151153150009,
    "output_throughput": 6913.980106955754,
    "total_throughput": 14733.131260105763,
    "itl": 123.36925033240937,
    "ttft": 1240267.4928542448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32116653664037587,
    "arrivals": 187655,
    "finished_requests": 113846,
    "scheduler_time": 147.55191248031534
}
#Debug simulation 
Total elapsed time: 117.62364033656195. Arrivals time: 0.4611999490298331 Scheduler time: 116.97993213264272 Scheduler overhead time: 0.0721885459497571 Adapter cache time: 0.013636461459100246 Engine time: 0.07066880166530609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 122.0209458223544,
    "estimated_duration": 3600.1133867130266,
    "input_throughput": 7819.0420623670925,
    "output_throughput": 6913.7908522168655,
    "total_throughput": 14732.832914583958,
    "itl": 123.3692774681794,
    "ttft": 1240297.0327497723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29003487860551136,
    "arrivals": 187655,
    "finished_requests": 113847,
    "scheduler_time": 147.5566639593273
}
#Debug simulation 
Total elapsed time: 122.02108058333397. Arrivals time: 0.46539605036377907 Scheduler time: 121.37266969541088 Scheduler overhead time: 0.07200484536588192 Adapter cache time: 0.013593600131571293 Engine time: 0.07079045288264751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 117.14193178387359,
    "estimated_duration": 3600.1250566730573,
    "input_throughput": 7819.016716606345,
    "output_throughput": 6913.768440866805,
    "total_throughput": 14732.78515747315,
    "itl": 123.36948748350743,
    "ttft": 1240294.691357599,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32531641159206603,
    "arrivals": 187655,
    "finished_requests": 113847,
    "scheduler_time": 147.5558015973439
}
#Debug simulation 
Total elapsed time: 117.14207611884922. Arrivals time: 0.46828431775793433 Scheduler time: 116.49151761876419 Scheduler overhead time: 0.0718783182092011 Adapter cache time: 0.013501305598765612 Engine time: 0.07075544679537416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 144.36535780364648,
    "estimated_duration": 3600.0718328212815,
    "input_throughput": 7817.884005370956,
    "output_throughput": 6886.990635564585,
    "total_throughput": 14704.874640935543,
    "itl": 123.78394824448516,
    "ttft": 1189905.3813627884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 68,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20811316658742707,
    "arrivals": 187118,
    "finished_requests": 113669,
    "scheduler_time": 147.19083239126903
}
#Debug simulation 
Total elapsed time: 144.365498914849. Arrivals time: 0.4899052632972598 Scheduler time: 143.67997148586437 Scheduler overhead time: 0.07830827310681343 Adapter cache time: 0.01381709985435009 Engine time: 0.07578018819913268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 144.76195804588497,
    "estimated_duration": 3600.0849657285194,
    "input_throughput": 7817.8554861703215,
    "output_throughput": 6886.965512210547,
    "total_throughput": 14704.820998380868,
    "itl": 123.78359575010063,
    "ttft": 1189909.674811288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 68,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22252740002004426,
    "arrivals": 187118,
    "finished_requests": 113669,
    "scheduler_time": 147.19132770120066
}
#Debug simulation 
Total elapsed time: 144.76214790390804. Arrivals time: 0.48390768142417073 Scheduler time: 144.08526505157351 Scheduler overhead time: 0.07660213438794017 Adapter cache time: 0.013613366987556219 Engine time: 0.0759533909149468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 144.4147788179107,
    "estimated_duration": 3600.0889753403912,
    "input_throughput": 7817.846779006031,
    "output_throughput": 6886.957841828267,
    "total_throughput": 14704.804620834298,
    "itl": 123.78366591079124,
    "ttft": 1189912.2646795607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 68,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22280991395935407,
    "arrivals": 187118,
    "finished_requests": 113669,
    "scheduler_time": 147.19155180977236
}
#Debug simulation 
Total elapsed time: 144.41492070723325. Arrivals time: 0.4824969004839659 Scheduler time: 143.74112867144868 Scheduler overhead time: 0.07593279657885432 Adapter cache time: 0.013747325632721186 Engine time: 0.07467664685100317 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 144.82880940614268,
    "estimated_duration": 3600.074478636243,
    "input_throughput": 7817.878259746917,
    "output_throughput": 6886.985574085172,
    "total_throughput": 14704.863833832089,
    "itl": 123.78374938550154,
    "ttft": 1189906.1286480222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 68,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21272111237747604,
    "arrivals": 187118,
    "finished_requests": 113669,
    "scheduler_time": 147.19093269938145
}
#Debug simulation 
Total elapsed time: 144.8289551581256. Arrivals time: 0.49889425840228796 Scheduler time: 144.13533275946975 Scheduler overhead time: 0.07735497690737247 Adapter cache time: 0.013791333418339491 Engine time: 0.0761795942671597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 144.09046263014898,
    "estimated_duration": 3600.089057447163,
    "input_throughput": 7817.846600705397,
    "output_throughput": 6886.9576847582985,
    "total_throughput": 14704.804285463695,
    "itl": 123.78384372580803,
    "ttft": 1189910.2612649966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 68,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22582800483331064,
    "arrivals": 187118,
    "finished_requests": 113669,
    "scheduler_time": 147.19153981653696
}
#Debug simulation 
Total elapsed time: 144.09060452599078. Arrivals time: 0.4940386530943215 Scheduler time: 143.40101117966697 Scheduler overhead time: 0.07793536828830838 Adapter cache time: 0.013683065306395292 Engine time: 0.07558958232402802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 144.60585576994345,
    "estimated_duration": 3600.0663919556264,
    "input_throughput": 7817.895820724328,
    "output_throughput": 6887.001044036746,
    "total_throughput": 14704.896864761073,
    "itl": 123.78402951816909,
    "ttft": 1189902.584905629,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 68,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20332342005334814,
    "arrivals": 187118,
    "finished_requests": 113669,
    "scheduler_time": 147.19061374224236
}
#Debug simulation 
Total elapsed time: 144.60599973099306. Arrivals time: 0.49122392339631915 Scheduler time: 143.92169507732615 Scheduler overhead time: 0.07660498889163136 Adapter cache time: 0.01368257449939847 Engine time: 0.07550727110356092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 144.58729498228058,
    "estimated_duration": 3600.09253084083,
    "input_throughput": 7817.839057994025,
    "output_throughput": 6886.95104017486,
    "total_throughput": 14704.790098168885,
    "itl": 123.78379860340213,
    "ttft": 1189911.8056919414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 68,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22872034192085236,
    "arrivals": 187118,
    "finished_requests": 113669,
    "scheduler_time": 147.19165627121635
}
#Debug simulation 
Total elapsed time: 144.58744049398229. Arrivals time: 0.4937568227760494 Scheduler time: 143.90053560957313 Scheduler overhead time: 0.07736635813489556 Adapter cache time: 0.013698399532586336 Engine time: 0.07481946982443333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 131.25290361093357,
    "estimated_duration": 3600.0244897450593,
    "input_throughput": 7896.031563388898,
    "output_throughput": 6973.283673905722,
    "total_throughput": 14869.31523729462,
    "itl": 122.48901832890922,
    "ttft": 1233685.9982789205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21117365433135982,
    "arrivals": 186916,
    "finished_requests": 114964,
    "scheduler_time": 148.93826904731003
}
#Debug simulation 
Total elapsed time: 131.25304166367278. Arrivals time: 0.4887203383259475 Scheduler time: 130.57464860239998 Scheduler overhead time: 0.07605709554627538 Adapter cache time: 0.013699885457754135 Engine time: 0.07358470559120178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 131.40940851299092,
    "estimated_duration": 3600.011374894443,
    "input_throughput": 7895.978384466486,
    "output_throughput": 6973.140744794469,
    "total_throughput": 14869.119129260956,
    "itl": 122.48966463908471,
    "ttft": 1233698.8312464328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2251088549965061,
    "arrivals": 186916,
    "finished_requests": 114963,
    "scheduler_time": 148.93648088324844
}
#Debug simulation 
Total elapsed time: 131.40955027425662. Arrivals time: 0.48892407212406397 Scheduler time: 130.73197925370187 Scheduler overhead time: 0.0746086691506207 Adapter cache time: 0.013486884068697691 Engine time: 0.073475934099406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 131.24092970835045,
    "estimated_duration": 3600.012400655359,
    "input_throughput": 7895.976134644786,
    "output_throughput": 6973.138757919303,
    "total_throughput": 14869.114892564088,
    "itl": 122.4895140203083,
    "ttft": 1233699.9136576648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22551618531346312,
    "arrivals": 186916,
    "finished_requests": 114963,
    "scheduler_time": 148.9365942918968
}
#Debug simulation 
Total elapsed time: 131.24107350641862. Arrivals time: 0.48319851281121373 Scheduler time: 130.56779077276587 Scheduler overhead time: 0.0749494293704629 Adapter cache time: 0.013409537263214588 Engine time: 0.07492255000397563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 130.87884191330522,
    "estimated_duration": 3600.032061719525,
    "input_throughput": 7896.014955606425,
    "output_throughput": 6973.269006945813,
    "total_throughput": 14869.28396255224,
    "itl": 122.48923571499698,
    "ttft": 1233688.39356498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2157111626723782,
    "arrivals": 186916,
    "finished_requests": 114964,
    "scheduler_time": 148.93852925244624
}
#Debug simulation 
Total elapsed time: 130.87898520985618. Arrivals time: 0.48864147160202265 Scheduler time: 130.2017405210063 Scheduler overhead time: 0.07610534969717264 Adapter cache time: 0.01348610408604145 Engine time: 0.07272862643003464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 130.43936872808263,
    "estimated_duration": 3600.014431414242,
    "input_throughput": 7895.971680544953,
    "output_throughput": 6973.134824389662,
    "total_throughput": 14869.106504934616,
    "itl": 122.48945170270564,
    "ttft": 1233700.181382876,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22840852240100482,
    "arrivals": 186916,
    "finished_requests": 114963,
    "scheduler_time": 148.93671834322143
}
#Debug simulation 
Total elapsed time: 130.43950107507408. Arrivals time: 0.4850593078881502 Scheduler time: 129.7656518672593 Scheduler overhead time: 0.07494977442547679 Adapter cache time: 0.013506155461072922 Engine time: 0.07373008318245411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 131.0836750259623,
    "estimated_duration": 3600.015612923861,
    "input_throughput": 7895.969089121056,
    "output_throughput": 6973.132535836846,
    "total_throughput": 14869.1016249579,
    "itl": 122.48832115366332,
    "ttft": 1233702.0827843386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20631347034825032,
    "arrivals": 186916,
    "finished_requests": 114963,
    "scheduler_time": 148.93756120188465
}
#Debug simulation 
Total elapsed time: 131.08381884219125. Arrivals time: 0.48541013710200787 Scheduler time: 130.41189962020144 Scheduler overhead time: 0.0735296243801713 Adapter cache time: 0.013343635946512222 Engine time: 0.07356332149356604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_64_slots_32_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 131.27030074084178,
    "estimated_duration": 3600.0183632058934,
    "input_throughput": 7895.963056890183,
    "output_throughput": 6973.127208619263,
    "total_throughput": 14869.090265509447,
    "itl": 122.48943184296215,
    "ttft": 1233701.8310239923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.23130085948854656,
    "arrivals": 186916,
    "finished_requests": 114963,
    "scheduler_time": 148.93682438327866
}
#Debug simulation 
Total elapsed time: 131.270440925844. Arrivals time: 0.4839814119040966 Scheduler time: 130.5969128506258 Scheduler overhead time: 0.07546724658459425 Adapter cache time: 0.013676762115210295 Engine time: 0.07342857029289007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 148.28768462873995,
    "estimated_duration": 3600.075388413063,
    "input_throughput": 7762.687717581074,
    "output_throughput": 6840.233979337588,
    "total_throughput": 14602.921696918662,
    "itl": 124.28249523426098,
    "ttft": 1010139.4252889211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21117365433135982,
    "arrivals": 164321,
    "finished_requests": 112328,
    "scheduler_time": 141.19501448713297
}
#Debug simulation 
Total elapsed time: 148.28789209295064. Arrivals time: 0.4712315141223371 Scheduler time: 147.6213489654474 Scheduler overhead time: 0.0781136155128479 Adapter cache time: 0.013046611566096544 Engine time: 0.07628218829631805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 149.12886442616582,
    "estimated_duration": 3600.093308021254,
    "input_throughput": 7762.649078492999,
    "output_throughput": 6840.199931799829,
    "total_throughput": 14602.849010292828,
    "itl": 124.28159688272177,
    "ttft": 1010144.0254603839,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22551745031494644,
    "arrivals": 164321,
    "finished_requests": 112328,
    "scheduler_time": 141.19555980874566
}
#Debug simulation 
Total elapsed time: 149.1290091080591. Arrivals time: 0.47731714183464646 Scheduler time: 148.45800910098478 Scheduler overhead time: 0.07789440173655748 Adapter cache time: 0.013506958726793528 Engine time: 0.07549123233184218 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 148.09014313295484,
    "estimated_duration": 3600.0936634897926,
    "input_throughput": 7762.648312019184,
    "output_throughput": 6840.199256407435,
    "total_throughput": 14602.84756842662,
    "itl": 124.28157167884139,
    "ttft": 1010144.3100002183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22585343280807127,
    "arrivals": 164321,
    "finished_requests": 112328,
    "scheduler_time": 141.19558011143988
}
#Debug simulation 
Total elapsed time: 148.0902885920368. Arrivals time: 0.47699826769530773 Scheduler time: 147.418943206314 Scheduler overhead time: 0.07811295567080379 Adapter cache time: 0.012926758732646704 Engine time: 0.07598351407796144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 148.73013285081834,
    "estimated_duration": 3600.0819908576873,
    "input_throughput": 7762.6734810398175,
    "output_throughput": 6840.221434549392,
    "total_throughput": 14602.89491558921,
    "itl": 124.28222936426305,
    "ttft": 1010140.188948492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21611975799081853,
    "arrivals": 164321,
    "finished_requests": 112328,
    "scheduler_time": 141.19512580838003
}
#Debug simulation 
Total elapsed time: 148.73028363799676. Arrivals time: 0.47902865102514625 Scheduler time: 148.05341892223805 Scheduler overhead time: 0.07917491067200899 Adapter cache time: 0.013652617111802101 Engine time: 0.07687202095985413 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 147.46531710587442,
    "estimated_duration": 3600.1003515364473,
    "input_throughput": 7762.633891045043,
    "output_throughput": 6840.186549102837,
    "total_throughput": 14602.82044014788,
    "itl": 124.2815499573743,
    "ttft": 1010144.2729582965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.22874576989561302,
    "arrivals": 164321,
    "finished_requests": 112328,
    "scheduler_time": 141.19587916201488
}
#Debug simulation 
Total elapsed time: 147.46546304784715. Arrivals time: 0.4737103986553848 Scheduler time: 146.79440344916657 Scheduler overhead time: 0.07853136723861098 Adapter cache time: 0.013453220017254353 Engine time: 0.07754464214667678 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_64_slots_32_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 148.76567889889702,
    "estimated_duration": 3600.0696117076463,
    "input_throughput": 7762.70017366249,
    "output_throughput": 6840.244955241096,
    "total_throughput": 14602.945128903586,
    "itl": 124.28263114771217,
    "ttft": 1010136.8379466025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20631347034825032,
    "arrivals": 164321,
    "finished_requests": 112328,
    "scheduler_time": 141.19477741252163
}
#Debug simulation 
Total elapsed time: 148.76582253770903. Arrivals time: 0.47648001881316304 Scheduler time: 148.09415005380288 Scheduler overhead time: 0.0784114645794034 Adapter cache time: 0.013587694149464369 Engine time: 0.07643599528819323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_64_slots_32_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 148.1940978928469,
    "estimated_duration": 3600.1037884512225,
    "input_throughput": 7762.626480283387,
    "output_throughput": 6840.18001897493,
    "total_throughput": 14602.806499258317,
    "itl": 124.28167187117974,
    "ttft": 1010144.919923973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 69,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2317638607695696,
    "arrivals": 164321,
    "finished_requests": 112328,
    "scheduler_time": 141.19597752184578
}
#Debug simulation 
Total elapsed time: 148.19425154896453. Arrivals time: 0.4729207921773195 Scheduler time: 147.5272601218894 Scheduler overhead time: 0.07800358952954412 Adapter cache time: 0.013641165103763342 Engine time: 0.07574917934834957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 147.17426076205447,
    "estimated_duration": 3600.075463356432,
    "input_throughput": 7736.244776945265,
    "output_throughput": 6855.314076386222,
    "total_throughput": 14591.558853331488,
    "itl": 124.87277740530465,
    "ttft": 891408.8027247617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 43,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13160097298910833,
    "arrivals": 160553,
    "finished_requests": 112378,
    "scheduler_time": 139.71737739066555
}
#Debug simulation 
Total elapsed time: 147.17442021006718. Arrivals time: 0.47770103672519326 Scheduler time: 146.49843662092462 Scheduler overhead time: 0.0791100850328803 Adapter cache time: 0.013320838566869497 Engine time: 0.07777616009116173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 146.7750484352,
    "estimated_duration": 3600.047574657536,
    "input_throughput": 7735.668049511592,
    "output_throughput": 6855.149407948488,
    "total_throughput": 14590.81745746008,
    "itl": 124.89308088525885,
    "ttft": 891685.9930292122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 43,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1400128315971233,
    "arrivals": 160553,
    "finished_requests": 112370,
    "scheduler_time": 139.71915348758833
}
#Debug simulation 
Total elapsed time: 146.77521384507418. Arrivals time: 0.4844678770750761 Scheduler time: 146.09174309019 Scheduler overhead time: 0.07887246645987034 Adapter cache time: 0.013717649970203638 Engine time: 0.07822320843115449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 147.3573458348401,
    "estimated_duration": 3600.0610136119535,
    "input_throughput": 7735.69139375753,
    "output_throughput": 6855.177150244856,
    "total_throughput": 14590.868544002387,
    "itl": 124.89383353692065,
    "ttft": 891661.215874076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 43,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1403142403438687,
    "arrivals": 160553,
    "finished_requests": 112371,
    "scheduler_time": 139.719623269781
}
#Debug simulation 
Total elapsed time: 147.3575169998221. Arrivals time: 0.4866167022846639 Scheduler time: 146.66836160700768 Scheduler overhead time: 0.08320712484419346 Adapter cache time: 0.013527736067771912 Engine time: 0.0784005057066679 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 147.17029821267352,
    "estimated_duration": 3600.083154119542,
    "input_throughput": 7736.228250208688,
    "output_throughput": 6855.29943155877,
    "total_throughput": 14591.527681767458,
    "itl": 124.87271807377795,
    "ttft": 891411.136518684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 43,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.13388390182051813,
    "arrivals": 160553,
    "finished_requests": 112378,
    "scheduler_time": 139.7177208827864
}
#Debug simulation 
Total elapsed time: 147.17046328401193. Arrivals time: 0.48684639809653163 Scheduler time: 146.48440551571548 Scheduler overhead time: 0.07974515156820416 Adapter cache time: 0.01341248769313097 Engine time: 0.07798110507428646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 146.30502456380054,
    "estimated_duration": 3600.066930805215,
    "input_throughput": 7735.678679110311,
    "output_throughput": 6855.165882840994,
    "total_throughput": 14590.844561951306,
    "itl": 124.89378281486871,
    "ttft": 891665.5676793576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 43,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.14220054714009156,
    "arrivals": 160553,
    "finished_requests": 112371,
    "scheduler_time": 139.72003969515836
}
#Debug simulation 
Total elapsed time: 146.3051959630102. Arrivals time: 0.4835747149772942 Scheduler time: 145.62583125848323 Scheduler overhead time: 0.07754604564979672 Adapter cache time: 0.01365248067304492 Engine time: 0.07726934272795916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_64_slots_32_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 147.7815312310122,
    "estimated_duration": 3600.0451003238018,
    "input_throughput": 7736.170026729669,
    "output_throughput": 6855.254951606094,
    "total_throughput": 14591.424978335763,
    "itl": 124.8719593418165,
    "ttft": 891477.460361889,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 43,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12857216268079363,
    "arrivals": 160553,
    "finished_requests": 112375,
    "scheduler_time": 139.7160450757397
}
#Debug simulation 
Total elapsed time: 147.78169902181253. Arrivals time: 0.4901222502812743 Scheduler time: 147.09225143119693 Scheduler overhead time: 0.07985178986564279 Adapter cache time: 0.013387196697294712 Engine time: 0.07871764013543725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_64_slots_32_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 150.82740747975186,
    "estimated_duration": 3600.0664239688595,
    "input_throughput": 7735.679768179992,
    "output_throughput": 6855.166847947435,
    "total_throughput": 14590.846616127426,
    "itl": 124.89345207485692,
    "ttft": 891664.4804182524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 43,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.14383534636348475,
    "arrivals": 160553,
    "finished_requests": 112371,
    "scheduler_time": 139.71996743292559
}
#Debug simulation 
Total elapsed time: 150.82762473681942. Arrivals time: 0.48616867884993553 Scheduler time: 150.14025395736098 Scheduler overhead time: 0.08111153822392225 Adapter cache time: 0.013678160030394793 Engine time: 0.07869302248582244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_64_slots_32_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_64_slots_32_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 106.34754102677107,
    "estimated_duration": 3600.0633558443724,
    "input_throughput": 7700.97363842018,
    "output_throughput": 6863.558098189247,
    "total_throughput": 14564.531736609428,
    "itl": 125.38817501499497,
    "ttft": 947039.4392895214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.12241950975731009,
    "arrivals": 158690,
    "finished_requests": 112224,
    "scheduler_time": 138.74435589936832
}
#Debug simulation 
Total elapsed time: 106.34769682493061. Arrivals time: 0.42053839610889554 Scheduler time: 105.74444903759286 Scheduler overhead time: 0.07323794951662421 Adapter cache time: 0.011739932000637054 Engine time: 0.07136482186615467 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_64_slots_32_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_64_slots_32_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.17522726720199,
    "estimated_duration": 3600.1120487516455,
    "input_throughput": 7701.019752875081,
    "output_throughput": 6863.621649933988,
    "total_throughput": 14564.64140280907,
    "itl": 125.38867282859839,
    "ttft": 947016.3879848496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1298168947570957,
    "arrivals": 158690,
    "finished_requests": 112226,
    "scheduler_time": 138.74658840578422
}
#Debug simulation 
Total elapsed time: 106.17537157284096. Arrivals time: 0.3867627619765699 Scheduler time: 105.6062474171631 Scheduler overhead time: 0.07230403227731586 Adapter cache time: 0.01230010949075222 Engine time: 0.07211345341056585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_64_slots_32_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_64_slots_32_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.64179728506133,
    "estimated_duration": 3600.1125632047642,
    "input_throughput": 7701.018652405704,
    "output_throughput": 6863.62066912811,
    "total_throughput": 14564.639321533814,
    "itl": 125.38871782882211,
    "ttft": 947016.2660154598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1301719413138926,
    "arrivals": 158690,
    "finished_requests": 112226,
    "scheduler_time": 138.7465744778307
}
#Debug simulation 
Total elapsed time: 106.64193922141567. Arrivals time: 0.3917981400154531 Scheduler time: 106.06539610913023 Scheduler overhead time: 0.0735168787650764 Adapter cache time: 0.012521788012236357 Engine time: 0.07275250041857362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_64_slots_32_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_64_slots_32_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 106.07281326828524,
    "estimated_duration": 3600.068906740877,
    "input_throughput": 7700.9617643953325,
    "output_throughput": 6863.54751536385,
    "total_throughput": 14564.509279759182,
    "itl": 125.38806581937517,
    "ttft": 947040.1104301863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1240965602989309,
    "arrivals": 158690,
    "finished_requests": 112224,
    "scheduler_time": 138.74454859362677
}
#Debug simulation 
Total elapsed time: 106.07295833295211. Arrivals time: 0.3968918239697814 Scheduler time: 105.49287343909964 Scheduler overhead time: 0.07331429747864604 Adapter cache time: 0.012256531044840813 Engine time: 0.07200724305585027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_64_slots_32_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_64_slots_32_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 106.21961409505457,
    "estimated_duration": 3600.1200218095873,
    "input_throughput": 7701.002697700163,
    "output_throughput": 6863.606449314905,
    "total_throughput": 14564.609147015068,
    "itl": 125.38855162543943,
    "ttft": 947037.1584272877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 40,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.1319324943237006,
    "arrivals": 158690,
    "finished_requests": 112226,
    "scheduler_time": 138.7469871522722
}
#Debug simulation 
Total elapsed time: 106.21975377993658. Arrivals time: 0.39954673778265715 Scheduler time: 105.63217115681618 Scheduler overhead time: 0.07449055090546608 Adapter cache time: 0.012793543748557568 Engine time: 0.07469766028225422 
