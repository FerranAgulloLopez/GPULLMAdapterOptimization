INFO 06-01 00:47:01 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:01 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 135.22356985788792,
    "estimated_duration": 3600.1269027049393,
    "input_throughput": 6824.2666616948345,
    "output_throughput": 6066.584759440078,
    "total_throughput": 12890.851421134914,
    "itl": 98.2076390455064,
    "ttft": 1211461.9627698702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6016522643854854,
    "arrivals": 139992,
    "finished_requests": 99553,
    "scheduler_time": 182.11153881310923
}
#Debug simulation 
Total elapsed time: 135.22381368093193. Arrivals time: 0.4500967115163803 Scheduler time: 134.54024498118088 Scheduler overhead time: 0.09018577868118882 Adapter cache time: 0.017651062458753586 Engine time: 0.09214141638949513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 135.6249063066207,
    "estimated_duration": 3600.127638268203,
    "input_throughput": 6824.265267388753,
    "output_throughput": 6066.5835199404455,
    "total_throughput": 12890.8487873292,
    "itl": 98.20765988259932,
    "ttft": 1211462.1128714024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6025006524845982,
    "arrivals": 139992,
    "finished_requests": 99553,
    "scheduler_time": 182.1115390932934
}
#Debug simulation 
Total elapsed time: 135.62511778390035. Arrivals time: 0.45392968226224184 Scheduler time: 134.93654041178524 Scheduler overhead time: 0.09148203302174807 Adapter cache time: 0.01839747140184045 Engine time: 0.09073653211817145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 137.37013364210725,
    "estimated_duration": 3600.1285705926553,
    "input_throughput": 6821.230552873661,
    "output_throughput": 6065.761978163212,
    "total_throughput": 12886.992531036873,
    "itl": 98.22948628053012,
    "ttft": 1210066.7285055586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5715650400077001,
    "arrivals": 139992,
    "finished_requests": 99534,
    "scheduler_time": 182.1176635753131
}
#Debug simulation 
Total elapsed time: 137.37043595593423. Arrivals time: 0.4599461411125958 Scheduler time: 136.67664545634761 Scheduler overhead time: 0.09075215877965093 Adapter cache time: 0.01820252137258649 Engine time: 0.09148232825100422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 136.23027051100507,
    "estimated_duration": 3600.137082923828,
    "input_throughput": 6824.247364505097,
    "output_throughput": 6066.567604770871,
    "total_throughput": 12890.814969275969,
    "itl": 98.20782167294082,
    "ttft": 1211465.1507414973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6099201258830742,
    "arrivals": 139992,
    "finished_requests": 99553,
    "scheduler_time": 182.11198080509675
}
#Debug simulation 
Total elapsed time: 136.23044109297916. Arrivals time: 0.4539159000851214 Scheduler time: 135.54246613243595 Scheduler overhead time: 0.09177176002413034 Adapter cache time: 0.01791452942416072 Engine time: 0.09069057647138834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 136.49663794925436,
    "estimated_duration": 3600.0233136002007,
    "input_throughput": 6821.742766837906,
    "output_throughput": 6064.613503340392,
    "total_throughput": 12886.356270178298,
    "itl": 98.1495117135297,
    "ttft": 1210818.2421809265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5471792039670988,
    "arrivals": 139992,
    "finished_requests": 99512,
    "scheduler_time": 182.18320081416314
}
#Debug simulation 
Total elapsed time: 136.49682309804484. Arrivals time: 0.45307705318555236 Scheduler time: 135.80630037700757 Scheduler overhead time: 0.09279776317998767 Adapter cache time: 0.018344022799283266 Engine time: 0.0919838105328381 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 136.131657268852,
    "estimated_duration": 3600.001839334883,
    "input_throughput": 6824.480402081983,
    "output_throughput": 6066.6971781422935,
    "total_throughput": 12891.177580224277,
    "itl": 98.20791957078872,
    "ttft": 1211413.4519326994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6183456295728695,
    "arrivals": 139992,
    "finished_requests": 99551,
    "scheduler_time": 182.1044097820493
}
#Debug simulation 
Total elapsed time: 136.13182464893907. Arrivals time: 0.44385775923728943 Scheduler time: 135.45082827936858 Scheduler overhead time: 0.09215682838112116 Adapter cache time: 0.01852193893864751 Engine time: 0.09182604867964983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 140.63254800625145,
    "estimated_duration": 3600.0392346008234,
    "input_throughput": 6844.3651289073,
    "output_throughput": 6075.717394903693,
    "total_throughput": 12920.082523810992,
    "itl": 98.1642182527374,
    "ttft": 1208093.8263188193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5447668184200295,
    "arrivals": 139506,
    "finished_requests": 99791,
    "scheduler_time": 181.33363239467332
}
#Debug simulation 
Total elapsed time: 140.63272492587566. Arrivals time: 0.4584549772553146 Scheduler time: 139.9374371576123 Scheduler overhead time: 0.09235790371894836 Adapter cache time: 0.018641735427081585 Engine time: 0.09183734701946378 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 143.6876994362101,
    "estimated_duration": 3600.0944470340382,
    "input_throughput": 6839.383622361727,
    "output_throughput": 6066.25834996976,
    "total_throughput": 12905.641972331487,
    "itl": 97.87976239480986,
    "ttft": 1201012.436650341,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5612771123438148,
    "arrivals": 139506,
    "finished_requests": 99763,
    "scheduler_time": 181.65736392058506
}
#Debug simulation 
Total elapsed time: 143.68787958938628. Arrivals time: 0.45908684097230434 Scheduler time: 142.98732388718054 Scheduler overhead time: 0.09311367245391011 Adapter cache time: 0.018560463562607765 Engine time: 0.09460849827155471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 144.1989581687376,
    "estimated_duration": 3600.095119904605,
    "input_throughput": 6839.382344056633,
    "output_throughput": 6066.257216164525,
    "total_throughput": 12905.639560221158,
    "itl": 97.87964172340357,
    "ttft": 1201012.7983566902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5622687038593015,
    "arrivals": 139506,
    "finished_requests": 99763,
    "scheduler_time": 181.65761072476738
}
#Debug simulation 
Total elapsed time: 144.1991493436508. Arrivals time: 0.46788771776482463 Scheduler time: 143.49095101468265 Scheduler overhead time: 0.09430482098832726 Adapter cache time: 0.018685071729123592 Engine time: 0.09315091324970126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 145.7505749007687,
    "estimated_duration": 3600.0423237701625,
    "input_throughput": 6834.835756661761,
    "output_throughput": 6064.79203198221,
    "total_throughput": 12899.62778864397,
    "itl": 97.79714161370629,
    "ttft": 1196678.2571121927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 169,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.528608432989568,
    "arrivals": 139506,
    "finished_requests": 99692,
    "scheduler_time": 181.9090476887327
}
#Debug simulation 
Total elapsed time: 145.75075319875032. Arrivals time: 0.4682256686501205 Scheduler time: 145.03771714586765 Scheduler overhead time: 0.09406743803992867 Adapter cache time: 0.019512498285621405 Engine time: 0.09659923240542412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 144.15126485191286,
    "estimated_duration": 3600.0009153440305,
    "input_throughput": 6839.624927608375,
    "output_throughput": 6066.344290891102,
    "total_throughput": 12905.969218499476,
    "itl": 97.88039087465486,
    "ttft": 1201066.9235036408,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5693109158985328,
    "arrivals": 139506,
    "finished_requests": 99761,
    "scheduler_time": 181.65128783169212
}
#Debug simulation 
Total elapsed time: 144.15144349215552. Arrivals time: 0.4610442970879376 Scheduler time: 143.44736513216048 Scheduler overhead time: 0.09449989860877395 Adapter cache time: 0.019197382032871246 Engine time: 0.09448538301512599 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 145.506830599159,
    "estimated_duration": 3600.124646995458,
    "input_throughput": 6844.607733391928,
    "output_throughput": 6073.127223038504,
    "total_throughput": 12917.734956430433,
    "itl": 97.95742820040954,
    "ttft": 1201430.4075794052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5083085501333705,
    "arrivals": 139506,
    "finished_requests": 99830,
    "scheduler_time": 181.61594955347124
}
#Debug simulation 
Total elapsed time: 145.50699856039137. Arrivals time: 0.4541263612918556 Scheduler time: 144.80985609674826 Scheduler overhead time: 0.09387081116437912 Adapter cache time: 0.01845882646739483 Engine time: 0.0955531862564385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 144.83809328079224,
    "estimated_duration": 3600.0063706169867,
    "input_throughput": 6839.614563176467,
    "output_throughput": 6066.335098250715,
    "total_throughput": 12905.949661427183,
    "itl": 97.88050699751055,
    "ttft": 1201068.1866097432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5767303892970091,
    "arrivals": 139506,
    "finished_requests": 99761,
    "scheduler_time": 181.6513652383946
}
#Debug simulation 
Total elapsed time: 144.83827951503918. Arrivals time: 0.4706279509700835 Scheduler time: 144.12318199826404 Scheduler overhead time: 0.09405112126842141 Adapter cache time: 0.019107462838292122 Engine time: 0.0962238316424191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_128_slots_32_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_128_slots_32_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 152.23953167209402,
    "estimated_duration": 3600.134553957635,
    "input_throughput": 6771.339691512782,
    "output_throughput": 5984.16161315884,
    "total_throughput": 12755.501304671621,
    "itl": 93.94487055910355,
    "ttft": 1183414.621425064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6427024262258775,
    "arrivals": 135164,
    "finished_requests": 98218,
    "scheduler_time": 181.16537012401432
}
#Debug simulation 
Total elapsed time: 152.23970746295527. Arrivals time: 0.4594185478053987 Scheduler time: 151.5322910156101 Scheduler overhead time: 0.09476099675521255 Adapter cache time: 0.01984009938314557 Engine time: 0.09794495347887278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_128_slots_32_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_128_slots_32_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 152.80735675990582,
    "estimated_duration": 3600.038047386614,
    "input_throughput": 6771.271214118402,
    "output_throughput": 5984.173699397137,
    "total_throughput": 12755.44491351554,
    "itl": 93.94433204597202,
    "ttft": 1183404.1176845212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6859310971479886,
    "arrivals": 135164,
    "finished_requests": 98215,
    "scheduler_time": 181.1590567467648
}
#Debug simulation 
Total elapsed time: 152.8075325358659. Arrivals time: 0.4713506787084043 Scheduler time: 152.08886997913942 Scheduler overhead time: 0.09597998578101397 Adapter cache time: 0.020172235555946827 Engine time: 0.09559739660471678 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_128_slots_32_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_128_slots_32_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 153.04710356611758,
    "estimated_duration": 3600.0389113737524,
    "input_throughput": 6771.269589055068,
    "output_throughput": 5984.172263232352,
    "total_throughput": 12755.441852287418,
    "itl": 93.94434724536782,
    "ttft": 1183404.3071768652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6870281024649769,
    "arrivals": 135164,
    "finished_requests": 98215,
    "scheduler_time": 181.15904993862796
}
#Debug simulation 
Total elapsed time: 153.04727129312232. Arrivals time: 0.46431497344747186 Scheduler time: 152.33318669814616 Scheduler overhead time: 0.0973260523751378 Adapter cache time: 0.01939046662300825 Engine time: 0.09735716274008155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_128_slots_32_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_128_slots_32_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 152.671703718137,
    "estimated_duration": 3600.0117918706833,
    "input_throughput": 6771.320598184208,
    "output_throughput": 5984.217343023041,
    "total_throughput": 12755.53794120725,
    "itl": 93.9436982594408,
    "ttft": 1183399.3350767184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6581466154940444,
    "arrivals": 135164,
    "finished_requests": 98215,
    "scheduler_time": 181.1584394377056
}
#Debug simulation 
Total elapsed time: 152.67188402311876. Arrivals time: 0.45781717356294394 Scheduler time: 151.9663891158998 Scheduler overhead time: 0.09643246745690703 Adapter cache time: 0.019822699949145317 Engine time: 0.09643224067986012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_128_slots_32_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_128_slots_32_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 152.99801789596677,
    "estimated_duration": 3600.046278263341,
    "input_throughput": 6771.255732790014,
    "output_throughput": 5984.160017629675,
    "total_throughput": 12755.415750419688,
    "itl": 93.94456274367323,
    "ttft": 1183405.527135936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6955793599411869,
    "arrivals": 135164,
    "finished_requests": 98215,
    "scheduler_time": 181.159109726107
}
#Debug simulation 
Total elapsed time: 152.99818902974948. Arrivals time: 0.4622331368736923 Scheduler time: 152.28450207132846 Scheduler overhead time: 0.09768845606595278 Adapter cache time: 0.019863605964928865 Engine time: 0.09807902807369828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_128_slots_32_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_128_slots_32_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 154.64996819011867,
    "estimated_duration": 3600.120359608831,
    "input_throughput": 6771.36638916393,
    "output_throughput": 5984.185207169247,
    "total_throughput": 12755.551596333178,
    "itl": 93.94444133946338,
    "ttft": 1183411.9538289239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6279105619294577,
    "arrivals": 135164,
    "finished_requests": 98218,
    "scheduler_time": 181.16493594591935
}
#Debug simulation 
Total elapsed time: 154.65014503104612. Arrivals time: 0.47167773684486747 Scheduler time: 153.9265498612076 Scheduler overhead time: 0.09862665040418506 Adapter cache time: 0.020379855297505856 Engine time: 0.09776638261973858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_128_slots_32_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_128_slots_32_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 152.74067418510094,
    "estimated_duration": 3600.0603002687126,
    "input_throughput": 6771.229359180591,
    "output_throughput": 5984.136709707886,
    "total_throughput": 12755.366068888477,
    "itl": 93.94421881331589,
    "ttft": 1183409.0565915036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7048851401358865,
    "arrivals": 135164,
    "finished_requests": 98215,
    "scheduler_time": 181.1597587992102
}
#Debug simulation 
Total elapsed time: 152.74084849935025. Arrivals time: 0.46687582693994045 Scheduler time: 152.02398017421365 Scheduler overhead time: 0.09647808037698269 Adapter cache time: 0.01997747551649809 Engine time: 0.09788361564278603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 152.65450815623626,
    "estimated_duration": 3600.0940216628182,
    "input_throughput": 6767.719357714584,
    "output_throughput": 5958.632155415729,
    "total_throughput": 12726.351513130312,
    "itl": 91.91371217850887,
    "ttft": 1166960.4438975665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5906741345790207,
    "arrivals": 133259,
    "finished_requests": 97933,
    "scheduler_time": 181.05716464677926
}
#Debug simulation 
Total elapsed time: 152.65467344410717. Arrivals time: 0.4507967736572027 Scheduler time: 151.95414577424526 Scheduler overhead time: 0.09581923205405474 Adapter cache time: 0.018878226168453693 Engine time: 0.09895051782950759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 154.39336377941072,
    "estimated_duration": 3600.026709204569,
    "input_throughput": 6778.678040805972,
    "output_throughput": 5970.5082034652805,
    "total_throughput": 12749.186244271254,
    "itl": 92.50059641071297,
    "ttft": 1165099.607707928,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6365857942216114,
    "arrivals": 133259,
    "finished_requests": 98101,
    "scheduler_time": 180.4424004481553
}
#Debug simulation 
Total elapsed time: 154.3935408210382. Arrivals time: 0.4536884888075292 Scheduler time: 153.6886357134208 Scheduler overhead time: 0.09638642752543092 Adapter cache time: 0.019805322866886854 Engine time: 0.09805198386311531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 152.72668475890532,
    "estimated_duration": 3600.028419608626,
    "input_throughput": 6778.674820198502,
    "output_throughput": 5970.50536682616,
    "total_throughput": 12749.180187024662,
    "itl": 92.50065570402361,
    "ttft": 1165099.989175618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6376655972935286,
    "arrivals": 133259,
    "finished_requests": 98101,
    "scheduler_time": 180.4424655239664
}
#Debug simulation 
Total elapsed time: 152.7268566940911. Arrivals time: 0.45382813224568963 Scheduler time: 152.02390564745292 Scheduler overhead time: 0.09749920666217804 Adapter cache time: 0.019376407377421856 Engine time: 0.096591932233423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 152.57772105792537,
    "estimated_duration": 3600.1177951392565,
    "input_throughput": 6763.114260559409,
    "output_throughput": 5951.141662344208,
    "total_throughput": 12714.255922903616,
    "itl": 91.77272416815028,
    "ttft": 1168099.3845080035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.612887265752071,
    "arrivals": 133259,
    "finished_requests": 97848,
    "scheduler_time": 181.22529300170862
}
#Debug simulation 
Total elapsed time: 152.57789235608652. Arrivals time: 0.4666790822520852 Scheduler time: 151.8599412641488 Scheduler overhead time: 0.09726102091372013 Adapter cache time: 0.020264323335140944 Engine time: 0.09813129343092442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 152.59508114680648,
    "estimated_duration": 3600.0366803261086,
    "input_throughput": 6778.65926571321,
    "output_throughput": 5970.491666782954,
    "total_throughput": 12749.150932496164,
    "itl": 92.50090910011328,
    "ttft": 1165101.6178241929,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6450850706920047,
    "arrivals": 133259,
    "finished_requests": 98101,
    "scheduler_time": 180.4427412429166
}
#Debug simulation 
Total elapsed time: 152.5952593330294. Arrivals time: 0.4597070412710309 Scheduler time: 151.88735192781314 Scheduler overhead time: 0.09652971429750323 Adapter cache time: 0.01899122539907694 Engine time: 0.09627497103065252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 152.60965265892446,
    "estimated_duration": 3600.080021159942,
    "input_throughput": 6767.745676983537,
    "output_throughput": 5958.655328191373,
    "total_throughput": 12726.40100517491,
    "itl": 91.91357699794287,
    "ttft": 1166957.7930981894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5770797069161206,
    "arrivals": 133259,
    "finished_requests": 97933,
    "scheduler_time": 181.05680130836478
}
#Debug simulation 
Total elapsed time: 152.60982642602175. Arrivals time: 0.4600857347249985 Scheduler time: 151.9002550286241 Scheduler overhead time: 0.096967828925699 Adapter cache time: 0.01968162041157484 Engine time: 0.09694862924516201 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 153.55329648125917,
    "estimated_duration": 3600.006965544966,
    "input_throughput": 6766.112741760403,
    "output_throughput": 5957.218195757996,
    "total_throughput": 12723.3309375184,
    "itl": 91.92510972068831,
    "ttft": 1167062.8231171996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.647589304335417,
    "arrivals": 133259,
    "finished_requests": 97909,
    "scheduler_time": 181.01867727654133
}
#Debug simulation 
Total elapsed time: 153.55347033590078. Arrivals time: 0.4586953353136778 Scheduler time: 152.84354416839778 Scheduler overhead time: 0.09829128393903375 Adapter cache time: 0.01951331878080964 Engine time: 0.0972900465130806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 160.2266800091602,
    "estimated_duration": 3600.052736143716,
    "input_throughput": 6725.792029906918,
    "output_throughput": 5928.357878129705,
    "total_throughput": 12654.149908036623,
    "itl": 90.90496938976096,
    "ttft": 1157193.1074927389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 177,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5417063306760967,
    "arrivals": 132319,
    "finished_requests": 97651,
    "scheduler_time": 180.7046745709906
}
#Debug simulation 
Total elapsed time: 160.22686271276325. Arrivals time: 0.45878314273431897 Scheduler time: 159.51598419528455 Scheduler overhead time: 0.09851824818179011 Adapter cache time: 0.019856475293636322 Engine time: 0.09815293550491333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 160.12262997590005,
    "estimated_duration": 3600.1167734825576,
    "input_throughput": 6724.222719193219,
    "output_throughput": 5926.820529035092,
    "total_throughput": 12651.04324822831,
    "itl": 90.85408911681624,
    "ttft": 1157248.800847627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5804432000685493,
    "arrivals": 132319,
    "finished_requests": 97640,
    "scheduler_time": 180.7770341090219
}
#Debug simulation 
Total elapsed time: 160.12280719168484. Arrivals time: 0.4573543523438275 Scheduler time: 159.41405212692916 Scheduler overhead time: 0.09784781374037266 Adapter cache time: 0.01960566220805049 Engine time: 0.09774152096360922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 161.22732671815902,
    "estimated_duration": 3600.117643148735,
    "input_throughput": 6724.221094849337,
    "output_throughput": 5926.819097316502,
    "total_throughput": 12651.040192165838,
    "itl": 90.85410492678969,
    "ttft": 1157248.9074875421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5815415594354294,
    "arrivals": 132319,
    "finished_requests": 97640,
    "scheduler_time": 180.7770316258772
}
#Debug simulation 
Total elapsed time: 161.22749890573323. Arrivals time: 0.4588504582643509 Scheduler time: 160.5133077064529 Scheduler overhead time: 0.0987117225304246 Adapter cache time: 0.019971817266196012 Engine time: 0.10038556065410376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 160.93467018473893,
    "estimated_duration": 3600.1094553514367,
    "input_throughput": 6725.170803907274,
    "output_throughput": 5928.270310858254,
    "total_throughput": 12653.441114765528,
    "itl": 90.90734440971094,
    "ttft": 1157174.9035742208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5571532669174493,
    "arrivals": 132319,
    "finished_requests": 97652,
    "scheduler_time": 180.70627562735862
}
#Debug simulation 
Total elapsed time: 160.93485260009766. Arrivals time: 0.46771781239658594 Scheduler time: 160.21279928460717 Scheduler overhead time: 0.09930614940822124 Adapter cache time: 0.02015942381694913 Engine time: 0.09910166449844837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 159.97919433796778,
    "estimated_duration": 3600.1234697393274,
    "input_throughput": 6724.210212088314,
    "output_throughput": 5926.809505104267,
    "total_throughput": 12651.019717192581,
    "itl": 90.85419569072894,
    "ttft": 1157249.5628917303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5887095252610758,
    "arrivals": 132319,
    "finished_requests": 97640,
    "scheduler_time": 180.77706057748512
}
#Debug simulation 
Total elapsed time: 159.97936341585591. Arrivals time: 0.4816036173142493 Scheduler time: 159.24352157348767 Scheduler overhead time: 0.0992923635058105 Adapter cache time: 0.01951450388878584 Engine time: 0.09884837130084634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 163.20829028170556,
    "estimated_duration": 3600.132771821796,
    "input_throughput": 6722.970660815968,
    "output_throughput": 5926.723360595037,
    "total_throughput": 12649.694021411005,
    "itl": 90.84561694287973,
    "ttft": 1157590.3027909019,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 177,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5292389021976858,
    "arrivals": 132319,
    "finished_requests": 97628,
    "scheduler_time": 180.77992511385233
}
#Debug simulation 
Total elapsed time: 163.20846804510802. Arrivals time: 0.46235305396839976 Scheduler time: 162.49170417990535 Scheduler overhead time: 0.09888437343761325 Adapter cache time: 0.019624431617558002 Engine time: 0.09986842144280672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 161.27508642710745,
    "estimated_duration": 3600.131558667285,
    "input_throughput": 6724.195103848215,
    "output_throughput": 5926.796188497826,
    "total_throughput": 12650.99129234604,
    "itl": 90.85444607641129,
    "ttft": 1157250.973797837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5963805062323819,
    "arrivals": 132319,
    "finished_requests": 97640,
    "scheduler_time": 180.777492475593
}
#Debug simulation 
Total elapsed time: 161.2752635567449. Arrivals time: 0.4686854467727244 Scheduler time: 160.5533827249892 Scheduler overhead time: 0.09854451939463615 Adapter cache time: 0.020057355985045433 Engine time: 0.09796857973560691 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 163.56969879800454,
    "estimated_duration": 3600.0252244374215,
    "input_throughput": 6754.2810075170555,
    "output_throughput": 5927.021803947058,
    "total_throughput": 12681.302811464113,
    "itl": 90.92002279581168,
    "ttft": 1150602.9726889867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5294643797003658,
    "arrivals": 131875,
    "finished_requests": 97804,
    "scheduler_time": 179.49477713736587
}
#Debug simulation 
Total elapsed time: 163.56987784290686. Arrivals time: 0.4573277700692415 Scheduler time: 162.85740664834157 Scheduler overhead time: 0.09904552111402154 Adapter cache time: 0.019643195904791355 Engine time: 0.09954374143853784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 162.40627002110705,
    "estimated_duration": 3600.070728398123,
    "input_throughput": 6754.164524656254,
    "output_throughput": 5927.002997936746,
    "total_throughput": 12681.167522593,
    "itl": 90.92160802930829,
    "ttft": 1150402.870052835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5752802901156254,
    "arrivals": 131875,
    "finished_requests": 97812,
    "scheduler_time": 179.49631638263475
}
#Debug simulation 
Total elapsed time: 162.4064387138933. Arrivals time: 0.4587339963763952 Scheduler time: 161.6940464237705 Scheduler overhead time: 0.09918936807662249 Adapter cache time: 0.019677691627293825 Engine time: 0.09828645968809724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 161.64819477684796,
    "estimated_duration": 3600.0721710433772,
    "input_throughput": 6754.161818081791,
    "output_throughput": 5927.000622828043,
    "total_throughput": 12681.162440909835,
    "itl": 90.9216224000619,
    "ttft": 1150403.1909810775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5761290167272113,
    "arrivals": 131875,
    "finished_requests": 97812,
    "scheduler_time": 179.49634477610468
}
#Debug simulation 
Total elapsed time: 161.64836861984804. Arrivals time: 0.4551154663786292 Scheduler time: 160.93903807317838 Scheduler overhead time: 0.09885882679373026 Adapter cache time: 0.0198371484875679 Engine time: 0.09897032333537936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 162.7341819088906,
    "estimated_duration": 3600.063783148903,
    "input_throughput": 6754.935874699799,
    "output_throughput": 5927.526645468151,
    "total_throughput": 12682.46252016795,
    "itl": 90.96138025587008,
    "ttft": 1150159.3341777988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.549817497306504,
    "arrivals": 131875,
    "finished_requests": 97812,
    "scheduler_time": 179.51687068500684
}
#Debug simulation 
Total elapsed time: 162.73436111118644. Arrivals time: 0.4578433530405164 Scheduler time: 162.02327912999317 Scheduler overhead time: 0.09802921861410141 Adapter cache time: 0.019853089936077595 Engine time: 0.0983913135714829 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 161.73770642420277,
    "estimated_duration": 3600.078445624396,
    "input_throughput": 6754.1500462450995,
    "output_throughput": 5926.990292651584,
    "total_throughput": 12681.140338896685,
    "itl": 90.92173048365365,
    "ttft": 1150403.8657863885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5829197211936129,
    "arrivals": 131875,
    "finished_requests": 97812,
    "scheduler_time": 179.49650728284593
}
#Debug simulation 
Total elapsed time: 161.7378834998235. Arrivals time: 0.4697327739559114 Scheduler time: 161.01218881085515 Scheduler overhead time: 0.09954673796892166 Adapter cache time: 0.019250495824962854 Engine time: 0.09991837339475751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 163.47152803000063,
    "estimated_duration": 3600.127547695262,
    "input_throughput": 6753.952652473686,
    "output_throughput": 5927.011395376869,
    "total_throughput": 12680.964047850555,
    "itl": 90.91733283909716,
    "ttft": 1150506.5479631298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5232588016078814,
    "arrivals": 131875,
    "finished_requests": 97810,
    "scheduler_time": 179.50067222513468
}
#Debug simulation 
Total elapsed time: 163.47170049604028. Arrivals time: 0.4672978175804019 Scheduler time: 162.7455463297665 Scheduler overhead time: 0.10086150746792555 Adapter cache time: 0.019916816614568233 Engine time: 0.10138046136125922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 161.85974841192365,
    "estimated_duration": 3600.082870910722,
    "input_throughput": 6754.141743922927,
    "output_throughput": 5926.983007088992,
    "total_throughput": 12681.12475101192,
    "itl": 90.92160768514907,
    "ttft": 1150404.328887107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5912194710969932,
    "arrivals": 131875,
    "finished_requests": 97812,
    "scheduler_time": 179.49648329027875
}
#Debug simulation 
Total elapsed time: 161.85991901578382. Arrivals time: 0.4686882244423032 Scheduler time: 161.13766177883372 Scheduler overhead time: 0.09856079239398241 Adapter cache time: 0.019491428043693304 Engine time: 0.09863751567900181 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 168.25762723991647,
    "estimated_duration": 3600.011231012564,
    "input_throughput": 6675.771951200372,
    "output_throughput": 5930.167332837827,
    "total_throughput": 12605.939284038199,
    "itl": 90.26283006153938,
    "ttft": 1132213.7356361388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6335209629940792,
    "arrivals": 129404,
    "finished_requests": 97174,
    "scheduler_time": 176.42809605957856
}
#Debug simulation 
Total elapsed time: 168.25780695909634. Arrivals time: 0.45498748030513525 Scheduler time: 167.54383106390014 Scheduler overhead time: 0.10128881502896547 Adapter cache time: 0.02069183625280857 Engine time: 0.10005214344710112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 170.66869036993012,
    "estimated_duration": 3600.0035054101077,
    "input_throughput": 6675.739888553866,
    "output_throughput": 5929.923114774327,
    "total_throughput": 12605.663003328193,
    "itl": 90.25993858657753,
    "ttft": 1132361.3754530353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.67491796967108,
    "arrivals": 129404,
    "finished_requests": 97170,
    "scheduler_time": 176.42786709290604
}
#Debug simulation 
Total elapsed time: 170.66887250356376. Arrivals time: 0.4479411877691746 Scheduler time: 169.9615259328857 Scheduler overhead time: 0.10122399684041739 Adapter cache time: 0.020361829083412886 Engine time: 0.10058523016050458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 168.47238940466195,
    "estimated_duration": 3600.0045684538454,
    "input_throughput": 6675.737917277623,
    "output_throughput": 5929.9213637299845,
    "total_throughput": 12605.659281007607,
    "itl": 90.2599697074238,
    "ttft": 1132361.526412566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6762113084457844,
    "arrivals": 129404,
    "finished_requests": 97170,
    "scheduler_time": 176.42786300791238
}
#Debug simulation 
Total elapsed time: 168.47256565559655. Arrivals time: 0.4628851115703583 Scheduler time: 167.74821201106533 Scheduler overhead time: 0.10100950999185443 Adapter cache time: 0.020770103204995394 Engine time: 0.10244180588051677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 167.94234724296257,
    "estimated_duration": 3600.0470624979052,
    "input_throughput": 6675.91494854631,
    "output_throughput": 5930.196641703604,
    "total_throughput": 12606.111590249913,
    "itl": 90.26019444106414,
    "ttft": 1132249.555633915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6459077020618147,
    "arrivals": 129404,
    "finished_requests": 97174,
    "scheduler_time": 176.43251966609833
}
#Debug simulation 
Total elapsed time: 167.9425236256793. Arrivals time: 0.4525934671983123 Scheduler time: 167.23306022165343 Scheduler overhead time: 0.1001172992400825 Adapter cache time: 0.020644138101488352 Engine time: 0.09968566242605448 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 167.02456000167876,
    "estimated_duration": 3600.016461773776,
    "input_throughput": 6675.715862743243,
    "output_throughput": 5929.901773138471,
    "total_throughput": 12605.617635881714,
    "itl": 90.26050451366278,
    "ttft": 1132361.721192808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6851398272812389,
    "arrivals": 129404,
    "finished_requests": 97170,
    "scheduler_time": 176.42823455984697
}
#Debug simulation 
Total elapsed time: 167.02474625781178. Arrivals time: 0.46021797927096486 Scheduler time: 166.3105808729306 Scheduler overhead time: 0.09743910748511553 Adapter cache time: 0.020008109975606203 Engine time: 0.09969914425164461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 167.8675975431688,
    "estimated_duration": 3600.064442577565,
    "input_throughput": 6675.792720752718,
    "output_throughput": 5930.348286964037,
    "total_throughput": 12606.141007716755,
    "itl": 90.26316620638649,
    "ttft": 1132130.5618777457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6189404110447512,
    "arrivals": 129404,
    "finished_requests": 97177,
    "scheduler_time": 176.4327853860041
}
#Debug simulation 
Total elapsed time: 167.8677707761526. Arrivals time: 0.45751402247697115 Scheduler time: 167.15218845382333 Scheduler overhead time: 0.10030574956908822 Adapter cache time: 0.01972873229533434 Engine time: 0.10079849697649479 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 167.48316627694294,
    "estimated_duration": 3600.022991051482,
    "input_throughput": 6675.703755153135,
    "output_throughput": 5929.891018213977,
    "total_throughput": 12605.594773367111,
    "itl": 90.26065743459128,
    "ttft": 1132362.8358300892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6934395771846195,
    "arrivals": 129404,
    "finished_requests": 97170,
    "scheduler_time": 176.4285653784976
}
#Debug simulation 
Total elapsed time: 167.48334324266762. Arrivals time: 0.4653232656419277 Scheduler time: 166.7613251232542 Scheduler overhead time: 0.09970394987612963 Adapter cache time: 0.0201933397911489 Engine time: 0.10007557459175587 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 164.61208811914548,
    "estimated_duration": 3600.12526419672,
    "input_throughput": 6711.3248086913645,
    "output_throughput": 5907.106403072634,
    "total_throughput": 12618.431211763998,
    "itl": 89.22678664016684,
    "ttft": 1143555.9052006423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.636581450738012,
    "arrivals": 128419,
    "finished_requests": 97068,
    "scheduler_time": 175.59120725777527
}
#Debug simulation 
Total elapsed time: 164.61227861698717. Arrivals time: 0.4438717416487634 Scheduler time: 163.91127427620813 Scheduler overhead time: 0.09972968278452754 Adapter cache time: 0.0209918562322855 Engine time: 0.0998614951968193 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 165.94926108093932,
    "estimated_duration": 3600.027983110519,
    "input_throughput": 6711.1745001284025,
    "output_throughput": 5907.122972312504,
    "total_throughput": 12618.297472440907,
    "itl": 89.22736579787868,
    "ttft": 1143586.603003704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6783166152844223,
    "arrivals": 128419,
    "finished_requests": 97065,
    "scheduler_time": 175.58531357263496
}
#Debug simulation 
Total elapsed time: 165.94944216869771. Arrivals time: 0.4559280569665134 Scheduler time: 165.23225810285658 Scheduler overhead time: 0.10182241024449468 Adapter cache time: 0.020130771212279797 Engine time: 0.10151935741305351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 165.3751438576728,
    "estimated_duration": 3600.029070563797,
    "input_throughput": 6711.172472897909,
    "output_throughput": 5907.12118796018,
    "total_throughput": 12618.29366085809,
    "itl": 89.22738742062062,
    "ttft": 1143586.7500202865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6795920747891098,
    "arrivals": 128419,
    "finished_requests": 97065,
    "scheduler_time": 175.585351776454
}
#Debug simulation 
Total elapsed time: 165.37532980693504. Arrivals time: 0.4520222940482199 Scheduler time: 164.66484010824934 Scheduler overhead time: 0.10033344849944115 Adapter cache time: 0.02019357541576028 Engine time: 0.10082694655284286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 165.44182262802497,
    "estimated_duration": 3600.138932891244,
    "input_throughput": 6711.299327716776,
    "output_throughput": 5907.083975484573,
    "total_throughput": 12618.38330320135,
    "itl": 89.2267024818124,
    "ttft": 1143558.934437756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6476719664013956,
    "arrivals": 128419,
    "finished_requests": 97068,
    "scheduler_time": 175.59232038206085
}
#Debug simulation 
Total elapsed time: 165.44200886692852. Arrivals time: 0.44908014917746186 Scheduler time: 164.7339476281777 Scheduler overhead time: 0.10081292223185301 Adapter cache time: 0.019964193925261497 Engine time: 0.10049711028113961 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 165.11444293614477,
    "estimated_duration": 3600.0368373997535,
    "input_throughput": 6711.157993997268,
    "output_throughput": 5907.108443745798,
    "total_throughput": 12618.266437743066,
    "itl": 89.22766240488048,
    "ttft": 1143588.0779777868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6890236087702238,
    "arrivals": 128419,
    "finished_requests": 97065,
    "scheduler_time": 175.5853918204476
}
#Debug simulation 
Total elapsed time: 165.1146239452064. Arrivals time: 0.4488910920917988 Scheduler time: 164.4079465540126 Scheduler overhead time: 0.0991551992483437 Adapter cache time: 0.02055057091638446 Engine time: 0.10132322693243623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 164.11229633307084,
    "estimated_duration": 3600.1117703204327,
    "input_throughput": 6711.34996396222,
    "output_throughput": 5907.128543986056,
    "total_throughput": 12618.478507948275,
    "itl": 89.22632491573388,
    "ttft": 1143553.5654675148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6219304613396534,
    "arrivals": 128419,
    "finished_requests": 97068,
    "scheduler_time": 175.59106461507696
}
#Debug simulation 
Total elapsed time: 164.11248418223113. Arrivals time: 0.44983702013269067 Scheduler time: 163.40221019042656 Scheduler overhead time: 0.10083201620727777 Adapter cache time: 0.020101603586226702 Engine time: 0.10240266565233469 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 164.68992021679878,
    "estimated_duration": 3600.010926109237,
    "input_throughput": 6711.129631517928,
    "output_throughput": 5907.15040495261,
    "total_throughput": 12618.280036470538,
    "itl": 89.22842903453689,
    "ttft": 1143590.026536708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 208,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6969460973143597,
    "arrivals": 128419,
    "finished_requests": 97064,
    "scheduler_time": 175.58158659405876
}
#Debug simulation 
Total elapsed time: 164.69009017571807. Arrivals time: 0.44693431025370955 Scheduler time: 163.98523267079145 Scheduler overhead time: 0.10051752533763647 Adapter cache time: 0.020249543245881796 Engine time: 0.09997113281860948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 169.05624761525542,
    "estimated_duration": 3600.0488598581614,
    "input_throughput": 6698.257978907006,
    "output_throughput": 5876.308023450962,
    "total_throughput": 12574.566002357968,
    "itl": 88.19552210973214,
    "ttft": 1148595.193349142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 212,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.648823401713743,
    "arrivals": 127927,
    "finished_requests": 96868,
    "scheduler_time": 175.58519388078003
}
#Debug simulation 
Total elapsed time: 169.05642050830647. Arrivals time: 0.45331577863544226 Scheduler time: 168.33937423909083 Scheduler overhead time: 0.10218266863375902 Adapter cache time: 0.020855430513620377 Engine time: 0.10336291231215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 168.27249145694077,
    "estimated_duration": 3600.0897990215103,
    "input_throughput": 6697.204610438646,
    "output_throughput": 5876.238422094315,
    "total_throughput": 12573.443032532961,
    "itl": 88.1955898562733,
    "ttft": 1148574.48644332,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.650588971993422,
    "arrivals": 127927,
    "finished_requests": 96857,
    "scheduler_time": 175.58488461488773
}
#Debug simulation 
Total elapsed time: 168.27267919573933. Arrivals time: 0.45611702371388674 Scheduler time: 167.5562108112499 Scheduler overhead time: 0.10158214252442122 Adapter cache time: 0.020450180862098932 Engine time: 0.10106395464390516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 167.486867220141,
    "estimated_duration": 3600.088851111941,
    "input_throughput": 6697.206373824108,
    "output_throughput": 5876.2399693179705,
    "total_throughput": 12573.446343142077,
    "itl": 88.19554503300259,
    "ttft": 1148574.2138842188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6515259101614383,
    "arrivals": 127927,
    "finished_requests": 96857,
    "scheduler_time": 175.5848266653523
}
#Debug simulation 
Total elapsed time: 167.4870545049198. Arrivals time: 0.4510754249058664 Scheduler time: 166.77459317212924 Scheduler overhead time: 0.10232836427167058 Adapter cache time: 0.02009177254512906 Engine time: 0.10136478953063488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 166.23885198915377,
    "estimated_duration": 3600.0602952282775,
    "input_throughput": 6697.259496447175,
    "output_throughput": 5876.286579988677,
    "total_throughput": 12573.54607643585,
    "itl": 88.19468040407426,
    "ttft": 1148570.0465545664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6219872997025973,
    "arrivals": 127927,
    "finished_requests": 96857,
    "scheduler_time": 175.58425232547336
}
#Debug simulation 
Total elapsed time: 166.23902722308412. Arrivals time: 0.4496783553622663 Scheduler time: 165.53177009802312 Scheduler overhead time: 0.09950925223529339 Adapter cache time: 0.019695858471095562 Engine time: 0.10083935968577862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 167.86553557682782,
    "estimated_duration": 3600.098023181191,
    "input_throughput": 6697.1893111663,
    "output_throughput": 5876.224998258966,
    "total_throughput": 12573.414309425267,
    "itl": 88.19586973937658,
    "ttft": 1148575.6157852283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.660328675210478,
    "arrivals": 127927,
    "finished_requests": 96857,
    "scheduler_time": 175.5849845272891
}
#Debug simulation 
Total elapsed time: 167.86571461567655. Arrivals time: 0.4460732634179294 Scheduler time: 167.1560379131697 Scheduler overhead time: 0.10398515872657299 Adapter cache time: 0.020829305052757263 Engine time: 0.10198227455839515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 167.45761155802757,
    "estimated_duration": 3600.069873094056,
    "input_throughput": 6698.235548210425,
    "output_throughput": 5876.377055375917,
    "total_throughput": 12574.612603586342,
    "itl": 88.19518500738346,
    "ttft": 1148607.392321582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 212,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6338906625192621,
    "arrivals": 127927,
    "finished_requests": 96869,
    "scheduler_time": 175.5886437477265
}
#Debug simulation 
Total elapsed time: 167.4578023739159. Arrivals time: 0.45295777916908264 Scheduler time: 166.74424767214805 Scheduler overhead time: 0.10146199585869908 Adapter cache time: 0.020630113314837217 Engine time: 0.10077075101435184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 167.77135794144124,
    "estimated_duration": 3600.108348588046,
    "input_throughput": 6697.170103076508,
    "output_throughput": 5876.208144762348,
    "total_throughput": 12573.378247838855,
    "itl": 88.19614853612622,
    "ttft": 1148577.078901042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6686284251138586,
    "arrivals": 127927,
    "finished_requests": 96857,
    "scheduler_time": 175.5854332470418
}
#Debug simulation 
Total elapsed time: 167.7715475652367. Arrivals time: 0.4568157494068146 Scheduler time: 167.05135738337412 Scheduler overhead time: 0.10147307859733701 Adapter cache time: 0.020635137800127268 Engine time: 0.10334076685830951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 166.54256142815575,
    "estimated_duration": 3600.020035140868,
    "input_throughput": 6633.137251155771,
    "output_throughput": 5875.930076364775,
    "total_throughput": 12509.067327520546,
    "itl": 87.36353133614728,
    "ttft": 1151350.2488358521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8079687643982459,
    "arrivals": 126456,
    "finished_requests": 96297,
    "scheduler_time": 174.07562097552534
}
#Debug simulation 
Total elapsed time: 166.54275041725487. Arrivals time: 0.4406511215493083 Scheduler time: 165.8427899205126 Scheduler overhead time: 0.10195252718403935 Adapter cache time: 0.01977725187316537 Engine time: 0.10100572556257248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 166.4109447961673,
    "estimated_duration": 3600.0753365326887,
    "input_throughput": 6633.0353583652495,
    "output_throughput": 5875.839815167136,
    "total_throughput": 12508.875173532386,
    "itl": 87.36510486192357,
    "ttft": 1151358.1868916373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8608774585812394,
    "arrivals": 126456,
    "finished_requests": 96297,
    "scheduler_time": 174.0771628680277
}
#Debug simulation 
Total elapsed time: 166.41112600499764. Arrivals time: 0.4468395090661943 Scheduler time: 165.70292999455705 Scheduler overhead time: 0.1015016115270555 Adapter cache time: 0.01999608986079693 Engine time: 0.10229648044332862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 166.26426563970745,
    "estimated_duration": 3600.076909064578,
    "input_throughput": 6633.032461021697,
    "output_throughput": 5875.837248570444,
    "total_throughput": 12508.86970959214,
    "itl": 87.36516247648058,
    "ttft": 1151358.4176593514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8625072876177771,
    "arrivals": 126456,
    "finished_requests": 96297,
    "scheduler_time": 174.07714320491334
}
#Debug simulation 
Total elapsed time: 166.2644585086964. Arrivals time: 0.43910197634249926 Scheduler time: 165.56960519589484 Scheduler overhead time: 0.09962321352213621 Adapter cache time: 0.01996672246605158 Engine time: 0.09968354227021337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 166.96234128717333,
    "estimated_duration": 3600.037014910079,
    "input_throughput": 6633.105965605316,
    "output_throughput": 5875.902362222897,
    "total_throughput": 12509.008327828213,
    "itl": 87.36393680575718,
    "ttft": 1151353.144866375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8220609033294045,
    "arrivals": 126456,
    "finished_requests": 96297,
    "scheduler_time": 174.07647924843064
}
#Debug simulation 
Total elapsed time: 166.96252208203077. Arrivals time: 0.44586447440087795 Scheduler time: 166.25531675713137 Scheduler overhead time: 0.10090636182576418 Adapter cache time: 0.020436232909560204 Engine time: 0.10262722289189696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 165.9892992428504,
    "estimated_duration": 3600.088664900088,
    "input_throughput": 6633.010801322227,
    "output_throughput": 5875.818061438513,
    "total_throughput": 12508.82886276074,
    "itl": 87.36549492740781,
    "ttft": 1151360.0256809685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8744538973271889,
    "arrivals": 126456,
    "finished_requests": 96297,
    "scheduler_time": 174.0772974624112
}
#Debug simulation 
Total elapsed time: 165.98948600981385. Arrivals time: 0.44918251782655716 Scheduler time: 165.27926723565906 Scheduler overhead time: 0.10153377242386341 Adapter cache time: 0.021103824023157358 Engine time: 0.10049331188201904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 166.31614215299487,
    "estimated_duration": 3600.1330804831587,
    "input_throughput": 6633.218680014768,
    "output_throughput": 5876.043059264059,
    "total_throughput": 12509.261739278827,
    "itl": 87.36460547739615,
    "ttft": 1151257.184372298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7893732778541754,
    "arrivals": 126456,
    "finished_requests": 96303,
    "scheduler_time": 174.08212675618614
}
#Debug simulation 
Total elapsed time: 166.3163285991177. Arrivals time: 0.440632950514555 Scheduler time: 165.6158216362819 Scheduler overhead time: 0.10122480941936374 Adapter cache time: 0.02040154254063964 Engine time: 0.10104343201965094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 166.1368598723784,
    "estimated_duration": 3600.098737954478,
    "input_throughput": 6632.992242198316,
    "output_throughput": 5875.801620935286,
    "total_throughput": 12508.793863133602,
    "itl": 87.36584718480528,
    "ttft": 1151361.2483648623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8845142002403779,
    "arrivals": 126456,
    "finished_requests": 96297,
    "scheduler_time": 174.07758875780965
}
#Debug simulation 
Total elapsed time: 166.13703645206988. Arrivals time: 0.4446989051066339 Scheduler time: 165.43185741640627 Scheduler overhead time: 0.10087285144254565 Adapter cache time: 0.02104158978909254 Engine time: 0.10148715693503618 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 164.12209002533928,
    "estimated_duration": 3600.093564491358,
    "input_throughput": 6582.883909948694,
    "output_throughput": 5864.062869983411,
    "total_throughput": 12446.946779932105,
    "itl": 86.80500030157341,
    "ttft": 1151539.5141042979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7773638869589184,
    "arrivals": 125984,
    "finished_requests": 96026,
    "scheduler_time": 173.22721693231387
}
#Debug simulation 
Total elapsed time: 164.12227541720495. Arrivals time: 0.4412244944833219 Scheduler time: 163.41882327245548 Scheduler overhead time: 0.10242762602865696 Adapter cache time: 0.02089408738538623 Engine time: 0.10185988480225205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 164.19852171419188,
    "estimated_duration": 3600.1151565628693,
    "input_throughput": 6582.699433044136,
    "output_throughput": 5863.819095207697,
    "total_throughput": 12446.518528251832,
    "itl": 86.80613328889604,
    "ttft": 1151582.2268907914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.829751169676896,
    "arrivals": 125984,
    "finished_requests": 96024,
    "scheduler_time": 173.22467727458897
}
#Debug simulation 
Total elapsed time: 164.19870534492657. Arrivals time: 0.43984727654606104 Scheduler time: 163.4964411118999 Scheduler overhead time: 0.10210632206872106 Adapter cache time: 0.020176326856017113 Engine time: 0.10267933085560799 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 164.11770741129294,
    "estimated_duration": 3600.1163591722297,
    "input_throughput": 6582.697234110778,
    "output_throughput": 5863.817136414416,
    "total_throughput": 12446.514370525194,
    "itl": 86.80618266629962,
    "ttft": 1151582.3540657875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8310603566467802,
    "arrivals": 125984,
    "finished_requests": 96024,
    "scheduler_time": 173.22468380198413
}
#Debug simulation 
Total elapsed time: 164.1178802489303. Arrivals time: 0.44463450787588954 Scheduler time: 163.41403931053355 Scheduler overhead time: 0.09974937932565808 Adapter cache time: 0.019848935306072235 Engine time: 0.10250844014808536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 164.19921205099672,
    "estimated_duration": 3600.107010346067,
    "input_throughput": 6582.859323873789,
    "output_throughput": 5864.040968596278,
    "total_throughput": 12446.900292470067,
    "itl": 86.80534296140345,
    "ttft": 1151541.5114789438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7917518050619422,
    "arrivals": 125984,
    "finished_requests": 96026,
    "scheduler_time": 173.22761225872594
}
#Debug simulation 
Total elapsed time: 164.1993712875992. Arrivals time: 0.44196248706430197 Scheduler time: 163.4959618439898 Scheduler overhead time: 0.10201647598296404 Adapter cache time: 0.01985433977097273 Engine time: 0.10259896144270897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 163.6302992058918,
    "estimated_duration": 3600.132418136126,
    "input_throughput": 6582.812865608297,
    "output_throughput": 5863.99958336248,
    "total_throughput": 12446.812448970777,
    "itl": 86.80766803319503,
    "ttft": 1151543.1464134152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.842755458783362,
    "arrivals": 125984,
    "finished_requests": 96026,
    "scheduler_time": 173.225271644271
}
#Debug simulation 
Total elapsed time: 163.63047504099086. Arrivals time: 0.43817083025351167 Scheduler time: 162.93107170052826 Scheduler overhead time: 0.10124820657074451 Adapter cache time: 0.020516869146376848 Engine time: 0.10218379413709044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 162.24965245602652,
    "estimated_duration": 3600.0726890971073,
    "input_throughput": 6582.9220814826585,
    "output_throughput": 5864.096873359146,
    "total_throughput": 12447.018954841804,
    "itl": 86.80491329079425,
    "ttft": 1151536.7722069041,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7594727749051536,
    "arrivals": 125984,
    "finished_requests": 96026,
    "scheduler_time": 173.2263816456646
}
#Debug simulation 
Total elapsed time: 162.24982177466154. Arrivals time: 0.4041032758541405 Scheduler time: 161.59798704041168 Scheduler overhead time: 0.09775200206786394 Adapter cache time: 0.01854249695315957 Engine time: 0.09563366090878844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 161.25336947431788,
    "estimated_duration": 3600.0005486382306,
    "input_throughput": 6582.718719019125,
    "output_throughput": 5863.880217458648,
    "total_throughput": 12446.598936477772,
    "itl": 86.80584614114142,
    "ttft": 1151564.1326607175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8526900079101362,
    "arrivals": 125984,
    "finished_requests": 96021,
    "scheduler_time": 173.21777647072406
}
#Debug simulation 
Total elapsed time: 161.25352272717282. Arrivals time: 0.3958355085924268 Scheduler time: 160.61064642574638 Scheduler overhead time: 0.09714281093329191 Adapter cache time: 0.018590315710753202 Engine time: 0.09591581206768751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 170.82325118407607,
    "estimated_duration": 3600.0205550335886,
    "input_throughput": 6621.05080668841,
    "output_throughput": 5820.355378444361,
    "total_throughput": 12441.406185132771,
    "itl": 84.2855333915548,
    "ttft": 1142736.2783531616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0650497348885992,
    "arrivals": 125093,
    "finished_requests": 95915,
    "scheduler_time": 172.56179463039442
}
#Debug simulation 
Total elapsed time: 170.82341593690217. Arrivals time: 0.3990274746902287 Scheduler time: 170.17291442723945 Scheduler overhead time: 0.09922469919547439 Adapter cache time: 0.01959369843825698 Engine time: 0.09621579805389047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 171.11659468803555,
    "estimated_duration": 3600.053038146085,
    "input_throughput": 6621.068564110627,
    "output_throughput": 5820.392304772963,
    "total_throughput": 12441.46086888359,
    "itl": 84.28829981989075,
    "ttft": 1142697.95250941,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1316542586381597,
    "arrivals": 125093,
    "finished_requests": 95916,
    "scheduler_time": 172.55892660230202
}
#Debug simulation 
Total elapsed time: 171.11675372021273. Arrivals time: 0.39502200903370976 Scheduler time: 170.46446892246604 Scheduler overhead time: 0.10195011645555496 Adapter cache time: 0.01976286992430687 Engine time: 0.09858117904514074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 170.63204695517197,
    "estimated_duration": 3600.054572911019,
    "input_throughput": 6621.065741435678,
    "output_throughput": 5820.389823440019,
    "total_throughput": 12441.455564875698,
    "itl": 84.28841269176527,
    "ttft": 1142698.0406775742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1343507506512167,
    "arrivals": 125093,
    "finished_requests": 95916,
    "scheduler_time": 172.55889592550028
}
#Debug simulation 
Total elapsed time: 170.63218889804557. Arrivals time: 0.4013561853207648 Scheduler time: 169.97524316003546 Scheduler overhead time: 0.10061713820323348 Adapter cache time: 0.019479455426335335 Engine time: 0.09861885709688067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 170.5962559208274,
    "estimated_duration": 3600.043331473148,
    "input_throughput": 6621.009194977182,
    "output_throughput": 5820.3191102774335,
    "total_throughput": 12441.328305254616,
    "itl": 84.28583896533465,
    "ttft": 1142740.3307881383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0890115163335587,
    "arrivals": 125093,
    "finished_requests": 95916,
    "scheduler_time": 172.5621334181042
}
#Debug simulation 
Total elapsed time: 170.5964090419002. Arrivals time: 0.39942054776474833 Scheduler time: 169.94059041002765 Scheduler overhead time: 0.10095353284850717 Adapter cache time: 0.019581210799515247 Engine time: 0.09952638112008572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 171.29097615880892,
    "estimated_duration": 3600.0976236584115,
    "input_throughput": 6620.909067398564,
    "output_throughput": 5820.230779938462,
    "total_throughput": 12441.139847337026,
    "itl": 84.28819440608046,
    "ttft": 1142743.9214694034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1460604386776736,
    "arrivals": 125093,
    "finished_requests": 95915,
    "scheduler_time": 172.56239907834313
}
#Debug simulation 
Total elapsed time: 171.29111859714612. Arrivals time: 0.3971120798960328 Scheduler time: 170.63830019440502 Scheduler overhead time: 0.10073272557929158 Adapter cache time: 0.019492257852107286 Engine time: 0.0989103689789772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 170.68095850432292,
    "estimated_duration": 3600.1346585371884,
    "input_throughput": 6621.175945036882,
    "output_throughput": 5820.462562507104,
    "total_throughput": 12441.638507543985,
    "itl": 84.28693249086905,
    "ttft": 1142685.0599123659,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0405375026259571,
    "arrivals": 125093,
    "finished_requests": 95920,
    "scheduler_time": 172.56909129076618
}
#Debug simulation 
Total elapsed time: 170.68108964106068. Arrivals time: 0.3991640447638929 Scheduler time: 170.02743848180398 Scheduler overhead time: 0.09990303358063102 Adapter cache time: 0.01954163471236825 Engine time: 0.0986524154432118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 170.3665669420734,
    "estimated_duration": 3600.0869886169485,
    "input_throughput": 6621.006124398453,
    "output_throughput": 5820.337415805007,
    "total_throughput": 12441.343540203461,
    "itl": 84.289566429636,
    "ttft": 1142721.863320738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1623938450217288,
    "arrivals": 125093,
    "finished_requests": 95916,
    "scheduler_time": 172.5597136248744
}
#Debug simulation 
Total elapsed time: 170.36669892212376. Arrivals time: 0.39705302100628614 Scheduler time: 169.71657712291926 Scheduler overhead time: 0.09960451116785407 Adapter cache time: 0.019709458108991385 Engine time: 0.09747295640408993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_128_slots_32_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_128_slots_32_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 36.46956594474614,
    "estimated_duration": 3600.028893722574,
    "input_throughput": 5521.418184909653,
    "output_throughput": 4933.559847525128,
    "total_throughput": 10454.978032434781,
    "itl": 58.5746094629686,
    "ttft": 320634.68632237706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 796,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4361482441705204,
    "arrivals": 85226,
    "finished_requests": 80639,
    "scheduler_time": 91.11318788776488
}
#Debug simulation 
Total elapsed time: 36.46969212312251. Arrivals time: 0.24102584412321448 Scheduler time: 35.97832334134728 Scheduler overhead time: 0.09652376500889659 Adapter cache time: 0.021194228902459145 Engine time: 0.09191924287006259 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_128_slots_32_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_128_slots_32_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 34.879199327901006,
    "estimated_duration": 3600.0242245689733,
    "input_throughput": 5533.554986670988,
    "output_throughput": 4960.87634025248,
    "total_throughput": 10494.431326923468,
    "itl": 59.39070379963751,
    "ttft": 306256.36535996466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 818,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6677283991430936,
    "arrivals": 85226,
    "finished_requests": 80885,
    "scheduler_time": 90.24565861141728
}
#Debug simulation 
Total elapsed time: 34.87933754501864. Arrivals time: 0.2458943324163556 Scheduler time: 34.38684810511768 Scheduler overhead time: 0.0956376944668591 Adapter cache time: 0.021518653258681297 Engine time: 0.08893639128655195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_128_slots_32_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_128_slots_32_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 33.900587115902454,
    "estimated_duration": 3600.0002054907714,
    "input_throughput": 5543.998016877658,
    "output_throughput": 4966.26666096612,
    "total_throughput": 10510.26467784378,
    "itl": 59.26222764785756,
    "ttft": 298081.0430809384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 830,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.711606766469762,
    "arrivals": 85226,
    "finished_requests": 81014,
    "scheduler_time": 89.64738432781365
}
#Debug simulation 
Total elapsed time: 33.90071171987802. Arrivals time: 0.23872698191553354 Scheduler time: 33.416190108750015 Scheduler overhead time: 0.09538710257038474 Adapter cache time: 0.021604823414236307 Engine time: 0.08835157239809632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_128_slots_32_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_128_slots_32_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 36.202583839185536,
    "estimated_duration": 3600.0104228345326,
    "input_throughput": 5525.876779083524,
    "output_throughput": 4953.20399265955,
    "total_throughput": 10479.080771743073,
    "itl": 58.23442708617389,
    "ttft": 314741.9965133207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 801,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4988134971004663,
    "arrivals": 85226,
    "finished_requests": 80717,
    "scheduler_time": 90.64459464882474
}
#Debug simulation 
Total elapsed time: 36.20269965706393. Arrivals time: 0.240732965990901 Scheduler time: 35.70785971311852 Scheduler overhead time: 0.09870272222906351 Adapter cache time: 0.021291034296154976 Engine time: 0.09324528556317091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_128_slots_32_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_128_slots_32_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 35.672001658938825,
    "estimated_duration": 3600.012910486245,
    "input_throughput": 5508.917743664791,
    "output_throughput": 4940.875058583478,
    "total_throughput": 10449.792802248268,
    "itl": 58.15166924408083,
    "ttft": 318319.9907787923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 816,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7025223253108606,
    "arrivals": 85226,
    "finished_requests": 80558,
    "scheduler_time": 90.05796784945844
}
#Debug simulation 
Total elapsed time: 35.672154685948044. Arrivals time: 0.24732336355373263 Scheduler time: 35.17387605179101 Scheduler overhead time: 0.09676529839634895 Adapter cache time: 0.021571843419224024 Engine time: 0.09162690304219723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_128_slots_32_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_128_slots_32_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 35.75597751699388,
    "estimated_duration": 3600.0293341463516,
    "input_throughput": 5514.602842731428,
    "output_throughput": 4940.868906618997,
    "total_throughput": 10455.471749350425,
    "itl": 58.47877006820807,
    "ttft": 320575.0625631037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.415960638280909,
    "arrivals": 85226,
    "finished_requests": 80543,
    "scheduler_time": 90.44411858626201
}
#Debug simulation 
Total elapsed time: 35.75608766172081. Arrivals time: 0.24400664353743196 Scheduler time: 35.26154632307589 Scheduler overhead time: 0.09651893051341176 Adapter cache time: 0.02154948329553008 Engine time: 0.09141317242756486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_128_slots_32_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_128_slots_32_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 34.28845547512174,
    "estimated_duration": 3600.05515112879,
    "input_throughput": 5548.50138719039,
    "output_throughput": 4962.128981384854,
    "total_throughput": 10510.630368575245,
    "itl": 59.98192227984583,
    "ttft": 297331.3930980819,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 826,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.76674526847901,
    "arrivals": 85226,
    "finished_requests": 81066,
    "scheduler_time": 89.91348406485595
}
#Debug simulation 
Total elapsed time: 34.28857601201162. Arrivals time: 0.23622035421431065 Scheduler time: 33.80373919662088 Scheduler overhead time: 0.09648421918973327 Adapter cache time: 0.021205143071711063 Engine time: 0.09024151274934411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_128_slots_32_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_128_slots_32_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 29.823536632116884,
    "estimated_duration": 3600.0406275414425,
    "input_throughput": 5398.965737026603,
    "output_throughput": 4764.135956908048,
    "total_throughput": 10163.101693934652,
    "itl": 55.663994113547936,
    "ttft": 254552.6418592984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 864,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.644261410757955,
    "arrivals": 81441,
    "finished_requests": 78136,
    "scheduler_time": 83.0924294310897
}
#Debug simulation 
Total elapsed time: 29.823646232951432. Arrivals time: 0.22314243018627167 Scheduler time: 29.348356500733644 Scheduler overhead time: 0.09780893148854375 Adapter cache time: 0.021949639543890953 Engine time: 0.09102236852049828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_128_slots_32_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_128_slots_32_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 29.658367482014,
    "estimated_duration": 3599.996642833845,
    "input_throughput": 5418.905886713178,
    "output_throughput": 4788.24724303933,
    "total_throughput": 10207.153129752507,
    "itl": 55.925587013982685,
    "ttft": 241335.13634203523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 866,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8251430541253697,
    "arrivals": 81441,
    "finished_requests": 78422,
    "scheduler_time": 82.89150244964989
}
#Debug simulation 
Total elapsed time: 29.658493150025606. Arrivals time: 0.22579093370586634 Scheduler time: 29.181035364046693 Scheduler overhead time: 0.09785701707005501 Adapter cache time: 0.021776416338980198 Engine time: 0.09046796662732959 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_128_slots_32_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_128_slots_32_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 29.690368231851608,
    "estimated_duration": 3600.0121754304814,
    "input_throughput": 5399.318961379817,
    "output_throughput": 4764.633330149478,
    "total_throughput": 10163.952291529295,
    "itl": 55.66887683705995,
    "ttft": 254416.97381876846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 864,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8225058522075237,
    "arrivals": 81441,
    "finished_requests": 78139,
    "scheduler_time": 83.08928444074236
}
#Debug simulation 
Total elapsed time: 29.690479442942888. Arrivals time: 0.22219910053536296 Scheduler time: 29.218118149321526 Scheduler overhead time: 0.09696688689291477 Adapter cache time: 0.021914989221841097 Engine time: 0.09014966851100326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_128_slots_32_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_128_slots_32_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 30.06424709968269,
    "estimated_duration": 3600.0687333938,
    "input_throughput": 5393.085365261054,
    "output_throughput": 4758.486648073146,
    "total_throughput": 10151.5720133342,
    "itl": 55.48593422624922,
    "ttft": 257910.76634391845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 858,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6778268655971353,
    "arrivals": 81441,
    "finished_requests": 78055,
    "scheduler_time": 83.013088521192
}
#Debug simulation 
Total elapsed time: 30.064357570838183. Arrivals time: 0.22074372367933393 Scheduler time: 29.591031856369227 Scheduler overhead time: 0.09799142042174935 Adapter cache time: 0.021530655678361654 Engine time: 0.09129042783752084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_128_slots_32_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_128_slots_32_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 29.504653324838728,
    "estimated_duration": 3600.0314066621704,
    "input_throughput": 5418.853558860257,
    "output_throughput": 4788.201005163507,
    "total_throughput": 10207.054564023765,
    "itl": 55.92841337292707,
    "ttft": 241336.49193876956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 866,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.867879509516064,
    "arrivals": 81441,
    "finished_requests": 78422,
    "scheduler_time": 82.89221366373724
}
#Debug simulation 
Total elapsed time: 29.504746784921736. Arrivals time: 0.2217814801260829 Scheduler time: 29.030747291166335 Scheduler overhead time: 0.09797085728496313 Adapter cache time: 0.02162663033232093 Engine time: 0.09139814786612988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_128_slots_32_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_128_slots_32_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 29.742086447309703,
    "estimated_duration": 3600.021143715828,
    "input_throughput": 5418.869006937113,
    "output_throughput": 4788.214655375001,
    "total_throughput": 10207.083662312114,
    "itl": 55.91829906296149,
    "ttft": 241332.0734628142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 866,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.589383555385229,
    "arrivals": 81441,
    "finished_requests": 78422,
    "scheduler_time": 82.893077636034
}
#Debug simulation 
Total elapsed time: 29.742231958080083. Arrivals time: 0.2277081524953246 Scheduler time: 29.260828962083906 Scheduler overhead time: 0.09818799933418632 Adapter cache time: 0.02186046727001667 Engine time: 0.09221176104620099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_128_slots_32_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_128_slots_32_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 29.502905416302383,
    "estimated_duration": 3600.000106752957,
    "input_throughput": 5418.9006726434245,
    "output_throughput": 4788.24263578915,
    "total_throughput": 10207.143308432574,
    "itl": 55.930014770701995,
    "ttft": 241336.91607702128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 866,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9028390621394173,
    "arrivals": 81441,
    "finished_requests": 78422,
    "scheduler_time": 82.89133009140588
}
#Debug simulation 
Total elapsed time: 29.50300207035616. Arrivals time: 0.22149033844470978 Scheduler time: 29.031114327721298 Scheduler overhead time: 0.09761112974956632 Adapter cache time: 0.02179655386134982 Engine time: 0.08989422908052802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 25.318802510853857,
    "estimated_duration": 3600.0070857614,
    "input_throughput": 5306.817332544052,
    "output_throughput": 4684.898001091828,
    "total_throughput": 9991.71533363588,
    "itl": 53.69051037903601,
    "ttft": 219604.37647974366,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 966,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9564311606391067,
    "arrivals": 79515,
    "finished_requests": 76618,
    "scheduler_time": 77.01261576544516
}
#Debug simulation 
Total elapsed time: 25.31894367514178. Arrivals time: 0.20995766017585993 Scheduler time: 24.852463421877474 Scheduler overhead time: 0.09921900322660804 Adapter cache time: 0.02252731192857027 Engine time: 0.09237284539267421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 24.97586247883737,
    "estimated_duration": 3600.021250026031,
    "input_throughput": 5321.92358582924,
    "output_throughput": 4695.8950589188225,
    "total_throughput": 10017.818644748062,
    "itl": 53.82110692792084,
    "ttft": 210927.00771206553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 980,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2011479851300892,
    "arrivals": 79515,
    "finished_requests": 76820,
    "scheduler_time": 77.06970451797133
}
#Debug simulation 
Total elapsed time: 24.975977099034935. Arrivals time: 0.20919597428292036 Scheduler time: 24.51077707624063 Scheduler overhead time: 0.09934847801923752 Adapter cache time: 0.02266750345006585 Engine time: 0.09184171818196774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 25.01145346602425,
    "estimated_duration": 3600.0292859863475,
    "input_throughput": 5322.019483169465,
    "output_throughput": 4695.95013179682,
    "total_throughput": 10017.969614966285,
    "itl": 53.822149203409566,
    "ttft": 210881.95544729565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 980,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.206243560668047,
    "arrivals": 79515,
    "finished_requests": 76821,
    "scheduler_time": 77.06993038202091
}
#Debug simulation 
Total elapsed time: 25.01156972395256. Arrivals time: 0.2165039675310254 Scheduler time: 24.539969104342163 Scheduler overhead time: 0.0984247769229114 Adapter cache time: 0.022722750902175903 Engine time: 0.09206367284059525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 25.760544528253376,
    "estimated_duration": 3600.065989994838,
    "input_throughput": 5296.150974173486,
    "output_throughput": 4670.326612547485,
    "total_throughput": 9966.477586720972,
    "itl": 54.067525690003514,
    "ttft": 229755.94215911647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 949,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9662652551708444,
    "arrivals": 79515,
    "finished_requests": 76471,
    "scheduler_time": 77.76303385259888
}
#Debug simulation 
Total elapsed time: 25.760676324367523. Arrivals time: 0.21253468189388514 Scheduler time: 25.293416409753263 Scheduler overhead time: 0.09906108770519495 Adapter cache time: 0.022422689478844404 Engine time: 0.0912230578251183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 24.966890714131296,
    "estimated_duration": 3600.021447446408,
    "input_throughput": 5321.923293982046,
    "output_throughput": 4695.894801402197,
    "total_throughput": 10017.818095384244,
    "itl": 53.82473694716256,
    "ttft": 210927.54131889503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 980,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2496286169812105,
    "arrivals": 79515,
    "finished_requests": 76820,
    "scheduler_time": 77.06966710196546
}
#Debug simulation 
Total elapsed time: 24.966982718091458. Arrivals time: 0.2088031736202538 Scheduler time: 24.50432790396735 Scheduler overhead time: 0.09812737768515944 Adapter cache time: 0.022345516830682755 Engine time: 0.0914758276194334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 25.73174736229703,
    "estimated_duration": 3600.0163744560105,
    "input_throughput": 5296.637019569478,
    "output_throughput": 4682.229536399563,
    "total_throughput": 9978.86655596904,
    "itl": 53.853462771662436,
    "ttft": 224454.96475286022,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 947,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8315776292722967,
    "arrivals": 79515,
    "finished_requests": 76528,
    "scheduler_time": 77.19943906453874
}
#Debug simulation 
Total elapsed time: 25.731866138987243. Arrivals time: 0.21241145906969905 Scheduler time: 25.26374299218878 Scheduler overhead time: 0.09914142172783613 Adapter cache time: 0.02245873399078846 Engine time: 0.0920459064655006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 25.447175316046923,
    "estimated_duration": 3600.011110464979,
    "input_throughput": 5330.00132811303,
    "output_throughput": 4702.420764977437,
    "total_throughput": 10032.422093090467,
    "itl": 54.26632332746264,
    "ttft": 208123.44891416802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 953,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1963312814012523,
    "arrivals": 79515,
    "finished_requests": 76932,
    "scheduler_time": 77.5018957975751
}
#Debug simulation 
Total elapsed time: 25.44731112709269. Arrivals time: 0.21668993728235364 Scheduler time: 24.974471523892134 Scheduler overhead time: 0.09934600442647934 Adapter cache time: 0.0224464344792068 Engine time: 0.09230354987084866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 21.66118635283783,
    "estimated_duration": 3600.025881852516,
    "input_throughput": 5350.214868479958,
    "output_throughput": 4658.865949977387,
    "total_throughput": 10009.080818457343,
    "itl": 52.30946961030052,
    "ttft": 171309.34119161533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.17372579045834,
    "arrivals": 78583,
    "finished_requests": 76462,
    "scheduler_time": 72.60145928461661
}
#Debug simulation 
Total elapsed time: 21.661276667844504. Arrivals time: 0.2077244925312698 Scheduler time: 21.195929835550487 Scheduler overhead time: 0.09960864344611764 Adapter cache time: 0.023109183181077242 Engine time: 0.09262839332222939 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 21.436429845169187,
    "estimated_duration": 3600.0009270961773,
    "input_throughput": 5364.31445187905,
    "output_throughput": 4671.301297015118,
    "total_throughput": 10035.615748894168,
    "itl": 53.115629463387656,
    "ttft": 160406.86308961277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1054,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4412070916011066,
    "arrivals": 78583,
    "finished_requests": 76697,
    "scheduler_time": 72.64543733167766
}
#Debug simulation 
Total elapsed time: 21.43654074938968. Arrivals time: 0.2043843069113791 Scheduler time: 20.977029785048217 Scheduler overhead time: 0.0987693197093904 Adapter cache time: 0.02271875273436308 Engine time: 0.09153450420126319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 21.87758846115321,
    "estimated_duration": 3600.003016662187,
    "input_throughput": 5346.49468650879,
    "output_throughput": 4650.992491535228,
    "total_throughput": 9997.487178044019,
    "itl": 52.60913225701636,
    "ttft": 173552.76267210068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1031,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.37056870430704,
    "arrivals": 78583,
    "finished_requests": 76437,
    "scheduler_time": 72.81099941117115
}
#Debug simulation 
Total elapsed time: 21.87771671405062. Arrivals time: 0.20421254308894277 Scheduler time: 21.416724127717316 Scheduler overhead time: 0.09928761422634125 Adapter cache time: 0.0228180973790586 Engine time: 0.09228875581175089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 21.72255055233836,
    "estimated_duration": 3600.0410656117083,
    "input_throughput": 5365.661571062601,
    "output_throughput": 4672.800030168992,
    "total_throughput": 10038.461601231593,
    "itl": 52.85208959208438,
    "ttft": 160694.04282792137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1046,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.269375183966441,
    "arrivals": 78583,
    "finished_requests": 76707,
    "scheduler_time": 72.82098946188104
}
#Debug simulation 
Total elapsed time: 21.72264071414247. Arrivals time: 0.203782444819808 Scheduler time: 21.262885706033558 Scheduler overhead time: 0.09882572572678328 Adapter cache time: 0.02327355882152915 Engine time: 0.09178554965183139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 21.453141503967345,
    "estimated_duration": 3600.0030166779734,
    "input_throughput": 5363.514672223175,
    "output_throughput": 4664.012202826978,
    "total_throughput": 10027.526875050153,
    "itl": 52.641049371292326,
    "ttft": 161520.79485845947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1064,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5232726468890934,
    "arrivals": 78583,
    "finished_requests": 76674,
    "scheduler_time": 72.62656096735384
}
#Debug simulation 
Total elapsed time: 21.45325760357082. Arrivals time: 0.2058576517738402 Scheduler time: 20.99031837610528 Scheduler overhead time: 0.09947610227391124 Adapter cache time: 0.022975420579314232 Engine time: 0.09261131985113025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 21.812399546150118,
    "estimated_duration": 3600.0144917919847,
    "input_throughput": 5345.8487580755045,
    "output_throughput": 4650.2304471743555,
    "total_throughput": 9996.07920524986,
    "itl": 52.64157886944437,
    "ttft": 173848.82466941487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1032,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0857319043389726,
    "arrivals": 78583,
    "finished_requests": 76429,
    "scheduler_time": 72.79554916377703
}
#Debug simulation 
Total elapsed time: 21.81252374825999. Arrivals time: 0.20255501195788383 Scheduler time: 21.35386084858328 Scheduler overhead time: 0.09914391161873937 Adapter cache time: 0.02269723452627659 Engine time: 0.0920755872502923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 21.78923080302775,
    "estimated_duration": 3600.0087147717054,
    "input_throughput": 5355.201480733795,
    "output_throughput": 4662.4514910554635,
    "total_throughput": 10017.652971789257,
    "itl": 52.79768295694019,
    "ttft": 166899.20015088306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1046,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5071586111933453,
    "arrivals": 78583,
    "finished_requests": 76575,
    "scheduler_time": 72.79319599045361
}
#Debug simulation 
Total elapsed time: 21.789333790075034. Arrivals time: 0.21225990261882544 Scheduler time: 21.32299963431433 Scheduler overhead time: 0.09747118037194014 Adapter cache time: 0.02274603070691228 Engine time: 0.09185548638924956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 21.402082523796707,
    "estimated_duration": 3600.029966681473,
    "input_throughput": 5282.551583183151,
    "output_throughput": 4626.5284328600355,
    "total_throughput": 9909.080016043186,
    "itl": 52.24127289258639,
    "ttft": 162538.8954956743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 966,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9564311606391067,
    "arrivals": 78101,
    "finished_requests": 76165,
    "scheduler_time": 71.74242144988378
}
#Debug simulation 
Total elapsed time: 21.402203677687794. Arrivals time: 0.19717054767534137 Scheduler time: 20.946010486688465 Scheduler overhead time: 0.10041296249255538 Adapter cache time: 0.022484965156763792 Engine time: 0.09354110527783632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 21.287945369724184,
    "estimated_duration": 3600.0221929776735,
    "input_throughput": 5289.591280060783,
    "output_throughput": 4625.568151352581,
    "total_throughput": 9915.159431413364,
    "itl": 52.532288892280114,
    "ttft": 158542.04322990804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 975,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1837461617449363,
    "arrivals": 78101,
    "finished_requests": 76263,
    "scheduler_time": 71.92591813821126
}
#Debug simulation 
Total elapsed time: 21.28804343007505. Arrivals time: 0.20863400725647807 Scheduler time: 20.824579810723662 Scheduler overhead time: 0.0986388036981225 Adapter cache time: 0.022397699765861034 Engine time: 0.09160327818244696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 21.72686206828803,
    "estimated_duration": 3600.006324623469,
    "input_throughput": 5271.461572219562,
    "output_throughput": 4614.7718925848285,
    "total_throughput": 9886.23346480439,
    "itl": 51.80407515175995,
    "ttft": 170452.97789496655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 941,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0764171582460116,
    "arrivals": 78101,
    "finished_requests": 76029,
    "scheduler_time": 72.15071089629089
}
#Debug simulation 
Total elapsed time: 21.726955568417907. Arrivals time: 0.1937123592942953 Scheduler time: 21.276810551062226 Scheduler overhead time: 0.09982503205537796 Adapter cache time: 0.02219991199672222 Engine time: 0.0920267403125763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 21.13030767114833,
    "estimated_duration": 3600.025402629883,
    "input_throughput": 5285.3810381726935,
    "output_throughput": 4628.929836946832,
    "total_throughput": 9914.310875119527,
    "itl": 52.35717624210215,
    "ttft": 158553.938132153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 991,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.101245059880861,
    "arrivals": 78101,
    "finished_requests": 76216,
    "scheduler_time": 71.4229315025246
}
#Debug simulation 
Total elapsed time: 21.130431837867945. Arrivals time: 0.20455051911994815 Scheduler time: 20.667985300067812 Scheduler overhead time: 0.10008927248418331 Adapter cache time: 0.022511077113449574 Engine time: 0.09296324476599693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 21.43742169626057,
    "estimated_duration": 3599.9840283864805,
    "input_throughput": 5278.872031139972,
    "output_throughput": 4626.898583065545,
    "total_throughput": 9905.770614205518,
    "itl": 52.336128265127286,
    "ttft": 165561.24443243965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 958,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1750116742402343,
    "arrivals": 78101,
    "finished_requests": 76121,
    "scheduler_time": 71.98240517184543
}
#Debug simulation 
Total elapsed time: 21.437523426953703. Arrivals time: 0.21032109251245856 Scheduler time: 20.97173007717356 Scheduler overhead time: 0.0995545731857419 Adapter cache time: 0.02228176686912775 Engine time: 0.09140735724940896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 21.375721847172827,
    "estimated_duration": 3600.009665441106,
    "input_throughput": 5262.777814703049,
    "output_throughput": 4608.54040456226,
    "total_throughput": 9871.31821926531,
    "itl": 51.76524831735086,
    "ttft": 172729.2269089166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 968,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.89436868546524,
    "arrivals": 78101,
    "finished_requests": 75926,
    "scheduler_time": 71.58194601524009
}
#Debug simulation 
Total elapsed time: 21.375816005282104. Arrivals time: 0.19328537909314036 Scheduler time: 20.926784076262265 Scheduler overhead time: 0.09978504246100783 Adapter cache time: 0.022232546005398035 Engine time: 0.09154539508745074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 21.241036131978035,
    "estimated_duration": 3600.0220374745372,
    "input_throughput": 5285.413200789214,
    "output_throughput": 4626.825843456469,
    "total_throughput": 9912.239044245682,
    "itl": 52.291122520150694,
    "ttft": 161192.5553848468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 971,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2580596398935096,
    "arrivals": 78101,
    "finished_requests": 76187,
    "scheduler_time": 71.68867990234772
}
#Debug simulation 
Total elapsed time: 21.2411691932939. Arrivals time: 0.20885417005047202 Scheduler time: 20.776254550088197 Scheduler overhead time: 0.0996589157730341 Adapter cache time: 0.022324235644191504 Engine time: 0.09172942629083991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_128_slots_32_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_128_slots_32_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 18.772343246731907,
    "estimated_duration": 3599.84161067864,
    "input_throughput": 4943.459441996164,
    "output_throughput": 4429.206538616059,
    "total_throughput": 9372.665980612224,
    "itl": 50.034982413343684,
    "ttft": 150400.45356660566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.917424312234025,
    "arrivals": 73678,
    "finished_requests": 71971,
    "scheduler_time": 66.56896044401772
}
#Debug simulation 
Total elapsed time: 18.772447044029832. Arrivals time: 0.1837280890904367 Scheduler time: 18.32273192424327 Scheduler overhead time: 0.10312399500980973 Adapter cache time: 0.024685877840965986 Engine time: 0.09479138720780611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_128_slots_32_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_128_slots_32_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 18.343814647756517,
    "estimated_duration": 3599.8427883931467,
    "input_throughput": 4943.10253141439,
    "output_throughput": 4429.774281092425,
    "total_throughput": 9372.876812506815,
    "itl": 50.03470614291327,
    "ttft": 149274.55503482535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1300,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.240091738505273,
    "arrivals": 73678,
    "finished_requests": 71966,
    "scheduler_time": 66.27743514483454
}
#Debug simulation 
Total elapsed time: 18.34390677558258. Arrivals time: 0.1971523859538138 Scheduler time: 17.88207703549415 Scheduler overhead time: 0.10262736631557345 Adapter cache time: 0.02479794668033719 Engine time: 0.09412116557359695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_128_slots_32_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_128_slots_32_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 18.29685951769352,
    "estimated_duration": 3599.9025552783955,
    "input_throughput": 4932.993526161343,
    "output_throughput": 4416.082589982934,
    "total_throughput": 9349.076116144277,
    "itl": 49.88316104440512,
    "ttft": 156268.75349897973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2357822632788995,
    "arrivals": 73678,
    "finished_requests": 71810,
    "scheduler_time": 66.05325803849959
}
#Debug simulation 
Total elapsed time: 18.296980269718915. Arrivals time: 0.19326593913137913 Scheduler time: 17.837156736757606 Scheduler overhead time: 0.1032534996047616 Adapter cache time: 0.024877297692000866 Engine time: 0.09500626567751169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_128_slots_32_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_128_slots_32_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 18.339447157923132,
    "estimated_duration": 3599.870307337683,
    "input_throughput": 4943.064744229635,
    "output_throughput": 4429.740418007829,
    "total_throughput": 9372.805162237464,
    "itl": 50.033431900895174,
    "ttft": 149273.92679406397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1300,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.057858226480841,
    "arrivals": 73678,
    "finished_requests": 71966,
    "scheduler_time": 66.27801937904437
}
#Debug simulation 
Total elapsed time: 18.339575053192675. Arrivals time: 0.19004168501123786 Scheduler time: 17.88405447360128 Scheduler overhead time: 0.10279499879106879 Adapter cache time: 0.0246005873195827 Engine time: 0.09460444748401642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_128_slots_32_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_128_slots_32_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 18.556267753709108,
    "estimated_duration": 3599.897243180987,
    "input_throughput": 4953.982515412425,
    "output_throughput": 4431.5748262589905,
    "total_throughput": 9385.557341671414,
    "itl": 50.58573155927705,
    "ttft": 141726.73720264662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.260812785997978,
    "arrivals": 73678,
    "finished_requests": 72142,
    "scheduler_time": 66.6329211833824
}
#Debug simulation 
Total elapsed time: 18.556379751767963. Arrivals time: 0.19097139546647668 Scheduler time: 18.102094823494554 Scheduler overhead time: 0.10132978903129697 Adapter cache time: 0.024532747454941273 Engine time: 0.09430404473096132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_128_slots_32_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_128_slots_32_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 18.48447043215856,
    "estimated_duration": 3599.88047960372,
    "input_throughput": 4929.578662554234,
    "output_throughput": 4411.985645075745,
    "total_throughput": 9341.564307629978,
    "itl": 49.66910589062171,
    "ttft": 161952.0415181891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.845204679244098,
    "arrivals": 73678,
    "finished_requests": 71717,
    "scheduler_time": 66.27770528987612
}
#Debug simulation 
Total elapsed time: 18.48459041491151. Arrivals time: 0.19000338157638907 Scheduler time: 18.030900387559086 Scheduler overhead time: 0.10275390604510903 Adapter cache time: 0.024693596176803112 Engine time: 0.09296398190781474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_128_slots_32_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_128_slots_32_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 18.290025502908975,
    "estimated_duration": 3599.8848510743733,
    "input_throughput": 4944.6595478429235,
    "output_throughput": 4429.100279482966,
    "total_throughput": 9373.759827325888,
    "itl": 49.610320257853374,
    "ttft": 147877.94307045173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.395642332844459,
    "arrivals": 73678,
    "finished_requests": 71994,
    "scheduler_time": 66.23251163775856
}
#Debug simulation 
Total elapsed time: 18.290117343887687. Arrivals time: 0.19014327693730593 Scheduler time: 17.834269097540528 Scheduler overhead time: 0.10370268858969212 Adapter cache time: 0.024784002918750048 Engine time: 0.09404979553073645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 16.296555762644857,
    "estimated_duration": 3600.0271622688206,
    "input_throughput": 4889.611440849893,
    "output_throughput": 4292.640667261179,
    "total_throughput": 9182.252108111072,
    "itl": 47.79255949179617,
    "ttft": 125867.82375785467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.131658454309325,
    "arrivals": 71837,
    "finished_requests": 70439,
    "scheduler_time": 61.603744452334084
}
#Debug simulation 
Total elapsed time: 16.296687808819115. Arrivals time: 0.18650837428867817 Scheduler time: 15.838367071934044 Scheduler overhead time: 0.105754722841084 Adapter cache time: 0.025456322357058525 Engine time: 0.09589675394818187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 16.36715395981446,
    "estimated_duration": 3600.012896780249,
    "input_throughput": 4881.021958481226,
    "output_throughput": 4286.238533700983,
    "total_throughput": 9167.260492182208,
    "itl": 47.87312638405486,
    "ttft": 130111.76589656444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.406346661306419,
    "arrivals": 71837,
    "finished_requests": 70356,
    "scheduler_time": 61.57766519340862
}
#Debug simulation 
Total elapsed time: 16.367245354689658. Arrivals time: 0.18603437719866633 Scheduler time: 15.90927422652021 Scheduler overhead time: 0.10592884477227926 Adapter cache time: 0.025390895549207926 Engine time: 0.09610823914408684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 16.335387258790433,
    "estimated_duration": 3600.004823559457,
    "input_throughput": 4889.641781811706,
    "output_throughput": 4292.667303906675,
    "total_throughput": 9182.309085718382,
    "itl": 47.79982108752067,
    "ttft": 125822.73775625227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4136221808939595,
    "arrivals": 71837,
    "finished_requests": 70439,
    "scheduler_time": 61.604304855471646
}
#Debug simulation 
Total elapsed time: 16.335502098780125. Arrivals time: 0.1927127754315734 Scheduler time: 15.87772070709616 Scheduler overhead time: 0.10051475837826729 Adapter cache time: 0.02491065952926874 Engine time: 0.0954544129781425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 16.19564027292654,
    "estimated_duration": 3600.008889987997,
    "input_throughput": 4886.881821022019,
    "output_throughput": 4284.858863237816,
    "total_throughput": 9171.740684259836,
    "itl": 47.821350357279655,
    "ttft": 125712.1960111428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2521005587046306,
    "arrivals": 71837,
    "finished_requests": 70445,
    "scheduler_time": 61.63081340739734
}
#Debug simulation 
Total elapsed time: 16.195740522816777. Arrivals time: 0.17956016398966312 Scheduler time: 15.745913395658135 Scheduler overhead time: 0.10522221308201551 Adapter cache time: 0.025184714701026678 Engine time: 0.09575484041124582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 16.277815649285913,
    "estimated_duration": 3600.0324907343534,
    "input_throughput": 4886.849784072733,
    "output_throughput": 4284.8307729726685,
    "total_throughput": 9171.680557045402,
    "itl": 47.826531966830245,
    "ttft": 125763.89308002002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.507811377588639,
    "arrivals": 71837,
    "finished_requests": 70445,
    "scheduler_time": 61.6320708969411
}
#Debug simulation 
Total elapsed time: 16.27793031092733. Arrivals time: 0.18965541198849678 Scheduler time: 15.81760895717889 Scheduler overhead time: 0.10480217961594462 Adapter cache time: 0.02530352585017681 Engine time: 0.09608072601258755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 16.22926728799939,
    "estimated_duration": 3600.0492834908227,
    "input_throughput": 4886.8614328915055,
    "output_throughput": 4284.975228184074,
    "total_throughput": 9171.83666107558,
    "itl": 47.816118189068696,
    "ttft": 125708.45548159031,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.069458451361753,
    "arrivals": 71837,
    "finished_requests": 70446,
    "scheduler_time": 61.63112329690104
}
#Debug simulation 
Total elapsed time: 16.229390724096447. Arrivals time: 0.18070671893656254 Scheduler time: 15.776648011989892 Scheduler overhead time: 0.10564336739480495 Adapter cache time: 0.025376944337040186 Engine time: 0.09670805465430021 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 16.257376474794,
    "estimated_duration": 3600.000053099943,
    "input_throughput": 4891.448261184549,
    "output_throughput": 4293.70188111241,
    "total_throughput": 9185.150142296958,
    "itl": 47.8315023654843,
    "ttft": 124298.27740201542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1351,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.530810123942803,
    "arrivals": 71837,
    "finished_requests": 70471,
    "scheduler_time": 61.62729816833777
}
#Debug simulation 
Total elapsed time: 16.257468884810805. Arrivals time: 0.18437899881973863 Scheduler time: 15.801906534936279 Scheduler overhead time: 0.10545183578506112 Adapter cache time: 0.02523092133924365 Engine time: 0.09626904362812638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 14.92219988675788,
    "estimated_duration": 3600.006153948708,
    "input_throughput": 4819.548150207747,
    "output_throughput": 4288.327113848583,
    "total_throughput": 9107.87526405633,
    "itl": 47.575943159344355,
    "ttft": 98353.23109689589,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.247956988578774,
    "arrivals": 70891,
    "finished_requests": 69925,
    "scheduler_time": 59.85922588751143
}
#Debug simulation 
Total elapsed time: 14.922321016900241. Arrivals time: 0.18512155814096332 Scheduler time: 14.469067096244544 Scheduler overhead time: 0.10303854383528233 Adapter cache time: 0.02516218414530158 Engine time: 0.0958256321027875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 15.46648197574541,
    "estimated_duration": 3600.065756879982,
    "input_throughput": 4805.347226488654,
    "output_throughput": 4276.2596684739465,
    "total_throughput": 9081.6068949626,
    "itl": 47.27131769478267,
    "ttft": 112143.49743430698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.318260587612143,
    "arrivals": 70891,
    "finished_requests": 69690,
    "scheduler_time": 60.11627843001832
}
#Debug simulation 
Total elapsed time: 15.466574563179165. Arrivals time: 0.18360021710395813 Scheduler time: 15.00957929994911 Scheduler overhead time: 0.1066473163664341 Adapter cache time: 0.025360362604260445 Engine time: 0.0967364376410842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 15.425130939111114,
    "estimated_duration": 3600.01957257207,
    "input_throughput": 4805.334707566699,
    "output_throughput": 4276.29452831032,
    "total_throughput": 9081.629235877019,
    "itl": 47.27139318026717,
    "ttft": 112092.98365140094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.325713964570252,
    "arrivals": 70891,
    "finished_requests": 69689,
    "scheduler_time": 60.11511166266115
}
#Debug simulation 
Total elapsed time: 15.425236688926816. Arrivals time: 0.19163412461057305 Scheduler time: 14.965763992164284 Scheduler overhead time: 0.10296917799860239 Adapter cache time: 0.02482001855969429 Engine time: 0.0958235408179462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 15.693421823903918,
    "estimated_duration": 3600.0473329109686,
    "input_throughput": 4805.743480603249,
    "output_throughput": 4275.962112851668,
    "total_throughput": 9081.705593454917,
    "itl": 47.729544836927516,
    "ttft": 112544.54404740952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.024967673236919,
    "arrivals": 70891,
    "finished_requests": 69706,
    "scheduler_time": 60.486367945153525
}
#Debug simulation 
Total elapsed time: 15.693511926103383. Arrivals time: 0.17800049064680934 Scheduler time: 15.245116910431534 Scheduler overhead time: 0.10549622494727373 Adapter cache time: 0.024769115261733532 Engine time: 0.09575109323486686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 15.532804552931339,
    "estimated_duration": 3600.0210013674623,
    "input_throughput": 4799.360890793985,
    "output_throughput": 4277.957543622677,
    "total_throughput": 9077.318434416662,
    "itl": 47.49455139648046,
    "ttft": 116644.98810964498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.33686445863916,
    "arrivals": 70891,
    "finished_requests": 69610,
    "scheduler_time": 60.22843757949817
}
#Debug simulation 
Total elapsed time: 15.532898142002523. Arrivals time: 0.18927442096173763 Scheduler time: 15.074936819728464 Scheduler overhead time: 0.10405291663482785 Adapter cache time: 0.024676458444446325 Engine time: 0.09576986590400338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 15.251617643982172,
    "estimated_duration": 3600.0415616563796,
    "input_throughput": 4800.744575862103,
    "output_throughput": 4281.480015166337,
    "total_throughput": 9082.22459102844,
    "itl": 47.15512838705394,
    "ttft": 112376.11544051662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.030587797528026,
    "arrivals": 70891,
    "finished_requests": 69664,
    "scheduler_time": 59.868098237923135
}
#Debug simulation 
Total elapsed time: 15.251713084988296. Arrivals time: 0.17704033106565475 Scheduler time: 14.803247021045536 Scheduler overhead time: 0.10584958596155047 Adapter cache time: 0.02508139004930854 Engine time: 0.09579615434631705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.4-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 15.972341526765376,
    "estimated_duration": 3600.0230558149115,
    "input_throughput": 4810.169749337935,
    "output_throughput": 4285.618108772806,
    "total_throughput": 9095.787858110742,
    "itl": 47.26818769763772,
    "ttft": 110218.93166629142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.230039353258946,
    "arrivals": 70891,
    "finished_requests": 69771,
    "scheduler_time": 60.68228649934609
}
#Debug simulation 
Total elapsed time: 15.972439763136208. Arrivals time: 0.1904377923347056 Scheduler time: 15.509630530141294 Scheduler overhead time: 0.10656305495649576 Adapter cache time: 0.024994454346597195 Engine time: 0.09634985914453864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 13.545408080331981,
    "estimated_duration": 3600.013959458254,
    "input_throughput": 4759.732932418564,
    "output_throughput": 4248.708525091864,
    "total_throughput": 9008.44145751043,
    "itl": 46.89070415813182,
    "ttft": 98472.21141649473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.116356015589661,
    "arrivals": 70422,
    "finished_requests": 69328,
    "scheduler_time": 57.54746141458774
}
#Debug simulation 
Total elapsed time: 13.545498944353312. Arrivals time: 0.1743787475861609 Scheduler time: 13.10090701514855 Scheduler overhead time: 0.10490232147276402 Adapter cache time: 0.025250857695937157 Engine time: 0.09588872501626611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 13.597866822965443,
    "estimated_duration": 3600.0261652271242,
    "input_throughput": 4761.682891523792,
    "output_throughput": 4245.9241401195895,
    "total_throughput": 9007.607031643382,
    "itl": 46.922235884894164,
    "ttft": 98215.44744382174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1340,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.376037563038961,
    "arrivals": 70422,
    "finished_requests": 69338,
    "scheduler_time": 57.605919547258424
}
#Debug simulation 
Total elapsed time: 13.597967709880322. Arrivals time: 0.18524131923913956 Scheduler time: 13.14083787985146 Scheduler overhead time: 0.1045618699863553 Adapter cache time: 0.025069117546081543 Engine time: 0.09827297925949097 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 13.653711354825646,
    "estimated_duration": 3600.0492321897846,
    "input_throughput": 4763.029584895424,
    "output_throughput": 4247.961906537005,
    "total_throughput": 9010.991491432429,
    "itl": 46.94359174526676,
    "ttft": 97325.29887232679,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3798062260634625,
    "arrivals": 70422,
    "finished_requests": 69355,
    "scheduler_time": 57.60253606938805
}
#Debug simulation 
Total elapsed time: 13.653832552954555. Arrivals time: 0.17983831278979778 Scheduler time: 13.201414380222559 Scheduler overhead time: 0.10599056025967002 Adapter cache time: 0.025259236805140972 Engine time: 0.09671534318476915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 13.539149823132902,
    "estimated_duration": 3600.0129696235736,
    "input_throughput": 4747.666506820154,
    "output_throughput": 4234.516688864592,
    "total_throughput": 8982.183195684745,
    "itl": 46.69213913975368,
    "ttft": 108515.56630641922,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1340,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.192986860377655,
    "arrivals": 70422,
    "finished_requests": 69122,
    "scheduler_time": 57.35725192619984
}
#Debug simulation 
Total elapsed time: 13.539267382118851. Arrivals time: 0.17847491707652807 Scheduler time: 13.088339609093964 Scheduler overhead time: 0.1056321207433939 Adapter cache time: 0.025163639336824417 Engine time: 0.09730351716279984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 13.603437338024378,
    "estimated_duration": 3600.0096197138964,
    "input_throughput": 4762.811995308684,
    "output_throughput": 4247.737816104862,
    "total_throughput": 9010.549811413546,
    "itl": 46.94681865396794,
    "ttft": 97475.21074543682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.43639542995019,
    "arrivals": 70422,
    "finished_requests": 69351,
    "scheduler_time": 57.60154345845143
}
#Debug simulation 
Total elapsed time: 13.603530101943761. Arrivals time: 0.1805577427148819 Scheduler time: 13.152625313960016 Scheduler overhead time: 0.10506975464522839 Adapter cache time: 0.025242679752409458 Engine time: 0.09570773923769593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 13.828420179430395,
    "estimated_duration": 3600.0458641159526,
    "input_throughput": 4756.151350923043,
    "output_throughput": 4240.200146380229,
    "total_throughput": 8996.351497303272,
    "itl": 46.5674014383506,
    "ttft": 102673.02063002123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9558365401554747,
    "arrivals": 70422,
    "finished_requests": 69267,
    "scheduler_time": 57.767327081084666
}
#Debug simulation 
Total elapsed time: 13.828510815277696. Arrivals time: 0.1769351363182068 Scheduler time: 13.38718214025721 Scheduler overhead time: 0.10069113550707698 Adapter cache time: 0.024115836713463068 Engine time: 0.09555220929905772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 13.674078947864473,
    "estimated_duration": 3600.0307192785067,
    "input_throughput": 4753.003330879705,
    "output_throughput": 4237.627728648221,
    "total_throughput": 8990.631059527925,
    "itl": 46.80485777612518,
    "ttft": 104645.02734449222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1324,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4430790996551695,
    "arrivals": 70422,
    "finished_requests": 69214,
    "scheduler_time": 57.558015579965485
}
#Debug simulation 
Total elapsed time: 13.674168625846505. Arrivals time: 0.17930749105289578 Scheduler time: 13.225869096349925 Scheduler overhead time: 0.1033612871542573 Adapter cache time: 0.024745778646320105 Engine time: 0.09641921892762184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 12.49506689189002,
    "estimated_duration": 3600.0382359627447,
    "input_throughput": 4617.978174210715,
    "output_throughput": 4124.384250054853,
    "total_throughput": 8742.362424265568,
    "itl": 45.770346655387016,
    "ttft": 79657.052916139,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.071228191696713,
    "arrivals": 68030,
    "finished_requests": 67279,
    "scheduler_time": 54.665936878747715
}
#Debug simulation 
Total elapsed time: 12.495155829936266. Arrivals time: 0.17136459751054645 Scheduler time: 12.049233043100685 Scheduler overhead time: 0.10534521657973528 Adapter cache time: 0.02715301513671875 Engine time: 0.09735833760350943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 12.710471886675805,
    "estimated_duration": 3600.012833323306,
    "input_throughput": 4616.032711377728,
    "output_throughput": 4124.501408038876,
    "total_throughput": 8740.534119416605,
    "itl": 46.06194202717663,
    "ttft": 83466.67486709238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1617,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.274559889498522,
    "arrivals": 68030,
    "finished_requests": 67222,
    "scheduler_time": 54.76416501396911
}
#Debug simulation 
Total elapsed time: 12.710564389824867. Arrivals time: 0.17247549165040255 Scheduler time: 12.266695769503713 Scheduler overhead time: 0.10316771268844604 Adapter cache time: 0.026541970670223236 Engine time: 0.09698244417086244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 12.402751537039876,
    "estimated_duration": 3600.0205595169214,
    "input_throughput": 4614.186704041772,
    "output_throughput": 4126.02643635824,
    "total_throughput": 8740.213140400012,
    "itl": 45.737157740264614,
    "ttft": 82204.71734996514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1672,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.463782729059407,
    "arrivals": 68030,
    "finished_requests": 67213,
    "scheduler_time": 54.392634458445485
}
#Debug simulation 
Total elapsed time: 12.402843817137182. Arrivals time: 0.1739094452932477 Scheduler time: 11.951566603500396 Scheduler overhead time: 0.10691858362406492 Adapter cache time: 0.027434994000941515 Engine time: 0.0981019027531147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 12.843810284975916,
    "estimated_duration": 3600.0182322296214,
    "input_throughput": 4612.473584534024,
    "output_throughput": 4122.666620720977,
    "total_throughput": 8735.140205255,
    "itl": 45.210825494132045,
    "ttft": 86143.70751785263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1623,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.070224338036379,
    "arrivals": 68030,
    "finished_requests": 67180,
    "scheduler_time": 54.83483346947116
}
#Debug simulation 
Total elapsed time: 12.843900312669575. Arrivals time: 0.17267532087862492 Scheduler time: 12.401011676061898 Scheduler overhead time: 0.1026038657873869 Adapter cache time: 0.026368903927505016 Engine time: 0.09633745392784476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 12.617608402390033,
    "estimated_duration": 3600.029202054439,
    "input_throughput": 4611.235928454829,
    "output_throughput": 4117.471878156129,
    "total_throughput": 8728.707806610959,
    "itl": 45.76721495695878,
    "ttft": 85967.60518123013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1646,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.449340151976728,
    "arrivals": 68030,
    "finished_requests": 67162,
    "scheduler_time": 54.61393864499677
}
#Debug simulation 
Total elapsed time: 12.617701072245836. Arrivals time: 0.17367176618427038 Scheduler time: 12.164407753385603 Scheduler overhead time: 0.11031018057838082 Adapter cache time: 0.027167249005287886 Engine time: 0.0971803585998714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 12.661441633012146,
    "estimated_duration": 3600.0239587791907,
    "input_throughput": 4615.566504628132,
    "output_throughput": 4123.408668933941,
    "total_throughput": 8738.975173562074,
    "itl": 45.67807677917519,
    "ttft": 83011.52783351913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1612,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.819961075382173,
    "arrivals": 68030,
    "finished_requests": 67224,
    "scheduler_time": 54.73304631344878
}
#Debug simulation 
Total elapsed time: 12.661533385980874. Arrivals time: 0.17697957996279 Scheduler time: 12.208656358532608 Scheduler overhead time: 0.10696524614468217 Adapter cache time: 0.026900448370724916 Engine time: 0.09723752178251743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 12.631899412255734,
    "estimated_duration": 3600.05480865369,
    "input_throughput": 4608.5403922515625,
    "output_throughput": 4116.345107962919,
    "total_throughput": 8724.885500214481,
    "itl": 45.80282975391894,
    "ttft": 89708.61154461186,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1656,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.553072632849125,
    "arrivals": 68030,
    "finished_requests": 67091,
    "scheduler_time": 54.52063146562725
}
#Debug simulation 
Total elapsed time: 12.632001803256571. Arrivals time: 0.17337365355342627 Scheduler time: 12.184018288273364 Scheduler overhead time: 0.10494683124125004 Adapter cache time: 0.027037074323743582 Engine time: 0.0977190094999969 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 11.64196591405198,
    "estimated_duration": 3600.040146896056,
    "input_throughput": 4567.766838427648,
    "output_throughput": 4058.57307246909,
    "total_throughput": 8626.339910896737,
    "itl": 44.221074248197496,
    "ttft": 71771.20357564103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1634,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.0008369735862575,
    "arrivals": 67080,
    "finished_requests": 66394,
    "scheduler_time": 52.209370132716586
}
#Debug simulation 
Total elapsed time: 11.642077049706131. Arrivals time: 0.1762820049189031 Scheduler time: 11.187748583033681 Scheduler overhead time: 0.1081092064268887 Adapter cache time: 0.027175814379006624 Engine time: 0.09722284879535437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.4419259368442,
    "estimated_duration": 3600.045902257531,
    "input_throughput": 4565.820116263662,
    "output_throughput": 4056.7610515304086,
    "total_throughput": 8622.581167794071,
    "itl": 44.1449346156226,
    "ttft": 72519.22954182756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1675,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.466369595932646,
    "arrivals": 67080,
    "finished_requests": 66363,
    "scheduler_time": 51.96802589784097
}
#Debug simulation 
Total elapsed time: 11.442037722095847. Arrivals time: 0.16989438841119409 Scheduler time: 10.992277843877673 Scheduler overhead time: 0.10847687628120184 Adapter cache time: 0.02749440586194396 Engine time: 0.09853756194934249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.501588630024344,
    "estimated_duration": 3600.0525207534474,
    "input_throughput": 4570.376100112135,
    "output_throughput": 4059.9485467898235,
    "total_throughput": 8630.324646901958,
    "itl": 44.336907211023686,
    "ttft": 68982.20094232378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.4502596636861025,
    "arrivals": 67080,
    "finished_requests": 66433,
    "scheduler_time": 52.05650376032537
}
#Debug simulation 
Total elapsed time: 11.50170087814331. Arrivals time: 0.16962266387417912 Scheduler time: 11.05652572773397 Scheduler overhead time: 0.10504757147282362 Adapter cache time: 0.027007649186998606 Engine time: 0.0981798218563199 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 11.558315206784755,
    "estimated_duration": 3600.022402386953,
    "input_throughput": 4565.157424882463,
    "output_throughput": 4056.3455911601254,
    "total_throughput": 8621.503016042589,
    "itl": 44.24856495578352,
    "ttft": 73173.43670178883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1655,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.171626281931408,
    "arrivals": 67080,
    "finished_requests": 66354,
    "scheduler_time": 52.00704283753489
}
#Debug simulation 
Total elapsed time: 11.55840556602925. Arrivals time: 0.17010136507451534 Scheduler time: 11.115720660891384 Scheduler overhead time: 0.10354996984824538 Adapter cache time: 0.02681373944506049 Engine time: 0.09705028450116515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 11.415930456016213,
    "estimated_duration": 3600.038246674628,
    "input_throughput": 4568.012302422158,
    "output_throughput": 4059.592981685585,
    "total_throughput": 8627.605284107744,
    "itl": 44.25522519278126,
    "ttft": 69789.05415968853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.576257080733701,
    "arrivals": 67080,
    "finished_requests": 66406,
    "scheduler_time": 51.9071203220602
}
#Debug simulation 
Total elapsed time: 11.41602039616555. Arrivals time: 0.17290666932240129 Scheduler time: 10.962828824296594 Scheduler overhead time: 0.10872227931395173 Adapter cache time: 0.027745702303946018 Engine time: 0.09830490965396166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 11.53182807797566,
    "estimated_duration": 3600.0341093522197,
    "input_throughput": 4566.961728859394,
    "output_throughput": 4057.878774551004,
    "total_throughput": 8624.840503410398,
    "itl": 44.20393395929795,
    "ttft": 71929.97462009607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.954513338652766,
    "arrivals": 67080,
    "finished_requests": 66381,
    "scheduler_time": 52.064621401881475
}
#Debug simulation 
Total elapsed time: 11.531921084038913. Arrivals time: 0.17031449265778065 Scheduler time: 11.081534683238715 Scheduler overhead time: 0.1079317182302475 Adapter cache time: 0.027491077315062284 Engine time: 0.09949663281440735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.4-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.48273047618568,
    "estimated_duration": 3600.055341579638,
    "input_throughput": 4566.84146216098,
    "output_throughput": 4053.848237120818,
    "total_throughput": 8620.689699281798,
    "itl": 44.28570606879149,
    "ttft": 71403.89273747182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1661,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.5687532283737315,
    "arrivals": 67080,
    "finished_requests": 66391,
    "scheduler_time": 52.08599167158669
}
#Debug simulation 
Total elapsed time: 11.482820167206228. Arrivals time: 0.1703445971943438 Scheduler time: 11.035812970716506 Scheduler overhead time: 0.10691898642107844 Adapter cache time: 0.02735433680936694 Engine time: 0.09733074437826872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 33, 33, 4320, 33, 270, 270, 270, 33, 4320, 270, 33, 4320, 270, 33, 33, 33, 33, 270, 270, 4320, 270, 33, 270, 270, 270, 270, 4320, 270, 33, 270, 33, 4320, 4320, 33, 270, 270, 33, 270, 33, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 33, 270, 4320, 4320, 270, 33, 33, 33, 4320, 270, 270, 270, 270, 4320, 270, 4320, 33, 33, 270, 33, 4320, 4320, 33, 33, 270, 270, 270, 33, 4320, 33, 4320, 270, 33, 4320, 4320, 270, 270, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 270, 4320, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 270, 270, 4320, 4320, 33, 33, 4320, 33, 33, 33, 270, 33]
Prompts retrieved: 198756 . Total input tokens: 44307814 . Total output tokens: 39759755
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 11.065306106116623,
    "estimated_duration": 3600.032298154556,
    "input_throughput": 4555.487462822735,
    "output_throughput": 4046.817859792029,
    "total_throughput": 8602.305322614764,
    "itl": 44.14179673606362,
    "ttft": 71971.33561270603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1523,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.66112283400971,
    "arrivals": 66631,
    "finished_requests": 65886,
    "scheduler_time": 51.216881380724836
}
#Debug simulation 
Total elapsed time: 11.06540336785838. Arrivals time: 0.16828650096431375 Scheduler time: 10.618626100476831 Scheduler overhead time: 0.10839748848229647 Adapter cache time: 0.026375851593911648 Engine time: 0.0984492739662528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 33, 33, 4320, 33, 270, 270, 270, 33, 4320, 270, 33, 4320, 270, 33, 33, 33, 33, 270, 270, 4320, 270, 33, 270, 270, 270, 270, 4320, 270, 33, 270, 33, 4320, 4320, 33, 270, 270, 33, 270, 33, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 33, 270, 4320, 4320, 270, 33, 33, 33, 4320, 270, 270, 270, 270, 4320, 270, 4320, 33, 33, 270, 33, 4320, 4320, 33, 33, 270, 270, 270, 33, 4320, 33, 4320, 270, 33, 4320, 4320, 270, 270, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 270, 4320, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 270, 270, 4320, 4320, 33, 33, 4320, 33, 33, 33, 270, 33]
Prompts retrieved: 198756 . Total input tokens: 44307814 . Total output tokens: 39759755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.173611536622047,
    "estimated_duration": 3600.009153609078,
    "input_throughput": 4553.930365306019,
    "output_throughput": 4037.8152887270326,
    "total_throughput": 8591.745654033051,
    "itl": 44.08911051358128,
    "ttft": 75064.72630835703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1498,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.8840133023377685,
    "arrivals": 66631,
    "finished_requests": 65834,
    "scheduler_time": 51.24495181180458
}
#Debug simulation 
Total elapsed time: 11.173727648798376. Arrivals time: 0.16929693520069122 Scheduler time: 10.726857903879136 Scheduler overhead time: 0.1074638026766479 Adapter cache time: 0.026430165860801935 Engine time: 0.09809712925925851 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 33, 33, 4320, 33, 270, 270, 270, 33, 4320, 270, 33, 4320, 270, 33, 33, 33, 33, 270, 270, 4320, 270, 33, 270, 270, 270, 270, 4320, 270, 33, 270, 33, 4320, 4320, 33, 270, 270, 33, 270, 33, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 33, 270, 4320, 4320, 270, 33, 33, 33, 4320, 270, 270, 270, 270, 4320, 270, 4320, 33, 33, 270, 33, 4320, 4320, 33, 33, 270, 270, 270, 33, 4320, 33, 4320, 270, 33, 4320, 4320, 270, 270, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 270, 4320, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 270, 270, 4320, 4320, 33, 33, 4320, 33, 33, 33, 270, 33]
Prompts retrieved: 198756 . Total input tokens: 44307814 . Total output tokens: 39759755
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.297409477178007,
    "estimated_duration": 3600.042626006908,
    "input_throughput": 4548.843083606471,
    "output_throughput": 4033.390020191427,
    "total_throughput": 8582.233103797898,
    "itl": 43.92556732736389,
    "ttft": 79950.1891891543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1479,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.830855179484876,
    "arrivals": 66631,
    "finished_requests": 65755,
    "scheduler_time": 51.29658168915366
}
#Debug simulation 
Total elapsed time: 11.297514734324068. Arrivals time: 0.16745391301810741 Scheduler time: 10.858071847818792 Scheduler overhead time: 0.1045699194073677 Adapter cache time: 0.025568504352122545 Engine time: 0.09684002911671996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 33, 33, 4320, 33, 270, 270, 270, 33, 4320, 270, 33, 4320, 270, 33, 33, 33, 33, 270, 270, 4320, 270, 33, 270, 270, 270, 270, 4320, 270, 33, 270, 33, 4320, 4320, 33, 270, 270, 33, 270, 33, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 33, 270, 4320, 4320, 270, 33, 33, 33, 4320, 270, 270, 270, 270, 4320, 270, 4320, 33, 33, 270, 33, 4320, 4320, 33, 33, 270, 270, 270, 33, 4320, 33, 4320, 270, 33, 4320, 4320, 270, 270, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 270, 4320, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 270, 270, 4320, 4320, 33, 33, 4320, 33, 33, 33, 270, 33]
Prompts retrieved: 198756 . Total input tokens: 44307814 . Total output tokens: 39759755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 11.137603959999979,
    "estimated_duration": 3600.0094256254292,
    "input_throughput": 4552.531413762261,
    "output_throughput": 4036.7944307466005,
    "total_throughput": 8589.325844508861,
    "itl": 44.06684241592688,
    "ttft": 76531.4289156738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1497,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.682445927280841,
    "arrivals": 66631,
    "finished_requests": 65806,
    "scheduler_time": 51.22431725313773
}
#Debug simulation 
Total elapsed time: 11.137711303774267. Arrivals time: 0.16819350561127067 Scheduler time: 10.693958685267717 Scheduler overhead time: 0.1062293304130435 Adapter cache time: 0.026140414644032717 Engine time: 0.09800639655441046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 33, 33, 4320, 33, 270, 270, 270, 33, 4320, 270, 33, 4320, 270, 33, 33, 33, 33, 270, 270, 4320, 270, 33, 270, 270, 270, 270, 4320, 270, 33, 270, 33, 4320, 4320, 33, 270, 270, 33, 270, 33, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 33, 270, 4320, 4320, 270, 33, 33, 33, 4320, 270, 270, 270, 270, 4320, 270, 4320, 33, 33, 270, 33, 4320, 4320, 33, 33, 270, 270, 270, 33, 4320, 33, 4320, 270, 33, 4320, 4320, 270, 270, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 270, 4320, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 270, 270, 4320, 4320, 33, 33, 4320, 33, 33, 33, 270, 33]
Prompts retrieved: 198756 . Total input tokens: 44307814 . Total output tokens: 39759755
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 11.544112843926996,
    "estimated_duration": 3600.0385730887742,
    "input_throughput": 4546.375175629652,
    "output_throughput": 4030.4384815385147,
    "total_throughput": 8576.813657168168,
    "itl": 43.918604976356214,
    "ttft": 82924.04169256671,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.7674433305673,
    "arrivals": 66631,
    "finished_requests": 65720,
    "scheduler_time": 51.55550066511134
}
#Debug simulation 
Total elapsed time: 11.544208341278136. Arrivals time: 0.1719503290951252 Scheduler time: 11.092880830634385 Scheduler overhead time: 0.10839970642700791 Adapter cache time: 0.02601174684241414 Engine time: 0.09960021125152707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 33, 33, 4320, 33, 270, 270, 270, 33, 4320, 270, 33, 4320, 270, 33, 33, 33, 33, 270, 270, 4320, 270, 33, 270, 270, 270, 270, 4320, 270, 33, 270, 33, 4320, 4320, 33, 270, 270, 33, 270, 33, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 33, 270, 4320, 4320, 270, 33, 33, 33, 4320, 270, 270, 270, 270, 4320, 270, 4320, 33, 33, 270, 33, 4320, 4320, 33, 33, 270, 270, 270, 33, 4320, 33, 4320, 270, 33, 4320, 4320, 270, 270, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 270, 4320, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 270, 270, 4320, 4320, 33, 33, 4320, 33, 33, 33, 270, 33]
Prompts retrieved: 198756 . Total input tokens: 44307814 . Total output tokens: 39759755
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 11.11893462901935,
    "estimated_duration": 3600.0159425511833,
    "input_throughput": 4555.508159327223,
    "output_throughput": 4046.83624530723,
    "total_throughput": 8602.344404634454,
    "itl": 44.139381967137126,
    "ttft": 71970.980135393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1523,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5538465991358885,
    "arrivals": 66631,
    "finished_requests": 65886,
    "scheduler_time": 51.21623566132039
}
#Debug simulation 
Total elapsed time: 11.119030030909926. Arrivals time: 0.17739637522026896 Scheduler time: 10.664035402238369 Scheduler overhead time: 0.10753598110750318 Adapter cache time: 0.026517310179769993 Engine time: 0.09787225537002087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 33, 33, 4320, 33, 270, 270, 270, 33, 4320, 270, 33, 4320, 270, 33, 33, 33, 33, 270, 270, 4320, 270, 33, 270, 270, 270, 270, 4320, 270, 33, 270, 33, 4320, 4320, 33, 270, 270, 33, 270, 33, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 33, 270, 4320, 4320, 270, 33, 33, 33, 4320, 270, 270, 270, 270, 4320, 270, 4320, 33, 33, 270, 33, 4320, 4320, 33, 33, 270, 270, 270, 33, 4320, 33, 4320, 270, 33, 4320, 4320, 270, 270, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 270, 4320, 4320, 270, 4320, 33, 270, 4320, 33, 4320, 270, 33, 270, 270, 4320, 4320, 33, 33, 4320, 33, 33, 33, 270, 33]
Prompts retrieved: 198756 . Total input tokens: 44307814 . Total output tokens: 39759755
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 11.383677030913532,
    "estimated_duration": 3600.0161065797656,
    "input_throughput": 4548.594093807335,
    "output_throughput": 4034.8583367312103,
    "total_throughput": 8583.452430538546,
    "itl": 43.99402039841165,
    "ttft": 80264.0728298765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1468,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.913292902260989,
    "arrivals": 66631,
    "finished_requests": 65754,
    "scheduler_time": 51.42208089544047
}
#Debug simulation 
Total elapsed time: 11.383775662165135. Arrivals time: 0.16913282219320536 Scheduler time: 10.935089320410043 Scheduler overhead time: 0.10846780892461538 Adapter cache time: 0.02631069766357541 Engine time: 0.09935896564275026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-8/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 66, 66, 4320, 66, 135, 135, 135, 66, 4320, 135, 66, 4320, 135, 66, 66, 66, 66, 135, 135, 4320, 135, 66, 135, 135, 135, 135, 4320, 135, 66, 135, 66, 4320, 4320, 66, 135, 135, 66, 135, 66, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 66, 135, 4320, 4320, 135, 66, 66, 66, 4320, 135, 135, 135, 135, 4320, 135, 4320, 66, 66, 135, 66, 4320, 4320, 66, 66, 135, 135, 135, 66, 4320, 66, 4320, 135, 66, 4320, 4320, 135, 135, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 135, 4320, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 135, 135, 4320, 4320, 66, 66, 4320, 66, 66, 66, 135, 66]
Prompts retrieved: 194337 . Total input tokens: 43344562 . Total output tokens: 38859128
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 10.103245648555458,
    "estimated_duration": 3599.8999179372167,
    "input_throughput": 4468.209496562045,
    "output_throughput": 3966.9650061224447,
    "total_throughput": 8435.174502684491,
    "itl": 43.26914873598588,
    "ttft": 60495.96027403776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.104893556879975,
    "arrivals": 65160,
    "finished_requests": 64553,
    "scheduler_time": 48.64089259775016
}
#Debug simulation 
Total elapsed time: 10.103338973596692. Arrivals time: 0.16601671930402517 Scheduler time: 9.660036421846598 Scheduler overhead time: 0.10659837862476707 Adapter cache time: 0.027134353760629892 Engine time: 0.0979046281427145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-16/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 66, 66, 4320, 66, 135, 135, 135, 66, 4320, 135, 66, 4320, 135, 66, 66, 66, 66, 135, 135, 4320, 135, 66, 135, 135, 135, 135, 4320, 135, 66, 135, 66, 4320, 4320, 66, 135, 135, 66, 135, 66, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 66, 135, 4320, 4320, 135, 66, 66, 66, 4320, 135, 135, 135, 135, 4320, 135, 4320, 66, 66, 135, 66, 4320, 4320, 66, 66, 135, 135, 135, 66, 4320, 66, 4320, 135, 66, 4320, 4320, 135, 135, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 135, 4320, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 135, 135, 4320, 4320, 66, 66, 4320, 66, 66, 66, 135, 66]
Prompts retrieved: 194337 . Total input tokens: 43344562 . Total output tokens: 38859128
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 10.047526891808957,
    "estimated_duration": 3599.8850905157205,
    "input_throughput": 4462.623832723098,
    "output_throughput": 3964.14736614707,
    "total_throughput": 8426.771198870167,
    "itl": 43.263820318336265,
    "ttft": 63371.57859301839,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1656,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.405064091826669,
    "arrivals": 65160,
    "finished_requests": 64499,
    "scheduler_time": 48.61477680524499
}
#Debug simulation 
Total elapsed time: 10.047630690038204. Arrivals time: 0.16644174233078957 Scheduler time: 9.601574511732906 Scheduler overhead time: 0.10744865518063307 Adapter cache time: 0.027173619717359543 Engine time: 0.09938461054116488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-8-32/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 66, 66, 4320, 66, 135, 135, 135, 66, 4320, 135, 66, 4320, 135, 66, 66, 66, 66, 135, 135, 4320, 135, 66, 135, 135, 135, 135, 4320, 135, 66, 135, 66, 4320, 4320, 66, 135, 135, 66, 135, 66, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 66, 135, 4320, 4320, 135, 66, 66, 66, 4320, 135, 135, 135, 135, 4320, 135, 4320, 66, 66, 135, 66, 4320, 4320, 66, 66, 135, 135, 135, 66, 4320, 66, 4320, 135, 66, 4320, 4320, 135, 135, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 135, 4320, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 135, 135, 4320, 4320, 66, 66, 4320, 66, 66, 66, 135, 66]
Prompts retrieved: 194337 . Total input tokens: 43344562 . Total output tokens: 38859128
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 10.050765402149409,
    "estimated_duration": 3599.8764185858604,
    "input_throughput": 4470.2470665150095,
    "output_throughput": 3969.831277045309,
    "total_throughput": 8440.078343560319,
    "itl": 43.307293609009434,
    "ttft": 57714.12152604645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1664,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.439771825764271,
    "arrivals": 65160,
    "finished_requests": 64602,
    "scheduler_time": 48.67985836570496
}
#Debug simulation 
Total elapsed time: 10.050870957784355. Arrivals time: 0.16475465334951878 Scheduler time: 9.611595473252237 Scheduler overhead time: 0.10465767560526729 Adapter cache time: 0.02683917200192809 Engine time: 0.09729530988261104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-16/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 66, 66, 4320, 66, 135, 135, 135, 66, 4320, 135, 66, 4320, 135, 66, 66, 66, 66, 135, 135, 4320, 135, 66, 135, 135, 135, 135, 4320, 135, 66, 135, 66, 4320, 4320, 66, 135, 135, 66, 135, 66, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 66, 135, 4320, 4320, 135, 66, 66, 66, 4320, 135, 135, 135, 135, 4320, 135, 4320, 66, 66, 135, 66, 4320, 4320, 66, 66, 135, 135, 135, 66, 4320, 66, 4320, 135, 66, 4320, 4320, 135, 135, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 135, 4320, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 135, 135, 4320, 4320, 66, 66, 4320, 66, 66, 66, 135, 66]
Prompts retrieved: 194337 . Total input tokens: 43344562 . Total output tokens: 38859128
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 10.002877349033952,
    "estimated_duration": 3599.9090141471897,
    "input_throughput": 4466.750391970473,
    "output_throughput": 3966.651919224353,
    "total_throughput": 8433.402311194826,
    "itl": 43.36069755608059,
    "ttft": 60599.09352130447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1667,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.211592838654635,
    "arrivals": 65160,
    "finished_requests": 64543,
    "scheduler_time": 48.54992185620944
}
#Debug simulation 
Total elapsed time: 10.002987843938172. Arrivals time: 0.16307885153219104 Scheduler time: 9.564066018909216 Scheduler overhead time: 0.1046873671002686 Adapter cache time: 0.027192580047994852 Engine time: 0.09834459144622087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_8-16-32/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 66, 66, 4320, 66, 135, 135, 135, 66, 4320, 135, 66, 4320, 135, 66, 66, 66, 66, 135, 135, 4320, 135, 66, 135, 135, 135, 135, 4320, 135, 66, 135, 66, 4320, 4320, 66, 135, 135, 66, 135, 66, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 66, 135, 4320, 4320, 135, 66, 66, 66, 4320, 135, 135, 135, 135, 4320, 135, 4320, 66, 66, 135, 66, 4320, 4320, 66, 66, 135, 135, 135, 66, 4320, 66, 4320, 135, 66, 4320, 4320, 135, 135, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 135, 4320, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 135, 135, 4320, 4320, 66, 66, 4320, 66, 66, 66, 135, 66]
Prompts retrieved: 194337 . Total input tokens: 43344562 . Total output tokens: 38859128
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 10.199525486212224,
    "estimated_duration": 3599.8796649780593,
    "input_throughput": 4467.559334402924,
    "output_throughput": 3965.9422893756687,
    "total_throughput": 8433.501623778593,
    "itl": 43.02664109751451,
    "ttft": 61912.274885037354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.3498580397106075,
    "arrivals": 65160,
    "finished_requests": 64539,
    "scheduler_time": 48.77603920006949
}
#Debug simulation 
Total elapsed time: 10.19961364613846. Arrivals time: 0.16430864157155156 Scheduler time: 9.742711183149368 Scheduler overhead time: 0.12083679484203458 Adapter cache time: 0.02725196909159422 Engine time: 0.09878801601007581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-16/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 66, 66, 4320, 66, 135, 135, 135, 66, 4320, 135, 66, 4320, 135, 66, 66, 66, 66, 135, 135, 4320, 135, 66, 135, 135, 135, 135, 4320, 135, 66, 135, 66, 4320, 4320, 66, 135, 135, 66, 135, 66, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 66, 135, 4320, 4320, 135, 66, 66, 66, 4320, 135, 135, 135, 135, 4320, 135, 4320, 66, 66, 135, 66, 4320, 4320, 66, 66, 135, 135, 135, 66, 4320, 66, 4320, 135, 66, 4320, 4320, 135, 135, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 135, 4320, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 135, 135, 4320, 4320, 66, 66, 4320, 66, 66, 66, 135, 66]
Prompts retrieved: 194337 . Total input tokens: 43344562 . Total output tokens: 38859128
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 10.265057823155075,
    "estimated_duration": 3599.882380029171,
    "input_throughput": 4468.150706598824,
    "output_throughput": 3966.676822336703,
    "total_throughput": 8434.827528935528,
    "itl": 43.16583632609413,
    "ttft": 61961.017270272096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.828931226266879,
    "arrivals": 65160,
    "finished_requests": 64546,
    "scheduler_time": 48.879661481193246
}
#Debug simulation 
Total elapsed time: 10.265156478155404. Arrivals time: 0.16370000503957272 Scheduler time: 9.820341720245779 Scheduler overhead time: 0.11021657241508365 Adapter cache time: 0.027109499089419842 Engine time: 0.09792398381978273 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.00625_size_16-16-32/adapters_128_slots_32_rate_0.4-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 66, 66, 4320, 66, 135, 135, 135, 66, 4320, 135, 66, 4320, 135, 66, 66, 66, 66, 135, 135, 4320, 135, 66, 135, 135, 135, 135, 4320, 135, 66, 135, 66, 4320, 4320, 66, 135, 135, 66, 135, 66, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 66, 135, 4320, 4320, 135, 66, 66, 66, 4320, 135, 135, 135, 135, 4320, 135, 4320, 66, 66, 135, 66, 4320, 4320, 66, 66, 135, 135, 135, 66, 4320, 66, 4320, 135, 66, 4320, 4320, 135, 135, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 135, 4320, 4320, 135, 4320, 66, 135, 4320, 66, 4320, 135, 66, 135, 135, 4320, 4320, 66, 66, 4320, 66, 66, 66, 135, 66]
Prompts retrieved: 194337 . Total input tokens: 43344562 . Total output tokens: 38859128
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 10.027519973926246,
    "estimated_duration": 3599.9037775662355,
    "input_throughput": 4464.205710205073,
    "output_throughput": 3964.9398656009994,
    "total_throughput": 8429.145575806073,
    "itl": 43.27566124531423,
    "ttft": 62505.91139359981,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1661,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.570605233497825,
    "arrivals": 65160,
    "finished_requests": 64515,
    "scheduler_time": 48.628596879998256
}
#Debug simulation 
Total elapsed time: 10.027608358766884. Arrivals time: 0.1638116561807692 Scheduler time: 9.58771642530337 Scheduler overhead time: 0.1060116607695818 Adapter cache time: 0.026933292858302593 Engine time: 0.09738484350964427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 33, 33, 4320, 33, 135, 135, 135, 33, 4320, 135, 33, 4320, 135, 33, 33, 33, 33, 135, 135, 4320, 135, 33, 135, 135, 135, 135, 4320, 135, 33, 135, 33, 4320, 4320, 33, 135, 135, 33, 135, 33, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 33, 135, 4320, 4320, 135, 33, 33, 33, 4320, 135, 135, 135, 135, 4320, 135, 4320, 33, 33, 135, 33, 4320, 4320, 33, 33, 135, 135, 135, 33, 4320, 33, 4320, 135, 33, 4320, 4320, 135, 135, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 135, 4320, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 135, 135, 4320, 4320, 33, 33, 4320, 33, 33, 33, 135, 33]
Prompts retrieved: 192951 . Total input tokens: 43038885 . Total output tokens: 38572296
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 9.043671129271388,
    "estimated_duration": 3599.948990560878,
    "input_throughput": 4437.623155741169,
    "output_throughput": 3921.1716713243486,
    "total_throughput": 8358.794827065518,
    "itl": 42.164162565454745,
    "ttft": 53828.52707796894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1617,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.948808681939399,
    "arrivals": 64686,
    "finished_requests": 64106,
    "scheduler_time": 46.56370973508554
}
#Debug simulation 
Total elapsed time: 9.043762227986008. Arrivals time: 0.16415815940126777 Scheduler time: 8.60168272536248 Scheduler overhead time: 0.10708776256069541 Adapter cache time: 0.027077133301645517 Engine time: 0.09790906263515353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 33, 33, 4320, 33, 135, 135, 135, 33, 4320, 135, 33, 4320, 135, 33, 33, 33, 33, 135, 135, 4320, 135, 33, 135, 135, 135, 135, 4320, 135, 33, 135, 33, 4320, 4320, 33, 135, 135, 33, 135, 33, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 33, 135, 4320, 4320, 135, 33, 33, 33, 4320, 135, 135, 135, 135, 4320, 135, 4320, 33, 33, 135, 33, 4320, 4320, 33, 33, 135, 135, 135, 33, 4320, 33, 4320, 135, 33, 4320, 4320, 135, 135, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 135, 4320, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 135, 135, 4320, 4320, 33, 33, 4320, 33, 33, 33, 135, 33]
Prompts retrieved: 192951 . Total input tokens: 43038885 . Total output tokens: 38572296
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.949350378010422,
    "estimated_duration": 3599.9876114113545,
    "input_throughput": 4437.497215091003,
    "output_throughput": 3920.984326517196,
    "total_throughput": 8358.4815416082,
    "itl": 42.251063858625464,
    "ttft": 53539.62826723832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1645,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.364410227532382,
    "arrivals": 64686,
    "finished_requests": 64104,
    "scheduler_time": 46.46027948280654
}
#Debug simulation 
Total elapsed time: 8.949443424120545. Arrivals time: 0.1627975390292704 Scheduler time: 8.510911913122982 Scheduler overhead time: 0.10536488052457571 Adapter cache time: 0.026986997574567795 Engine time: 0.09769502421841025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 33, 33, 4320, 33, 135, 135, 135, 33, 4320, 135, 33, 4320, 135, 33, 33, 33, 33, 135, 135, 4320, 135, 33, 135, 135, 135, 135, 4320, 135, 33, 135, 33, 4320, 4320, 33, 135, 135, 33, 135, 33, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 33, 135, 4320, 4320, 135, 33, 33, 33, 4320, 135, 135, 135, 135, 4320, 135, 4320, 33, 33, 135, 33, 4320, 4320, 33, 33, 135, 135, 135, 33, 4320, 33, 4320, 135, 33, 4320, 4320, 135, 135, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 135, 4320, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 135, 135, 4320, 4320, 33, 33, 4320, 33, 33, 33, 135, 33]
Prompts retrieved: 192951 . Total input tokens: 43038885 . Total output tokens: 38572296
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.080147085245699,
    "estimated_duration": 3599.9732610149235,
    "input_throughput": 4438.172131171773,
    "output_throughput": 3921.497460239463,
    "total_throughput": 8359.669591411235,
    "itl": 42.262114291057934,
    "ttft": 53909.80905744614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.25348413798952,
    "arrivals": 64686,
    "finished_requests": 64114,
    "scheduler_time": 46.70287951263668
}
#Debug simulation 
Total elapsed time: 9.080245139077306. Arrivals time: 0.16227583680301905 Scheduler time: 8.64443173399195 Scheduler overhead time: 0.10457265563309193 Adapter cache time: 0.026450755540281534 Engine time: 0.09704431938007474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-16/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 33, 33, 4320, 33, 135, 135, 135, 33, 4320, 135, 33, 4320, 135, 33, 33, 33, 33, 135, 135, 4320, 135, 33, 135, 135, 135, 135, 4320, 135, 33, 135, 33, 4320, 4320, 33, 135, 135, 33, 135, 33, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 33, 135, 4320, 4320, 135, 33, 33, 33, 4320, 135, 135, 135, 135, 4320, 135, 4320, 33, 33, 135, 33, 4320, 4320, 33, 33, 135, 135, 135, 33, 4320, 33, 4320, 135, 33, 4320, 4320, 135, 135, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 135, 4320, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 135, 135, 4320, 4320, 33, 33, 4320, 33, 33, 33, 135, 33]
Prompts retrieved: 192951 . Total input tokens: 43038885 . Total output tokens: 38572296
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 9.052364612929523,
    "estimated_duration": 3599.945652171614,
    "input_throughput": 4437.5308806018775,
    "output_throughput": 3921.1747520368044,
    "total_throughput": 8358.705632638681,
    "itl": 42.16593671440519,
    "ttft": 53883.05998992133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1617,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.066584872412381,
    "arrivals": 64686,
    "finished_requests": 64105,
    "scheduler_time": 46.564421506216206
}
#Debug simulation 
Total elapsed time: 9.052464071195573. Arrivals time: 0.1627154857851565 Scheduler time: 8.613559785764664 Scheduler overhead time: 0.1055368254892528 Adapter cache time: 0.026816740166395903 Engine time: 0.09776532184332609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_8-16-32/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 33, 33, 4320, 33, 135, 135, 135, 33, 4320, 135, 33, 4320, 135, 33, 33, 33, 33, 135, 135, 4320, 135, 33, 135, 135, 135, 135, 4320, 135, 33, 135, 33, 4320, 4320, 33, 135, 135, 33, 135, 33, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 33, 135, 4320, 4320, 135, 33, 33, 33, 4320, 135, 135, 135, 135, 4320, 135, 4320, 33, 33, 135, 33, 4320, 4320, 33, 33, 135, 135, 135, 33, 4320, 33, 4320, 135, 33, 4320, 4320, 135, 135, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 135, 4320, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 135, 135, 4320, 4320, 33, 33, 4320, 33, 33, 33, 135, 33]
Prompts retrieved: 192951 . Total input tokens: 43038885 . Total output tokens: 38572296
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 9.033446766901761,
    "estimated_duration": 3599.9635889258448,
    "input_throughput": 4437.6493276607625,
    "output_throughput": 3921.2432712998702,
    "total_throughput": 8358.892598960632,
    "itl": 42.171493521500054,
    "ttft": 53774.00424405566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1617,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.346867956016143,
    "arrivals": 64686,
    "finished_requests": 64107,
    "scheduler_time": 46.56630896221446
}
#Debug simulation 
Total elapsed time: 9.033543339930475. Arrivals time: 0.16413196874782443 Scheduler time: 8.594596422277391 Scheduler overhead time: 0.10489140450954437 Adapter cache time: 0.02664153790101409 Engine time: 0.09774001874029636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-16/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 33, 33, 4320, 33, 135, 135, 135, 33, 4320, 135, 33, 4320, 135, 33, 33, 33, 33, 135, 135, 4320, 135, 33, 135, 135, 135, 135, 4320, 135, 33, 135, 33, 4320, 4320, 33, 135, 135, 33, 135, 33, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 33, 135, 4320, 4320, 135, 33, 33, 33, 4320, 135, 135, 135, 135, 4320, 135, 4320, 33, 33, 135, 33, 4320, 4320, 33, 33, 135, 135, 135, 33, 4320, 33, 4320, 135, 33, 4320, 4320, 135, 135, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 135, 4320, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 135, 135, 4320, 4320, 33, 33, 4320, 33, 33, 33, 135, 33]
Prompts retrieved: 192951 . Total input tokens: 43038885 . Total output tokens: 38572296
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.999791032634676,
    "estimated_duration": 3599.967675049135,
    "input_throughput": 4437.547900977183,
    "output_throughput": 3921.238264953957,
    "total_throughput": 8358.78616593114,
    "itl": 42.16140220881487,
    "ttft": 53825.93419275478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1617,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.834911326856683,
    "arrivals": 64686,
    "finished_requests": 64106,
    "scheduler_time": 46.56312192193512
}
#Debug simulation 
Total elapsed time: 8.999876929912716. Arrivals time: 0.16217654617503285 Scheduler time: 8.56402183137834 Scheduler overhead time: 0.10470201633870602 Adapter cache time: 0.02655636379495263 Engine time: 0.09684227406978607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.0125-0.003125_size_16-16-32/adapters_128_slots_32_rate_0.4-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 135, 4320, 4320, 33, 33, 4320, 33, 135, 135, 135, 33, 4320, 135, 33, 4320, 135, 33, 33, 33, 33, 135, 135, 4320, 135, 33, 135, 135, 135, 135, 4320, 135, 33, 135, 33, 4320, 4320, 33, 135, 135, 33, 135, 33, 135, 135, 4320, 4320, 4320, 4320, 135, 135, 33, 135, 4320, 4320, 135, 33, 33, 33, 4320, 135, 135, 135, 135, 4320, 135, 4320, 33, 33, 135, 33, 4320, 4320, 33, 33, 135, 135, 135, 33, 4320, 33, 4320, 135, 33, 4320, 4320, 135, 135, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 135, 4320, 4320, 135, 4320, 33, 135, 4320, 33, 4320, 135, 33, 135, 135, 4320, 4320, 33, 33, 4320, 33, 33, 33, 135, 33]
Prompts retrieved: 192951 . Total input tokens: 43038885 . Total output tokens: 38572296
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 9.001702466979623,
    "estimated_duration": 3599.9807567524963,
    "input_throughput": 4438.162890185266,
    "output_throughput": 3921.9551308759114,
    "total_throughput": 8360.118021061178,
    "itl": 42.18498376571244,
    "ttft": 53799.62430293354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1618,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.421676873043101,
    "arrivals": 64686,
    "finished_requests": 64107,
    "scheduler_time": 46.568001958847155
}
#Debug simulation 
Total elapsed time: 9.001792673952878. Arrivals time: 0.1633065459318459 Scheduler time: 8.563527683261782 Scheduler overhead time: 0.10482742777094245 Adapter cache time: 0.02655989769846201 Engine time: 0.0978601984679699 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-8/adapters_128_slots_32_rate_0.4-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 66, 4320, 4320, 33, 33, 4320, 33, 66, 66, 66, 33, 4320, 66, 33, 4320, 66, 33, 33, 33, 33, 66, 66, 4320, 66, 33, 66, 66, 66, 66, 4320, 66, 33, 66, 33, 4320, 4320, 33, 66, 66, 33, 66, 33, 66, 66, 4320, 4320, 4320, 4320, 66, 66, 33, 66, 4320, 4320, 66, 33, 33, 33, 4320, 66, 66, 66, 66, 4320, 66, 4320, 33, 33, 66, 33, 4320, 4320, 33, 33, 66, 66, 66, 33, 4320, 33, 4320, 66, 33, 4320, 4320, 66, 66, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 66, 4320, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 66, 66, 4320, 4320, 33, 33, 4320, 33, 33, 33, 66, 33]
Prompts retrieved: 189984 . Total input tokens: 42380508 . Total output tokens: 37972669
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.206420199014246,
    "estimated_duration": 3600.0179345515944,
    "input_throughput": 4400.224189986733,
    "output_throughput": 3865.588797888406,
    "total_throughput": 8265.812987875139,
    "itl": 41.46066373181651,
    "ttft": 43813.944421951564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1494,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.572368689435657,
    "arrivals": 63755,
    "finished_requests": 63284,
    "scheduler_time": 44.54348973008656
}
#Debug simulation 
Total elapsed time: 8.2065046899952. Arrivals time: 0.16140236845239997 Scheduler time: 7.7705642762593925 Scheduler overhead time: 0.10463301604613662 Adapter cache time: 0.025789655279368162 Engine time: 0.09782344102859497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-16/adapters_128_slots_32_rate_0.4-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 66, 4320, 4320, 33, 33, 4320, 33, 66, 66, 66, 33, 4320, 66, 33, 4320, 66, 33, 33, 33, 33, 66, 66, 4320, 66, 33, 66, 66, 66, 66, 4320, 66, 33, 66, 33, 4320, 4320, 33, 66, 66, 33, 66, 33, 66, 66, 4320, 4320, 4320, 4320, 66, 66, 33, 66, 4320, 4320, 66, 33, 33, 33, 4320, 66, 66, 66, 66, 4320, 66, 4320, 33, 33, 66, 33, 4320, 4320, 33, 33, 66, 66, 66, 33, 4320, 33, 4320, 66, 33, 4320, 4320, 66, 66, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 66, 4320, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 66, 66, 4320, 4320, 33, 33, 4320, 33, 33, 33, 66, 33]
Prompts retrieved: 189984 . Total input tokens: 42380508 . Total output tokens: 37972669
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.149935754947364,
    "estimated_duration": 3600.0426167412124,
    "input_throughput": 4401.349008013428,
    "output_throughput": 3864.480919004654,
    "total_throughput": 8265.829927018081,
    "itl": 41.43952747909845,
    "ttft": 43178.4915679368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1503,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.899780744449157,
    "arrivals": 63755,
    "finished_requests": 63294,
    "scheduler_time": 44.549069165183056
}
#Debug simulation 
Total elapsed time: 8.150028786156327. Arrivals time: 0.16568954335525632 Scheduler time: 7.709021154325455 Scheduler overhead time: 0.105576794128865 Adapter cache time: 0.025897805113345385 Engine time: 0.0979022216051817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-8-32/adapters_128_slots_32_rate_0.4-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 66, 4320, 4320, 33, 33, 4320, 33, 66, 66, 66, 33, 4320, 66, 33, 4320, 66, 33, 33, 33, 33, 66, 66, 4320, 66, 33, 66, 66, 66, 66, 4320, 66, 33, 66, 33, 4320, 4320, 33, 66, 66, 33, 66, 33, 66, 66, 4320, 4320, 4320, 4320, 66, 66, 33, 66, 4320, 4320, 66, 33, 33, 33, 4320, 66, 66, 66, 66, 4320, 66, 4320, 33, 33, 66, 33, 4320, 4320, 33, 33, 66, 66, 66, 33, 4320, 33, 4320, 66, 33, 4320, 4320, 66, 66, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 66, 4320, 4320, 66, 4320, 33, 66, 4320, 33, 4320, 66, 33, 66, 66, 4320, 4320, 33, 33, 4320, 33, 33, 33, 66, 33]
Prompts retrieved: 189984 . Total input tokens: 42380508 . Total output tokens: 37972669
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.068818267900497,
    "estimated_duration": 3600.0216388773883,
    "input_throughput": 4400.589660049975,
    "output_throughput": 3863.3556670334483,
    "total_throughput": 8263.945327083424,
    "itl": 41.45616434348857,
    "ttft": 42650.8969276627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.960344334412314,
    "arrivals": 63755,
    "finished_requests": 63297,
    "scheduler_time": 44.48704535273149
}
#Debug simulation 
Total elapsed time: 8.068911441136152. Arrivals time: 0.1596908033825457 Scheduler time: 7.636252368800342 Scheduler overhead time: 0.10390518745407462 Adapter cache time: 0.025946295354515314 Engine time: 0.09738003183156252 
