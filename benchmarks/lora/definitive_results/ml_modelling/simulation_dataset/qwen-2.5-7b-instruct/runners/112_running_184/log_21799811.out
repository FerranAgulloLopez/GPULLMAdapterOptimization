INFO 06-01 00:47:10 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:10 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 28.500213076826185,
    "estimated_duration": 3600.1509209864844,
    "input_throughput": 5384.189836877333,
    "output_throughput": 4749.673937367748,
    "total_throughput": 10133.863774245081,
    "itl": 180.59349970014375,
    "ttft": 1870521.0864663206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 535,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6373609430040441,
    "arrivals": 299534,
    "finished_requests": 78332,
    "scheduler_time": 106.35756891373468
}
#Debug simulation 
Total elapsed time: 28.50042338995263. Arrivals time: 0.340400458779186 Scheduler time: 28.03895386820659 Scheduler overhead time: 0.043641351629048586 Adapter cache time: 0.015650588553398848 Engine time: 0.044960251078009605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 32.37936720903963,
    "estimated_duration": 3600.011035781859,
    "input_throughput": 5383.797940438988,
    "output_throughput": 4746.600727097168,
    "total_throughput": 10130.398667536156,
    "itl": 180.68371177608196,
    "ttft": 1859757.0422244167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 383,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.248155283194506,
    "arrivals": 299534,
    "finished_requests": 78210,
    "scheduler_time": 106.28063896550916
}
#Debug simulation 
Total elapsed time: 32.37954307394102. Arrivals time: 0.363844595849514 Scheduler time: 31.889746569562703 Scheduler overhead time: 0.0462992493994534 Adapter cache time: 0.014282166957855225 Engine time: 0.04754529381170869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 29.028478553984314,
    "estimated_duration": 3600.0857183594076,
    "input_throughput": 5375.963105906202,
    "output_throughput": 4748.970257239076,
    "total_throughput": 10124.933363145277,
    "itl": 178.4630271098624,
    "ttft": 1872755.9007160196,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5924736100621617,
    "arrivals": 299534,
    "finished_requests": 78276,
    "scheduler_time": 106.790457507448
}
#Debug simulation 
Total elapsed time: 29.028590139001608. Arrivals time: 0.3540094248019159 Scheduler time: 28.55128166684881 Scheduler overhead time: 0.0450301356613636 Adapter cache time: 0.015291462652385235 Engine time: 0.04585592541843653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 32.75157233700156,
    "estimated_duration": 3600.098101017614,
    "input_throughput": 5370.266436499367,
    "output_throughput": 4741.245799711747,
    "total_throughput": 10111.512236211114,
    "itl": 181.03230553891865,
    "ttft": 1860786.4099710851,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 376,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1757419210067024,
    "arrivals": 299534,
    "finished_requests": 78075,
    "scheduler_time": 106.13777336514104
}
#Debug simulation 
Total elapsed time: 32.75169798405841. Arrivals time: 0.34678993141278625 Scheduler time: 32.280463964212686 Scheduler overhead time: 0.04600436706095934 Adapter cache time: 0.013894239440560341 Engine time: 0.046744792722165585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 29.04203030373901,
    "estimated_duration": 3600.1072707576204,
    "input_throughput": 5375.930922171407,
    "output_throughput": 4748.941827058977,
    "total_throughput": 10124.872749230382,
    "itl": 178.46349536097998,
    "ttft": 1872764.6964855955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6132229848206083,
    "arrivals": 299534,
    "finished_requests": 78276,
    "scheduler_time": 106.79057019671414
}
#Debug simulation 
Total elapsed time: 29.042176310904324. Arrivals time: 0.3335545468144119 Scheduler time: 28.586041316390038 Scheduler overhead time: 0.04485932597890496 Adapter cache time: 0.014730576425790787 Engine time: 0.04581229807808995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 32.1097979801707,
    "estimated_duration": 3600.1538144233805,
    "input_throughput": 5386.325418183795,
    "output_throughput": 4753.766056170884,
    "total_throughput": 10140.09147435468,
    "itl": 180.58896699514145,
    "ttft": 1868397.5146822361,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 456,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3634629344753806,
    "arrivals": 299534,
    "finished_requests": 78340,
    "scheduler_time": 106.40718425028696
}
#Debug simulation 
Total elapsed time: 32.109958610963076. Arrivals time: 0.3541131727397442 Scheduler time: 31.63033869676292 Scheduler overhead time: 0.04609960690140724 Adapter cache time: 0.015048405155539513 Engine time: 0.04697214858606458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 24.577521453611553,
    "estimated_duration": 3600.197379409742,
    "input_throughput": 5359.970570047958,
    "output_throughput": 4737.279710701921,
    "total_throughput": 10097.25028074988,
    "itl": 178.29346214493395,
    "ttft": 1872784.562774181,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 567,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9006896714493593,
    "arrivals": 299534,
    "finished_requests": 78073,
    "scheduler_time": 106.67017781351099
}
#Debug simulation 
Total elapsed time: 24.57763732271269. Arrivals time: 0.3453423213213682 Scheduler time: 24.113900636322796 Scheduler overhead time: 0.042103104293346405 Adapter cache time: 0.016233792528510094 Engine time: 0.043203390669077635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 33.229579153936356,
    "estimated_duration": 3600.179498997393,
    "input_throughput": 5363.3370239948645,
    "output_throughput": 4746.744712245353,
    "total_throughput": 10110.081736240218,
    "itl": 181.15792681429414,
    "ttft": 1863509.1952563976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 510,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5608487494057226,
    "arrivals": 287913,
    "finished_requests": 78205,
    "scheduler_time": 106.10231683641844
}
#Debug simulation 
Total elapsed time: 33.22979666991159. Arrivals time: 0.35409303894266486 Scheduler time: 32.74878066312522 Scheduler overhead time: 0.04601573385298252 Adapter cache time: 0.016430494375526905 Engine time: 0.04695484507828951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 30.656068125274032,
    "estimated_duration": 3600.0349651825973,
    "input_throughput": 5359.8901640171425,
    "output_throughput": 4740.125350180999,
    "total_throughput": 10100.015514198141,
    "itl": 181.2403076601026,
    "ttft": 1863133.8461905592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.638293275036855,
    "arrivals": 287913,
    "finished_requests": 78166,
    "scheduler_time": 106.0323890082754
}
#Debug simulation 
Total elapsed time: 30.656183551065624. Arrivals time: 0.3512234673835337 Scheduler time: 30.180115913506597 Scheduler overhead time: 0.04542897921055555 Adapter cache time: 0.015953763853758574 Engine time: 0.046058965381234884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 29.09496133774519,
    "estimated_duration": 3600.0430073481066,
    "input_throughput": 5337.374570464965,
    "output_throughput": 4726.731865499237,
    "total_throughput": 10064.106435964202,
    "itl": 179.35901664494688,
    "ttft": 1863705.316158234,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 510,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6658470185287397,
    "arrivals": 287913,
    "finished_requests": 77887,
    "scheduler_time": 106.2292871986024
}
#Debug simulation 
Total elapsed time: 29.095091147813946. Arrivals time: 0.3531863885000348 Scheduler time: 28.617807004600763 Scheduler overhead time: 0.04466398945078254 Adapter cache time: 0.015965977683663368 Engine time: 0.046421713661402464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 33.25605134200305,
    "estimated_duration": 3600.0151097842504,
    "input_throughput": 5363.387211215663,
    "output_throughput": 4746.918687522993,
    "total_throughput": 10110.305898738658,
    "itl": 181.15836631836356,
    "ttft": 1863535.7582317914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 510,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5927524732612004,
    "arrivals": 287913,
    "finished_requests": 78202,
    "scheduler_time": 106.09656570286282
}
#Debug simulation 
Total elapsed time: 33.256218522787094. Arrivals time: 0.3561776801943779 Scheduler time: 32.773126190528274 Scheduler overhead time: 0.04596300609409809 Adapter cache time: 0.016448866110295057 Engine time: 0.04709809785708785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 27.315182036254555,
    "estimated_duration": 3600.064932214495,
    "input_throughput": 5335.907924356147,
    "output_throughput": 4728.687765508814,
    "total_throughput": 10064.595689864962,
    "itl": 179.38095835619208,
    "ttft": 1861764.4097113793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 492,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.629029334131632,
    "arrivals": 287913,
    "finished_requests": 77864,
    "scheduler_time": 106.21299777145019
}
#Debug simulation 
Total elapsed time: 27.31533119501546. Arrivals time: 0.36398602463304996 Scheduler time: 26.82701815618202 Scheduler overhead time: 0.04521290585398674 Adapter cache time: 0.01566908648237586 Engine time: 0.04585168696939945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 33.252688977401704,
    "estimated_duration": 3600.0623024821098,
    "input_throughput": 5369.5748505997035,
    "output_throughput": 4750.6625060928345,
    "total_throughput": 10120.237356692538,
    "itl": 180.96319990953705,
    "ttft": 1864095.7397797974,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.426253990668324,
    "arrivals": 287913,
    "finished_requests": 78300,
    "scheduler_time": 106.20667023612984
}
#Debug simulation 
Total elapsed time: 33.25280308909714. Arrivals time: 0.3523784796707332 Scheduler time: 32.77440656675026 Scheduler overhead time: 0.046006033662706614 Adapter cache time: 0.01625508861616254 Engine time: 0.04646797711029649 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 26.691247678827494,
    "estimated_duration": 3600.057630036578,
    "input_throughput": 5338.071490762426,
    "output_throughput": 4728.987630074973,
    "total_throughput": 10067.059120837399,
    "itl": 179.3270768340731,
    "ttft": 1864043.2451102051,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.939724394157524,
    "arrivals": 287913,
    "finished_requests": 77848,
    "scheduler_time": 106.25478773625953
}
#Debug simulation 
Total elapsed time: 26.6913618799299. Arrivals time: 0.3605201309546828 Scheduler time: 26.207014963962138 Scheduler overhead time: 0.044036800507456064 Adapter cache time: 0.016155991703271866 Engine time: 0.04639245057478547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 29.518858809024096,
    "estimated_duration": 3600.161049275788,
    "input_throughput": 5392.559314507709,
    "output_throughput": 4761.381995243865,
    "total_throughput": 10153.941309751575,
    "itl": 180.07091321069652,
    "ttft": 1852181.376545489,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4812760680634682,
    "arrivals": 282093,
    "finished_requests": 78458,
    "scheduler_time": 106.48149913479749
}
#Debug simulation 
Total elapsed time: 29.51898214640096. Arrivals time: 0.34996352577582 Scheduler time: 29.045178232714534 Scheduler overhead time: 0.04493337916210294 Adapter cache time: 0.015602472238242626 Engine time: 0.04628605814650655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 29.602225795853883,
    "estimated_duration": 3600.056911666961,
    "input_throughput": 5390.645891487867,
    "output_throughput": 4759.831141687577,
    "total_throughput": 10150.477033175443,
    "itl": 180.166054306346,
    "ttft": 1852033.943132347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5807950118626533,
    "arrivals": 282093,
    "finished_requests": 78431,
    "scheduler_time": 106.42442338077721
}
#Debug simulation 
Total elapsed time: 29.602437869179994. Arrivals time: 0.35325249191373587 Scheduler time: 29.12413128092885 Scheduler overhead time: 0.04504855955019593 Adapter cache time: 0.016105955466628075 Engine time: 0.04655430791899562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 36.18128285929561,
    "estimated_duration": 3600.17811120642,
    "input_throughput": 5363.718239353748,
    "output_throughput": 4740.6118455291735,
    "total_throughput": 10104.33008488292,
    "itl": 178.75053563304385,
    "ttft": 1839878.4996110122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 316,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0332484250515754,
    "arrivals": 282093,
    "finished_requests": 78070,
    "scheduler_time": 106.41382787601296
}
#Debug simulation 
Total elapsed time: 36.18144618906081. Arrivals time: 0.39401237526908517 Scheduler time: 35.65932480804622 Scheduler overhead time: 0.04818335035815835 Adapter cache time: 0.013442446943372488 Engine time: 0.04897600458934903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 29.512357359286398,
    "estimated_duration": 3600.0186441640653,
    "input_throughput": 5391.1062464807355,
    "output_throughput": 4760.098958870568,
    "total_throughput": 10151.205205351303,
    "itl": 180.14087531049947,
    "ttft": 1851802.2189729696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 482,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5078052790486203,
    "arrivals": 282093,
    "finished_requests": 78431,
    "scheduler_time": 106.45556528352583
}
#Debug simulation 
Total elapsed time: 29.51249686907977. Arrivals time: 0.3820077176205814 Scheduler time: 29.00356831913814 Scheduler overhead time: 0.04564693383872509 Adapter cache time: 0.016252183355391026 Engine time: 0.047516689635813236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 38.58623204380274,
    "estimated_duration": 3600.068821556159,
    "input_throughput": 5360.824183260393,
    "output_throughput": 4738.963571431232,
    "total_throughput": 10099.787754691624,
    "itl": 178.779068894881,
    "ttft": 1835813.93543798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0266767019033487,
    "arrivals": 282093,
    "finished_requests": 78065,
    "scheduler_time": 106.40080340050606
}
#Debug simulation 
Total elapsed time: 38.58640516316518. Arrivals time: 0.3694737032055855 Scheduler time: 38.08680131286383 Scheduler overhead time: 0.049226779490709305 Adapter cache time: 0.013109651394188404 Engine time: 0.04951552674174309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 29.559930734802037,
    "estimated_duration": 3600.0925927154794,
    "input_throughput": 5391.505218303143,
    "output_throughput": 4760.501725616161,
    "total_throughput": 10152.006943919305,
    "itl": 180.10015422972356,
    "ttft": 1852027.9812469773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 486,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4531644433224427,
    "arrivals": 282093,
    "finished_requests": 78439,
    "scheduler_time": 106.48385237148656
}
#Debug simulation 
Total elapsed time: 29.5600817441009. Arrivals time: 0.37680870248004794 Scheduler time: 29.059357415884733 Scheduler overhead time: 0.04444735823199153 Adapter cache time: 0.015844324603676796 Engine time: 0.046490775886923075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 38.50660802377388,
    "estimated_duration": 3600.0806628287246,
    "input_throughput": 5360.8065506053845,
    "output_throughput": 4738.9479841806715,
    "total_throughput": 10099.754534786056,
    "itl": 178.77920876274408,
    "ttft": 1835819.2776137376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0367515907064133,
    "arrivals": 282093,
    "finished_requests": 78065,
    "scheduler_time": 106.40086041153812
}
#Debug simulation 
Total elapsed time: 38.506747723091394. Arrivals time: 0.3567364579066634 Scheduler time: 38.01949283014983 Scheduler overhead time: 0.04907642165198922 Adapter cache time: 0.013452807441353798 Engine time: 0.05014816951006651 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 24.859707013703883,
    "estimated_duration": 3600.152084683792,
    "input_throughput": 5437.7841656429555,
    "output_throughput": 4777.894820938046,
    "total_throughput": 10215.678986581,
    "itl": 179.15913367562283,
    "ttft": 1845162.8510262459,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5853326513571855,
    "arrivals": 279137,
    "finished_requests": 78982,
    "scheduler_time": 106.86230496275728
}
#Debug simulation 
Total elapsed time: 24.859819928649813. Arrivals time: 0.3450249577872455 Scheduler time: 24.394510532729328 Scheduler overhead time: 0.043402462266385555 Adapter cache time: 0.014958649408072233 Engine time: 0.04490745114162564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 24.96572528174147,
    "estimated_duration": 3600.1544943849503,
    "input_throughput": 5437.139442357212,
    "output_throughput": 4774.750924386846,
    "total_throughput": 10211.890366744059,
    "itl": 179.1664098639402,
    "ttft": 1845393.1391898687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6881770563474912,
    "arrivals": 279137,
    "finished_requests": 78961,
    "scheduler_time": 106.833101001091
}
#Debug simulation 
Total elapsed time: 24.965840477962047. Arrivals time: 0.3424197328276932 Scheduler time: 24.504450250416994 Scheduler overhead time: 0.04254804830998182 Adapter cache time: 0.014657401945441961 Engine time: 0.04474089294672012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 29.300079485867172,
    "estimated_duration": 3600.1532955990688,
    "input_throughput": 5448.742703256426,
    "output_throughput": 4779.5456435242495,
    "total_throughput": 10228.288346780675,
    "itl": 176.89460570774648,
    "ttft": 1846300.0454181004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.161051342505968,
    "arrivals": 279137,
    "finished_requests": 79123,
    "scheduler_time": 107.38173387686791
}
#Debug simulation 
Total elapsed time: 29.30027486057952. Arrivals time: 0.3558796918950975 Scheduler time: 28.81924069672823 Scheduler overhead time: 0.04618680849671364 Adapter cache time: 0.013620934449136257 Engine time: 0.04789200471714139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 29.094903143588454,
    "estimated_duration": 3600.1842640819305,
    "input_throughput": 5456.824306466365,
    "output_throughput": 4791.7541810597195,
    "total_throughput": 10248.578487526085,
    "itl": 178.63180103696615,
    "ttft": 1844889.6248631743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 351,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.098947687041948,
    "arrivals": 279137,
    "finished_requests": 79249,
    "scheduler_time": 107.1874475437221
}
#Debug simulation 
Total elapsed time: 29.09502547280863. Arrivals time: 0.3601035065948963 Scheduler time: 28.60902735264972 Scheduler overhead time: 0.04620153829455376 Adapter cache time: 0.013438891153782606 Engine time: 0.048957588616758585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 24.83136497111991,
    "estimated_duration": 3600.0938813243624,
    "input_throughput": 5410.325853178798,
    "output_throughput": 4756.0493043868255,
    "total_throughput": 10166.375157565622,
    "itl": 177.68078274475158,
    "ttft": 1844131.0718191776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 511,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.689262650776659,
    "arrivals": 279137,
    "finished_requests": 78638,
    "scheduler_time": 106.85269325388751
}
#Debug simulation 
Total elapsed time: 24.831514615099877. Arrivals time: 0.3469792101532221 Scheduler time: 24.36464053951204 Scheduler overhead time: 0.042997837997972965 Adapter cache time: 0.015081949532032013 Engine time: 0.04471321078017354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 24.86234416393563,
    "estimated_duration": 3600.0994578380496,
    "input_throughput": 5437.809490895642,
    "output_throughput": 4776.137215477312,
    "total_throughput": 10213.946706372953,
    "itl": 179.11105998151592,
    "ttft": 1845430.4265399235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5488460527593089,
    "arrivals": 279137,
    "finished_requests": 78975,
    "scheduler_time": 106.86444192295892
}
#Debug simulation 
Total elapsed time: 24.862492127809674. Arrivals time: 0.3425957909785211 Scheduler time: 24.399787686765194 Scheduler overhead time: 0.043207104317843914 Adapter cache time: 0.014937958680093288 Engine time: 0.04489785432815552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 24.694340704008937,
    "estimated_duration": 3600.117780934419,
    "input_throughput": 5410.289936387726,
    "output_throughput": 4756.017731052089,
    "total_throughput": 10166.307667439816,
    "itl": 177.68148858682895,
    "ttft": 1844141.420559489,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 511,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7112695633992532,
    "arrivals": 279137,
    "finished_requests": 78638,
    "scheduler_time": 106.85281806310022
}
#Debug simulation 
Total elapsed time: 24.694486815016717. Arrivals time: 0.34750086022540927 Scheduler time: 24.226264438591897 Scheduler overhead time: 0.043657530564814806 Adapter cache time: 0.015092569403350353 Engine time: 0.04487498244270682 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 27.256772393826395,
    "estimated_duration": 3600.1720696520165,
    "input_throughput": 5463.121656266023,
    "output_throughput": 4820.594033906132,
    "total_throughput": 10283.715690172154,
    "itl": 177.82286367936928,
    "ttft": 1839986.877420051,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4904575312952668,
    "arrivals": 277607,
    "finished_requests": 79589,
    "scheduler_time": 107.664455052818
}
#Debug simulation 
Total elapsed time: 27.256934073753655. Arrivals time: 0.34984877286478877 Scheduler time: 26.78350623510778 Scheduler overhead time: 0.04474369203671813 Adapter cache time: 0.014757006894797087 Engine time: 0.046725597232580185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 34.004302609711885,
    "estimated_duration": 3600.153712032267,
    "input_throughput": 5445.965802646894,
    "output_throughput": 4809.616862227134,
    "total_throughput": 10255.582664874028,
    "itl": 178.57560349865938,
    "ttft": 1849828.8258123377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3003607533499664,
    "arrivals": 277607,
    "finished_requests": 79388,
    "scheduler_time": 107.38803071800407
}
#Debug simulation 
Total elapsed time: 34.00446961866692. Arrivals time: 0.37735883705317974 Scheduler time: 33.496929372195154 Scheduler overhead time: 0.048189503606408834 Adapter cache time: 0.014919509179890156 Engine time: 0.049226817674934864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 25.18503272300586,
    "estimated_duration": 3600.0100316953576,
    "input_throughput": 5401.115782681077,
    "output_throughput": 4768.811433537911,
    "total_throughput": 10169.927216218988,
    "itl": 177.47654722186329,
    "ttft": 1842248.390325763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5579914516396913,
    "arrivals": 277607,
    "finished_requests": 78619,
    "scheduler_time": 106.96680879055687
}
#Debug simulation 
Total elapsed time: 25.185200068168342. Arrivals time: 0.34617300191894174 Scheduler time: 24.716997603420168 Scheduler overhead time: 0.044520263094455004 Adapter cache time: 0.014257865026593208 Engine time: 0.045827009715139866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 29.79299019323662,
    "estimated_duration": 3600.194547425087,
    "input_throughput": 5462.385640816318,
    "output_throughput": 4827.331626406123,
    "total_throughput": 10289.71726722244,
    "itl": 177.77447158656753,
    "ttft": 1842965.4589858735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 489,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5283270357944942,
    "arrivals": 277607,
    "finished_requests": 79633,
    "scheduler_time": 107.73039771970424
}
#Debug simulation 
Total elapsed time: 29.793202709406614. Arrivals time: 0.3583206757903099 Scheduler time: 29.30701681552455 Scheduler overhead time: 0.04701955150812864 Adapter cache time: 0.015503805130720139 Engine time: 0.04736950807273388 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 25.208319711964577,
    "estimated_duration": 3600.0310117318277,
    "input_throughput": 5401.084306394975,
    "output_throughput": 4768.783642155707,
    "total_throughput": 10169.867948550682,
    "itl": 177.4764008279513,
    "ttft": 1842257.792837667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 477,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5771060271747457,
    "arrivals": 277607,
    "finished_requests": 78619,
    "scheduler_time": 106.96694031808184
}
#Debug simulation 
Total elapsed time: 25.20842194184661. Arrivals time: 0.35701003298163414 Scheduler time: 24.72798969130963 Scheduler overhead time: 0.045270446222275496 Adapter cache time: 0.014420202933251858 Engine time: 0.04646879713982344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 25.767370890360326,
    "estimated_duration": 3600.037061582661,
    "input_throughput": 5419.98809073982,
    "output_throughput": 4784.42018939324,
    "total_throughput": 10204.40828013306,
    "itl": 179.2255295438115,
    "ttft": 1839094.662029501,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 463,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.384393286539695,
    "arrivals": 277607,
    "finished_requests": 78933,
    "scheduler_time": 106.8296196753123
}
#Debug simulation 
Total elapsed time: 25.767526170238853. Arrivals time: 0.3437689561396837 Scheduler time: 25.3029154879041 Scheduler overhead time: 0.04347678739577532 Adapter cache time: 0.01398619543761015 Engine time: 0.04607611568644643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 25.280949771869928,
    "estimated_duration": 3600.0519319846435,
    "input_throughput": 5400.945421718138,
    "output_throughput": 4768.551766567469,
    "total_throughput": 10169.497188285608,
    "itl": 177.46452465493584,
    "ttft": 1842299.4510665552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 479,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6046169346198433,
    "arrivals": 277607,
    "finished_requests": 78617,
    "scheduler_time": 106.96815436965989
}
#Debug simulation 
Total elapsed time: 25.281096775084734. Arrivals time: 0.3639334407635033 Scheduler time: 24.792801660019904 Scheduler overhead time: 0.04550522333011031 Adapter cache time: 0.014150192495435476 Engine time: 0.046848734840750694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 26.08271725429222,
    "estimated_duration": 3600.112633987075,
    "input_throughput": 5466.717294954127,
    "output_throughput": 4820.01391739298,
    "total_throughput": 10286.731212347107,
    "itl": 177.91219830063534,
    "ttft": 1845119.7978056201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4108848499530124,
    "arrivals": 276925,
    "finished_requests": 79417,
    "scheduler_time": 107.72080575569639
}
#Debug simulation 
Total elapsed time: 26.082814312074333. Arrivals time: 0.3415038916282356 Scheduler time: 25.62038579909131 Scheduler overhead time: 0.044496496208012104 Adapter cache time: 0.013495443388819695 Engine time: 0.0458992812782526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 25.268784936051816,
    "estimated_duration": 3600.128315702749,
    "input_throughput": 5470.4927916314045,
    "output_throughput": 4820.100418174311,
    "total_throughput": 10290.593209805716,
    "itl": 177.96018581763403,
    "ttft": 1843514.7520985887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.424736025901517,
    "arrivals": 276925,
    "finished_requests": 79447,
    "scheduler_time": 107.68164430346921
}
#Debug simulation 
Total elapsed time: 25.268939146772027. Arrivals time: 0.33928003208711743 Scheduler time: 24.80805864650756 Scheduler overhead time: 0.04452440235763788 Adapter cache time: 0.013365371152758598 Engine time: 0.04646641295403242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 25.32192278979346,
    "estimated_duration": 3600.0956075329646,
    "input_throughput": 5465.507071209034,
    "output_throughput": 4824.170214718658,
    "total_throughput": 10289.677285927692,
    "itl": 175.53174483128663,
    "ttft": 1842745.5103495468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 430,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.40415414592252,
    "arrivals": 276925,
    "finished_requests": 79449,
    "scheduler_time": 108.24041259753172
}
#Debug simulation 
Total elapsed time: 25.322053926065564. Arrivals time: 0.34573720255866647 Scheduler time: 24.85528263868764 Scheduler overhead time: 0.04461919842287898 Adapter cache time: 0.013357067946344614 Engine time: 0.045549407601356506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 26.334553232882172,
    "estimated_duration": 3600.0156702817044,
    "input_throughput": 5466.456760856287,
    "output_throughput": 4819.634576380132,
    "total_throughput": 10286.09133723642,
    "itl": 177.9085818279532,
    "ttft": 1845124.967152487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.445144105921496,
    "arrivals": 276925,
    "finished_requests": 79411,
    "scheduler_time": 107.71829408064171
}
#Debug simulation 
Total elapsed time: 26.334667942021042. Arrivals time: 0.3528841664083302 Scheduler time: 25.859072795137763 Scheduler overhead time: 0.045531367883086205 Adapter cache time: 0.013705046381801367 Engine time: 0.0461624450981617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 25.220873760059476,
    "estimated_duration": 3600.133255391772,
    "input_throughput": 5459.994285089574,
    "output_throughput": 4821.642636144871,
    "total_throughput": 10281.636921234445,
    "itl": 175.7435351075979,
    "ttft": 1842081.828062945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.435799760129308,
    "arrivals": 276925,
    "finished_requests": 79412,
    "scheduler_time": 108.13100419921093
}
#Debug simulation 
Total elapsed time: 25.221063856966794. Arrivals time: 0.3459190805442631 Scheduler time: 24.752529717981815 Scheduler overhead time: 0.044686386827379465 Adapter cache time: 0.013430262450128794 Engine time: 0.04696620488539338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 26.13387064030394,
    "estimated_duration": 3600.152232580886,
    "input_throughput": 5466.332179484538,
    "output_throughput": 4819.296207250088,
    "total_throughput": 10285.628386734626,
    "itl": 177.92543798778624,
    "ttft": 1845091.944150375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 462,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.381403236244793,
    "arrivals": 276925,
    "finished_requests": 79410,
    "scheduler_time": 107.71467648535769
}
#Debug simulation 
Total elapsed time: 26.1340036643669. Arrivals time: 0.35179675556719303 Scheduler time: 25.662182092200965 Scheduler overhead time: 0.04372529359534383 Adapter cache time: 0.013331298716366291 Engine time: 0.04566918406635523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 25.17097127297893,
    "estimated_duration": 3600.1522088157426,
    "input_throughput": 5459.965540308643,
    "output_throughput": 4821.61725204114,
    "total_throughput": 10281.582792349782,
    "itl": 175.7438142390688,
    "ttft": 1842089.3910213893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4540340591594587,
    "arrivals": 276925,
    "finished_requests": 79412,
    "scheduler_time": 108.13108192232727
}
#Debug simulation 
Total elapsed time: 25.171113561838865. Arrivals time: 0.35688505368307233 Scheduler time: 24.6932282326743 Scheduler overhead time: 0.04389027925208211 Adapter cache time: 0.013226494193077087 Engine time: 0.04638250544667244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_192_slots_96_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_192_slots_96_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.491248403210193,
    "estimated_duration": 3600.1997054622043,
    "input_throughput": 5347.695565551488,
    "output_throughput": 4731.762789201428,
    "total_throughput": 10079.458354752915,
    "itl": 181.8139772935361,
    "ttft": 1747298.1877082996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 798,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.442269219658386,
    "arrivals": 218234,
    "finished_requests": 78094,
    "scheduler_time": 104.40102974334751
}
#Debug simulation 
Total elapsed time: 9.49136570515111. Arrivals time: 0.2789324992336333 Scheduler time: 9.110360338818282 Scheduler overhead time: 0.0342910336330533 Adapter cache time: 0.018016743939369917 Engine time: 0.03458729712292552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_192_slots_96_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_192_slots_96_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.493028396740556,
    "estimated_duration": 3600.032511336992,
    "input_throughput": 5346.266718255854,
    "output_throughput": 4731.707823847887,
    "total_throughput": 10077.97454210374,
    "itl": 181.8337793549322,
    "ttft": 1747400.4973382414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 795,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.595279884494382,
    "arrivals": 218234,
    "finished_requests": 78092,
    "scheduler_time": 104.39051816671419
}
#Debug simulation 
Total elapsed time: 9.493152244016528. Arrivals time: 0.2835867051035166 Scheduler time: 9.107505538035184 Scheduler overhead time: 0.03375193336978555 Adapter cache time: 0.01806033169850707 Engine time: 0.03504981053993106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_192_slots_96_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_192_slots_96_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.318816129118204,
    "estimated_duration": 3600.1735368073823,
    "input_throughput": 5330.554986807226,
    "output_throughput": 4718.968634791384,
    "total_throughput": 10049.52362159861,
    "itl": 179.79740132175704,
    "ttft": 1750523.687269176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 805,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.633495310284185,
    "arrivals": 218234,
    "finished_requests": 77875,
    "scheduler_time": 104.61809649243527
}
#Debug simulation 
Total elapsed time: 9.31893300730735. Arrivals time: 0.27745229145511985 Scheduler time: 8.938701564911753 Scheduler overhead time: 0.03406453598290682 Adapter cache time: 0.018344519194215536 Engine time: 0.035013383720070124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_192_slots_96_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_192_slots_96_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 9.51055079465732,
    "estimated_duration": 3600.061891442939,
    "input_throughput": 5346.380029118193,
    "output_throughput": 4731.700874501478,
    "total_throughput": 10078.080903619672,
    "itl": 181.82883170845855,
    "ttft": 1747392.0908439618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 795,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4841419578785793,
    "arrivals": 218234,
    "finished_requests": 78094,
    "scheduler_time": 104.39416281779141
}
#Debug simulation 
Total elapsed time: 9.510686453897506. Arrivals time: 0.2813707129098475 Scheduler time: 9.127519569825381 Scheduler overhead time: 0.03377676708623767 Adapter cache time: 0.018142067827284336 Engine time: 0.034654762130230665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_192_slots_96_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_192_slots_96_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 9.327696156222373,
    "estimated_duration": 3600.005094070397,
    "input_throughput": 5330.6619014536145,
    "output_throughput": 4719.055266888962,
    "total_throughput": 10049.717168342577,
    "itl": 179.7984447871594,
    "ttft": 1750524.445345227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 804,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.665159836485985,
    "arrivals": 218234,
    "finished_requests": 77872,
    "scheduler_time": 104.61216074346935
}
#Debug simulation 
Total elapsed time: 9.327789574861526. Arrivals time: 0.28556152479723096 Scheduler time: 8.939725213684142 Scheduler overhead time: 0.03403033735230565 Adapter cache time: 0.018271852750331163 Engine time: 0.03486759774386883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_192_slots_96_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_192_slots_96_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.501618310809135,
    "estimated_duration": 3600.0508039245838,
    "input_throughput": 5346.542326296353,
    "output_throughput": 4731.830445678582,
    "total_throughput": 10078.372771974935,
    "itl": 181.82363492310003,
    "ttft": 1747363.4862591664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 796,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3800800347420843,
    "arrivals": 218234,
    "finished_requests": 78096,
    "scheduler_time": 104.39629121661298
}
#Debug simulation 
Total elapsed time: 9.50177350686863. Arrivals time: 0.27825577510520816 Scheduler time: 9.121849910821766 Scheduler overhead time: 0.033699245657771826 Adapter cache time: 0.018204376567155123 Engine time: 0.034497412387281656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_192_slots_96_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_192_slots_96_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.388933492358774,
    "estimated_duration": 3600.020479685328,
    "input_throughput": 5329.5730144504805,
    "output_throughput": 4719.348152565245,
    "total_throughput": 10048.921167015726,
    "itl": 179.7973080816956,
    "ttft": 1750537.8900731043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 801,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.688342290855962,
    "arrivals": 218234,
    "finished_requests": 77872,
    "scheduler_time": 104.61153212708335
}
#Debug simulation 
Total elapsed time: 9.389056512154639. Arrivals time: 0.28149903286248446 Scheduler time: 9.00510021392256 Scheduler overhead time: 0.03404538379982114 Adapter cache time: 0.017943273298442364 Engine time: 0.035081869922578335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_192_slots_96_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_192_slots_96_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 8.461294974200428,
    "estimated_duration": 3600.142522001118,
    "input_throughput": 5371.650950432566,
    "output_throughput": 4725.554862351285,
    "total_throughput": 10097.20581278385,
    "itl": 181.38252386631024,
    "ttft": 1736974.5974366912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 834,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.552446778439969,
    "arrivals": 212464,
    "finished_requests": 77752,
    "scheduler_time": 104.31951054332916
}
#Debug simulation 
Total elapsed time: 8.461384105030447. Arrivals time: 0.2779469331726432 Scheduler time: 8.08243951946497 Scheduler overhead time: 0.033299724105745554 Adapter cache time: 0.018114300444722176 Engine time: 0.03441071044653654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_192_slots_96_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_192_slots_96_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 8.408327741082758,
    "estimated_duration": 3600.161750429873,
    "input_throughput": 5371.946968130181,
    "output_throughput": 4726.106541732014,
    "total_throughput": 10098.053509862195,
    "itl": 181.39647454719974,
    "ttft": 1736982.1838797508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 837,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7347542377072456,
    "arrivals": 212464,
    "finished_requests": 77747,
    "scheduler_time": 104.31584876704522
}
#Debug simulation 
Total elapsed time: 8.408420480322093. Arrivals time: 0.27429079730063677 Scheduler time: 8.03338139783591 Scheduler overhead time: 0.03293848177418113 Adapter cache time: 0.01817915542051196 Engine time: 0.03433790383860469 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_192_slots_96_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_192_slots_96_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 8.246945628896356,
    "estimated_duration": 3600.0007314667178,
    "input_throughput": 5360.397244179951,
    "output_throughput": 4713.740708904321,
    "total_throughput": 10074.137953084271,
    "itl": 179.36016179075796,
    "ttft": 1739620.8949411646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 852,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7876698634959576,
    "arrivals": 212464,
    "finished_requests": 77546,
    "scheduler_time": 104.53657041463258
}
#Debug simulation 
Total elapsed time: 8.247067600023001. Arrivals time: 0.27015330735594034 Scheduler time: 7.874924201052636 Scheduler overhead time: 0.033485685009509325 Adapter cache time: 0.01837771385908127 Engine time: 0.0348411095328629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_192_slots_96_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_192_slots_96_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 8.474652255885303,
    "estimated_duration": 3600.012226069903,
    "input_throughput": 5371.515368743782,
    "output_throughput": 4725.315063324367,
    "total_throughput": 10096.83043206815,
    "itl": 181.3844974774825,
    "ttft": 1737025.7819490202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 834,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.606474253837926,
    "arrivals": 212464,
    "finished_requests": 77747,
    "scheduler_time": 104.31421504685025
}
#Debug simulation 
Total elapsed time: 8.47474621515721. Arrivals time: 0.2824246189557016 Scheduler time: 8.090590607374907 Scheduler overhead time: 0.033472736831754446 Adapter cache time: 0.01848662318661809 Engine time: 0.03454093914479017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_192_slots_96_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_192_slots_96_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 8.52885588305071,
    "estimated_duration": 3600.173663785241,
    "input_throughput": 5359.431183581644,
    "output_throughput": 4713.662890961113,
    "total_throughput": 10073.094074542758,
    "itl": 179.3444017050654,
    "ttft": 1739673.2214411139,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 850,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8194804415665584,
    "arrivals": 212464,
    "finished_requests": 77548,
    "scheduler_time": 104.54455543218809
}
#Debug simulation 
Total elapsed time: 8.528921735007316. Arrivals time: 0.5035625933669508 Scheduler time: 7.9237222648225725 Scheduler overhead time: 0.03355592908337712 Adapter cache time: 0.01845173677429557 Engine time: 0.03437138628214598 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_192_slots_96_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_192_slots_96_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 8.384069489315152,
    "estimated_duration": 3600.1483809934775,
    "input_throughput": 5372.631612106613,
    "output_throughput": 4725.957988238492,
    "total_throughput": 10098.589600345105,
    "itl": 181.3776633598285,
    "ttft": 1736941.210729049,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 836,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.499682046538167,
    "arrivals": 212464,
    "finished_requests": 77751,
    "scheduler_time": 104.3216635806715
}
#Debug simulation 
Total elapsed time: 8.384191147051752. Arrivals time: 0.28127079037949443 Scheduler time: 8.001922722440213 Scheduler overhead time: 0.03336106473580003 Adapter cache time: 0.018106896430253983 Engine time: 0.03440366219729185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_192_slots_96_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_192_slots_96_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 8.247742177918553,
    "estimated_duration": 3600.167558686798,
    "input_throughput": 5360.020522777775,
    "output_throughput": 4713.355065670773,
    "total_throughput": 10073.375588448549,
    "itl": 179.36406873597298,
    "ttft": 1739638.2982422228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 853,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.864199319668138,
    "arrivals": 212464,
    "finished_requests": 77546,
    "scheduler_time": 104.53901827317353
}
#Debug simulation 
Total elapsed time: 8.247899225912988. Arrivals time: 0.2739985268563032 Scheduler time: 7.872375280596316 Scheduler overhead time: 0.033512603025883436 Adapter cache time: 0.01841988181695342 Engine time: 0.03417725954204798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.725698614958674,
    "estimated_duration": 3600.09566177012,
    "input_throughput": 5340.0850438922525,
    "output_throughput": 4729.561267165915,
    "total_throughput": 10069.646311058168,
    "itl": 181.82857142586218,
    "ttft": 1731023.465588855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 854,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6136565333186264,
    "arrivals": 209539,
    "finished_requests": 77765,
    "scheduler_time": 104.16554029998096
}
#Debug simulation 
Total elapsed time: 7.725790688302368. Arrivals time: 0.26756965462118387 Scheduler time: 7.357747391331941 Scheduler overhead time: 0.03307245625182986 Adapter cache time: 0.018379502929747105 Engine time: 0.033983328845351934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.673841586802155,
    "estimated_duration": 3600.1690798920545,
    "input_throughput": 5339.822262063428,
    "output_throughput": 4729.438151974385,
    "total_throughput": 10069.260414037813,
    "itl": 181.83865832675127,
    "ttft": 1731073.4053308794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 853,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7854552096547622,
    "arrivals": 209539,
    "finished_requests": 77763,
    "scheduler_time": 104.16324315364938
}
#Debug simulation 
Total elapsed time: 7.673962092027068. Arrivals time: 0.2649787738919258 Scheduler time: 7.309003146830946 Scheduler overhead time: 0.033008978702127934 Adapter cache time: 0.01826320542022586 Engine time: 0.033722187858074903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.617662591859698,
    "estimated_duration": 3600.084078372324,
    "input_throughput": 5327.0858631365,
    "output_throughput": 4720.265313270978,
    "total_throughput": 10047.35117640748,
    "itl": 179.87679114512233,
    "ttft": 1733391.740492378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 862,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8191167944669537,
    "arrivals": 209539,
    "finished_requests": 77584,
    "scheduler_time": 104.36113231758381
}
#Debug simulation 
Total elapsed time: 7.617769603617489. Arrivals time: 0.2744951373897493 Scheduler time: 7.242353736422956 Scheduler overhead time: 0.03343435749411583 Adapter cache time: 0.018582251854240894 Engine time: 0.033768496476113796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 7.729943111073226,
    "estimated_duration": 3600.1533009495142,
    "input_throughput": 5339.999548055243,
    "output_throughput": 4729.485545937526,
    "total_throughput": 10069.48509399277,
    "itl": 181.83028997173741,
    "ttft": 1731045.9563406545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 854,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6646408784622064,
    "arrivals": 209539,
    "finished_requests": 77765,
    "scheduler_time": 104.16595562925141
}
#Debug simulation 
Total elapsed time: 7.730037620291114. Arrivals time: 0.2715450953692198 Scheduler time: 7.358137383591384 Scheduler overhead time: 0.03322385950013995 Adapter cache time: 0.018092407379299402 Engine time: 0.03397742798551917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 7.656056893058121,
    "estimated_duration": 3600.002704195185,
    "input_throughput": 5327.206276165122,
    "output_throughput": 4720.372009775761,
    "total_throughput": 10047.578285940883,
    "itl": 179.90053505416833,
    "ttft": 1733327.3908276055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 862,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8574716993235096,
    "arrivals": 209539,
    "finished_requests": 77584,
    "scheduler_time": 104.35666360065076
}
#Debug simulation 
Total elapsed time: 7.6561702326871455. Arrivals time: 0.2739891214296222 Scheduler time: 7.280722665600479 Scheduler overhead time: 0.03329797927290201 Adapter cache time: 0.018574601504951715 Engine time: 0.03439034568145871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.660824730060995,
    "estimated_duration": 3600.1610663726774,
    "input_throughput": 5340.496618217057,
    "output_throughput": 4729.798385702922,
    "total_throughput": 10070.29500391998,
    "itl": 181.82317521220781,
    "ttft": 1731000.1311410605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 853,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5505129015515022,
    "arrivals": 209539,
    "finished_requests": 77769,
    "scheduler_time": 104.16896981925835
}
#Debug simulation 
Total elapsed time: 7.660920770838857. Arrivals time: 0.2670765486545861 Scheduler time: 7.293450671248138 Scheduler overhead time: 0.03294460754841566 Adapter cache time: 0.018394422717392445 Engine time: 0.03402290027588606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.704000066034496,
    "estimated_duration": 3600.158208058856,
    "input_throughput": 5326.976174844391,
    "output_throughput": 4720.168119823414,
    "total_throughput": 10047.144294667803,
    "itl": 179.87838670371283,
    "ttft": 1733407.8637246992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 862,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.892053990587617,
    "arrivals": 209539,
    "finished_requests": 77584,
    "scheduler_time": 104.36253159129318
}
#Debug simulation 
Total elapsed time: 7.704092739149928. Arrivals time: 0.2751226634718478 Scheduler time: 7.327521249186248 Scheduler overhead time: 0.033439773600548506 Adapter cache time: 0.018503142055124044 Engine time: 0.034333011601120234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.038776020053774,
    "estimated_duration": 3600.1014031141617,
    "input_throughput": 5378.694051020307,
    "output_throughput": 4726.052434323741,
    "total_throughput": 10104.746485344047,
    "itl": 181.12454452671702,
    "ttft": 1720703.460445051,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 910,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7850438469788665,
    "arrivals": 208072,
    "finished_requests": 78342,
    "scheduler_time": 104.13933263502764
}
#Debug simulation 
Total elapsed time: 7.038926401175559. Arrivals time: 0.26184770883992314 Scheduler time: 6.675905267242342 Scheduler overhead time: 0.03306921245530248 Adapter cache time: 0.01926765264943242 Engine time: 0.03377451607957482 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.144498190842569,
    "estimated_duration": 3600.136050330229,
    "input_throughput": 5378.164527483336,
    "output_throughput": 4725.687796837411,
    "total_throughput": 10103.852324320747,
    "itl": 181.14322340612586,
    "ttft": 1720850.062083033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 894,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.916627773432999,
    "arrivals": 208072,
    "finished_requests": 78337,
    "scheduler_time": 104.13650506071912
}
#Debug simulation 
Total elapsed time: 7.144594457931817. Arrivals time: 0.2642948334105313 Scheduler time: 6.779335660394281 Scheduler overhead time: 0.0329656396061182 Adapter cache time: 0.01906490931287408 Engine time: 0.033847988583147526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.080512967891991,
    "estimated_duration": 3600.1954901817385,
    "input_throughput": 5367.065497608464,
    "output_throughput": 4716.854694782912,
    "total_throughput": 10083.920192391375,
    "itl": 179.46205296357812,
    "ttft": 1723189.7356607257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 923,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.016237859055372,
    "arrivals": 208072,
    "finished_requests": 78202,
    "scheduler_time": 104.31874226429717
}
#Debug simulation 
Total elapsed time: 7.080606060102582. Arrivals time: 0.26310817897319794 Scheduler time: 6.7157249776646495 Scheduler overhead time: 0.03343012882396579 Adapter cache time: 0.019332409370690584 Engine time: 0.03381197853013873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 7.3570149121806026,
    "estimated_duration": 3600.038741793135,
    "input_throughput": 5378.063234496306,
    "output_throughput": 4726.508024056633,
    "total_throughput": 10104.57125855294,
    "itl": 181.1420377425922,
    "ttft": 1720739.4832566953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 901,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8141623393282944,
    "arrivals": 208072,
    "finished_requests": 78335,
    "scheduler_time": 104.13545017388701
}
#Debug simulation 
Total elapsed time: 7.357086019124836. Arrivals time: 0.5154914814047515 Scheduler time: 6.741211952175945 Scheduler overhead time: 0.03287345916032791 Adapter cache time: 0.019012754783034325 Engine time: 0.03354869782924652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 7.022157963830978,
    "estimated_duration": 3600.0331349874145,
    "input_throughput": 5367.141711063042,
    "output_throughput": 4716.860751910653,
    "total_throughput": 10084.002462973694,
    "itl": 179.46489868596697,
    "ttft": 1723081.8790023485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 923,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.055473040416838,
    "arrivals": 208072,
    "finished_requests": 78198,
    "scheduler_time": 104.31281105566659
}
#Debug simulation 
Total elapsed time: 7.022284312173724. Arrivals time: 0.2665433092042804 Scheduler time: 6.6528279897756875 Scheduler overhead time: 0.03338472684845328 Adapter cache time: 0.019702552817761898 Engine time: 0.03421166306361556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.129801162984222,
    "estimated_duration": 3600.1297701917333,
    "input_throughput": 5378.409178558244,
    "output_throughput": 4725.910754904228,
    "total_throughput": 10104.319933462471,
    "itl": 181.1300267028809,
    "ttft": 1720789.5595749717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 894,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.673104963642487,
    "arrivals": 208072,
    "finished_requests": 78342,
    "scheduler_time": 104.14240093468631
}
#Debug simulation 
Total elapsed time: 7.12989443494007. Arrivals time: 0.2630716790445149 Scheduler time: 6.766352545469999 Scheduler overhead time: 0.03311819350346923 Adapter cache time: 0.01869622291997075 Engine time: 0.03359281038865447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.3002555491402745,
    "estimated_duration": 3600.101139806576,
    "input_throughput": 5366.490342834648,
    "output_throughput": 4716.27694351727,
    "total_throughput": 10082.767286351918,
    "itl": 179.46013513666193,
    "ttft": 1723262.972845505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 922,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0908701663464635,
    "arrivals": 208072,
    "finished_requests": 78195,
    "scheduler_time": 104.31509971875366
}
#Debug simulation 
Total elapsed time: 7.300368296913803. Arrivals time: 0.2694911421276629 Scheduler time: 6.929234741721302 Scheduler overhead time: 0.03324622893705964 Adapter cache time: 0.0191845684312284 Engine time: 0.033937425352633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.681837858632207,
    "estimated_duration": 3600.0576861430186,
    "input_throughput": 5361.325201618792,
    "output_throughput": 4730.2153144785725,
    "total_throughput": 10091.540516097364,
    "itl": 181.29897715805228,
    "ttft": 1722869.0152833676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 834,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.552446778439969,
    "arrivals": 207395,
    "finished_requests": 77882,
    "scheduler_time": 104.06149414169282
}
#Debug simulation 
Total elapsed time: 6.681929348967969. Arrivals time: 0.25961197121068835 Scheduler time: 6.323076441884041 Scheduler overhead time: 0.0325687974691391 Adapter cache time: 0.01817436283454299 Engine time: 0.03356422623619437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.72784122498706,
    "estimated_duration": 3600.143136171647,
    "input_throughput": 5360.286041438082,
    "output_throughput": 4728.889756895572,
    "total_throughput": 10089.175798333654,
    "itl": 181.30107158769994,
    "ttft": 1723130.5995486376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 824,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.684034319638745,
    "arrivals": 207395,
    "finished_requests": 77871,
    "scheduler_time": 104.06174148043588
}
#Debug simulation 
Total elapsed time: 6.728006792720407. Arrivals time: 0.26921845600008965 Scheduler time: 6.359673081897199 Scheduler overhead time: 0.0326939788646996 Adapter cache time: 0.017559760250151157 Engine time: 0.033881896175444126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.648170060943812,
    "estimated_duration": 3600.1302935719755,
    "input_throughput": 5343.565213278132,
    "output_throughput": 4716.776231771265,
    "total_throughput": 10060.341445049398,
    "itl": 179.16245876752453,
    "ttft": 1725746.0131885402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 839,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7400101785920405,
    "arrivals": 207395,
    "finished_requests": 77652,
    "scheduler_time": 104.29196637375964
}
#Debug simulation 
Total elapsed time: 6.648261931724846. Arrivals time: 0.26519942563027143 Scheduler time: 6.282576230820268 Scheduler overhead time: 0.03340390743687749 Adapter cache time: 0.01787356799468398 Engine time: 0.03402299806475639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.990777574013919,
    "estimated_duration": 3600.004633371429,
    "input_throughput": 5360.36032318323,
    "output_throughput": 4729.589746126111,
    "total_throughput": 10089.950069309341,
    "itl": 181.29506104011142,
    "ttft": 1723025.9740067702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 817,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5458371111820197,
    "arrivals": 207395,
    "finished_requests": 77878,
    "scheduler_time": 104.06087346923803
}
#Debug simulation 
Total elapsed time: 6.990896217059344. Arrivals time: 0.28272889694198966 Scheduler time: 6.607830495107919 Scheduler overhead time: 0.033171399496495724 Adapter cache time: 0.017780825030058622 Engine time: 0.033959738444536924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.672041623387486,
    "estimated_duration": 3600.145413260275,
    "input_throughput": 5343.518328216265,
    "output_throughput": 4717.568611935436,
    "total_throughput": 10061.086940151701,
    "itl": 179.1001179267159,
    "ttft": 1725771.7652898638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 847,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.803056193925444,
    "arrivals": 207395,
    "finished_requests": 77655,
    "scheduler_time": 104.30028226100316
}
#Debug simulation 
Total elapsed time: 6.67213377635926. Arrivals time: 0.25847276393324137 Scheduler time: 6.313500587362796 Scheduler overhead time: 0.03307944908738136 Adapter cache time: 0.017944217659533024 Engine time: 0.03405778296291828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.6922154990024865,
    "estimated_duration": 3600.1752625836666,
    "input_throughput": 5361.330377607258,
    "output_throughput": 4730.179993453649,
    "total_throughput": 10091.510371060907,
    "itl": 181.295975719508,
    "ttft": 1722846.6932274487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 834,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.493701945948363,
    "arrivals": 207395,
    "finished_requests": 77885,
    "scheduler_time": 104.06647970865671
}
#Debug simulation 
Total elapsed time: 6.692340143024921. Arrivals time: 0.25930939009413123 Scheduler time: 6.333659574389458 Scheduler overhead time: 0.0328800305724144 Adapter cache time: 0.017846690025180578 Engine time: 0.03359072469174862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.6479182709008455,
    "estimated_duration": 3600.1711370580915,
    "input_throughput": 5343.504591206767,
    "output_throughput": 4716.722720541604,
    "total_throughput": 10060.227311748371,
    "itl": 179.17179567467383,
    "ttft": 1725754.3702631532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 839,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8095520224794965,
    "arrivals": 207395,
    "finished_requests": 77652,
    "scheduler_time": 104.29119045690366
}
#Debug simulation 
Total elapsed time: 6.648030629847199. Arrivals time: 0.2842093841172755 Scheduler time: 6.264001783914864 Scheduler overhead time: 0.033018411602824926 Adapter cache time: 0.017798970453441143 Engine time: 0.03381810104474425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_192_slots_96_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_192_slots_96_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.011536893900484,
    "estimated_duration": 3600.0025884393326,
    "input_throughput": 5387.326682008811,
    "output_throughput": 4724.68826956418,
    "total_throughput": 10112.014951572992,
    "itl": 180.77060917245012,
    "ttft": 1705590.798517144,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4491696874122972,
    "arrivals": 200999,
    "finished_requests": 77948,
    "scheduler_time": 103.89268266322779
}
#Debug simulation 
Total elapsed time: 7.011628740932792. Arrivals time: 0.26798129500821233 Scheduler time: 6.639553032815456 Scheduler overhead time: 0.03288859874010086 Adapter cache time: 0.022256063297390938 Engine time: 0.03381257317960262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_192_slots_96_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_192_slots_96_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.2292440589517355,
    "estimated_duration": 3600.036688153527,
    "input_throughput": 5387.0267666485925,
    "output_throughput": 4724.848515008969,
    "total_throughput": 10111.875281657562,
    "itl": 180.79161698884613,
    "ttft": 1705645.34443497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1125,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.673930428461179,
    "arrivals": 200999,
    "finished_requests": 77949,
    "scheduler_time": 103.8867773782158
}
#Debug simulation 
Total elapsed time: 7.229328392073512. Arrivals time: 0.26441315608099103 Scheduler time: 6.8609034842811525 Scheduler overhead time: 0.03280656039714813 Adapter cache time: 0.02239560568705201 Engine time: 0.03385862661525607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_192_slots_96_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_192_slots_96_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.990881422068924,
    "estimated_duration": 3600.1862110425886,
    "input_throughput": 5373.1440170140595,
    "output_throughput": 4712.949554652711,
    "total_throughput": 10086.09357166677,
    "itl": 178.85068729707996,
    "ttft": 1707981.3212886609,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1138,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.722530525699203,
    "arrivals": 200999,
    "finished_requests": 77726,
    "scheduler_time": 104.11080331092394
}
#Debug simulation 
Total elapsed time: 6.991029658354819. Arrivals time: 0.26556518068537116 Scheduler time: 6.619760984554887 Scheduler overhead time: 0.033331953920423985 Adapter cache time: 0.022959750145673752 Engine time: 0.03424455877393484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_192_slots_96_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_192_slots_96_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 7.026564282830805,
    "estimated_duration": 3600.0789893761767,
    "input_throughput": 5387.293183631151,
    "output_throughput": 4724.674666915395,
    "total_throughput": 10111.967850546547,
    "itl": 180.77428837005354,
    "ttft": 1705626.4883220694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.518923973585438,
    "arrivals": 200999,
    "finished_requests": 77949,
    "scheduler_time": 103.89314826046049
}
#Debug simulation 
Total elapsed time: 7.026655938010663. Arrivals time: 0.2623869255185127 Scheduler time: 6.659919128753245 Scheduler overhead time: 0.03303689882159233 Adapter cache time: 0.022313793189823627 Engine time: 0.034015465062111616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_192_slots_96_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_192_slots_96_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 7.170496734790504,
    "estimated_duration": 3600.0424719634643,
    "input_throughput": 5373.341606564335,
    "output_throughput": 4713.0580075479165,
    "total_throughput": 10086.399614112252,
    "itl": 178.87875148023093,
    "ttft": 1707981.1364543438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.809048741422596,
    "arrivals": 200999,
    "finished_requests": 77725,
    "scheduler_time": 104.1010702575446
}
#Debug simulation 
Total elapsed time: 7.1705680959858. Arrivals time: 0.26539613911882043 Scheduler time: 6.799747587647289 Scheduler overhead time: 0.033184525556862354 Adapter cache time: 0.022771294694393873 Engine time: 0.03428772557526827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_192_slots_96_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_192_slots_96_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.036672980058938,
    "estimated_duration": 3600.125960488385,
    "input_throughput": 5387.562050014716,
    "output_throughput": 4724.716075682119,
    "total_throughput": 10112.278125696834,
    "itl": 180.7700366518518,
    "ttft": 1705666.1023353108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1127,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.369786682354669,
    "arrivals": 200999,
    "finished_requests": 77957,
    "scheduler_time": 103.89743395208042
}
#Debug simulation 
Total elapsed time: 7.03679200867191. Arrivals time: 0.27579024992883205 Scheduler time: 6.654903172980994 Scheduler overhead time: 0.03448481718078256 Adapter cache time: 0.022345708683133125 Engine time: 0.03396811056882143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_192_slots_96_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_192_slots_96_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.980048710014671,
    "estimated_duration": 3600.089714810129,
    "input_throughput": 5373.271093890011,
    "output_throughput": 4712.996159567891,
    "total_throughput": 10086.267253457901,
    "itl": 178.88056766439024,
    "ttft": 1708000.5377795554,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8558291499689803,
    "arrivals": 200999,
    "finished_requests": 77725,
    "scheduler_time": 104.1012709015706
}
#Debug simulation 
Total elapsed time: 6.980142478831112. Arrivals time: 0.26518505485728383 Scheduler time: 6.6096026273444295 Scheduler overhead time: 0.03344768797978759 Adapter cache time: 0.022655911277979612 Engine time: 0.03418645355850458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.440080858767033,
    "estimated_duration": 3600.1023636007844,
    "input_throughput": 5357.057397865318,
    "output_throughput": 4725.760070606314,
    "total_throughput": 10082.817468471632,
    "itl": 181.2849719655309,
    "ttft": 1696054.7414593583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.006178456808078,
    "arrivals": 198139,
    "finished_requests": 78030,
    "scheduler_time": 103.66411403106396
}
#Debug simulation 
Total elapsed time: 6.4401885671541095. Arrivals time: 0.2586041768081486 Scheduler time: 6.075443241279572 Scheduler overhead time: 0.032982966396957636 Adapter cache time: 0.024466963950544596 Engine time: 0.0335920094512403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.638585331849754,
    "estimated_duration": 3600.097652594188,
    "input_throughput": 5356.309984009663,
    "output_throughput": 4725.362932236441,
    "total_throughput": 10081.672916246105,
    "itl": 181.2981327663452,
    "ttft": 1696170.1406795494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.271905334980674,
    "arrivals": 198139,
    "finished_requests": 78022,
    "scheduler_time": 103.65710116650054
}
#Debug simulation 
Total elapsed time: 6.638679701834917. Arrivals time: 0.48729605320841074 Scheduler time: 6.045864148065448 Scheduler overhead time: 0.03268177807331085 Adapter cache time: 0.024387573823332787 Engine time: 0.03350174380466342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.437653131317347,
    "estimated_duration": 3600.104554977206,
    "input_throughput": 5343.855353701472,
    "output_throughput": 4714.7125148168,
    "total_throughput": 10058.567868518272,
    "itl": 179.38643043463003,
    "ttft": 1698390.8344770458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1327,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.338562534954336,
    "arrivals": 198139,
    "finished_requests": 77837,
    "scheduler_time": 103.86282028747517
}
#Debug simulation 
Total elapsed time: 6.437747246120125. Arrivals time: 0.26064486568793654 Scheduler time: 6.070069049950689 Scheduler overhead time: 0.03322249371558428 Adapter cache time: 0.02462982339784503 Engine time: 0.033972824923694134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.372251949738711,
    "estimated_duration": 3600.0263326578925,
    "input_throughput": 5356.782761575593,
    "output_throughput": 4725.683211166859,
    "total_throughput": 10082.465972742451,
    "itl": 181.2876534915026,
    "ttft": 1696108.056590069,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.088037441682482,
    "arrivals": 198139,
    "finished_requests": 78026,
    "scheduler_time": 103.65965720858853
}
#Debug simulation 
Total elapsed time: 6.372413845732808. Arrivals time: 0.269148466642946 Scheduler time: 5.997517177369446 Scheduler overhead time: 0.03265201160684228 Adapter cache time: 0.024362795520573854 Engine time: 0.03378622140735388 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.6219221777282655,
    "estimated_duration": 3600.1655029662684,
    "input_throughput": 5343.831827772573,
    "output_throughput": 4714.6738076388465,
    "total_throughput": 10058.50563541142,
    "itl": 179.37874548475162,
    "ttft": 1698614.7199173246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.399538535475712,
    "arrivals": 198139,
    "finished_requests": 77837,
    "scheduler_time": 103.86422724360388
}
#Debug simulation 
Total elapsed time: 6.621989401988685. Arrivals time: 0.25925891753286123 Scheduler time: 6.256258969660848 Scheduler overhead time: 0.033048003911972046 Adapter cache time: 0.024588970467448235 Engine time: 0.033741097431629896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.393538273405284,
    "estimated_duration": 3600.1896485583134,
    "input_throughput": 5357.268333828886,
    "output_throughput": 4725.960202350272,
    "total_throughput": 10083.228536179158,
    "itl": 181.28147032095595,
    "ttft": 1696019.0298204005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9139758360268457,
    "arrivals": 198139,
    "finished_requests": 78033,
    "scheduler_time": 103.66898065775038
}
#Debug simulation 
Total elapsed time: 6.3936566123738885. Arrivals time: 0.26266017789021134 Scheduler time: 6.0249772276729345 Scheduler overhead time: 0.0326943090185523 Adapter cache time: 0.024590944405645132 Engine time: 0.03366245748475194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.446389340329915,
    "estimated_duration": 3600.0112137119127,
    "input_throughput": 5343.946131813223,
    "output_throughput": 4714.719480692776,
    "total_throughput": 10058.665612505998,
    "itl": 179.37990376394782,
    "ttft": 1698677.2047476238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.45386417120697,
    "arrivals": 198139,
    "finished_requests": 77834,
    "scheduler_time": 103.85816033562654
}
#Debug simulation 
Total elapsed time: 6.446499821264297. Arrivals time: 0.26860826881602407 Scheduler time: 6.070192562881857 Scheduler overhead time: 0.03325512632727623 Adapter cache time: 0.02495248755440116 Engine time: 0.034321075305342674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.14110464323312,
    "estimated_duration": 3600.1059510278287,
    "input_throughput": 5320.771183006756,
    "output_throughput": 4729.576082375799,
    "total_throughput": 10050.347265382556,
    "itl": 182.0414315791831,
    "ttft": 1705558.6085084118,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.333650645408894,
    "arrivals": 196768,
    "finished_requests": 77349,
    "scheduler_time": 103.63590352130984
}
#Debug simulation 
Total elapsed time: 6.141200629994273. Arrivals time: 0.25609247805550694 Scheduler time: 5.778760420158505 Scheduler overhead time: 0.03234241669997573 Adapter cache time: 0.025283357594162226 Engine time: 0.03382135136052966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.152231629937887,
    "estimated_duration": 3600.0397312484365,
    "input_throughput": 5320.891276207445,
    "output_throughput": 4729.630857183723,
    "total_throughput": 10050.522133391167,
    "itl": 182.05485531632323,
    "ttft": 1705555.9254812533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.601944662551347,
    "arrivals": 196768,
    "finished_requests": 77347,
    "scheduler_time": 103.62733283268606
}
#Debug simulation 
Total elapsed time: 6.1523534040898085. Arrivals time: 0.2732281410135329 Scheduler time: 5.77311726892367 Scheduler overhead time: 0.03243378270417452 Adapter cache time: 0.02527162153273821 Engine time: 0.03338602418079972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.079968853387982,
    "estimated_duration": 3600.0682572036044,
    "input_throughput": 5298.830365738017,
    "output_throughput": 4712.073713062547,
    "total_throughput": 10010.904078800564,
    "itl": 179.2146798827942,
    "ttft": 1709066.4130356726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1453,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.745315987020676,
    "arrivals": 196768,
    "finished_requests": 77045,
    "scheduler_time": 103.93221560619416
}
#Debug simulation 
Total elapsed time: 6.080056639388204. Arrivals time: 0.2717257309705019 Scheduler time: 5.70091784466058 Scheduler overhead time: 0.03271099552512169 Adapter cache time: 0.026082840282469988 Engine time: 0.03356186766177416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.606288697104901,
    "estimated_duration": 3600.067562234284,
    "input_throughput": 5320.692922805054,
    "output_throughput": 4729.639848601241,
    "total_throughput": 10050.332771406296,
    "itl": 182.0488788501284,
    "ttft": 1705540.1655887698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.449313995102341,
    "arrivals": 196768,
    "finished_requests": 77346,
    "scheduler_time": 103.6315102497011
}
#Debug simulation 
Total elapsed time: 6.606357529759407. Arrivals time: 0.5139300925657153 Scheduler time: 5.986387857701629 Scheduler overhead time: 0.03228289308026433 Adapter cache time: 0.02536377450451255 Engine time: 0.033512752037495375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.066571448929608,
    "estimated_duration": 3600.1297201177376,
    "input_throughput": 5298.739901898907,
    "output_throughput": 4711.993266577411,
    "total_throughput": 10010.733168476318,
    "itl": 179.21649733875765,
    "ttft": 1709090.297523046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1453,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.80743835750964,
    "arrivals": 196768,
    "finished_requests": 77045,
    "scheduler_time": 103.93249360665085
}
#Debug simulation 
Total elapsed time: 6.066759708337486. Arrivals time: 0.255885471124202 Scheduler time: 5.702814492862672 Scheduler overhead time: 0.03276457637548447 Adapter cache time: 0.026129124220460653 Engine time: 0.03404513793066144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.153763114940375,
    "estimated_duration": 3600.1223574299916,
    "input_throughput": 5321.033592223544,
    "output_throughput": 4729.626193081721,
    "total_throughput": 10050.659785305264,
    "itl": 182.03342435292856,
    "ttft": 1705528.6011803965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.224941066696661,
    "arrivals": 196768,
    "finished_requests": 77351,
    "scheduler_time": 103.63958041892616
}
#Debug simulation 
Total elapsed time: 6.153854186180979. Arrivals time: 0.2712523718364537 Scheduler time: 5.777165321167558 Scheduler overhead time: 0.03222860721871257 Adapter cache time: 0.025140985380858183 Engine time: 0.03317544609308243 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.329127594362944,
    "estimated_duration": 3600.087535748262,
    "input_throughput": 5298.673382405744,
    "output_throughput": 4712.047924266418,
    "total_throughput": 10010.721306672162,
    "itl": 179.22528197175873,
    "ttft": 1709114.2233771493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1454,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.869294634535884,
    "arrivals": 196768,
    "finished_requests": 77044,
    "scheduler_time": 103.9294793529524
}
#Debug simulation 
Total elapsed time: 6.329236753284931. Arrivals time: 0.27509757224470377 Scheduler time: 5.946992389392108 Scheduler overhead time: 0.03272052714601159 Adapter cache time: 0.02582094445824623 Engine time: 0.03354433039203286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.258931436110288,
    "estimated_duration": 3600.010460868223,
    "input_throughput": 5401.315415986445,
    "output_throughput": 4725.627657175502,
    "total_throughput": 10126.943073161947,
    "itl": 180.64318541269319,
    "ttft": 1684460.8226978115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1351,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.134718942053258,
    "arrivals": 196098,
    "finished_requests": 78184,
    "scheduler_time": 103.57421877011464
}
#Debug simulation 
Total elapsed time: 6.258998780976981. Arrivals time: 0.2614949457347393 Scheduler time: 5.891398663632572 Scheduler overhead time: 0.03268533246591687 Adapter cache time: 0.02491491800174117 Engine time: 0.03356942441314459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.237718280404806,
    "estimated_duration": 3600.1575440437346,
    "input_throughput": 5401.063359621628,
    "output_throughput": 4725.417094078519,
    "total_throughput": 10126.480453700146,
    "itl": 180.6560587025547,
    "ttft": 1684521.624463858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.44166984034,
    "arrivals": 196098,
    "finished_requests": 78184,
    "scheduler_time": 103.5703776288889
}
#Debug simulation 
Total elapsed time: 6.237788024358451. Arrivals time: 0.4931420302018523 Scheduler time: 5.638609872199595 Scheduler overhead time: 0.03255577990785241 Adapter cache time: 0.02491524163633585 Engine time: 0.03355969209223986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.218503179959953,
    "estimated_duration": 3600.120398873608,
    "input_throughput": 5385.656825829036,
    "output_throughput": 4713.117096114012,
    "total_throughput": 10098.77392194305,
    "itl": 178.72037223027954,
    "ttft": 1686947.4274631164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.531644921004709,
    "arrivals": 196098,
    "finished_requests": 77962,
    "scheduler_time": 103.778933040362
}
#Debug simulation 
Total elapsed time: 6.2186004957184196. Arrivals time: 0.4913061452098191 Scheduler time: 5.620672766584903 Scheduler overhead time: 0.032703653909265995 Adapter cache time: 0.02517773024737835 Engine time: 0.033651260659098625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.22810284094885,
    "estimated_duration": 3600.0928920565307,
    "input_throughput": 5401.191742275373,
    "output_throughput": 4725.519454661023,
    "total_throughput": 10126.711196936396,
    "itl": 180.6461877594355,
    "ttft": 1684491.7557864909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1351,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.209533600883959,
    "arrivals": 196098,
    "finished_requests": 78184,
    "scheduler_time": 103.5747507060191
}
#Debug simulation 
Total elapsed time: 6.228200151119381. Arrivals time: 0.26562700839713216 Scheduler time: 5.856788688339293 Scheduler overhead time: 0.03258829517289996 Adapter cache time: 0.02468501729890704 Engine time: 0.033529384061694145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.218907438218594,
    "estimated_duration": 3600.101489585232,
    "input_throughput": 5385.5851164445285,
    "output_throughput": 4712.889358559377,
    "total_throughput": 10098.474475003904,
    "itl": 178.6904488451354,
    "ttft": 1687000.6533501432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1404,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.640357501171501,
    "arrivals": 196098,
    "finished_requests": 77958,
    "scheduler_time": 103.78055113813046
}
#Debug simulation 
Total elapsed time: 6.21897822804749. Arrivals time: 0.2602032800205052 Scheduler time: 5.851680162362754 Scheduler overhead time: 0.03277422022074461 Adapter cache time: 0.0252980450168252 Engine time: 0.033794411923736334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.236628734041005,
    "estimated_duration": 3600.0974787431073,
    "input_throughput": 5401.432076441724,
    "output_throughput": 4725.670374330992,
    "total_throughput": 10127.102450772716,
    "itl": 180.63993386735496,
    "ttft": 1684420.9077596168,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1352,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.042547998707635,
    "arrivals": 196098,
    "finished_requests": 78187,
    "scheduler_time": 103.5792183245839
}
#Debug simulation 
Total elapsed time: 6.236748670693487. Arrivals time: 0.5131860142573714 Scheduler time: 5.618238850031048 Scheduler overhead time: 0.0324837202206254 Adapter cache time: 0.02420168463140726 Engine time: 0.03361896611750126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.191475679166615,
    "estimated_duration": 3600.006710898366,
    "input_throughput": 5385.767182406616,
    "output_throughput": 4713.045380897626,
    "total_throughput": 10098.812563304242,
    "itl": 178.72956555682202,
    "ttft": 1686957.9170050179,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.6450748363509735,
    "arrivals": 196098,
    "finished_requests": 77959,
    "scheduler_time": 103.77271060317746
}
#Debug simulation 
Total elapsed time: 6.191542249172926. Arrivals time: 0.49336746195331216 Scheduler time: 5.591837253887206 Scheduler overhead time: 0.032745946664363146 Adapter cache time: 0.024941719602793455 Engine time: 0.033562809228897095 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.821356897242367,
    "estimated_duration": 3600.1988282682596,
    "input_throughput": 5377.216627036109,
    "output_throughput": 4726.267023475664,
    "total_throughput": 10103.483650511773,
    "itl": 181.26786547093192,
    "ttft": 1690055.6540178554,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1979,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.056705245243094,
    "arrivals": 192525,
    "finished_requests": 77725,
    "scheduler_time": 103.5324375290247
}
#Debug simulation 
Total elapsed time: 5.821454598102719. Arrivals time: 0.2539550894871354 Scheduler time: 5.454890409950167 Scheduler overhead time: 0.03223067848011851 Adapter cache time: 0.032212152145802975 Engine time: 0.03329199133440852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.876978317741305,
    "estimated_duration": 3600.090492380772,
    "input_throughput": 5376.8706761587255,
    "output_throughput": 4725.560381330725,
    "total_throughput": 10102.43105748945,
    "itl": 181.28657494599582,
    "ttft": 1690049.6772903453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1987,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.485478900132946,
    "arrivals": 192525,
    "finished_requests": 77716,
    "scheduler_time": 103.5179076409422
}
#Debug simulation 
Total elapsed time: 5.877073687966913. Arrivals time: 0.25454409001395106 Scheduler time: 5.5100649683736265 Scheduler overhead time: 0.03242554981261492 Adapter cache time: 0.03206738969311118 Engine time: 0.03310427675023675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.831154815852642,
    "estimated_duration": 3600.2003755458336,
    "input_throughput": 5362.9995516770905,
    "output_throughput": 4712.623251539907,
    "total_throughput": 10075.622803216998,
    "itl": 179.2102417104308,
    "ttft": 1693252.5027901158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2016,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.590680869240268,
    "arrivals": 192525,
    "finished_requests": 77507,
    "scheduler_time": 103.75823291051704
}
#Debug simulation 
Total elapsed time: 5.831269870977849. Arrivals time: 0.26974708680063486 Scheduler time: 5.445307624060661 Scheduler overhead time: 0.032595576252788305 Adapter cache time: 0.03494039596989751 Engine time: 0.03344321018084884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.919924723915756,
    "estimated_duration": 3600.159258053975,
    "input_throughput": 5377.080737940447,
    "output_throughput": 4726.053149436789,
    "total_throughput": 10103.133887377235,
    "itl": 181.27526998637285,
    "ttft": 1690069.8701767381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1979,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.178401942094601,
    "arrivals": 192525,
    "finished_requests": 77721,
    "scheduler_time": 103.52812006461365
}
#Debug simulation 
Total elapsed time: 5.92001862032339. Arrivals time: 0.2558809397742152 Scheduler time: 5.551226631272584 Scheduler overhead time: 0.032272761687636375 Adapter cache time: 0.03232754860073328 Engine time: 0.03331893589347601 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.846094086300582,
    "estimated_duration": 3600.189697080009,
    "input_throughput": 5362.725196302603,
    "output_throughput": 4712.509736295419,
    "total_throughput": 10075.234932598023,
    "itl": 179.20716542589778,
    "ttft": 1693266.8002519263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2016,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.67820550458493,
    "arrivals": 192525,
    "finished_requests": 77505,
    "scheduler_time": 103.75620156391598
}
#Debug simulation 
Total elapsed time: 5.846189351286739. Arrivals time: 0.255325841717422 Scheduler time: 5.476620273664594 Scheduler overhead time: 0.032692987471818924 Adapter cache time: 0.03292967984452844 Engine time: 0.03363915113732219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.8737249043770134,
    "estimated_duration": 3600.0032376783956,
    "input_throughput": 5377.4765526278725,
    "output_throughput": 4726.549360264792,
    "total_throughput": 10104.025912892665,
    "itl": 181.2614325810878,
    "ttft": 1689919.077926966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1987,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.941229935970449,
    "arrivals": 192525,
    "finished_requests": 77726,
    "scheduler_time": 103.5296008540393
}
#Debug simulation 
Total elapsed time: 5.873818204272538. Arrivals time: 0.25487657357007265 Scheduler time: 5.506092520896345 Scheduler overhead time: 0.03232645383104682 Adapter cache time: 0.0323879043571651 Engine time: 0.03314425935968757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.827964197844267,
    "estimated_duration": 3600.0236037949817,
    "input_throughput": 5362.580672984784,
    "output_throughput": 4712.354102933298,
    "total_throughput": 10074.934775918082,
    "itl": 179.2079958608051,
    "ttft": 1693322.893226702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2016,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.760322727113751,
    "arrivals": 192525,
    "finished_requests": 77500,
    "scheduler_time": 103.74908493563784
}
#Debug simulation 
Total elapsed time: 5.82812375202775. Arrivals time: 0.2502229446545243 Scheduler time: 5.461586129385978 Scheduler overhead time: 0.03261468652635813 Adapter cache time: 0.03486851928755641 Engine time: 0.03368671843782067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.757844882085919,
    "estimated_duration": 3600.137152091294,
    "input_throughput": 5422.343698395014,
    "output_throughput": 4786.240154764816,
    "total_throughput": 10208.583853159831,
    "itl": 179.19337068213028,
    "ttft": 1662644.2345094061,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2034,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.225032071159402,
    "arrivals": 191098,
    "finished_requests": 79159,
    "scheduler_time": 104.62448370533144
}
#Debug simulation 
Total elapsed time: 5.7579364432021976. Arrivals time: 0.2554787164554 Scheduler time: 5.389331983402371 Scheduler overhead time: 0.03162355162203312 Adapter cache time: 0.03356274403631687 Engine time: 0.033176359720528126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.708800791762769,
    "estimated_duration": 3600.127321283849,
    "input_throughput": 5422.085736967452,
    "output_throughput": 4785.606858441885,
    "total_throughput": 10207.692595409337,
    "itl": 179.213069612514,
    "ttft": 1662760.125108056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2035,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.636356030020161,
    "arrivals": 191098,
    "finished_requests": 79152,
    "scheduler_time": 104.61225399799278
}
#Debug simulation 
Total elapsed time: 5.7088961116969585. Arrivals time: 0.2549906144849956 Scheduler time: 5.3409754522144794 Scheduler overhead time: 0.03167316457256675 Adapter cache time: 0.03340160334482789 Engine time: 0.0331054488196969 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.747062691953033,
    "estimated_duration": 3600.132363364469,
    "input_throughput": 5417.784134406668,
    "output_throughput": 4781.549749441053,
    "total_throughput": 10199.333883847721,
    "itl": 177.25833578715748,
    "ttft": 1663962.514752271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2043,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.674542115628678,
    "arrivals": 191098,
    "finished_requests": 79085,
    "scheduler_time": 104.94292942307315
}
#Debug simulation 
Total elapsed time: 5.747187365312129. Arrivals time: 0.25512304436415434 Scheduler time: 5.378162503708154 Scheduler overhead time: 0.03184444038197398 Adapter cache time: 0.03369229566305876 Engine time: 0.033427848014980555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.801033729687333,
    "estimated_duration": 3600.1344555278415,
    "input_throughput": 5422.136101063473,
    "output_throughput": 4785.842921378844,
    "total_throughput": 10207.979022442318,
    "itl": 179.19916242321062,
    "ttft": 1662707.0059863653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2036,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.345974641674932,
    "arrivals": 191098,
    "finished_requests": 79154,
    "scheduler_time": 104.62098687232971
}
#Debug simulation 
Total elapsed time: 5.801125960890204. Arrivals time: 0.26361702056601644 Scheduler time: 5.423943370115012 Scheduler overhead time: 0.03162477910518646 Adapter cache time: 0.0338070229627192 Engine time: 0.03334142826497555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.741634896956384,
    "estimated_duration": 3600.010917208354,
    "input_throughput": 5417.56079315557,
    "output_throughput": 4781.405222334157,
    "total_throughput": 10198.966015489726,
    "itl": 177.2621403580674,
    "ttft": 1663950.8057593647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2042,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.761578321717541,
    "arrivals": 191098,
    "finished_requests": 79079,
    "scheduler_time": 104.9374096057846
}
#Debug simulation 
Total elapsed time: 5.741732560098171. Arrivals time: 0.2559217377565801 Scheduler time: 5.371651848312467 Scheduler overhead time: 0.03191516920924187 Adapter cache time: 0.03384435782209039 Engine time: 0.03341421764343977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.788412711117417,
    "estimated_duration": 3600.0258432061423,
    "input_throughput": 5422.503018093526,
    "output_throughput": 4786.263141004166,
    "total_throughput": 10208.766159097691,
    "itl": 179.18815623049116,
    "ttft": 1662627.090811287,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2034,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.081762299830846,
    "arrivals": 191098,
    "finished_requests": 79158,
    "scheduler_time": 104.62409225534948
}
#Debug simulation 
Total elapsed time: 5.788525735959411. Arrivals time: 0.2651481661014259 Scheduler time: 5.409761314280331 Scheduler overhead time: 0.03180233435705304 Adapter cache time: 0.033703065011650324 Engine time: 0.033276332542300224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.746761268936098,
    "estimated_duration": 3600.0891593620127,
    "input_throughput": 5417.44305117606,
    "output_throughput": 4781.301306173876,
    "total_throughput": 10198.744357349935,
    "itl": 177.26542358746394,
    "ttft": 1663980.781079258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2042,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.841306222304484,
    "arrivals": 191098,
    "finished_requests": 79079,
    "scheduler_time": 104.93766829812576
}
#Debug simulation 
Total elapsed time: 5.746854371856898. Arrivals time: 0.2545971446670592 Scheduler time: 5.378018822986633 Scheduler overhead time: 0.031886300537735224 Adapter cache time: 0.03412670735269785 Engine time: 0.033393533900380135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.825071446131915,
    "estimated_duration": 3600.0342909524493,
    "input_throughput": 5447.282002086636,
    "output_throughput": 4855.818469266602,
    "total_throughput": 10303.100471353238,
    "itl": 177.50001156367478,
    "ttft": 1652329.6308588928,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.150800873038968,
    "arrivals": 190328,
    "finished_requests": 79584,
    "scheduler_time": 105.90113929353824
}
#Debug simulation 
Total elapsed time: 5.8252100963145494. Arrivals time: 0.2586249178275466 Scheduler time: 5.456454902887344 Scheduler overhead time: 0.03159537259489298 Adapter cache time: 0.030295470263808966 Engine time: 0.033340588212013245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.858527611941099,
    "estimated_duration": 3600.036371583459,
    "input_throughput": 5446.722748352494,
    "output_throughput": 4855.155113977935,
    "total_throughput": 10301.87786233043,
    "itl": 177.5151825215759,
    "ttft": 1652501.7291923515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1686,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.4906796474893165,
    "arrivals": 190328,
    "finished_requests": 79576,
    "scheduler_time": 105.89224284661057
}
#Debug simulation 
Total elapsed time: 5.858618650119752. Arrivals time: 0.25764059042558074 Scheduler time: 5.490544301457703 Scheduler overhead time: 0.03163918387144804 Adapter cache time: 0.030568831600248814 Engine time: 0.03323971526697278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.789866786915809,
    "estimated_duration": 3600.1131050759177,
    "input_throughput": 5439.926587969522,
    "output_throughput": 4849.996233557723,
    "total_throughput": 10289.922821527245,
    "itl": 175.1081134358119,
    "ttft": 1653005.2217704172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1687,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.507074779327912,
    "arrivals": 190328,
    "finished_requests": 79473,
    "scheduler_time": 106.26679033075595
}
#Debug simulation 
Total elapsed time: 5.789961163885891. Arrivals time: 0.25752225844189525 Scheduler time: 5.421330601442605 Scheduler overhead time: 0.0319410334341228 Adapter cache time: 0.03061807155609131 Engine time: 0.03356128744781017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.099557218607515,
    "estimated_duration": 3600.112896074083,
    "input_throughput": 5447.163065743052,
    "output_throughput": 4855.712446979961,
    "total_throughput": 10302.875512723012,
    "itl": 177.50282138286053,
    "ttft": 1652357.4103378796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.2430898306354505,
    "arrivals": 190328,
    "finished_requests": 79584,
    "scheduler_time": 105.90138023624483
}
#Debug simulation 
Total elapsed time: 6.099624913651496. Arrivals time: 0.25738908536732197 Scheduler time: 5.7320912894792855 Scheduler overhead time: 0.031803491059690714 Adapter cache time: 0.030048539396375418 Engine time: 0.03329480113461614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.822757158894092,
    "estimated_duration": 3600.152803433852,
    "input_throughput": 5439.8666026953915,
    "output_throughput": 4849.94275335925,
    "total_throughput": 10289.809356054642,
    "itl": 175.1285620094407,
    "ttft": 1653009.1777487628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1688,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.584061524588544,
    "arrivals": 190328,
    "finished_requests": 79473,
    "scheduler_time": 106.26385860426292
}
#Debug simulation 
Total elapsed time: 5.822848012205213. Arrivals time: 0.26558764884248376 Scheduler time: 5.445374998264015 Scheduler overhead time: 0.032114369329065084 Adapter cache time: 0.03082414995878935 Engine time: 0.033881052397191525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.850783957634121,
    "estimated_duration": 3600.1108498372087,
    "input_throughput": 5447.372544344513,
    "output_throughput": 4855.791593414542,
    "total_throughput": 10303.164137759055,
    "itl": 177.49528826643882,
    "ttft": 1652296.7813288167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1682,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.0292645960253175,
    "arrivals": 190328,
    "finished_requests": 79586,
    "scheduler_time": 105.90641951643116
}
#Debug simulation 
Total elapsed time: 5.850876650772989. Arrivals time: 0.2591462410055101 Scheduler time: 5.481335369870067 Scheduler overhead time: 0.031760267447680235 Adapter cache time: 0.030464442912489176 Engine time: 0.03333829529583454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.853041968308389,
    "estimated_duration": 3600.017627888936,
    "input_throughput": 5439.800307723428,
    "output_throughput": 4849.900696246993,
    "total_throughput": 10289.70100397042,
    "itl": 175.13004073714495,
    "ttft": 1652997.8274962213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1687,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.645569712035257,
    "arrivals": 190328,
    "finished_requests": 79469,
    "scheduler_time": 106.25832028796677
}
#Debug simulation 
Total elapsed time: 5.853151449933648. Arrivals time: 0.27414694940671325 Scheduler time: 5.466406647581607 Scheduler overhead time: 0.03233645763248205 Adapter cache time: 0.030843749176710844 Engine time: 0.034029298927634954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.030007460154593,
    "estimated_duration": 3600.150905416725,
    "input_throughput": 5696.237890790924,
    "output_throughput": 5013.8284405916065,
    "total_throughput": 10710.06633138253,
    "itl": 170.81372124614592,
    "ttft": 1608238.4593287278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.391799912543618,
    "arrivals": 188114,
    "finished_requests": 82715,
    "scheduler_time": 109.3877658859478
}
#Debug simulation 
Total elapsed time: 6.030102141201496. Arrivals time: 0.2622948647476733 Scheduler time: 5.658313842955977 Scheduler overhead time: 0.03260907344520092 Adapter cache time: 0.026942726224660873 Engine time: 0.034426940605044365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.053253518883139,
    "estimated_duration": 3600.1184144854747,
    "input_throughput": 5695.555712139127,
    "output_throughput": 5013.193157031034,
    "total_throughput": 10708.748869170162,
    "itl": 170.82583166698248,
    "ttft": 1608376.7235728428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.682026605184602,
    "arrivals": 188114,
    "finished_requests": 82706,
    "scheduler_time": 109.37800469935463
}
#Debug simulation 
Total elapsed time: 6.053427477832884. Arrivals time: 0.271021100692451 Scheduler time: 5.673279717564583 Scheduler overhead time: 0.03263716772198677 Adapter cache time: 0.02668076427653432 Engine time: 0.03438941482454538 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.015477068256587,
    "estimated_duration": 3600.135200909274,
    "input_throughput": 5686.113120093314,
    "output_throughput": 5006.184766463217,
    "total_throughput": 10692.297886556531,
    "itl": 168.6227790313681,
    "ttft": 1609823.713572324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.689175366368083,
    "arrivals": 188114,
    "finished_requests": 82567,
    "scheduler_time": 109.74536308527597
}
#Debug simulation 
Total elapsed time: 6.015571742318571. Arrivals time: 0.26555336778983474 Scheduler time: 5.639612187631428 Scheduler overhead time: 0.03315769275650382 Adapter cache time: 0.026609689462929964 Engine time: 0.03502681013196707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.983118801843375,
    "estimated_duration": 3600.058877217636,
    "input_throughput": 5696.112119101967,
    "output_throughput": 5013.558559894872,
    "total_throughput": 10709.67067899684,
    "itl": 170.81779792041246,
    "ttft": 1608295.9227160208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.485622140080591,
    "arrivals": 188114,
    "finished_requests": 82710,
    "scheduler_time": 109.3820645166166
}
#Debug simulation 
Total elapsed time: 5.9832121399231255. Arrivals time: 0.26432360243052244 Scheduler time: 5.60960642201826 Scheduler overhead time: 0.03263305081054568 Adapter cache time: 0.026877428870648146 Engine time: 0.034398247953504324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.274224325083196,
    "estimated_duration": 3600.0625015685523,
    "input_throughput": 5686.138224289438,
    "output_throughput": 5005.9014231413985,
    "total_throughput": 10692.039647430836,
    "itl": 168.61721482446742,
    "ttft": 1609881.80746808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.750417460352148,
    "arrivals": 188114,
    "finished_requests": 82563,
    "scheduler_time": 109.74269679067652
}
#Debug simulation 
Total elapsed time: 6.274337049107999. Arrivals time: 0.2697946014814079 Scheduler time: 5.894353875424713 Scheduler overhead time: 0.03310281690210104 Adapter cache time: 0.0266278306953609 Engine time: 0.03481103526428342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.012723168823868,
    "estimated_duration": 3600.0154361807995,
    "input_throughput": 5696.45196348266,
    "output_throughput": 5013.949889934161,
    "total_throughput": 10710.40185341682,
    "itl": 170.80807422727375,
    "ttft": 1608199.0325857918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2937122234794085,
    "arrivals": 188114,
    "finished_requests": 82714,
    "scheduler_time": 109.3860802473984
}
#Debug simulation 
Total elapsed time: 6.012816522270441. Arrivals time: 0.2614892888814211 Scheduler time: 5.642371709924191 Scheduler overhead time: 0.03262575715780258 Adapter cache time: 0.02648015646263957 Engine time: 0.03453263035044074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.965271919034421,
    "estimated_duration": 3600.086251828915,
    "input_throughput": 5686.090156756834,
    "output_throughput": 5005.853398885852,
    "total_throughput": 10691.943555642685,
    "itl": 168.62707000635953,
    "ttft": 1609919.297543427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.81027626268564,
    "arrivals": 188114,
    "finished_requests": 82561,
    "scheduler_time": 109.74021315322317
}
#Debug simulation 
Total elapsed time: 5.965399629902095. Arrivals time: 0.2635396118275821 Scheduler time: 5.592094060499221 Scheduler overhead time: 0.03292412031441927 Adapter cache time: 0.02651493577286601 Engine time: 0.03476553410291672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.135507605969906,
    "estimated_duration": 3600.1090713195185,
    "input_throughput": 5777.018581377902,
    "output_throughput": 5115.479457751548,
    "total_throughput": 10892.49803912945,
    "itl": 168.27748493601834,
    "ttft": 1590041.737553273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.540984319730283,
    "arrivals": 187444,
    "finished_requests": 84018,
    "scheduler_time": 111.15007139642746
}
#Debug simulation 
Total elapsed time: 6.135599577799439. Arrivals time: 0.2663399283774197 Scheduler time: 5.761065282393247 Scheduler overhead time: 0.033191236201673746 Adapter cache time: 0.024283832404762506 Engine time: 0.03511693840846419 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.110414306167513,
    "estimated_duration": 3600.0657213067134,
    "input_throughput": 5776.788428309968,
    "output_throughput": 5115.230783430215,
    "total_throughput": 10892.019211740182,
    "itl": 168.28931545976496,
    "ttft": 1589988.6610078637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7738278741482687,
    "arrivals": 187444,
    "finished_requests": 84014,
    "scheduler_time": 111.14163016373988
}
#Debug simulation 
Total elapsed time: 6.110506712924689. Arrivals time: 0.2768153278157115 Scheduler time: 5.725282276514918 Scheduler overhead time: 0.03328479267656803 Adapter cache time: 0.02447191858664155 Engine time: 0.03496226342394948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.111275899223983,
    "estimated_duration": 3600.029936842258,
    "input_throughput": 5764.826783136097,
    "output_throughput": 5106.680589474648,
    "total_throughput": 10871.507372610744,
    "itl": 165.9230563677914,
    "ttft": 1592441.3413481389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7874644554033337,
    "arrivals": 187444,
    "finished_requests": 83845,
    "scheduler_time": 111.53352289764182
}
#Debug simulation 
Total elapsed time: 6.111407638993114. Arrivals time: 0.26551759615540504 Scheduler time: 5.735934378579259 Scheduler overhead time: 0.03366607706993818 Adapter cache time: 0.02475209254771471 Engine time: 0.03564203530550003 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.113312711007893,
    "estimated_duration": 3600.0867688155317,
    "input_throughput": 5776.90020700322,
    "output_throughput": 5115.290042317202,
    "total_throughput": 10892.190249320422,
    "itl": 168.28248929784232,
    "ttft": 1590081.047686008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6099811514536415,
    "arrivals": 187444,
    "finished_requests": 84016,
    "scheduler_time": 111.14652624106597
}
#Debug simulation 
Total elapsed time: 6.11340392427519. Arrivals time: 0.26677907491102815 Scheduler time: 5.738305125851184 Scheduler overhead time: 0.03332623280584812 Adapter cache time: 0.02429503295570612 Engine time: 0.03518079500645399 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.090811672154814,
    "estimated_duration": 3600.0803857773185,
    "input_throughput": 5764.745999003285,
    "output_throughput": 5106.609028128837,
    "total_throughput": 10871.355027132122,
    "itl": 165.92450248789538,
    "ttft": 1592461.0172130077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8378917237557517,
    "arrivals": 187444,
    "finished_requests": 83845,
    "scheduler_time": 111.53385714375908
}
#Debug simulation 
Total elapsed time: 6.0909297852776945. Arrivals time: 0.2676570825278759 Scheduler time: 5.714331300929189 Scheduler overhead time: 0.03333520516753197 Adapter cache time: 0.02411160059273243 Engine time: 0.03572269482538104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.041862090118229,
    "estimated_duration": 3600.17289482397,
    "input_throughput": 5776.916444735611,
    "output_throughput": 5115.389326573012,
    "total_throughput": 10892.305771308622,
    "itl": 168.27222009705034,
    "ttft": 1590123.5110776038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4624782414966333,
    "arrivals": 187444,
    "finished_requests": 84019,
    "scheduler_time": 111.15473139433209
}
#Debug simulation 
Total elapsed time: 6.041956378147006. Arrivals time: 0.2661791811697185 Scheduler time: 5.668360369745642 Scheduler overhead time: 0.03288328927010298 Adapter cache time: 0.024266306776553392 Engine time: 0.0347639680840075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.1289120321162045,
    "estimated_duration": 3600.1275192635017,
    "input_throughput": 5764.670525961166,
    "output_throughput": 5106.542171528679,
    "total_throughput": 10871.212697489846,
    "itl": 165.92514754456624,
    "ttft": 1592480.714819113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.883288840651569,
    "arrivals": 187444,
    "finished_requests": 83845,
    "scheduler_time": 111.53452033898706
}
#Debug simulation 
Total elapsed time: 6.129024663940072. Arrivals time: 0.2643541293218732 Scheduler time: 5.7558035724796355 Scheduler overhead time: 0.03347455803304911 Adapter cache time: 0.024274857714772224 Engine time: 0.035292706452310085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.283907734323293,
    "estimated_duration": 3600.0836482606096,
    "input_throughput": 5992.688533896601,
    "output_throughput": 5298.501330438853,
    "total_throughput": 11291.189864335454,
    "itl": 162.26037458438725,
    "ttft": 1545696.452839874,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.402482878987259,
    "arrivals": 185958,
    "finished_requests": 87254,
    "scheduler_time": 114.94016644497201
}
#Debug simulation 
Total elapsed time: 6.284000817220658. Arrivals time: 0.2713237311691046 Scheduler time: 5.908102995716035 Scheduler overhead time: 0.034112634137272835 Adapter cache time: 0.0183762451633811 Engine time: 0.03599344985559583 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.204837658908218,
    "estimated_duration": 3600.137787509316,
    "input_throughput": 5992.474808839293,
    "output_throughput": 5298.293044832702,
    "total_throughput": 11290.767853671994,
    "itl": 162.2681302547642,
    "ttft": 1545755.0311215774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 786,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5651006692927387,
    "arrivals": 185958,
    "finished_requests": 87253,
    "scheduler_time": 114.93648587252315
}
#Debug simulation 
Total elapsed time: 6.204931480810046. Arrivals time: 0.26674533635377884 Scheduler time: 5.834469421301037 Scheduler overhead time: 0.033724946435540915 Adapter cache time: 0.018305683508515358 Engine time: 0.035667622927576303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.131786262150854,
    "estimated_duration": 3600.091687801934,
    "input_throughput": 5978.388848518714,
    "output_throughput": 5288.238925832441,
    "total_throughput": 11266.627774351156,
    "itl": 160.23623091798558,
    "ttft": 1548146.0138994388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.56621723091229,
    "arrivals": 185958,
    "finished_requests": 87069,
    "scheduler_time": 115.24921423060789
}
#Debug simulation 
Total elapsed time: 6.131878128275275. Arrivals time: 0.2560754716396332 Scheduler time: 5.772152478341013 Scheduler overhead time: 0.03394765732809901 Adapter cache time: 0.018201946280896664 Engine time: 0.035437030252069235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.210660723038018,
    "estimated_duration": 3600.164879907066,
    "input_throughput": 5992.567762773386,
    "output_throughput": 5298.585380481913,
    "total_throughput": 11291.153143255298,
    "itl": 162.2637566491002,
    "ttft": 1545725.0584905904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4534242642926785,
    "arrivals": 185958,
    "finished_requests": 87255,
    "scheduler_time": 114.9412248881381
}
#Debug simulation 
Total elapsed time: 6.210765418130904. Arrivals time: 0.2589924316853285 Scheduler time: 5.848842891398817 Scheduler overhead time: 0.033519035670906305 Adapter cache time: 0.018028338439762592 Engine time: 0.035476087126880884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.210753459949046,
    "estimated_duration": 3600.123429245584,
    "input_throughput": 5978.336138466829,
    "output_throughput": 5288.192300670507,
    "total_throughput": 11266.528439137335,
    "itl": 160.23736725854636,
    "ttft": 1548159.5063417687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.59954198431224,
    "arrivals": 185958,
    "finished_requests": 87069,
    "scheduler_time": 115.24928962871824
}
#Debug simulation 
Total elapsed time: 6.210848032031208. Arrivals time: 0.2678217743523419 Scheduler time: 5.838604404591024 Scheduler overhead time: 0.03400662587955594 Adapter cache time: 0.01825460884720087 Engine time: 0.03603040473535657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.242506302893162,
    "estimated_duration": 3600.1570793891306,
    "input_throughput": 5992.785182490095,
    "output_throughput": 5298.721022260736,
    "total_throughput": 11291.50620475083,
    "itl": 162.2583640219653,
    "ttft": 1545673.3206960487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3471894814981615,
    "arrivals": 185958,
    "finished_requests": 87257,
    "scheduler_time": 114.94385533704228
}
#Debug simulation 
Total elapsed time: 6.242629434913397. Arrivals time: 0.2606721902266145 Scheduler time: 5.87710418459028 Scheduler overhead time: 0.03370076743885875 Adapter cache time: 0.018207301385700703 Engine time: 0.03682049177587032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.175144032109529,
    "estimated_duration": 3600.1093801380216,
    "input_throughput": 5978.35946839339,
    "output_throughput": 5288.212937371951,
    "total_throughput": 11266.57240576534,
    "itl": 160.25150092263434,
    "ttft": 1548145.4267091136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.632237968780115,
    "arrivals": 185958,
    "finished_requests": 87069,
    "scheduler_time": 115.24566710615468
}
#Debug simulation 
Total elapsed time: 6.1752754230983555. Arrivals time: 0.2607672158628702 Scheduler time: 5.810423087794334 Scheduler overhead time: 0.033905986696481705 Adapter cache time: 0.018169627524912357 Engine time: 0.03587055392563343 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_192_slots_96_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_192_slots_96_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 17.05219258274883,
    "estimated_duration": 3600.0613591684237,
    "input_throughput": 5379.098317500103,
    "output_throughput": 4727.427480272508,
    "total_throughput": 10106.52579777261,
    "itl": 179.91808805172042,
    "ttft": 1393586.3401230758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 494,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5118809455027968,
    "arrivals": 126417,
    "finished_requests": 78056,
    "scheduler_time": 96.58556340338254
}
#Debug simulation 
Total elapsed time: 17.052311837673187. Arrivals time: 0.2907692543230951 Scheduler time: 16.656092182267457 Scheduler overhead time: 0.03694778308272362 Adapter cache time: 0.013913949951529503 Engine time: 0.03887063730508089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_192_slots_96_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_192_slots_96_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 17.20085955876857,
    "estimated_duration": 3600.186629910955,
    "input_throughput": 5378.59922014069,
    "output_throughput": 4726.150544151329,
    "total_throughput": 10104.749764292019,
    "itl": 179.91299747554973,
    "ttft": 1393997.890688643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.634077438786632,
    "arrivals": 126417,
    "finished_requests": 78039,
    "scheduler_time": 96.58995044358124
}
#Debug simulation 
Total elapsed time: 17.200963333714753. Arrivals time: 0.2768050907179713 Scheduler time: 16.818388415966183 Scheduler overhead time: 0.036956531926989555 Adapter cache time: 0.014089697040617466 Engine time: 0.03861565142869949 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_192_slots_96_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_192_slots_96_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 16.78180590691045,
    "estimated_duration": 3600.033807596297,
    "input_throughput": 5367.020987200319,
    "output_throughput": 4716.57959549476,
    "total_throughput": 10083.600582695079,
    "itl": 178.0220760932222,
    "ttft": 1396789.4368760688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 499,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6306820737198093,
    "arrivals": 126417,
    "finished_requests": 77883,
    "scheduler_time": 96.7787032457581
}
#Debug simulation 
Total elapsed time: 16.781915762927383. Arrivals time: 0.2676810319535434 Scheduler time: 16.408463195431978 Scheduler overhead time: 0.03741161758080125 Adapter cache time: 0.013887066394090652 Engine time: 0.03863669652491808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_192_slots_96_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_192_slots_96_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 17.36187071306631,
    "estimated_duration": 3600.095724262648,
    "input_throughput": 5380.097220600162,
    "output_throughput": 4727.067084719119,
    "total_throughput": 10107.16430531928,
    "itl": 179.90463722463585,
    "ttft": 1393780.9062327088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 492,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.536888591360761,
    "arrivals": 126417,
    "finished_requests": 78043,
    "scheduler_time": 96.5889525228334
}
#Debug simulation 
Total elapsed time: 17.36199742089957. Arrivals time: 0.2718351436778903 Scheduler time: 16.982510306872427 Scheduler overhead time: 0.038657383527606726 Adapter cache time: 0.01389144966378808 Engine time: 0.03914685593917966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_192_slots_96_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_192_slots_96_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 16.831969544291496,
    "estimated_duration": 3600.1447588085734,
    "input_throughput": 5366.365325374758,
    "output_throughput": 4716.080084962712,
    "total_throughput": 10082.44541033747,
    "itl": 178.05552625963173,
    "ttft": 1397190.000655988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6617395151592833,
    "arrivals": 126417,
    "finished_requests": 77889,
    "scheduler_time": 96.77609730254606
}
#Debug simulation 
Total elapsed time: 16.832090119365603. Arrivals time: 0.2673821384087205 Scheduler time: 16.4585039508529 Scheduler overhead time: 0.0373599398881197 Adapter cache time: 0.013909911271184683 Engine time: 0.03901721024885774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_192_slots_96_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_192_slots_96_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 17.057321744039655,
    "estimated_duration": 3600.0774470710726,
    "input_throughput": 5378.7623418362855,
    "output_throughput": 4726.293878439468,
    "total_throughput": 10105.056220275754,
    "itl": 179.90660362014424,
    "ttft": 1393954.209339023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4980151977459737,
    "arrivals": 126417,
    "finished_requests": 78039,
    "scheduler_time": 96.5887351897514
}
#Debug simulation 
Total elapsed time: 17.05744807375595. Arrivals time: 0.26910179387778044 Scheduler time: 16.682525974232703 Scheduler overhead time: 0.03744660038501024 Adapter cache time: 0.014069515746086836 Engine time: 0.0384531756862998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_192_slots_96_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_192_slots_96_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 16.86257589608431,
    "estimated_duration": 3600.0602915164254,
    "input_throughput": 5366.525956670038,
    "output_throughput": 4716.093794320426,
    "total_throughput": 10082.619750990465,
    "itl": 178.05224775269923,
    "ttft": 1397095.0581205084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6829518911987524,
    "arrivals": 126417,
    "finished_requests": 77880,
    "scheduler_time": 96.77397653316073
}
#Debug simulation 
Total elapsed time: 16.862659065052867. Arrivals time: 0.2692001615650952 Scheduler time: 16.487892417237163 Scheduler overhead time: 0.0372069189324975 Adapter cache time: 0.01409999094903469 Engine time: 0.03845517057925463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_192_slots_96_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_192_slots_96_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 13.448282449971884,
    "estimated_duration": 3600.1732441440786,
    "input_throughput": 5418.19703585891,
    "output_throughput": 4731.321479516645,
    "total_throughput": 10149.518515375554,
    "itl": 179.3512719504485,
    "ttft": 1356239.3994694285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 579,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.77202240373709,
    "arrivals": 120741,
    "finished_requests": 78292,
    "scheduler_time": 95.28130238527969
}
#Debug simulation 
Total elapsed time: 13.448382055852562. Arrivals time: 0.2565045957453549 Scheduler time: 13.091379913501441 Scheduler overhead time: 0.03459279704838991 Adapter cache time: 0.014719801023602486 Engine time: 0.035822274163365364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_192_slots_96_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_192_slots_96_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 13.691087357699871,
    "estimated_duration": 3600.190219271759,
    "input_throughput": 5417.795952999801,
    "output_throughput": 4729.393160632534,
    "total_throughput": 10147.189113632334,
    "itl": 179.3517441033923,
    "ttft": 1355974.0217156892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 567,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8493989522615508,
    "arrivals": 120741,
    "finished_requests": 78291,
    "scheduler_time": 95.28112737021857
}
#Debug simulation 
Total elapsed time: 13.691172679886222. Arrivals time: 0.2588360230438411 Scheduler time: 13.331116090528667 Scheduler overhead time: 0.03515923302620649 Adapter cache time: 0.014541362877935171 Engine time: 0.036245512310415506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_192_slots_96_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_192_slots_96_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 13.229723687749356,
    "estimated_duration": 3600.145345240631,
    "input_throughput": 5403.997654072389,
    "output_throughput": 4719.961104477084,
    "total_throughput": 10123.958758549472,
    "itl": 177.629710731222,
    "ttft": 1359823.2371373917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 577,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8856134136393783,
    "arrivals": 120741,
    "finished_requests": 78082,
    "scheduler_time": 95.43374334946228
}
#Debug simulation 
Total elapsed time: 13.229849309660494. Arrivals time: 0.25810997001826763 Scheduler time: 12.870914092287421 Scheduler overhead time: 0.03500456875190139 Adapter cache time: 0.014799203723669052 Engine time: 0.035501097328960896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_192_slots_96_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_192_slots_96_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 13.736006525345147,
    "estimated_duration": 3600.0078789769073,
    "input_throughput": 5416.095090753292,
    "output_throughput": 4730.612146560039,
    "total_throughput": 10146.70723731333,
    "itl": 179.39901561769256,
    "ttft": 1357117.846066386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8058821809571137,
    "arrivals": 120741,
    "finished_requests": 78267,
    "scheduler_time": 95.27504455028414
}
#Debug simulation 
Total elapsed time: 13.736086414195597. Arrivals time: 0.2579353991895914 Scheduler time: 13.376090893521905 Scheduler overhead time: 0.035155808087438345 Adapter cache time: 0.015030627604573965 Engine time: 0.03631280828267336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_192_slots_96_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_192_slots_96_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 13.248173980042338,
    "estimated_duration": 3600.1241700174605,
    "input_throughput": 5403.628619816646,
    "output_throughput": 4720.354131540296,
    "total_throughput": 10123.982751356943,
    "itl": 177.7625065587319,
    "ttft": 1360044.564975713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.899655855465684,
    "arrivals": 120741,
    "finished_requests": 78090,
    "scheduler_time": 95.41636963177599
}
#Debug simulation 
Total elapsed time: 13.248254016041756. Arrivals time: 0.2564352382905781 Scheduler time: 12.890031651128083 Scheduler overhead time: 0.03510219231247902 Adapter cache time: 0.014890171587467194 Engine time: 0.036267746705561876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_192_slots_96_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_192_slots_96_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 13.627839596942067,
    "estimated_duration": 3600.0633710684015,
    "input_throughput": 5417.986848995776,
    "output_throughput": 4729.5598007617655,
    "total_throughput": 10147.546649757542,
    "itl": 179.3445273481327,
    "ttft": 1355923.8280311218,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 567,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6953585172095103,
    "arrivals": 120741,
    "finished_requests": 78291,
    "scheduler_time": 95.2795962547933
}
#Debug simulation 
Total elapsed time: 13.627957100979984. Arrivals time: 0.25977536384016275 Scheduler time: 13.26735217962414 Scheduler overhead time: 0.03471744293347001 Adapter cache time: 0.014771915972232819 Engine time: 0.035913772881031036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_192_slots_96_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_192_slots_96_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 13.351279562804848,
    "estimated_duration": 3600.086839360806,
    "input_throughput": 5405.365167095546,
    "output_throughput": 4719.408380442461,
    "total_throughput": 10124.773547538007,
    "itl": 177.67242230025514,
    "ttft": 1359662.7646809618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 573,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9198767871037083,
    "arrivals": 120741,
    "finished_requests": 78081,
    "scheduler_time": 95.42497199730938
}
#Debug simulation 
Total elapsed time: 13.351363023743033. Arrivals time: 0.2630184143781662 Scheduler time: 12.985871105920523 Scheduler overhead time: 0.03532557003200054 Adapter cache time: 0.014982292428612709 Engine time: 0.03646878991276026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 12.613986117299646,
    "estimated_duration": 3600.10394287134,
    "input_throughput": 5332.083824415851,
    "output_throughput": 4736.101865548093,
    "total_throughput": 10068.185689963944,
    "itl": 180.9178335364504,
    "ttft": 1344336.8089909137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6036955778207826,
    "arrivals": 117840,
    "finished_requests": 77768,
    "scheduler_time": 94.40897236168853
}
#Debug simulation 
Total elapsed time: 12.614091063383967. Arrivals time: 0.2594725866802037 Scheduler time: 12.256946452893317 Scheduler overhead time: 0.033865089528262615 Adapter cache time: 0.01399785140529275 Engine time: 0.03449080418795347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 12.55687311803922,
    "estimated_duration": 3600.06437499296,
    "input_throughput": 5334.157114909244,
    "output_throughput": 4737.240844487218,
    "total_throughput": 10071.397959396463,
    "itl": 180.90522039808238,
    "ttft": 1344503.0660867956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 531,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7339938305947065,
    "arrivals": 117840,
    "finished_requests": 77786,
    "scheduler_time": 94.4076880885828
}
#Debug simulation 
Total elapsed time: 12.556977747939527. Arrivals time: 0.26146159833297133 Scheduler time: 12.19716585520655 Scheduler overhead time: 0.03372865775600076 Adapter cache time: 0.013832466676831245 Engine time: 0.03536854963749647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 12.18090766388923,
    "estimated_duration": 3600.1005291718757,
    "input_throughput": 5319.930608828183,
    "output_throughput": 4721.906752951237,
    "total_throughput": 10041.837361779419,
    "itl": 178.5378558999701,
    "ttft": 1349246.7663556037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 544,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7804558267071953,
    "arrivals": 117840,
    "finished_requests": 77547,
    "scheduler_time": 94.6277515245042
}
#Debug simulation 
Total elapsed time: 12.180985264014453. Arrivals time: 0.24870446184650064 Scheduler time: 11.833324728533626 Scheduler overhead time: 0.03416286362335086 Adapter cache time: 0.014129227958619595 Engine time: 0.03526226105168462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 12.58473049197346,
    "estimated_duration": 3600.1289170977684,
    "input_throughput": 5331.718513978628,
    "output_throughput": 4736.965645593539,
    "total_throughput": 10068.684159572167,
    "itl": 180.95155635274045,
    "ttft": 1343795.6305505831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 531,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6584036966832276,
    "arrivals": 117840,
    "finished_requests": 77767,
    "scheduler_time": 94.40551714847305
}
#Debug simulation 
Total elapsed time: 12.584820452146232. Arrivals time: 0.24968076730147004 Scheduler time: 12.236466977745295 Scheduler overhead time: 0.03409254550933838 Adapter cache time: 0.013770835008472204 Engine time: 0.035494972951710224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 11.808366524986923,
    "estimated_duration": 3600.041885463488,
    "input_throughput": 5318.927003965883,
    "output_throughput": 4722.018948902146,
    "total_throughput": 10040.945952868029,
    "itl": 178.64311861407424,
    "ttft": 1348780.9483542582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 554,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8379852297157082,
    "arrivals": 117840,
    "finished_requests": 77532,
    "scheduler_time": 94.61413079756224
}
#Debug simulation 
Total elapsed time: 11.808473385870457. Arrivals time: 0.24503658106550574 Scheduler time: 11.465604769065976 Scheduler overhead time: 0.03372929757460952 Adapter cache time: 0.013996541034430265 Engine time: 0.03477553138509393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 12.498518748674542,
    "estimated_duration": 3600.034200617615,
    "input_throughput": 5331.349073491379,
    "output_throughput": 4736.484724804746,
    "total_throughput": 10067.833798296126,
    "itl": 180.93477597010536,
    "ttft": 1344114.6630884483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 531,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5877167065930358,
    "arrivals": 117840,
    "finished_requests": 77760,
    "scheduler_time": 94.40394994749862
}
#Debug simulation 
Total elapsed time: 12.498619614634663. Arrivals time: 0.2470105802640319 Scheduler time: 12.154322955291718 Scheduler overhead time: 0.03357894904911518 Adapter cache time: 0.013608977664262056 Engine time: 0.03494848310947418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_192_slots_96_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 11.817386280279607,
    "estimated_duration": 3600.037352909341,
    "input_throughput": 5320.410351998463,
    "output_throughput": 4723.067105472944,
    "total_throughput": 10043.477457471407,
    "itl": 178.6302552982588,
    "ttft": 1348613.7407332147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8685999679565384,
    "arrivals": 117840,
    "finished_requests": 77551,
    "scheduler_time": 94.61396229642406
}
#Debug simulation 
Total elapsed time: 11.817494254093617. Arrivals time: 0.24451535427942872 Scheduler time: 11.474831346888095 Scheduler overhead time: 0.03376338770613074 Adapter cache time: 0.014189560431987047 Engine time: 0.034713400062173605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 10.808796717319638,
    "estimated_duration": 3600.04092257498,
    "input_throughput": 5299.754200114269,
    "output_throughput": 4740.9699964721785,
    "total_throughput": 10040.724196586449,
    "itl": 181.30038270370727,
    "ttft": 1332964.1139698457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.75059898952956,
    "arrivals": 116430,
    "finished_requests": 77604,
    "scheduler_time": 93.73106261130458
}
#Debug simulation 
Total elapsed time: 10.808893397450447. Arrivals time: 0.23995904577896 Scheduler time: 10.473205107729882 Scheduler overhead time: 0.03301443113014102 Adapter cache time: 0.014031521510332823 Engine time: 0.03367196815088391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.91710343817249,
    "estimated_duration": 3600.1284616736452,
    "input_throughput": 5299.63477779076,
    "output_throughput": 4738.891453909057,
    "total_throughput": 10038.526231699818,
    "itl": 181.2923236381897,
    "ttft": 1333297.5860483018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8386645370372634,
    "arrivals": 116430,
    "finished_requests": 77606,
    "scheduler_time": 93.73273815696784
}
#Debug simulation 
Total elapsed time: 10.917204890865833. Arrivals time: 0.24576213350519538 Scheduler time: 10.574963061138988 Scheduler overhead time: 0.033190031070262194 Adapter cache time: 0.014116075355559587 Engine time: 0.03411206044256687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.636449400801212,
    "estimated_duration": 3600.1895218681498,
    "input_throughput": 5282.913270113271,
    "output_throughput": 4725.79371076318,
    "total_throughput": 10008.70698087645,
    "itl": 178.79057435033673,
    "ttft": 1337781.2485598207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8791891284473357,
    "arrivals": 116430,
    "finished_requests": 77348,
    "scheduler_time": 93.9763089498417
}
#Debug simulation 
Total elapsed time: 10.636538351885974. Arrivals time: 0.24286067625507712 Scheduler time: 10.296533717773855 Scheduler overhead time: 0.03345417510718107 Adapter cache time: 0.01427333103492856 Engine time: 0.034216648899018764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 10.872014196123928,
    "estimated_duration": 3600.1015345606534,
    "input_throughput": 5299.508310208901,
    "output_throughput": 4740.791846050767,
    "total_throughput": 10040.30015625967,
    "itl": 181.30096909517556,
    "ttft": 1333006.0840723938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7875332838692606,
    "arrivals": 116430,
    "finished_requests": 77603,
    "scheduler_time": 93.73233759340691
}
#Debug simulation 
Total elapsed time: 10.872118010185659. Arrivals time: 0.2445157584734261 Scheduler time: 10.530415834393352 Scheduler overhead time: 0.03342423355206847 Adapter cache time: 0.013914485927671194 Engine time: 0.03471305035054684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 10.582294035702944,
    "estimated_duration": 3600.0568519207272,
    "input_throughput": 5282.341857977621,
    "output_throughput": 4725.164823695561,
    "total_throughput": 10007.506681673181,
    "itl": 178.66178593641658,
    "ttft": 1338658.9245493961,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 579,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9175714296661353,
    "arrivals": 116430,
    "finished_requests": 77324,
    "scheduler_time": 93.98855893247078
}
#Debug simulation 
Total elapsed time: 10.58237052662298. Arrivals time: 0.24528891500085592 Scheduler time: 10.239681701175869 Scheduler overhead time: 0.033244125079363585 Adapter cache time: 0.014411743730306625 Engine time: 0.034575365483760834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 10.829534077085555,
    "estimated_duration": 3600.0881990927705,
    "input_throughput": 5299.6309937095375,
    "output_throughput": 4740.920515308792,
    "total_throughput": 10040.55150901833,
    "itl": 181.31016090166543,
    "ttft": 1333066.9231226644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 567,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6953585172095103,
    "arrivals": 116430,
    "finished_requests": 77600,
    "scheduler_time": 93.73264210966543
}
#Debug simulation 
Total elapsed time: 10.829610907007009. Arrivals time: 0.24589650705456734 Scheduler time: 10.488039714284241 Scheduler overhead time: 0.03293989319354296 Adapter cache time: 0.014040162786841393 Engine time: 0.03366873413324356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.615318899042904,
    "estimated_duration": 3600.127478762923,
    "input_throughput": 5282.545441011693,
    "output_throughput": 4725.4490571110955,
    "total_throughput": 10007.994498122789,
    "itl": 178.68123263235728,
    "ttft": 1338649.589218071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 580,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9453484305739361,
    "arrivals": 116430,
    "finished_requests": 77324,
    "scheduler_time": 93.98771486722292
}
#Debug simulation 
Total elapsed time: 10.615404322277755. Arrivals time: 0.24372528353706002 Scheduler time: 10.274288333486766 Scheduler overhead time: 0.033415501937270164 Adapter cache time: 0.01439881743863225 Engine time: 0.034329378977417946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.690762093290687,
    "estimated_duration": 3600.16449795423,
    "input_throughput": 5356.6643999068665,
    "output_throughput": 4735.808602548127,
    "total_throughput": 10092.473002454994,
    "itl": 180.5548647316508,
    "ttft": 1320463.4024741796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 556,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7016311856266342,
    "arrivals": 115680,
    "finished_requests": 77707,
    "scheduler_time": 93.20244454296602
}
#Debug simulation 
Total elapsed time: 9.690866634249687. Arrivals time: 0.23558564018458128 Scheduler time: 9.359727062750608 Scheduler overhead time: 0.03282453026622534 Adapter cache time: 0.01395797124132514 Engine time: 0.033733023796230555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.71898753521964,
    "estimated_duration": 3600.0955970424043,
    "input_throughput": 5356.169157241653,
    "output_throughput": 4736.027569381006,
    "total_throughput": 10092.196726622658,
    "itl": 180.55770904723386,
    "ttft": 1320549.1406085652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 558,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8208541183336697,
    "arrivals": 115680,
    "finished_requests": 77705,
    "scheduler_time": 93.19927095426601
}
#Debug simulation 
Total elapsed time: 9.719076538924128. Arrivals time: 0.23763526091352105 Scheduler time: 9.384797321166843 Scheduler overhead time: 0.033207801170647144 Adapter cache time: 0.01402311772108078 Engine time: 0.03428802825510502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.474466392304748,
    "estimated_duration": 3600.0735581621425,
    "input_throughput": 5342.255287088718,
    "output_throughput": 4722.721834794867,
    "total_throughput": 10064.977121883585,
    "itl": 177.9750366779595,
    "ttft": 1324843.2040755309,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8761456095986184,
    "arrivals": 115680,
    "finished_requests": 77472,
    "scheduler_time": 93.43715245197532
}
#Debug simulation 
Total elapsed time: 9.474555269349366. Arrivals time: 0.23639287799596786 Scheduler time: 9.141098659019917 Scheduler overhead time: 0.03343634819611907 Adapter cache time: 0.01442047581076622 Engine time: 0.03392488695681095 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 9.683228749781847,
    "estimated_duration": 3600.128874452236,
    "input_throughput": 5356.643795962493,
    "output_throughput": 4736.109899008621,
    "total_throughput": 10092.753694971114,
    "itl": 180.55551963617057,
    "ttft": 1320479.0453631002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7471581318275904,
    "arrivals": 115680,
    "finished_requests": 77708,
    "scheduler_time": 93.20135326144796
}
#Debug simulation 
Total elapsed time: 9.68331450689584. Arrivals time: 0.23724125185981393 Scheduler time: 9.350285950116813 Scheduler overhead time: 0.03297543665394187 Adapter cache time: 0.014176692813634872 Engine time: 0.0335819385945797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 9.573248436208814,
    "estimated_duration": 3600.1002321640735,
    "input_throughput": 5343.001794268754,
    "output_throughput": 4723.31599217125,
    "total_throughput": 10066.317786440004,
    "itl": 178.07671173791297,
    "ttft": 1324764.0174721538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.865754160713408,
    "arrivals": 115680,
    "finished_requests": 77492,
    "scheduler_time": 93.43210347363757
}
#Debug simulation 
Total elapsed time: 9.573347825091332. Arrivals time: 0.23684601532295346 Scheduler time: 9.239266115706414 Scheduler overhead time: 0.033426309004426 Adapter cache time: 0.014375848695635796 Engine time: 0.034139607567340136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.656839584000409,
    "estimated_duration": 3600.0900135520114,
    "input_throughput": 5357.927420533739,
    "output_throughput": 4736.680731817385,
    "total_throughput": 10094.608152351124,
    "itl": 180.5451194728647,
    "ttft": 1320459.1827667358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 554,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6564878633757834,
    "arrivals": 115680,
    "finished_requests": 77721,
    "scheduler_time": 93.20355045528606
}
#Debug simulation 
Total elapsed time: 9.656920122914016. Arrivals time: 0.23553747218102217 Scheduler time: 9.326183679513633 Scheduler overhead time: 0.03258432541042566 Adapter cache time: 0.014009853359311819 Engine time: 0.03360397880896926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.588867239188403,
    "estimated_duration": 3600.113745352971,
    "input_throughput": 5343.338672237967,
    "output_throughput": 4723.213543446762,
    "total_throughput": 10066.55221568473,
    "itl": 178.10065715383124,
    "ttft": 1324803.5644519397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9584485085308503,
    "arrivals": 115680,
    "finished_requests": 77477,
    "scheduler_time": 93.42237519393133
}
#Debug simulation 
Total elapsed time: 9.588941154070199. Arrivals time: 0.2373350327834487 Scheduler time: 9.25465232366696 Scheduler overhead time: 0.03321637585759163 Adapter cache time: 0.014639298431575298 Engine time: 0.033852989319711924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_192_slots_96_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_192_slots_96_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.321296458132565,
    "estimated_duration": 3600.1055768622437,
    "input_throughput": 5403.25070604015,
    "output_throughput": 4733.2939649097525,
    "total_throughput": 10136.544670949903,
    "itl": 179.20668883295096,
    "ttft": 1266575.3156204934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 798,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.442269219658386,
    "arrivals": 109355,
    "finished_requests": 78095,
    "scheduler_time": 90.1575489287981
}
#Debug simulation 
Total elapsed time: 9.321360278874636. Arrivals time: 0.2268037418834865 Scheduler time: 8.995432431809604 Scheduler overhead time: 0.032872261479496956 Adapter cache time: 0.017498650588095188 Engine time: 0.033681866712868214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_192_slots_96_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_192_slots_96_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.438025931827724,
    "estimated_duration": 3600.0984937460835,
    "input_throughput": 5404.991011718808,
    "output_throughput": 4734.049646032283,
    "total_throughput": 10139.040657751091,
    "itl": 179.1802566301717,
    "ttft": 1266796.1532945468,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5773395827249694,
    "arrivals": 109355,
    "finished_requests": 78094,
    "scheduler_time": 90.15782631171672
}
#Debug simulation 
Total elapsed time: 9.438117552082986. Arrivals time: 0.2289817095734179 Scheduler time: 9.109367024153471 Scheduler overhead time: 0.033251124899834394 Adapter cache time: 0.017626900225877762 Engine time: 0.033767787739634514 
