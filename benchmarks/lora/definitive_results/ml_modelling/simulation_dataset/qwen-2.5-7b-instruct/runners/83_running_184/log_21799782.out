INFO 06-01 00:47:07 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:08 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_160_slots_96_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.918875112198293,
    "estimated_duration": 3600.032874980626,
    "input_throughput": 5560.4811664692725,
    "output_throughput": 4946.326219339269,
    "total_throughput": 10506.807385808543,
    "itl": 174.5643430363483,
    "ttft": 2040843.2037599306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.078772249535944,
    "arrivals": 650482,
    "finished_requests": 81310,
    "scheduler_time": 99.78387759429337
}
#Debug simulation 
Total elapsed time: 5.919005125295371. Arrivals time: 0.26931350072845817 Scheduler time: 5.535813770722598 Scheduler overhead time: 0.032298913691192865 Adapter cache time: 0.03332614433020353 Engine time: 0.03326389845460653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_160_slots_96_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.926998462993652,
    "estimated_duration": 3600.057731648303,
    "input_throughput": 5555.911457798255,
    "output_throughput": 4942.166300164807,
    "total_throughput": 10498.077757963061,
    "itl": 172.38277258724415,
    "ttft": 2041491.758387742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.7949315001440835,
    "arrivals": 650482,
    "finished_requests": 81242,
    "scheduler_time": 100.0631719526595
}
#Debug simulation 
Total elapsed time: 5.927148594055325. Arrivals time: 0.29779504612088203 Scheduler time: 5.514345111325383 Scheduler overhead time: 0.03281283937394619 Adapter cache time: 0.03331658011302352 Engine time: 0.03357114503160119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.38265446992591,
    "estimated_duration": 3600.0104961993416,
    "input_throughput": 5793.620052502505,
    "output_throughput": 5062.469684252509,
    "total_throughput": 10856.089736755015,
    "itl": 168.87062820438538,
    "ttft": 2018922.557644152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1636,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.006957949074123,
    "arrivals": 645743,
    "finished_requests": 84036,
    "scheduler_time": 102.3997446307755
}
#Debug simulation 
Total elapsed time: 6.382733395788819. Arrivals time: 0.6521136132068932 Scheduler time: 5.61711653182283 Scheduler overhead time: 0.03310457058250904 Adapter cache time: 0.030699538998305798 Engine time: 0.03430370939895511 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.1341166640631855,
    "estimated_duration": 3600.1418738995617,
    "input_throughput": 5792.901427356868,
    "output_throughput": 5061.841349119121,
    "total_throughput": 10854.742776475989,
    "itl": 168.88229699367494,
    "ttft": 2019063.8577030487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1634,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.329068102377818,
    "arrivals": 645743,
    "finished_requests": 84028,
    "scheduler_time": 102.39513533093135
}
#Debug simulation 
Total elapsed time: 6.13425675407052. Arrivals time: 0.38039451418444514 Scheduler time: 5.63967296294868 Scheduler overhead time: 0.03328443365171552 Adapter cache time: 0.031102273147553205 Engine time: 0.034285164438188076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.187578917015344,
    "estimated_duration": 3600.0690554554208,
    "input_throughput": 5790.202820807564,
    "output_throughput": 5059.792109375431,
    "total_throughput": 10849.994930182995,
    "itl": 167.4904181982317,
    "ttft": 2019362.9588134694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1636,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.345447615645771,
    "arrivals": 645743,
    "finished_requests": 83975,
    "scheduler_time": 102.577594434752
}
#Debug simulation 
Total elapsed time: 6.187693063169718. Arrivals time: 0.33519962802529335 Scheduler time: 5.737052088603377 Scheduler overhead time: 0.03351116180419922 Adapter cache time: 0.03147121798247099 Engine time: 0.034762570168823004 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.457507110666484,
    "estimated_duration": 3600.111970399224,
    "input_throughput": 5793.456751204078,
    "output_throughput": 5062.326991451601,
    "total_throughput": 10855.78374265568,
    "itl": 168.87491390055933,
    "ttft": 2018960.2361280636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1636,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.118084088875796,
    "arrivals": 645743,
    "finished_requests": 84036,
    "scheduler_time": 102.39972016505772
}
#Debug simulation 
Total elapsed time: 6.457584196701646. Arrivals time: 0.6520715649239719 Scheduler time: 5.690984241198748 Scheduler overhead time: 0.03352968627586961 Adapter cache time: 0.03089574445039034 Engine time: 0.03448572801426053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.186280512716621,
    "estimated_duration": 3600.136232546329,
    "input_throughput": 5790.09477795692,
    "output_throughput": 5059.697695694239,
    "total_throughput": 10849.792473651158,
    "itl": 167.49334151261243,
    "ttft": 2019388.406410121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1636,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.412474383804878,
    "arrivals": 645743,
    "finished_requests": 83975,
    "scheduler_time": 102.57765941874077
}
#Debug simulation 
Total elapsed time: 6.18641809374094. Arrivals time: 0.378452573902905 Scheduler time: 5.692013813182712 Scheduler overhead time: 0.033680310007184744 Adapter cache time: 0.031885948963463306 Engine time: 0.034684406127780676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.226017825305462,
    "estimated_duration": 3600.071862394758,
    "input_throughput": 5794.141560864407,
    "output_throughput": 5062.570608764561,
    "total_throughput": 10856.712169628969,
    "itl": 168.86477502490604,
    "ttft": 2018898.9608082427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1635,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.88873223216492,
    "arrivals": 645743,
    "finished_requests": 84041,
    "scheduler_time": 102.40441513843786
}
#Debug simulation 
Total elapsed time: 6.2261298024095595. Arrivals time: 0.38361971732228994 Scheduler time: 5.728273443412036 Scheduler overhead time: 0.033214766532182693 Adapter cache time: 0.031333649065345526 Engine time: 0.03418319346383214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.413730958942324,
    "estimated_duration": 3600.0240764219166,
    "input_throughput": 5789.9840549725,
    "output_throughput": 5059.50477367455,
    "total_throughput": 10849.48882864705,
    "itl": 167.5041074411854,
    "ttft": 2019362.5980446872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1636,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.482016227692281,
    "arrivals": 645743,
    "finished_requests": 83971,
    "scheduler_time": 102.5715628047293
}
#Debug simulation 
Total elapsed time: 6.413838550914079. Arrivals time: 0.659795300103724 Scheduler time: 5.639279222581536 Scheduler overhead time: 0.03315040934830904 Adapter cache time: 0.03144552977755666 Engine time: 0.03454233659431338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.159504617098719,
    "estimated_duration": 3600.16249460755,
    "input_throughput": 5819.469546549968,
    "output_throughput": 5132.605272033504,
    "total_throughput": 10952.074818583471,
    "itl": 167.41241495924695,
    "ttft": 2018459.8307258808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7337950475980533,
    "arrivals": 643390,
    "finished_requests": 84771,
    "scheduler_time": 103.59287647603563
}
#Debug simulation 
Total elapsed time: 6.159618674777448. Arrivals time: 0.2761704628355801 Scheduler time: 5.7707094005309045 Scheduler overhead time: 0.03358139004558325 Adapter cache time: 0.02840967895463109 Engine time: 0.0350902727805078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.113487070892006,
    "estimated_duration": 3600.1284027266265,
    "input_throughput": 5819.201610735109,
    "output_throughput": 5132.409440176033,
    "total_throughput": 10951.611050911142,
    "itl": 167.4237841435935,
    "ttft": 2018453.4220085726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.968219035558872,
    "arrivals": 643390,
    "finished_requests": 84766,
    "scheduler_time": 103.5860766812547
}
#Debug simulation 
Total elapsed time: 6.11360242869705. Arrivals time: 0.2785909762606025 Scheduler time: 5.723304559942335 Scheduler overhead time: 0.03336291713640094 Adapter cache time: 0.028147663455456495 Engine time: 0.034632901661098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.203118399716914,
    "estimated_duration": 3600.0913246240625,
    "input_throughput": 5813.559744122751,
    "output_throughput": 5127.911026514799,
    "total_throughput": 10941.47077063755,
    "itl": 165.54853058569105,
    "ttft": 2019371.4947592726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.983219947218834,
    "arrivals": 643390,
    "finished_requests": 84684,
    "scheduler_time": 103.82980229699571
}
#Debug simulation 
Total elapsed time: 6.2032440807670355. Arrivals time: 0.28388929460197687 Scheduler time: 5.8061152352020144 Scheduler overhead time: 0.033773292787373066 Adapter cache time: 0.02841090364381671 Engine time: 0.03508965251967311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.165675961878151,
    "estimated_duration": 3600.077134479881,
    "input_throughput": 5819.430589235638,
    "output_throughput": 5132.483085718523,
    "total_throughput": 10951.913674954161,
    "itl": 167.4158234040441,
    "ttft": 2018418.8756998475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.821514370117759,
    "arrivals": 643390,
    "finished_requests": 84767,
    "scheduler_time": 103.58900651366383
}
#Debug simulation 
Total elapsed time: 6.165808169171214. Arrivals time: 0.2713463199324906 Scheduler time: 5.782109721098095 Scheduler overhead time: 0.03363247122615576 Adapter cache time: 0.028176142368465662 Engine time: 0.03483298001810908 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.134996796026826,
    "estimated_duration": 3600.1143971289202,
    "input_throughput": 5813.3319365325,
    "output_throughput": 5127.804831624889,
    "total_throughput": 10941.136768157388,
    "itl": 165.55658007832616,
    "ttft": 2019377.6284571297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.0386481952480935,
    "arrivals": 643390,
    "finished_requests": 84683,
    "scheduler_time": 103.828394096372
}
#Debug simulation 
Total elapsed time: 6.135104572866112. Arrivals time: 0.2716724746860564 Scheduler time: 5.750734816305339 Scheduler overhead time: 0.033809070475399494 Adapter cache time: 0.028299895115196705 Engine time: 0.03475844580680132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.194468756206334,
    "estimated_duration": 3600.0775509473337,
    "input_throughput": 5819.606856659766,
    "output_throughput": 5132.7263756131015,
    "total_throughput": 10952.333232272867,
    "itl": 167.40860961434035,
    "ttft": 2018428.0142639496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1220,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6478613597805616,
    "arrivals": 643390,
    "finished_requests": 84771,
    "scheduler_time": 103.59273589937739
}
#Debug simulation 
Total elapsed time: 6.19459388917312. Arrivals time: 0.28482344932854176 Scheduler time: 5.797837403137237 Scheduler overhead time: 0.03351093363016844 Adapter cache time: 0.02806921536102891 Engine time: 0.03471737168729305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.147199383936822,
    "estimated_duration": 3600.1678980561423,
    "input_throughput": 5813.245546492462,
    "output_throughput": 5127.728628980769,
    "total_throughput": 10940.974175473231,
    "itl": 165.55897545728408,
    "ttft": 2019398.8530781905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.092093554474466,
    "arrivals": 643390,
    "finished_requests": 84683,
    "scheduler_time": 103.82844966441709
}
#Debug simulation 
Total elapsed time: 6.147403256967664. Arrivals time: 0.27560290042310953 Scheduler time: 5.75823094509542 Scheduler overhead time: 0.033843235578387976 Adapter cache time: 0.028825953602790833 Engine time: 0.03499577799811959 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.226391415111721,
    "estimated_duration": 3600.016686454298,
    "input_throughput": 5898.734047512195,
    "output_throughput": 5187.600677038441,
    "total_throughput": 11086.334724550636,
    "itl": 165.4443889461265,
    "ttft": 2016026.673073792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 956,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.925826283199778,
    "arrivals": 642182,
    "finished_requests": 85479,
    "scheduler_time": 104.79107824560302
}
#Debug simulation 
Total elapsed time: 6.226502580102533. Arrivals time: 0.3434504307806492 Scheduler time: 5.772290633060038 Scheduler overhead time: 0.03360829642042518 Adapter cache time: 0.02630772441625595 Engine time: 0.034947667736560106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.3237685090862215,
    "estimated_duration": 3600.0650738814825,
    "input_throughput": 5897.926999721508,
    "output_throughput": 5187.190680380122,
    "total_throughput": 11085.11768010163,
    "itl": 165.45443958984268,
    "ttft": 2016092.4894881835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 956,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1163117278623433,
    "arrivals": 642182,
    "finished_requests": 85472,
    "scheduler_time": 104.78624031275916
}
#Debug simulation 
Total elapsed time: 6.323879132047296. Arrivals time: 0.2855046554468572 Scheduler time: 5.926685904152691 Scheduler overhead time: 0.03397524822503328 Adapter cache time: 0.026383780408650637 Engine time: 0.035402263049036264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.254827675409615,
    "estimated_duration": 3600.0607521383054,
    "input_throughput": 5892.923053423232,
    "output_throughput": 5182.117826461415,
    "total_throughput": 11075.040879884647,
    "itl": 163.61759916588042,
    "ttft": 2016952.6824496728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 958,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.128831473663419,
    "arrivals": 642182,
    "finished_requests": 85389,
    "scheduler_time": 105.03791891126637
}
#Debug simulation 
Total elapsed time: 6.254964109975845. Arrivals time: 0.27427110029384494 Scheduler time: 5.868319824337959 Scheduler overhead time: 0.034339230973273516 Adapter cache time: 0.02662798250094056 Engine time: 0.03537666192278266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.532976693008095,
    "estimated_duration": 3600.0846775176274,
    "input_throughput": 5898.622644243629,
    "output_throughput": 5187.502704207867,
    "total_throughput": 11086.125348451496,
    "itl": 165.4474085325157,
    "ttft": 2016054.1629430777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 956,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.993733132330206,
    "arrivals": 642182,
    "finished_requests": 85479,
    "scheduler_time": 104.7911624597048
}
#Debug simulation 
Total elapsed time: 6.533052993938327. Arrivals time: 0.28298210306093097 Scheduler time: 6.139510416891426 Scheduler overhead time: 0.0335237649269402 Adapter cache time: 0.026342745404690504 Engine time: 0.03497895272448659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.580169535242021,
    "estimated_duration": 3600.144053979839,
    "input_throughput": 5892.919194871419,
    "output_throughput": 5182.061806492501,
    "total_throughput": 11074.98100136392,
    "itl": 163.61831911384115,
    "ttft": 2016969.359124344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 960,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1736964036338042,
    "arrivals": 642182,
    "finished_requests": 85391,
    "scheduler_time": 105.03988562699017
}
#Debug simulation 
Total elapsed time: 6.5802854942157865. Arrivals time: 0.28980353102087975 Scheduler time: 6.178093416616321 Scheduler overhead time: 0.03400766383856535 Adapter cache time: 0.026934747118502855 Engine time: 0.035388548858463764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.562918204814196,
    "estimated_duration": 3600.1371255182275,
    "input_throughput": 5898.670872694369,
    "output_throughput": 5187.654066735476,
    "total_throughput": 11086.324939429845,
    "itl": 165.44201203338207,
    "ttft": 2016013.5405315924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 957,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8614781322213174,
    "arrivals": 642182,
    "finished_requests": 85482,
    "scheduler_time": 104.79643697336644
}
#Debug simulation 
Total elapsed time: 6.563023671973497. Arrivals time: 0.6580334887839854 Scheduler time: 5.7933812867850065 Scheduler overhead time: 0.03378412174060941 Adapter cache time: 0.026566567365080118 Engine time: 0.035529572516679764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.545932457316667,
    "estimated_duration": 3600.0690854052405,
    "input_throughput": 5892.909134995656,
    "output_throughput": 5182.076665037113,
    "total_throughput": 11074.98580003277,
    "itl": 163.62100138130634,
    "ttft": 2016966.6861126255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 958,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2083078666776776,
    "arrivals": 642182,
    "finished_requests": 85388,
    "scheduler_time": 105.03583124552652
}
#Debug simulation 
Total elapsed time: 6.546008456032723. Arrivals time: 0.5857268781401217 Scheduler time: 5.8480543945916 Scheduler overhead time: 0.03391809854656458 Adapter cache time: 0.02689099172130227 Engine time: 0.03558463230729103 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.277623428963125,
    "estimated_duration": 3600.1237713573864,
    "input_throughput": 5917.805706987471,
    "output_throughput": 5218.673910457318,
    "total_throughput": 11136.47961744479,
    "itl": 164.67309134949275,
    "ttft": 2007848.227449432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 794,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4300272686826547,
    "arrivals": 641592,
    "finished_requests": 86159,
    "scheduler_time": 105.39193635980958
}
#Debug simulation 
Total elapsed time: 6.277733974158764. Arrivals time: 0.3244804861024022 Scheduler time: 5.842547855339944 Scheduler overhead time: 0.034122409764677286 Adapter cache time: 0.02548169670626521 Engine time: 0.03521687351167202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.258192464243621,
    "estimated_duration": 3600.134821170925,
    "input_throughput": 5917.529219939329,
    "output_throughput": 5218.491788007411,
    "total_throughput": 11136.02100794674,
    "itl": 164.68061294590706,
    "ttft": 2007859.7294589847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 793,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5888911885861376,
    "arrivals": 641592,
    "finished_requests": 86154,
    "scheduler_time": 105.38767027318892
}
#Debug simulation 
Total elapsed time: 6.258296119980514. Arrivals time: 0.27939597330987453 Scheduler time: 5.868909220676869 Scheduler overhead time: 0.033836452290415764 Adapter cache time: 0.025224281009286642 Engine time: 0.03502531675621867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.541011503897607,
    "estimated_duration": 3600.1101730147075,
    "input_throughput": 5910.602169761172,
    "output_throughput": 5212.59686458027,
    "total_throughput": 11123.199034341444,
    "itl": 162.95877199337667,
    "ttft": 2008654.9709383538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 795,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6000248943455406,
    "arrivals": 641592,
    "finished_requests": 86053,
    "scheduler_time": 105.60342077576766
}
#Debug simulation 
Total elapsed time: 6.5411308710463345. Arrivals time: 0.28123160591349006 Scheduler time: 6.148550303652883 Scheduler overhead time: 0.034212693106383085 Adapter cache time: 0.02576906280592084 Engine time: 0.03538287617266178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.266364497598261,
    "estimated_duration": 3600.0257977551273,
    "input_throughput": 5917.708426779747,
    "output_throughput": 5218.649825152699,
    "total_throughput": 11136.358251932446,
    "itl": 164.67597201282027,
    "ttft": 2007813.7335010306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 793,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4806134291994173,
    "arrivals": 641592,
    "finished_requests": 86154,
    "scheduler_time": 105.38749136831916
}
#Debug simulation 
Total elapsed time: 6.266475970856845. Arrivals time: 0.3162381504662335 Scheduler time: 5.839438132476062 Scheduler overhead time: 0.03406487638130784 Adapter cache time: 0.025502169970422983 Engine time: 0.03542106179520488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.338641784153879,
    "estimated_duration": 3600.1436748460496,
    "input_throughput": 5910.547167512677,
    "output_throughput": 5212.54835775477,
    "total_throughput": 11123.095525267448,
    "itl": 162.96019423748817,
    "ttft": 2008669.0088793398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 795,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6334754015319066,
    "arrivals": 641592,
    "finished_requests": 86053,
    "scheduler_time": 105.60347209995253
}
#Debug simulation 
Total elapsed time: 6.338767647277564. Arrivals time: 0.2933237492106855 Scheduler time: 5.933564671780914 Scheduler overhead time: 0.034349520690739155 Adapter cache time: 0.02574252523481846 Engine time: 0.03558744676411152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.576607841067016,
    "estimated_duration": 3600.0669158536675,
    "input_throughput": 5917.899166312603,
    "output_throughput": 5218.756328462555,
    "total_throughput": 11136.655494775157,
    "itl": 164.67066630949301,
    "ttft": 2007823.7428794445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 794,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.37409993415228,
    "arrivals": 641592,
    "finished_requests": 86159,
    "scheduler_time": 105.39182966699559
}
#Debug simulation 
Total elapsed time: 6.576720688026398. Arrivals time: 0.2805756665766239 Scheduler time: 6.185573681257665 Scheduler overhead time: 0.034009318333119154 Adapter cache time: 0.025592748075723648 Engine time: 0.03507592808455229 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.265127031132579,
    "estimated_duration": 3600.177551485269,
    "input_throughput": 5910.491550957349,
    "output_throughput": 5212.499309168248,
    "total_throughput": 11122.990860125597,
    "itl": 162.96164310536037,
    "ttft": 2008683.3073422078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 795,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6673031700775196,
    "arrivals": 641592,
    "finished_requests": 86053,
    "scheduler_time": 105.60352097065568
}
#Debug simulation 
Total elapsed time: 6.265228684991598. Arrivals time: 0.3091354304924607 Scheduler time: 5.844099744223058 Scheduler overhead time: 0.03441650699824095 Adapter cache time: 0.02577381068840623 Engine time: 0.03571113105863333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_160_slots_96_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.4226675922982395,
    "estimated_duration": 3600.0394739549856,
    "input_throughput": 6013.306286393982,
    "output_throughput": 5327.3193637875665,
    "total_throughput": 11340.62565018155,
    "itl": 162.12713674363238,
    "ttft": 1999057.961480643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.761339437293449,
    "arrivals": 636231,
    "finished_requests": 87646,
    "scheduler_time": 107.4051281616838
}
#Debug simulation 
Total elapsed time: 6.4228071621619165. Arrivals time: 0.34729102021083236 Scheduler time: 5.96324212802574 Scheduler overhead time: 0.034606303088366985 Adapter cache time: 0.025703654158860445 Engine time: 0.0357415871694684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_160_slots_96_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.462859648745507,
    "estimated_duration": 3600.017250280214,
    "input_throughput": 6013.105075625692,
    "output_throughput": 5326.954752371621,
    "total_throughput": 11340.059827997313,
    "itl": 162.1386937141563,
    "ttft": 1999136.6208002768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.009819973555864,
    "arrivals": 636231,
    "finished_requests": 87642,
    "scheduler_time": 107.39668364134508
}
#Debug simulation 
Total elapsed time: 6.4629702270030975. Arrivals time: 0.34930608747527003 Scheduler time: 6.0012500593438745 Scheduler overhead time: 0.034656813368201256 Adapter cache time: 0.02566609438508749 Engine time: 0.03584743198007345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_160_slots_96_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.4045758452266455,
    "estimated_duration": 3600.0559596052326,
    "input_throughput": 6003.985283153815,
    "output_throughput": 5319.135095361079,
    "total_throughput": 11323.120378514894,
    "itl": 160.24300300472066,
    "ttft": 2000095.3723303808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.016690363157476,
    "arrivals": 636231,
    "finished_requests": 87515,
    "scheduler_time": 107.66115947790766
}
#Debug simulation 
Total elapsed time: 6.4047035039402544. Arrivals time: 0.35767358262091875 Scheduler time: 5.934030042029917 Scheduler overhead time: 0.03499700827524066 Adapter cache time: 0.025692346040159464 Engine time: 0.035953315906226635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_160_slots_96_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.423598848748952,
    "estimated_duration": 3600.1209563401685,
    "input_throughput": 6013.170185817087,
    "output_throughput": 5327.198789314193,
    "total_throughput": 11340.36897513128,
    "itl": 162.13068786572754,
    "ttft": 1999090.3565669793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8427044883137116,
    "arrivals": 636231,
    "finished_requests": 87646,
    "scheduler_time": 107.4051847245049
}
#Debug simulation 
Total elapsed time: 6.423737074714154. Arrivals time: 0.2838533758185804 Scheduler time: 6.027586738578975 Scheduler overhead time: 0.03450270276516676 Adapter cache time: 0.025713578332215548 Engine time: 0.035866640973836184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_160_slots_96_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.734616247005761,
    "estimated_duration": 3600.1075726342106,
    "input_throughput": 6003.899206874106,
    "output_throughput": 5319.058837452593,
    "total_throughput": 11322.958044326699,
    "itl": 160.24519555013737,
    "ttft": 2000115.8267934842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.068249415587628,
    "arrivals": 636231,
    "finished_requests": 87515,
    "scheduler_time": 107.66121345450517
}
#Debug simulation 
Total elapsed time: 6.734695116989315. Arrivals time: 0.5935109839774668 Scheduler time: 6.028139720670879 Scheduler overhead time: 0.03496851772069931 Adapter cache time: 0.02583121182397008 Engine time: 0.035958233289420605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_160_slots_96_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.397570428904146,
    "estimated_duration": 3600.1083353851177,
    "input_throughput": 6013.254875477015,
    "output_throughput": 5327.344127811739,
    "total_throughput": 11340.599003288753,
    "itl": 162.12298134410867,
    "ttft": 1999127.3801976354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.67477181243468,
    "arrivals": 636231,
    "finished_requests": 87649,
    "scheduler_time": 107.40934896357295
}
#Debug simulation 
Total elapsed time: 6.397708864882588. Arrivals time: 0.35121373645961285 Scheduler time: 5.933700197841972 Scheduler overhead time: 0.03464507823809981 Adapter cache time: 0.02580522559583187 Engine time: 0.036083102226257324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_160_slots_96_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.417694136034697,
    "estimated_duration": 3600.178570290275,
    "input_throughput": 6003.780806421847,
    "output_throughput": 5318.953942458483,
    "total_throughput": 11322.73474888033,
    "itl": 160.23932090432584,
    "ttft": 2000151.2812748302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.123189234361103,
    "arrivals": 636231,
    "finished_requests": 87515,
    "scheduler_time": 107.66289134718896
}
#Debug simulation 
Total elapsed time: 6.417806996032596. Arrivals time: 0.351157006341964 Scheduler time: 5.952940582297742 Scheduler overhead time: 0.03494020318612456 Adapter cache time: 0.02602388709783554 Engine time: 0.036350940354168415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.539259058889002,
    "estimated_duration": 3600.045854720509,
    "input_throughput": 6141.975933725052,
    "output_throughput": 5403.374230495793,
    "total_throughput": 11545.350164220845,
    "itl": 158.73021169898354,
    "ttft": 1984884.7203981506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 927,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.837072138625725,
    "arrivals": 633808,
    "finished_requests": 89098,
    "scheduler_time": 109.12259515804016
}
#Debug simulation 
Total elapsed time: 6.5393843511119485. Arrivals time: 0.36131205782294273 Scheduler time: 6.066378541756421 Scheduler overhead time: 0.03533882601186633 Adapter cache time: 0.023406788241118193 Engine time: 0.036455730907619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.481778669171035,
    "estimated_duration": 3600.108265683565,
    "input_throughput": 6141.6830740232635,
    "output_throughput": 5402.971401002219,
    "total_throughput": 11544.654475025483,
    "itl": 158.7389178975898,
    "ttft": 1984945.936984103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 928,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0203324600518715,
    "arrivals": 633808,
    "finished_requests": 89094,
    "scheduler_time": 109.11945430264208
}
#Debug simulation 
Total elapsed time: 6.48191245412454. Arrivals time: 0.32226255908608437 Scheduler time: 6.048407599329948 Scheduler overhead time: 0.03511789534240961 Adapter cache time: 0.023227748926728964 Engine time: 0.03644582163542509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.451419138349593,
    "estimated_duration": 3600.05128575608,
    "input_throughput": 6135.29065193893,
    "output_throughput": 5397.920323215269,
    "total_throughput": 11533.210975154198,
    "itl": 157.1510142409713,
    "ttft": 1985260.5189157503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 926,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0209841981716195,
    "arrivals": 633808,
    "finished_requests": 88991,
    "scheduler_time": 109.33220276485777
}
#Debug simulation 
Total elapsed time: 6.4515294772572815. Arrivals time: 0.28544682636857033 Scheduler time: 6.053935328032821 Scheduler overhead time: 0.03544033784419298 Adapter cache time: 0.02331221057102084 Engine time: 0.036901724990457296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.46079016989097,
    "estimated_duration": 3600.1007545286075,
    "input_throughput": 6141.882271540824,
    "output_throughput": 5403.291831632938,
    "total_throughput": 11545.174103173762,
    "itl": 158.73246520106798,
    "ttft": 1984907.3219722523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 927,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8919036469957478,
    "arrivals": 633808,
    "finished_requests": 89098,
    "scheduler_time": 109.122663457666
}
#Debug simulation 
Total elapsed time: 6.460925864987075. Arrivals time: 0.35212588496506214 Scheduler time: 5.99795704940334 Scheduler overhead time: 0.03494963003322482 Adapter cache time: 0.023257588036358356 Engine time: 0.03632605401799083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.531494756694883,
    "estimated_duration": 3600.0896927144963,
    "input_throughput": 6135.225198610525,
    "output_throughput": 5397.8627363996375,
    "total_throughput": 11533.087935010162,
    "itl": 157.15258833335366,
    "ttft": 1985275.493599777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 926,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.05933910302818,
    "arrivals": 633808,
    "finished_requests": 88991,
    "scheduler_time": 109.33225481845611
}
#Debug simulation 
Total elapsed time: 6.531603914685547. Arrivals time: 0.28890184918418527 Scheduler time: 6.1305348123423755 Scheduler overhead time: 0.035419362131506205 Adapter cache time: 0.023398991208523512 Engine time: 0.036756377667188644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.589634891599417,
    "estimated_duration": 3600.1546840792253,
    "input_throughput": 6142.351632220414,
    "output_throughput": 5403.805310375348,
    "total_throughput": 11546.156942595762,
    "itl": 158.7291484058186,
    "ttft": 1984861.2592434823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 927,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7717766233742553,
    "arrivals": 633808,
    "finished_requests": 89106,
    "scheduler_time": 109.12783890674648
}
#Debug simulation 
Total elapsed time: 6.589758538641036. Arrivals time: 0.2927244189195335 Scheduler time: 6.185799052007496 Scheduler overhead time: 0.03517933934926987 Adapter cache time: 0.023160270880907774 Engine time: 0.036428137216717005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.503636367153376,
    "estimated_duration": 3600.1269626503226,
    "input_throughput": 6135.161684336777,
    "output_throughput": 5397.806855593246,
    "total_throughput": 11532.968539930023,
    "itl": 157.15412488394486,
    "ttft": 1985290.4214652097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 926,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0965622238070094,
    "arrivals": 633808,
    "finished_requests": 88991,
    "scheduler_time": 109.33230163353946
}
#Debug simulation 
Total elapsed time: 6.503768810071051. Arrivals time: 0.35376852191984653 Scheduler time: 6.038091676309705 Scheduler overhead time: 0.03561260877177119 Adapter cache time: 0.023088155314326286 Engine time: 0.03671232145279646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.5455501498654485,
    "estimated_duration": 3600.124199536021,
    "input_throughput": 6192.807460051888,
    "output_throughput": 5462.302384605063,
    "total_throughput": 11655.10984465695,
    "itl": 157.29017537413185,
    "ttft": 1988144.148588594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 697,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1331599575211673,
    "arrivals": 632594,
    "finished_requests": 90047,
    "scheduler_time": 110.23757239585382
}
#Debug simulation 
Total elapsed time: 6.545661510899663. Arrivals time: 0.29011946404352784 Scheduler time: 6.144149886909872 Scheduler overhead time: 0.035396299790591 Adapter cache time: 0.02095468109473586 Engine time: 0.038443137891590595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.810420565772802,
    "estimated_duration": 3600.131971574914,
    "input_throughput": 6192.704649726222,
    "output_throughput": 5462.1514309092345,
    "total_throughput": 11654.856080635456,
    "itl": 157.29588351357611,
    "ttft": 1988175.2555323916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 698,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2807292867824502,
    "arrivals": 632594,
    "finished_requests": 90045,
    "scheduler_time": 110.2342594326213
}
#Debug simulation 
Total elapsed time: 6.810537181794643. Arrivals time: 0.5939437630586326 Scheduler time: 6.107862995006144 Scheduler overhead time: 0.03524627489969134 Adapter cache time: 0.020530090667307377 Engine time: 0.036385391373187304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.494904222898185,
    "estimated_duration": 3600.0439045707594,
    "input_throughput": 6184.826516068498,
    "output_throughput": 5454.860418526326,
    "total_throughput": 11639.686934594823,
    "itl": 155.52963760345185,
    "ttft": 1989254.335160454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 697,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2805134550109525,
    "arrivals": 632594,
    "finished_requests": 89930,
    "scheduler_time": 110.44832730651615
}
#Debug simulation 
Total elapsed time: 6.495019195135683. Arrivals time: 0.2874897886067629 Scheduler time: 6.096889846026897 Scheduler overhead time: 0.03579979483038187 Adapter cache time: 0.020742188207805157 Engine time: 0.03736463375389576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.510791357140988,
    "estimated_duration": 3600.171380111796,
    "input_throughput": 6192.726302742753,
    "output_throughput": 5462.23080063187,
    "total_throughput": 11654.957103374623,
    "itl": 157.29202012159743,
    "ttft": 1988164.24798942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 697,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.179676360061842,
    "arrivals": 632594,
    "finished_requests": 90047,
    "scheduler_time": 110.23765634410805
}
#Debug simulation 
Total elapsed time: 6.510922440793365. Arrivals time: 0.30336832208558917 Scheduler time: 6.098255488090217 Scheduler overhead time: 0.03531302744522691 Adapter cache time: 0.020701850298792124 Engine time: 0.036776488181203604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.5110956421121955,
    "estimated_duration": 3600.0740049720685,
    "input_throughput": 6184.7748044203745,
    "output_throughput": 5454.81481016176,
    "total_throughput": 11639.589614582135,
    "itl": 155.53087318864345,
    "ttft": 1989267.23790216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 697,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3103171023912754,
    "arrivals": 632594,
    "finished_requests": 89930,
    "scheduler_time": 110.44838526173264
}
#Debug simulation 
Total elapsed time: 6.511206468101591. Arrivals time: 0.2880375301465392 Scheduler time: 6.112739974167198 Scheduler overhead time: 0.035753896459937096 Adapter cache time: 0.020648987498134375 Engine time: 0.037272481713443995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.471113222185522,
    "estimated_duration": 3600.06641146917,
    "input_throughput": 6192.906866654598,
    "output_throughput": 5462.39006518072,
    "total_throughput": 11655.296931835319,
    "itl": 157.28822128607771,
    "ttft": 1988125.5056067042,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 697,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0840650555467795,
    "arrivals": 632594,
    "finished_requests": 90047,
    "scheduler_time": 110.23732221066815
}
#Debug simulation 
Total elapsed time: 6.471221653278917. Arrivals time: 0.29375102696940303 Scheduler time: 6.068461168091744 Scheduler overhead time: 0.035020300187170506 Adapter cache time: 0.02075847052037716 Engine time: 0.036765749566257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.4821789199486375,
    "estimated_duration": 3600.1035716516217,
    "input_throughput": 6184.724010533168,
    "output_throughput": 5454.7700112390885,
    "total_throughput": 11639.494021772256,
    "itl": 155.53204508775087,
    "ttft": 1989279.7531492608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 697,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.339869242198771,
    "arrivals": 632594,
    "finished_requests": 89930,
    "scheduler_time": 110.44842756771448
}
#Debug simulation 
Total elapsed time: 6.482291488908231. Arrivals time: 0.2892402703873813 Scheduler time: 6.0828641639091074 Scheduler overhead time: 0.03584823105484247 Adapter cache time: 0.02059884136542678 Engine time: 0.03707157215103507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.584538975730538,
    "estimated_duration": 3600.052792302106,
    "input_throughput": 6240.57598489647,
    "output_throughput": 5486.007883615126,
    "total_throughput": 11726.583868511596,
    "itl": 156.6283931665163,
    "ttft": 1979124.57996106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 494,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5118809455027968,
    "arrivals": 632045,
    "finished_requests": 90705,
    "scheduler_time": 110.77700247243618
}
#Debug simulation 
Total elapsed time: 6.5846455888822675. Arrivals time: 0.3535893796943128 Scheduler time: 6.122673840261996 Scheduler overhead time: 0.035658581648021936 Adapter cache time: 0.019001570530235767 Engine time: 0.03707481315359473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.935993887018412,
    "estimated_duration": 3600.0847972403158,
    "input_throughput": 6240.26773403259,
    "output_throughput": 5485.708285298952,
    "total_throughput": 11725.976019331541,
    "itl": 156.63366350762928,
    "ttft": 1979130.3254685297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 494,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.613555682040757,
    "arrivals": 632045,
    "finished_requests": 90702,
    "scheduler_time": 110.77488585313499
}
#Debug simulation 
Total elapsed time: 6.936071895062923. Arrivals time: 0.5970919192768633 Scheduler time: 6.230040301568806 Scheduler overhead time: 0.03563938522711396 Adapter cache time: 0.01955792959779501 Engine time: 0.03711733780801296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.656194213312119,
    "estimated_duration": 3600.1037728144715,
    "input_throughput": 6233.229766723294,
    "output_throughput": 5478.956509239631,
    "total_throughput": 11712.186275962924,
    "itl": 155.21449818737472,
    "ttft": 1979755.7536569643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 497,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6259440260008076,
    "arrivals": 632045,
    "finished_requests": 90593,
    "scheduler_time": 110.96220661059941
}
#Debug simulation 
Total elapsed time: 6.656299067195505. Arrivals time: 0.3615100225433707 Scheduler time: 6.184660887811333 Scheduler overhead time: 0.03580358391627669 Adapter cache time: 0.019570304080843925 Engine time: 0.037955152336508036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.621278956066817,
    "estimated_duration": 3600.1273980921965,
    "input_throughput": 6240.4655490540645,
    "output_throughput": 5485.8953076121725,
    "total_throughput": 11726.360856666237,
    "itl": 156.62814167874927,
    "ttft": 1979160.672992719,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 495,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5479017188376691,
    "arrivals": 632045,
    "finished_requests": 90707,
    "scheduler_time": 110.77856431191867
}
#Debug simulation 
Total elapsed time: 6.621415908914059. Arrivals time: 0.2941049300134182 Scheduler time: 6.217911719810218 Scheduler overhead time: 0.035891239531338215 Adapter cache time: 0.019474304281175137 Engine time: 0.03720319923013449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.58110654912889,
    "estimated_duration": 3600.1259865578336,
    "input_throughput": 6233.191306023065,
    "output_throughput": 5478.922702607795,
    "total_throughput": 11712.11400863086,
    "itl": 155.21533898780524,
    "ttft": 1979765.7666587804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 497,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6471964159049133,
    "arrivals": 632045,
    "finished_requests": 90593,
    "scheduler_time": 110.96228319840851
}
#Debug simulation 
Total elapsed time: 6.581220507156104. Arrivals time: 0.35378671530634165 Scheduler time: 6.117619066964835 Scheduler overhead time: 0.03604383114725351 Adapter cache time: 0.019613830372691154 Engine time: 0.03734341310337186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.629003384150565,
    "estimated_duration": 3600.011226039675,
    "input_throughput": 6240.64803951042,
    "output_throughput": 5486.0712258741,
    "total_throughput": 11726.71926538452,
    "itl": 156.62669296380415,
    "ttft": 1979107.5817842176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 494,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4770848456816592,
    "arrivals": 632045,
    "finished_requests": 90705,
    "scheduler_time": 110.77681700527054
}
#Debug simulation 
Total elapsed time: 6.629135040100664. Arrivals time: 0.36492238799110055 Scheduler time: 6.155275498982519 Scheduler overhead time: 0.035574092995375395 Adapter cache time: 0.019614513497799635 Engine time: 0.036846031434834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.648506515193731,
    "estimated_duration": 3600.1532036535727,
    "input_throughput": 6233.144183204969,
    "output_throughput": 5478.881282047249,
    "total_throughput": 11712.025465252218,
    "itl": 155.21583493699237,
    "ttft": 1979779.3240121729,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 497,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6681972982361897,
    "arrivals": 632045,
    "finished_requests": 90593,
    "scheduler_time": 110.96241837818677
}
#Debug simulation 
Total elapsed time: 6.6486171069554985. Arrivals time: 0.38980896631255746 Scheduler time: 6.149140004534274 Scheduler overhead time: 0.03587522264569998 Adapter cache time: 0.019589917734265327 Engine time: 0.03740942943841219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.785945521201938,
    "estimated_duration": 3600.120834373357,
    "input_throughput": 6350.437402465536,
    "output_throughput": 5602.9241594917285,
    "total_throughput": 11953.361561957265,
    "itl": 153.42383990878366,
    "ttft": 1967326.2686345782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 662,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.026042886483517,
    "arrivals": 629108,
    "finished_requests": 92561,
    "scheduler_time": 113.07819541421416
}
#Debug simulation 
Total elapsed time: 6.786057694349438. Arrivals time: 0.3669754066504538 Scheduler time: 6.310440094675869 Scheduler overhead time: 0.03614715812727809 Adapter cache time: 0.017962098587304354 Engine time: 0.037510768976062536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.7289395509287715,
    "estimated_duration": 3600.174033554842,
    "input_throughput": 6350.343563093124,
    "output_throughput": 5602.8413659999605,
    "total_throughput": 11953.184929093084,
    "itl": 153.42832170688038,
    "ttft": 1967363.832617197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 660,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1519893487938724,
    "arrivals": 629108,
    "finished_requests": 92561,
    "scheduler_time": 113.07693736245808
}
#Debug simulation 
Total elapsed time: 6.729070263914764. Arrivals time: 0.36965310433879495 Scheduler time: 6.249981665518135 Scheduler overhead time: 0.036601771134883165 Adapter cache time: 0.0179512700997293 Engine time: 0.03789397096261382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.737302380148321,
    "estimated_duration": 3600.0506191192962,
    "input_throughput": 6342.078880431268,
    "output_throughput": 5595.798818220017,
    "total_throughput": 11937.877698651286,
    "itl": 152.00016127831287,
    "ttft": 1968090.5444259497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 659,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1523815814592018,
    "arrivals": 629108,
    "finished_requests": 92441,
    "scheduler_time": 113.25242580448081
}
#Debug simulation 
Total elapsed time: 6.73741015419364. Arrivals time: 0.3549621938727796 Scheduler time: 6.273146891500801 Scheduler overhead time: 0.03639084519818425 Adapter cache time: 0.017837444320321083 Engine time: 0.03790099173784256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.684801180381328,
    "estimated_duration": 3600.047806324701,
    "input_throughput": 6350.2709491347405,
    "output_throughput": 5602.933095655875,
    "total_throughput": 11953.204044790617,
    "itl": 153.42572801868042,
    "ttft": 1967310.2968394272,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 660,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0670015225582614,
    "arrivals": 629108,
    "finished_requests": 92559,
    "scheduler_time": 113.07438181682498
}
#Debug simulation 
Total elapsed time: 6.684913109987974. Arrivals time: 0.357519022654742 Scheduler time: 6.218969023320824 Scheduler overhead time: 0.03597870655357838 Adapter cache time: 0.017751514445990324 Engine time: 0.037692968267947435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.755575750023127,
    "estimated_duration": 3600.0647759614585,
    "input_throughput": 6342.053940932876,
    "output_throughput": 5595.776813382446,
    "total_throughput": 11937.830754315322,
    "itl": 151.9995094862902,
    "ttft": 1968093.8209989788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 659,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1785383690334896,
    "arrivals": 629108,
    "finished_requests": 92441,
    "scheduler_time": 113.25257454300682
}
#Debug simulation 
Total elapsed time: 6.755704069044441. Arrivals time: 0.3701114822179079 Scheduler time: 6.276011970825493 Scheduler overhead time: 0.036458068527281284 Adapter cache time: 0.01786732394248247 Engine time: 0.038099854718893766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.73373336205259,
    "estimated_duration": 3600.0741419651845,
    "input_throughput": 6350.519766662377,
    "output_throughput": 5602.996828556725,
    "total_throughput": 11953.516595219102,
    "itl": 153.42198451544468,
    "ttft": 1967306.4078911499,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 662,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9794132952252068,
    "arrivals": 629108,
    "finished_requests": 92561,
    "scheduler_time": 113.07811789745435
}
#Debug simulation 
Total elapsed time: 6.73385172104463. Arrivals time: 0.3572947597131133 Scheduler time: 6.267752528190613 Scheduler overhead time: 0.036211642902344465 Adapter cache time: 0.01779993250966072 Engine time: 0.03781716711819172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.746275896206498,
    "estimated_duration": 3600.0934940173574,
    "input_throughput": 6342.003350174638,
    "output_throughput": 5595.732175699677,
    "total_throughput": 11937.735525874315,
    "itl": 152.00064363145523,
    "ttft": 1968106.0581588969,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 659,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2070844785496604,
    "arrivals": 629108,
    "finished_requests": 92441,
    "scheduler_time": 113.25261950128005
}
#Debug simulation 
Total elapsed time: 6.7463882220909. Arrivals time: 0.36970628052949905 Scheduler time: 6.267010212410241 Scheduler overhead time: 0.03675523726269603 Adapter cache time: 0.01791231194511056 Engine time: 0.03791854111477733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.640673392917961,
    "estimated_duration": 3600.031426347607,
    "input_throughput": 6374.912127720975,
    "output_throughput": 5647.014315268108,
    "total_throughput": 12021.926442989083,
    "itl": 152.65872644412147,
    "ttft": 1964162.7530647323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5333043597103269,
    "arrivals": 627971,
    "finished_requests": 93124,
    "scheduler_time": 113.86026912254775
}
#Debug simulation 
Total elapsed time: 6.64081229781732. Arrivals time: 0.29783402290195227 Scheduler time: 6.235220566391945 Scheduler overhead time: 0.036348361521959305 Adapter cache time: 0.016580319497734308 Engine time: 0.03774840058758855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.73572735209018,
    "estimated_duration": 3600.1289823578663,
    "input_throughput": 6374.739380856631,
    "output_throughput": 5646.861292921082,
    "total_throughput": 12021.600673777713,
    "itl": 152.66199647249846,
    "ttft": 1964229.5300811157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6328516528313093,
    "arrivals": 627971,
    "finished_requests": 93124,
    "scheduler_time": 113.86093921612814
}
#Debug simulation 
Total elapsed time: 6.735836806241423. Arrivals time: 0.29917482007294893 Scheduler time: 6.328641206026077 Scheduler overhead time: 0.03653394756838679 Adapter cache time: 0.016521871089935303 Engine time: 0.03789179679006338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.023569566197693,
    "estimated_duration": 3600.0518558009685,
    "input_throughput": 6365.373310685697,
    "output_throughput": 5639.185437645089,
    "total_throughput": 12004.558748330786,
    "itl": 151.18862655994843,
    "ttft": 1965353.4678677262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 499,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6293330837413766,
    "arrivals": 627971,
    "finished_requests": 92987,
    "scheduler_time": 114.0355972570064
}
#Debug simulation 
Total elapsed time: 7.023688232991844. Arrivals time: 0.5990824438631535 Scheduler time: 6.3156868070364 Scheduler overhead time: 0.03671065904200077 Adapter cache time: 0.016640170477330685 Engine time: 0.03822666918858886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.747415420599282,
    "estimated_duration": 3600.0659379950544,
    "input_throughput": 6374.851015307022,
    "output_throughput": 5646.960180768758,
    "total_throughput": 12021.81119607578,
    "itl": 152.65981939513475,
    "ttft": 1964178.921423619,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5662506159255214,
    "arrivals": 627971,
    "finished_requests": 93124,
    "scheduler_time": 113.86038069403455
}
#Debug simulation 
Total elapsed time: 6.747523048892617. Arrivals time: 0.2989029185846448 Scheduler time: 6.340681745670736 Scheduler overhead time: 0.03625647583976388 Adapter cache time: 0.016500070225447416 Engine time: 0.038105472922325134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.733954209834337,
    "estimated_duration": 3600.099606253771,
    "input_throughput": 6365.427767662893,
    "output_throughput": 5639.232582546752,
    "total_throughput": 12004.660350209644,
    "itl": 151.1902602479047,
    "ttft": 1965356.025141073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 499,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.649705197140579,
    "arrivals": 627971,
    "finished_requests": 92988,
    "scheduler_time": 114.03702700876254
}
#Debug simulation 
Total elapsed time: 6.734082541894168. Arrivals time: 0.3033715318888426 Scheduler time: 6.320411696564406 Scheduler overhead time: 0.03735009813681245 Adapter cache time: 0.01675703004002571 Engine time: 0.038859897293150425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.658338122069836,
    "estimated_duration": 3600.1620728200965,
    "input_throughput": 6374.6813437270575,
    "output_throughput": 5646.905497250345,
    "total_throughput": 12021.586840977403,
    "itl": 152.6577840278231,
    "ttft": 1964219.6163137113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4980151977459737,
    "arrivals": 627971,
    "finished_requests": 93126,
    "scheduler_time": 113.86554138456881
}
#Debug simulation 
Total elapsed time: 6.658474841155112. Arrivals time: 0.2948287548497319 Scheduler time: 6.256391001865268 Scheduler overhead time: 0.036316897720098495 Adapter cache time: 0.01633957913145423 Engine time: 0.037508712615817785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.709250452928245,
    "estimated_duration": 3600.0631182225256,
    "input_throughput": 6365.353397279949,
    "output_throughput": 5639.167796042275,
    "total_throughput": 12004.521193322224,
    "itl": 151.19454688088882,
    "ttft": 1965350.884470865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 499,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6705803256854403,
    "arrivals": 627971,
    "finished_requests": 92987,
    "scheduler_time": 114.03466316835426
}
#Debug simulation 
Total elapsed time: 6.709369809832424. Arrivals time: 0.2940756753087044 Scheduler time: 6.306508210487664 Scheduler overhead time: 0.036683350801467896 Adapter cache time: 0.016649835743010044 Engine time: 0.03816810483112931 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.753710065968335,
    "estimated_duration": 3600.131365925554,
    "input_throughput": 6436.84956036107,
    "output_throughput": 5687.372742504364,
    "total_throughput": 12124.222302865433,
    "itl": 151.3685943075408,
    "ttft": 1957395.4982655887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2058321711095108,
    "arrivals": 627360,
    "finished_requests": 93979,
    "scheduler_time": 114.68538468532994
}
#Debug simulation 
Total elapsed time: 6.753848045133054. Arrivals time: 0.36224748427048326 Scheduler time: 6.28463685605675 Scheduler overhead time: 0.036324642598629 Adapter cache time: 0.015623386483639479 Engine time: 0.03791377367451787 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.842671494930983,
    "estimated_duration": 3600.0401176181067,
    "input_throughput": 6436.47382888916,
    "output_throughput": 5687.137457108715,
    "total_throughput": 12123.611285997877,
    "itl": 151.37026921595123,
    "ttft": 1957431.2651491622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2859489802597148,
    "arrivals": 627360,
    "finished_requests": 93973,
    "scheduler_time": 114.68011309017099
}
#Debug simulation 
Total elapsed time: 6.842781590297818. Arrivals time: 0.37475905008614063 Scheduler time: 6.360425886232406 Scheduler overhead time: 0.03670875634998083 Adapter cache time: 0.01571175968274474 Engine time: 0.03806986100971699 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.812439300119877,
    "estimated_duration": 3600.016719425135,
    "input_throughput": 6423.164335662434,
    "output_throughput": 5676.270304451,
    "total_throughput": 12099.434640113433,
    "itl": 149.723770647832,
    "ttft": 1959009.0158720496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2851362461224272,
    "arrivals": 627360,
    "finished_requests": 93775,
    "scheduler_time": 114.8720955109119
}
#Debug simulation 
Total elapsed time: 6.812561583239585. Arrivals time: 0.3785681505687535 Scheduler time: 6.325573836453259 Scheduler overhead time: 0.03687746822834015 Adapter cache time: 0.015865375753492117 Engine time: 0.038338120095431805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.861645324155688,
    "estimated_duration": 3600.1588275173985,
    "input_throughput": 6436.800460823005,
    "output_throughput": 5687.329359888095,
    "total_throughput": 12124.1298207111,
    "itl": 151.36963602643254,
    "ttft": 1957406.06662291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2332401841809044,
    "arrivals": 627360,
    "finished_requests": 93979,
    "scheduler_time": 114.68543826406555
}
#Debug simulation 
Total elapsed time: 6.861756782978773. Arrivals time: 0.37037035962566733 Scheduler time: 6.383335246704519 Scheduler overhead time: 0.036709688138216734 Adapter cache time: 0.01571665983647108 Engine time: 0.03832621080800891 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.846509154886007,
    "estimated_duration": 3600.0329872187535,
    "input_throughput": 6423.135310730672,
    "output_throughput": 5676.244654576633,
    "total_throughput": 12099.379965307306,
    "itl": 149.72435452360557,
    "ttft": 1959016.3221693416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3013584845699422,
    "arrivals": 627360,
    "finished_requests": 93775,
    "scheduler_time": 114.87214106609925
}
#Debug simulation 
Total elapsed time: 6.846624625846744. Arrivals time: 0.3705215738154948 Scheduler time: 6.367428004741669 Scheduler overhead time: 0.03693604515865445 Adapter cache time: 0.01576839853078127 Engine time: 0.03848892357200384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.910071710124612,
    "estimated_duration": 3600.103551769473,
    "input_throughput": 6436.8992910245815,
    "output_throughput": 5687.41668276077,
    "total_throughput": 12124.315973785351,
    "itl": 151.36755573563346,
    "ttft": 1957383.65292657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1780798161914523,
    "arrivals": 627360,
    "finished_requests": 93979,
    "scheduler_time": 114.6853228841151
}
#Debug simulation 
Total elapsed time: 6.91020625596866. Arrivals time: 0.3668090966530144 Scheduler time: 6.4348834729753435 Scheduler overhead time: 0.036966842133551836 Adapter cache time: 0.015888644382357597 Engine time: 0.03830351447686553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.811432681046426,
    "estimated_duration": 3600.050000450574,
    "input_throughput": 6423.104956071698,
    "output_throughput": 5676.21782959749,
    "total_throughput": 12099.322785669188,
    "itl": 149.72498780775183,
    "ttft": 1959023.2026416964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3183352457359456,
    "arrivals": 627360,
    "finished_requests": 93775,
    "scheduler_time": 114.87217753677324
}
#Debug simulation 
Total elapsed time: 6.811549658887088. Arrivals time: 0.36361336149275303 Scheduler time: 6.3388314810581505 Scheduler overhead time: 0.03709755092859268 Adapter cache time: 0.015873394906520844 Engine time: 0.03871221328154206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.8859663512557745,
    "estimated_duration": 3600.0056647979145,
    "input_throughput": 6558.70301285357,
    "output_throughput": 5776.160077561233,
    "total_throughput": 12334.863090414803,
    "itl": 148.97763342543348,
    "ttft": 1954483.3557775405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1078965633036593,
    "arrivals": 625657,
    "finished_requests": 95192,
    "scheduler_time": 116.51445117134529
}
#Debug simulation 
Total elapsed time: 6.886104096192867. Arrivals time: 0.3656318332068622 Scheduler time: 6.4137900192290545 Scheduler overhead time: 0.037318611051887274 Adapter cache time: 0.01325640408322215 Engine time: 0.038676424883306026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.878491771873087,
    "estimated_duration": 3600.1028066082877,
    "input_throughput": 6558.526038939603,
    "output_throughput": 5776.004219054662,
    "total_throughput": 12334.530257994265,
    "itl": 148.97984540456793,
    "ttft": 1954531.0397242827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1808696784987172,
    "arrivals": 625657,
    "finished_requests": 95192,
    "scheduler_time": 116.51582536014978
}
#Debug simulation 
Total elapsed time: 6.878602257929742. Arrivals time: 0.3663346394896507 Scheduler time: 6.406012237071991 Scheduler overhead time: 0.036940672900527716 Adapter cache time: 0.013163597323000431 Engine time: 0.038653118535876274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.944613833911717,
    "estimated_duration": 3600.088017191512,
    "input_throughput": 6545.8713474411115,
    "output_throughput": 5764.455174679147,
    "total_throughput": 12310.326522120258,
    "itl": 147.39773640361344,
    "ttft": 1956058.9687384558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1799869505874878,
    "arrivals": 625657,
    "finished_requests": 95006,
    "scheduler_time": 116.69117072853346
}
#Debug simulation 
Total elapsed time: 6.944740954320878. Arrivals time: 0.3119662143290043 Scheduler time: 6.524924416095018 Scheduler overhead time: 0.037533119320869446 Adapter cache time: 0.013392944354563951 Engine time: 0.039260693825781345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.922385774087161,
    "estimated_duration": 3600.030393721697,
    "input_throughput": 6558.657960548679,
    "output_throughput": 5776.120400612238,
    "total_throughput": 12334.778361160916,
    "itl": 148.97848505576997,
    "ttft": 1954494.2395031815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.131838240285871,
    "arrivals": 625657,
    "finished_requests": 95192,
    "scheduler_time": 116.5145182775067
}
#Debug simulation 
Total elapsed time: 6.9225202882662416. Arrivals time: 0.370285436976701 Scheduler time: 6.4454415761865675 Scheduler overhead time: 0.03734779544174671 Adapter cache time: 0.013452871236950159 Engine time: 0.0385806611739099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.913707101717591,
    "estimated_duration": 3600.1059042601537,
    "input_throughput": 6545.83882438395,
    "output_throughput": 5764.426534075749,
    "total_throughput": 12310.2653584597,
    "itl": 147.39749552437496,
    "ttft": 1956068.281447575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1950774049572699,
    "arrivals": 625657,
    "finished_requests": 95006,
    "scheduler_time": 116.6915035140245
}
#Debug simulation 
Total elapsed time: 6.913817287888378. Arrivals time: 0.3046845206990838 Scheduler time: 6.5015644361265 Scheduler overhead time: 0.037429024931043386 Adapter cache time: 0.013334372080862522 Engine time: 0.03906487440690398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.8951478051021695,
    "estimated_duration": 3600.1165698973355,
    "input_throughput": 6558.715125347551,
    "output_throughput": 5776.06435688081,
    "total_throughput": 12334.779482228361,
    "itl": 148.9767256528439,
    "ttft": 1954528.7998443441,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.082398206754586,
    "arrivals": 625657,
    "finished_requests": 95194,
    "scheduler_time": 116.51912910515604
}
#Debug simulation 
Total elapsed time: 6.895260456949472. Arrivals time: 0.36690408270806074 Scheduler time: 6.4215246881358325 Scheduler overhead time: 0.03731440380215645 Adapter cache time: 0.013240945991128683 Engine time: 0.03875961201265454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_160_slots_96_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.84985271980986,
    "estimated_duration": 3600.123295316476,
    "input_throughput": 6545.807203508126,
    "output_throughput": 5764.3986879554095,
    "total_throughput": 12310.205891463536,
    "itl": 147.39694453599276,
    "ttft": 1956076.562811443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2102936131134663,
    "arrivals": 625657,
    "finished_requests": 95006,
    "scheduler_time": 116.69179070254955
}
#Debug simulation 
Total elapsed time: 6.849960993044078. Arrivals time: 0.3623822773806751 Scheduler time: 6.379947905428708 Scheduler overhead time: 0.037638109177351 Adapter cache time: 0.0131513811647892 Engine time: 0.039396414533257484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.95022391108796,
    "estimated_duration": 3600.030133690311,
    "input_throughput": 6533.685865537094,
    "output_throughput": 5806.17778845462,
    "total_throughput": 12339.863653991715,
    "itl": 148.92959009368306,
    "ttft": 1953174.1243589162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8569365683011699,
    "arrivals": 625114,
    "finished_requests": 95283,
    "scheduler_time": 116.97431671370077
}
#Debug simulation 
Total elapsed time: 6.950350868981332. Arrivals time: 0.31281993025913835 Scheduler time: 6.53189263259992 Scheduler overhead time: 0.03719050996005535 Adapter cache time: 0.011988248210400343 Engine time: 0.03884554421529174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.875932955183089,
    "estimated_duration": 3600.1440852292794,
    "input_throughput": 6533.556558612568,
    "output_throughput": 5805.994567205997,
    "total_throughput": 12339.551125818563,
    "itl": 148.92987566690192,
    "ttft": 1953255.6292695857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9094055708707357,
    "arrivals": 625114,
    "finished_requests": 95284,
    "scheduler_time": 116.97757915247267
}
#Debug simulation 
Total elapsed time: 6.876049624290317. Arrivals time: 0.34190889867022634 Scheduler time: 6.42882657982409 Scheduler overhead time: 0.037222739309072495 Adapter cache time: 0.01202384801581502 Engine time: 0.03858375595882535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.203660151921213,
    "estimated_duration": 3600.0056722276654,
    "input_throughput": 6520.017504715647,
    "output_throughput": 5793.636149215764,
    "total_throughput": 12313.653653931411,
    "itl": 147.19665475194532,
    "ttft": 1954463.394787662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9145760641433345,
    "arrivals": 625114,
    "finished_requests": 95069,
    "scheduler_time": 117.15664608964663
}
#Debug simulation 
Total elapsed time: 7.203742219135165. Arrivals time: 0.6115671894513071 Scheduler time: 6.485385864041746 Scheduler overhead time: 0.03763447469100356 Adapter cache time: 0.012248894665390253 Engine time: 0.039185055531561375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.89748872211203,
    "estimated_duration": 3600.0513650646753,
    "input_throughput": 6533.647332995049,
    "output_throughput": 5806.143546405896,
    "total_throughput": 12339.790879400944,
    "itl": 148.9300003556715,
    "ttft": 1953182.960184157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8772564237797663,
    "arrivals": 625114,
    "finished_requests": 95283,
    "scheduler_time": 116.97464554013307
}
#Debug simulation 
Total elapsed time: 6.897625160869211. Arrivals time: 0.33831495326012373 Scheduler time: 6.453768435865641 Scheduler overhead time: 0.037094732746481895 Adapter cache time: 0.012206397019326687 Engine time: 0.03881267039105296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.863742331042886,
    "estimated_duration": 3600.0167769953814,
    "input_throughput": 6519.9973927871815,
    "output_throughput": 5793.6182779146975,
    "total_throughput": 12313.615670701878,
    "itl": 147.19702125387477,
    "ttft": 1954469.1662307293,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9255166435614276,
    "arrivals": 625114,
    "finished_requests": 95069,
    "scheduler_time": 117.15669880622687
}
#Debug simulation 
Total elapsed time: 6.863853023853153. Arrivals time: 0.3010402121581137 Scheduler time: 6.456384927500039 Scheduler overhead time: 0.037524473387748 Adapter cache time: 0.012031559366732836 Engine time: 0.03918034303933382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.905860716011375,
    "estimated_duration": 3600.155480666237,
    "input_throughput": 6533.706981912626,
    "output_throughput": 5806.05590848865,
    "total_throughput": 12339.762890401276,
    "itl": 148.9283929236294,
    "ttft": 1953236.632030512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8342240322777081,
    "arrivals": 625114,
    "finished_requests": 95287,
    "scheduler_time": 116.97938772730626
}
#Debug simulation 
Total elapsed time: 6.906001404859126. Arrivals time: 0.3051643958315253 Scheduler time: 6.494925202801824 Scheduler overhead time: 0.03727374039590359 Adapter cache time: 0.012059902306646109 Engine time: 0.03909200616180897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.8611454288475215,
    "estimated_duration": 3600.0297420424754,
    "input_throughput": 6519.9739118497155,
    "output_throughput": 5793.597412938794,
    "total_throughput": 12313.571324788509,
    "itl": 147.19743730855393,
    "ttft": 1954475.8409348337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9378405146300841,
    "arrivals": 625114,
    "finished_requests": 95069,
    "scheduler_time": 117.15675410878819
}
#Debug simulation 
Total elapsed time: 6.861256907694042. Arrivals time: 0.30346472142264247 Scheduler time: 6.451330518350005 Scheduler overhead time: 0.037589294370263815 Adapter cache time: 0.012117352802306414 Engine time: 0.03908851509913802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.029555128421634,
    "estimated_duration": 3600.1415599026727,
    "input_throughput": 6604.406689119965,
    "output_throughput": 5876.370317108289,
    "total_throughput": 12480.777006228254,
    "itl": 147.4082167146741,
    "ttft": 1942414.5777521688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6702468159212722,
    "arrivals": 623927,
    "finished_requests": 96299,
    "scheduler_time": 118.29556245260831
}
#Debug simulation 
Total elapsed time: 7.029682798311114. Arrivals time: 0.38277083868160844 Scheduler time: 6.542157446499914 Scheduler overhead time: 0.037429476622492075 Adapter cache time: 0.010350200347602367 Engine time: 0.03922265628352761 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.022420333698392,
    "estimated_duration": 3600.0919353475415,
    "input_throughput": 6604.086903048715,
    "output_throughput": 5876.141604133172,
    "total_throughput": 12480.228507181888,
    "itl": 147.40966556694858,
    "ttft": 1942405.5870664679,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7120243591652274,
    "arrivals": 623927,
    "finished_requests": 96296,
    "scheduler_time": 118.29273672509218
}
#Debug simulation 
Total elapsed time: 7.022526676766574. Arrivals time: 0.37126552825793624 Scheduler time: 6.54679423943162 Scheduler overhead time: 0.03760971827432513 Adapter cache time: 0.0103521510027349 Engine time: 0.039023874793201685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.253094736952335,
    "estimated_duration": 3600.041586313784,
    "input_throughput": 6586.241417360488,
    "output_throughput": 5860.01786207178,
    "total_throughput": 12446.259279432268,
    "itl": 145.77842600959892,
    "ttft": 1943950.774081546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7137452771142156,
    "arrivals": 623927,
    "finished_requests": 96040,
    "scheduler_time": 118.45290269948015
}
#Debug simulation 
Total elapsed time: 7.253173136617988. Arrivals time: 0.6775031564757228 Scheduler time: 6.470792860258371 Scheduler overhead time: 0.03768630092963576 Adapter cache time: 0.010283070150762796 Engine time: 0.03910291055217385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 7.042228295933455,
    "estimated_duration": 3600.157626150932,
    "input_throughput": 6604.377215955596,
    "output_throughput": 5876.344092916412,
    "total_throughput": 12480.721308872007,
    "itl": 147.40881657486634,
    "ttft": 1942422.8719413828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6862828541034846,
    "arrivals": 623927,
    "finished_requests": 96299,
    "scheduler_time": 118.29561961222227
}
#Debug simulation 
Total elapsed time: 7.042363716755062. Arrivals time: 0.30563949048519135 Scheduler time: 6.631565785501152 Scheduler overhead time: 0.037569545675069094 Adapter cache time: 0.010312041267752647 Engine time: 0.03952496265992522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 7.010740282014012,
    "estimated_duration": 3600.0496305642214,
    "input_throughput": 6586.226700514657,
    "output_throughput": 5860.004767960285,
    "total_throughput": 12446.231468474942,
    "itl": 145.77808071588643,
    "ttft": 1943956.0562006664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7216677656583518,
    "arrivals": 623927,
    "finished_requests": 96040,
    "scheduler_time": 118.45300819919335
}
#Debug simulation 
Total elapsed time: 7.0108557040803134. Arrivals time: 0.3732437468133867 Scheduler time: 6.531828168779612 Scheduler overhead time: 0.03781107300892472 Adapter cache time: 0.010291998274624348 Engine time: 0.039778102189302444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.015032975003123,
    "estimated_duration": 3600.097210431348,
    "input_throughput": 6604.287220664038,
    "output_throughput": 5876.420764056318,
    "total_throughput": 12480.707984720355,
    "itl": 147.40734562924484,
    "ttft": 1942409.1347036758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6548210145835773,
    "arrivals": 623927,
    "finished_requests": 96298,
    "scheduler_time": 118.29472921523958
}
#Debug simulation 
Total elapsed time: 7.015148512087762. Arrivals time: 0.3700403939001262 Scheduler time: 6.539720233995467 Scheduler overhead time: 0.03778724279254675 Adapter cache time: 0.010283926501870155 Engine time: 0.039549282286316156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_160_slots_96_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.9464868269860744,
    "estimated_duration": 3600.059036662653,
    "input_throughput": 6586.209492270012,
    "output_throughput": 5859.9894571609075,
    "total_throughput": 12446.19894943092,
    "itl": 145.7784038998104,
    "ttft": 1943960.6772170316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7313508072122955,
    "arrivals": 623927,
    "finished_requests": 96040,
    "scheduler_time": 118.45303014827184
}
#Debug simulation 
Total elapsed time: 6.946597769856453. Arrivals time: 0.3656028709374368 Scheduler time: 6.475317555479705 Scheduler overhead time: 0.03805152419954538 Adapter cache time: 0.010407980531454086 Engine time: 0.03938091965392232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 31.652531321160495,
    "estimated_duration": 3600.08040583359,
    "input_throughput": 5546.453897986351,
    "output_throughput": 4893.779031004891,
    "total_throughput": 10440.232928991243,
    "itl": 175.50074291499823,
    "ttft": 2031077.547041636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.377219484769751,
    "arrivals": 540089,
    "finished_requests": 80933,
    "scheduler_time": 98.76705494385688
}
#Debug simulation 
Total elapsed time: 31.652697219047695. Arrivals time: 0.42438489478081465 Scheduler time: 31.10341634694487 Scheduler overhead time: 0.04647444374859333 Adapter cache time: 0.013849013019353151 Engine time: 0.04694404453039169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 25.956318261101842,
    "estimated_duration": 3600.0972901787363,
    "input_throughput": 5513.87404283579,
    "output_throughput": 4863.552173372155,
    "total_throughput": 10377.426216207945,
    "itl": 176.64566207576954,
    "ttft": 2022572.2655460127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 467,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5238352270727102,
    "arrivals": 540089,
    "finished_requests": 80420,
    "scheduler_time": 98.09410210236231
}
#Debug simulation 
Total elapsed time: 25.95649712299928. Arrivals time: 0.42331668734550476 Scheduler time: 25.411230388097465 Scheduler overhead time: 0.04566091299057007 Adapter cache time: 0.01378420228138566 Engine time: 0.0453886678442359 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 21.927800028119236,
    "estimated_duration": 3600.109962837131,
    "input_throughput": 5496.602105010542,
    "output_throughput": 4845.527270020553,
    "total_throughput": 10342.129375031096,
    "itl": 175.19775457021942,
    "ttft": 2024232.3027674141,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.906918045580399,
    "arrivals": 540089,
    "finished_requests": 80153,
    "scheduler_time": 98.06022598824698
}
#Debug simulation 
Total elapsed time: 21.927930381149054. Arrivals time: 0.41917670518159866 Scheduler time: 21.38968826457858 Scheduler overhead time: 0.04377483669668436 Adapter cache time: 0.014792087022215128 Engine time: 0.0438078660517931 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 31.746926322113723,
    "estimated_duration": 3600.117730211903,
    "input_throughput": 5546.396394882537,
    "output_throughput": 4893.728294536358,
    "total_throughput": 10440.124689418895,
    "itl": 175.50201396095895,
    "ttft": 2031093.5577594573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4113064789748726,
    "arrivals": 540089,
    "finished_requests": 80933,
    "scheduler_time": 98.76720765469663
}
#Debug simulation 
Total elapsed time: 31.74712102022022. Arrivals time: 0.42362850718200207 Scheduler time: 31.197606374509633 Scheduler overhead time: 0.046923909801989794 Adapter cache time: 0.014315114356577396 Engine time: 0.047235794365406036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 20.42011193698272,
    "estimated_duration": 3600.1511600103495,
    "input_throughput": 5458.653019431414,
    "output_throughput": 4814.75227833216,
    "total_throughput": 10273.405297763573,
    "itl": 176.16747223652584,
    "ttft": 2023343.061068233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9799082590080834,
    "arrivals": 540089,
    "finished_requests": 79672,
    "scheduler_time": 97.46409719785247
}
#Debug simulation 
Total elapsed time: 20.42025091731921. Arrivals time: 0.35187529399991035 Scheduler time: 19.95069998735562 Scheduler overhead time: 0.04267601668834686 Adapter cache time: 0.015584460459649563 Engine time: 0.04294456401839852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 31.575129210948944,
    "estimated_duration": 3600.13959444832,
    "input_throughput": 5543.628094526237,
    "output_throughput": 4890.3200940178795,
    "total_throughput": 10433.948188544116,
    "itl": 175.62071130929644,
    "ttft": 2030789.0458416387,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 453,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3544927835906744,
    "arrivals": 540089,
    "finished_requests": 80891,
    "scheduler_time": 98.70568877751411
}
#Debug simulation 
Total elapsed time: 31.575306909158826. Arrivals time: 0.37691058311611414 Scheduler time: 31.072897990234196 Scheduler overhead time: 0.046709898859262466 Adapter cache time: 0.013999317772686481 Engine time: 0.04721162188798189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 20.489544009789824,
    "estimated_duration": 3600.1769942425553,
    "input_throughput": 5458.6138491045485,
    "output_throughput": 4814.717728522923,
    "total_throughput": 10273.331577627472,
    "itl": 176.16856681102533,
    "ttft": 2023353.351249878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0056877852231207,
    "arrivals": 540089,
    "finished_requests": 79672,
    "scheduler_time": 97.46415190386347
}
#Debug simulation 
Total elapsed time: 20.489707069005817. Arrivals time: 0.37339216470718384 Scheduler time: 19.998458571732044 Scheduler overhead time: 0.04267612285912037 Adapter cache time: 0.015248788520693779 Engine time: 0.04325987724587321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 16.905138438101858,
    "estimated_duration": 3600.12210900458,
    "input_throughput": 5368.673454619039,
    "output_throughput": 4763.740362335077,
    "total_throughput": 10132.413816954117,
    "itl": 181.09571708641886,
    "ttft": 2004742.2909031166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.028323881961405,
    "arrivals": 483089,
    "finished_requests": 78463,
    "scheduler_time": 95.83575447574883
}
#Debug simulation 
Total elapsed time: 16.905251702293754. Arrivals time: 0.3226206046529114 Scheduler time: 16.477585799060762 Scheduler overhead time: 0.03859437443315983 Adapter cache time: 0.01059557031840086 Engine time: 0.04007453890517354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 17.19343700259924,
    "estimated_duration": 3600.0164928168524,
    "input_throughput": 5366.583191646194,
    "output_throughput": 4762.619569718805,
    "total_throughput": 10129.202761364999,
    "itl": 181.10619346555097,
    "ttft": 2004912.8802204803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1025709463260187,
    "arrivals": 483089,
    "finished_requests": 78440,
    "scheduler_time": 95.83080351812987
}
#Debug simulation 
Total elapsed time: 17.193579132668674. Arrivals time: 0.3267865451052785 Scheduler time: 16.760643486864865 Scheduler overhead time: 0.039589397609233856 Adapter cache time: 0.010714686010032892 Engine time: 0.03996479930356145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 16.704379177186638,
    "estimated_duration": 3600.149109285415,
    "input_throughput": 5357.323381483014,
    "output_throughput": 4753.108963144168,
    "total_throughput": 10110.432344627183,
    "itl": 178.97456014669905,
    "ttft": 2006570.3108747394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 349,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1417784869298402,
    "arrivals": 483089,
    "finished_requests": 78317,
    "scheduler_time": 96.02319980357892
}
#Debug simulation 
Total elapsed time: 16.70454074209556. Arrivals time: 0.338713260833174 Scheduler time: 16.25938592525199 Scheduler overhead time: 0.038981389719992876 Adapter cache time: 0.01146557368338108 Engine time: 0.03996223537251353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 17.084768277592957,
    "estimated_duration": 3600.07342165219,
    "input_throughput": 5365.674178704635,
    "output_throughput": 4761.238728329471,
    "total_throughput": 10126.912907034106,
    "itl": 181.10084393235053,
    "ttft": 2005200.8777570198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0576254612975777,
    "arrivals": 483089,
    "finished_requests": 78382,
    "scheduler_time": 95.83350698718553
}
#Debug simulation 
Total elapsed time: 17.08489688159898. Arrivals time: 0.3382264422252774 Scheduler time: 16.64167116722092 Scheduler overhead time: 0.039178306236863136 Adapter cache time: 0.010722103994339705 Engine time: 0.039396023377776146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 16.58989175921306,
    "estimated_duration": 3600.02776790236,
    "input_throughput": 5357.332010591061,
    "output_throughput": 4753.268058810181,
    "total_throughput": 10110.600069401242,
    "itl": 178.9877916952844,
    "ttft": 2006511.8126715561,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 349,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1556114034354739,
    "arrivals": 483089,
    "finished_requests": 78315,
    "scheduler_time": 96.01799402065035
}
#Debug simulation 
Total elapsed time: 16.590078135952353. Arrivals time: 0.3173067420721054 Scheduler time: 16.166918939445168 Scheduler overhead time: 0.038748740684241056 Adapter cache time: 0.010748726781457663 Engine time: 0.040055578108876944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 17.06843351619318,
    "estimated_duration": 3600.1335152592696,
    "input_throughput": 5366.409584009179,
    "output_throughput": 4762.636698701768,
    "total_throughput": 10129.046282710948,
    "itl": 181.0992119220747,
    "ttft": 2005030.179433482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0076469493820344,
    "arrivals": 483089,
    "finished_requests": 78443,
    "scheduler_time": 95.83626913751766
}
#Debug simulation 
Total elapsed time: 17.06856450624764. Arrivals time: 0.3311678096652031 Scheduler time: 16.632209259085357 Scheduler overhead time: 0.03889523120597005 Adapter cache time: 0.010242976248264313 Engine time: 0.039999411441385746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 16.78521415311843,
    "estimated_duration": 3600.0434699426787,
    "input_throughput": 5357.30864391676,
    "output_throughput": 4753.247326836435,
    "total_throughput": 10110.555970753196,
    "itl": 178.98741105414953,
    "ttft": 2006517.2904473587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 349,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1714563805237452,
    "arrivals": 483089,
    "finished_requests": 78315,
    "scheduler_time": 96.01842152568497
}
#Debug simulation 
Total elapsed time: 16.785359614994377. Arrivals time: 0.583689312916249 Scheduler time: 16.096375200431794 Scheduler overhead time: 0.038445009384304285 Adapter cache time: 0.01085432805120945 Engine time: 0.03985447762534022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 14.469920502975583,
    "estimated_duration": 3600.066693964058,
    "input_throughput": 5389.380155798221,
    "output_throughput": 4754.571360774598,
    "total_throughput": 10143.951516572819,
    "itl": 180.52589895678008,
    "ttft": 2000784.8938640414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0436263206810692,
    "arrivals": 473564,
    "finished_requests": 78528,
    "scheduler_time": 95.81783624264847
}
#Debug simulation 
Total elapsed time: 14.470028951764107. Arrivals time: 0.28454491635784507 Scheduler time: 14.086380697321147 Scheduler overhead time: 0.03767659002915025 Adapter cache time: 0.009621197823435068 Engine time: 0.03654484637081623 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 14.479674856178463,
    "estimated_duration": 3600.0900108395917,
    "input_throughput": 5390.2530052226075,
    "output_throughput": 4753.842250741034,
    "total_throughput": 10144.095255963643,
    "itl": 180.49351023926687,
    "ttft": 2001460.1936212773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4511080001713763,
    "arrivals": 473564,
    "finished_requests": 78554,
    "scheduler_time": 95.82771778626726
}
#Debug simulation 
Total elapsed time: 14.479782618116587. Arrivals time: 0.27768367901444435 Scheduler time: 14.103708264417946 Scheduler overhead time: 0.03555848728865385 Adapter cache time: 0.010932587552815676 Engine time: 0.036512743681669235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 15.008137639146298,
    "estimated_duration": 3600.1154096923283,
    "input_throughput": 5388.0283803617685,
    "output_throughput": 4753.429835590325,
    "total_throughput": 10141.458215952094,
    "itl": 178.36387076390307,
    "ttft": 2001977.5150975538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 315,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0281814212352092,
    "arrivals": 473564,
    "finished_requests": 78516,
    "scheduler_time": 96.09796773344377
}
#Debug simulation 
Total elapsed time: 15.008275777101517. Arrivals time: 0.2787409662269056 Scheduler time: 14.631777847185731 Scheduler overhead time: 0.03595009678974748 Adapter cache time: 0.009162340313196182 Engine time: 0.037092078011482954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 14.56275840383023,
    "estimated_duration": 3600.1653666804445,
    "input_throughput": 5387.469192251022,
    "output_throughput": 4755.239344959668,
    "total_throughput": 10142.70853721069,
    "itl": 180.52112659360992,
    "ttft": 2001597.2219929297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.358971125753594,
    "arrivals": 473564,
    "finished_requests": 78577,
    "scheduler_time": 95.83720294099106
}
#Debug simulation 
Total elapsed time: 14.56289093196392. Arrivals time: 0.27714526606723666 Scheduler time: 14.187912782188505 Scheduler overhead time: 0.03563768183812499 Adapter cache time: 0.010638045612722635 Engine time: 0.03618847718462348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 14.901511637028307,
    "estimated_duration": 3600.1649799347847,
    "input_throughput": 5387.954193241271,
    "output_throughput": 4753.364386181544,
    "total_throughput": 10141.318579422816,
    "itl": 178.3553312046445,
    "ttft": 2002004.9268334392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 315,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0408825536631099,
    "arrivals": 473564,
    "finished_requests": 78516,
    "scheduler_time": 96.09960577605241
}
#Debug simulation 
Total elapsed time: 14.901633344125003. Arrivals time: 0.27971714455634356 Scheduler time: 14.525101903825998 Scheduler overhead time: 0.03560581197962165 Adapter cache time: 0.009137443266808987 Engine time: 0.03674818715080619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 14.380231626797467,
    "estimated_duration": 3600.033739031113,
    "input_throughput": 5389.429490519649,
    "output_throughput": 4754.614884416801,
    "total_throughput": 10144.044374936451,
    "itl": 180.52485170826074,
    "ttft": 2000773.3453737844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0196071505616426,
    "arrivals": 473564,
    "finished_requests": 78528,
    "scheduler_time": 95.81753698916269
}
#Debug simulation 
Total elapsed time: 14.380343948956579. Arrivals time: 0.2804307551123202 Scheduler time: 14.003269053529948 Scheduler overhead time: 0.035271487198770046 Adapter cache time: 0.00978651363402605 Engine time: 0.03629971342161298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 15.04545655567199,
    "estimated_duration": 3600.140333697215,
    "input_throughput": 5387.991078692046,
    "output_throughput": 4753.396927287462,
    "total_throughput": 10141.388005979508,
    "itl": 178.3627019595935,
    "ttft": 2001988.49845429,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 315,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0540867012366701,
    "arrivals": 473564,
    "finished_requests": 78516,
    "scheduler_time": 96.09821865434849
}
#Debug simulation 
Total elapsed time: 15.04562203772366. Arrivals time: 0.28302036598324776 Scheduler time: 14.66456813365221 Scheduler overhead time: 0.035865156911313534 Adapter cache time: 0.009533345699310303 Engine time: 0.03701474703848362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 14.29915564879775,
    "estimated_duration": 3600.050589024585,
    "input_throughput": 5376.205006397989,
    "output_throughput": 4752.424049861917,
    "total_throughput": 10128.629056259906,
    "itl": 180.98092340566436,
    "ttft": 1993744.1459143213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8783599825086992,
    "arrivals": 468761,
    "finished_requests": 78142,
    "scheduler_time": 95.74415422339118
}
#Debug simulation 
Total elapsed time: 14.299236001912504. Arrivals time: 0.5875119441188872 Scheduler time: 13.61538218241185 Scheduler overhead time: 0.03560716938227415 Adapter cache time: 0.00895222881808877 Engine time: 0.03649362921714783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 13.878439485095441,
    "estimated_duration": 3600.0393006575623,
    "input_throughput": 5373.633281299596,
    "output_throughput": 4748.7289921744505,
    "total_throughput": 10122.362273474047,
    "itl": 181.06238673162173,
    "ttft": 1995090.8357754454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 302,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9859400387038533,
    "arrivals": 468761,
    "finished_requests": 78113,
    "scheduler_time": 95.707148747087
}
#Debug simulation 
Total elapsed time: 13.878573752008379. Arrivals time: 0.3266125079244375 Scheduler time: 13.455579936970025 Scheduler overhead time: 0.035512218717485666 Adapter cache time: 0.009061639197170734 Engine time: 0.036513105034828186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.812761710956693,
    "estimated_duration": 3600.0865949506265,
    "input_throughput": 5360.504668711911,
    "output_throughput": 4737.491876979117,
    "total_throughput": 10097.996545691027,
    "itl": 178.77707028393752,
    "ttft": 1996447.17898966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.964275816809391,
    "arrivals": 468761,
    "finished_requests": 77940,
    "scheduler_time": 95.86450811583164
}
#Debug simulation 
Total elapsed time: 12.81290684407577. Arrivals time: 0.32011411571875215 Scheduler time: 12.39726975467056 Scheduler overhead time: 0.034903022926300764 Adapter cache time: 0.009040889795869589 Engine time: 0.03629752341657877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 13.900970621034503,
    "estimated_duration": 3600.196506146285,
    "input_throughput": 5373.817225523937,
    "output_throughput": 4748.846061822525,
    "total_throughput": 10122.663287346462,
    "itl": 181.06110902933398,
    "ttft": 1995083.2495088314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 302,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9422203396307335,
    "arrivals": 468761,
    "finished_requests": 78118,
    "scheduler_time": 95.71243817875111
}
#Debug simulation 
Total elapsed time: 13.901080987881869. Arrivals time: 0.2793126702308655 Scheduler time: 13.525443162303418 Scheduler overhead time: 0.0350778098218143 Adapter cache time: 0.00895148515701294 Engine time: 0.03700760658830404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 12.783946712035686,
    "estimated_duration": 3600.099344521721,
    "input_throughput": 5360.3595771226555,
    "output_throughput": 4737.270382816543,
    "total_throughput": 10097.629959939199,
    "itl": 178.76680844272002,
    "ttft": 1996481.712607937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.983612728137528,
    "arrivals": 468761,
    "finished_requests": 77937,
    "scheduler_time": 95.86345980559122
}
#Debug simulation 
Total elapsed time: 12.78405011817813. Arrivals time: 0.3195303110405803 Scheduler time: 12.3688537706621 Scheduler overhead time: 0.035180664621293545 Adapter cache time: 0.009147616568952799 Engine time: 0.03602935140952468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 13.45896578580141,
    "estimated_duration": 3600.0989344878826,
    "input_throughput": 5378.916899891985,
    "output_throughput": 4752.591890209786,
    "total_throughput": 10131.50879010177,
    "itl": 180.9577324950868,
    "ttft": 1995608.7883883752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9089752896502626,
    "arrivals": 468761,
    "finished_requests": 78181,
    "scheduler_time": 95.74501851840647
}
#Debug simulation 
Total elapsed time: 13.459109108895063. Arrivals time: 0.3202536841854453 Scheduler time: 13.043520820792764 Scheduler overhead time: 0.03497618855908513 Adapter cache time: 0.009009209927171469 Engine time: 0.036086609587073326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.780436071101576,
    "estimated_duration": 3600.166555139726,
    "input_throughput": 5360.405610248499,
    "output_throughput": 4737.463597524602,
    "total_throughput": 10097.869207773101,
    "itl": 178.76882543479925,
    "ttft": 1996484.7548815776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9925558328628594,
    "arrivals": 468761,
    "finished_requests": 77941,
    "scheduler_time": 95.86693715977437
}
#Debug simulation 
Total elapsed time: 12.780560961924493. Arrivals time: 0.31791438395157456 Scheduler time: 12.366512231063098 Scheduler overhead time: 0.03524261014536023 Adapter cache time: 0.009003618266433477 Engine time: 0.03655102849006653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 12.248842379078269,
    "estimated_duration": 3600.089380642962,
    "input_throughput": 5367.34090656077,
    "output_throughput": 4748.692377450576,
    "total_throughput": 10116.033284011346,
    "itl": 180.987350369225,
    "ttft": 2000762.7511616745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7192146198241962,
    "arrivals": 466310,
    "finished_requests": 78100,
    "scheduler_time": 95.57041619797525
}
#Debug simulation 
Total elapsed time: 12.248947080224752. Arrivals time: 0.3136822837404907 Scheduler time: 11.842506895773113 Scheduler overhead time: 0.034082372672855854 Adapter cache time: 0.008340176660567522 Engine time: 0.0351501009427011 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.918992029037327,
    "estimated_duration": 3600.1670656074402,
    "input_throughput": 5367.490910241848,
    "output_throughput": 4748.780178376361,
    "total_throughput": 10116.271088618209,
    "itl": 180.98932167723117,
    "ttft": 2000771.291916395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7680370702524699,
    "arrivals": 466310,
    "finished_requests": 78102,
    "scheduler_time": 95.57149038116759
}
#Debug simulation 
Total elapsed time: 12.919108892790973. Arrivals time: 0.32013135123997927 Scheduler time: 12.504678569734097 Scheduler overhead time: 0.03460262482985854 Adapter cache time: 0.008571928832679987 Engine time: 0.03576715895906091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.758941124659032,
    "estimated_duration": 3600.149549237942,
    "input_throughput": 5354.095360866365,
    "output_throughput": 4738.104838897792,
    "total_throughput": 10092.200199764156,
    "itl": 178.86802659275224,
    "ttft": 2002522.198617171,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7857531128078736,
    "arrivals": 466310,
    "finished_requests": 77923,
    "scheduler_time": 95.68591281379194
}
#Debug simulation 
Total elapsed time: 11.759048115927726. Arrivals time: 0.311644971370697 Scheduler time: 11.353942872490734 Scheduler overhead time: 0.03450925089418888 Adapter cache time: 0.008608999196439981 Engine time: 0.034999542869627476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 12.369384189136326,
    "estimated_duration": 3600.1076953825846,
    "input_throughput": 5367.313601418957,
    "output_throughput": 4748.668219544258,
    "total_throughput": 10115.981820963214,
    "itl": 180.98796596041325,
    "ttft": 2000771.9688351823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7361666354141216,
    "arrivals": 466310,
    "finished_requests": 78100,
    "scheduler_time": 95.57053197493882
}
#Debug simulation 
Total elapsed time: 12.369483502116054. Arrivals time: 0.3163718287833035 Scheduler time: 11.959725856315345 Scheduler overhead time: 0.03424827987328172 Adapter cache time: 0.00844336673617363 Engine time: 0.0355594945140183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 11.701198902912438,
    "estimated_duration": 3600.1595475608738,
    "input_throughput": 5354.08049153246,
    "output_throughput": 4738.091680285893,
    "total_throughput": 10092.172171818353,
    "itl": 178.86831110936532,
    "ttft": 2002527.0448309574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7959391695074772,
    "arrivals": 466310,
    "finished_requests": 77923,
    "scheduler_time": 95.68595578311945
}
#Debug simulation 
Total elapsed time: 11.7013217699714. Arrivals time: 0.3364765280857682 Scheduler time: 11.271802835632116 Scheduler overhead time: 0.03407585388049483 Adapter cache time: 0.008467191364616156 Engine time: 0.035263088531792164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 12.313364021014422,
    "estimated_duration": 3600.1844343579282,
    "input_throughput": 5367.531678539225,
    "output_throughput": 4748.8303201469425,
    "total_throughput": 10116.361998686167,
    "itl": 180.9866336062996,
    "ttft": 2000778.4110533309,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7026618193020122,
    "arrivals": 466310,
    "finished_requests": 78103,
    "scheduler_time": 95.57298982264246
}
#Debug simulation 
Total elapsed time: 12.313464971724898. Arrivals time: 0.31433084793388844 Scheduler time: 11.906679146923125 Scheduler overhead time: 0.034007704351097345 Adapter cache time: 0.008298570290207863 Engine time: 0.03503562603145838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.81900262273848,
    "estimated_duration": 3600.167983133332,
    "input_throughput": 5354.067946358416,
    "output_throughput": 4738.080578438459,
    "total_throughput": 10092.148524796876,
    "itl": 178.86852556846,
    "ttft": 2002530.755196967,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8063767337799105,
    "arrivals": 466310,
    "finished_requests": 77923,
    "scheduler_time": 95.68594736340687
}
#Debug simulation 
Total elapsed time: 11.819117557723075. Arrivals time: 0.31616460299119353 Scheduler time: 11.409623463172466 Scheduler overhead time: 0.03420728212222457 Adapter cache time: 0.008609241805970669 Engine time: 0.035198504105210304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 12.125953072682023,
    "estimated_duration": 3600.0852911550824,
    "input_throughput": 5422.776523646263,
    "output_throughput": 4745.726175425776,
    "total_throughput": 10168.502699072038,
    "itl": 180.04214991628157,
    "ttft": 1996720.8094914898,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9518116883630852,
    "arrivals": 465116,
    "finished_requests": 78404,
    "scheduler_time": 95.7042097656725
}
#Debug simulation 
Total elapsed time: 12.126059458591044. Arrivals time: 0.272474252153188 Scheduler time: 11.759241508319974 Scheduler overhead time: 0.03468881547451019 Adapter cache time: 0.008979331701993942 Engine time: 0.03548152418807149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.443829928059131,
    "estimated_duration": 3600.0213419115726,
    "input_throughput": 5423.618124894882,
    "output_throughput": 4746.669360278403,
    "total_throughput": 10170.287485173285,
    "itl": 180.05238688997463,
    "ttft": 1996449.8083873815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0119034176552724,
    "arrivals": 465116,
    "finished_requests": 78408,
    "scheduler_time": 95.70387372321501
}
#Debug simulation 
Total elapsed time: 12.443944917991757. Arrivals time: 0.5073249563574791 Scheduler time: 11.842903057578951 Scheduler overhead time: 0.034251678735017776 Adapter cache time: 0.00881832605227828 Engine time: 0.03544193506240845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.11472699092701,
    "estimated_duration": 3600.031295040219,
    "input_throughput": 5409.266310220351,
    "output_throughput": 4734.700507599219,
    "total_throughput": 10143.966817819572,
    "itl": 178.6298786280584,
    "ttft": 1997061.5768536371,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7316608513146672,
    "arrivals": 465116,
    "finished_requests": 78207,
    "scheduler_time": 95.7229218637377
}
#Debug simulation 
Total elapsed time: 12.114830961916596. Arrivals time: 0.2986460402607918 Scheduler time: 11.723460399080068 Scheduler overhead time: 0.034043995663523674 Adapter cache time: 0.008075747638940811 Engine time: 0.035232548136264086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 12.081317988224328,
    "estimated_duration": 3600.108975791882,
    "input_throughput": 5422.740847922757,
    "output_throughput": 4745.694953926212,
    "total_throughput": 10168.43580184897,
    "itl": 180.04281918866323,
    "ttft": 1996730.984523689,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9728081501508167,
    "arrivals": 465116,
    "finished_requests": 78404,
    "scheduler_time": 95.70436559297585
}
#Debug simulation 
Total elapsed time: 12.081422325223684. Arrivals time: 0.27778617898002267 Scheduler time: 11.709794336929917 Scheduler overhead time: 0.03445909544825554 Adapter cache time: 0.008700720500200987 Engine time: 0.0355280302464962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 12.70788543811068,
    "estimated_duration": 3600.1899128614796,
    "input_throughput": 5418.956352914663,
    "output_throughput": 4735.910719345438,
    "total_throughput": 10154.8670722601,
    "itl": 178.42287848715193,
    "ttft": 1996412.5949144678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6924901150353279,
    "arrivals": 465116,
    "finished_requests": 78234,
    "scheduler_time": 95.73919111657997
}
#Debug simulation 
Total elapsed time: 12.708011656999588. Arrivals time: 0.27496520709246397 Scheduler time: 12.339672220405191 Scheduler overhead time: 0.03452428011223674 Adapter cache time: 0.007819203659892082 Engine time: 0.035709002055227757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 12.533741517923772,
    "estimated_duration": 3600.1654835250833,
    "input_throughput": 5425.486158729212,
    "output_throughput": 4745.107989667491,
    "total_throughput": 10170.594148396704,
    "itl": 180.10641662307458,
    "ttft": 1995353.5876598621,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5980100589804359,
    "arrivals": 465116,
    "finished_requests": 78374,
    "scheduler_time": 95.64312685274045
}
#Debug simulation 
Total elapsed time: 12.533863636199385. Arrivals time: 0.2720776963979006 Scheduler time: 12.169265492353588 Scheduler overhead time: 0.03453474584966898 Adapter cache time: 0.0075501445680856705 Engine time: 0.035253760404884815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.766146541107446,
    "estimated_duration": 3600.004735395074,
    "input_throughput": 5418.913149807046,
    "output_throughput": 4735.925437089449,
    "total_throughput": 10154.838586896494,
    "itl": 178.423781102438,
    "ttft": 1996376.082815894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7009156187251233,
    "arrivals": 465116,
    "finished_requests": 78231,
    "scheduler_time": 95.73400903367902
}
#Debug simulation 
Total elapsed time: 12.766255986876786. Arrivals time: 0.27794426027685404 Scheduler time: 12.394057699013501 Scheduler overhead time: 0.0349725428968668 Adapter cache time: 0.00787825183942914 Engine time: 0.03601587889716029 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 10.382959875743836,
    "estimated_duration": 3600.003818775291,
    "input_throughput": 5387.892062458589,
    "output_throughput": 4743.332746188365,
    "total_throughput": 10131.224808646954,
    "itl": 180.68111220272178,
    "ttft": 1995595.0331738417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5845531590911552,
    "arrivals": 464531,
    "finished_requests": 78464,
    "scheduler_time": 95.61287017170007
}
#Debug simulation 
Total elapsed time: 10.383070074953139. Arrivals time: 0.3082311493344605 Scheduler time: 9.985034425742924 Scheduler overhead time: 0.03327491320669651 Adapter cache time: 0.00746139558032155 Engine time: 0.03407559311017394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.212715520057827,
    "estimated_duration": 3600.1810823101696,
    "input_throughput": 5391.284092728252,
    "output_throughput": 4746.362921565906,
    "total_throughput": 10137.647014294158,
    "itl": 180.66343153348583,
    "ttft": 1996630.911927742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6659478187863733,
    "arrivals": 464531,
    "finished_requests": 78494,
    "scheduler_time": 95.61723678558775
}
#Debug simulation 
Total elapsed time: 10.212816339917481. Arrivals time: 0.32773140259087086 Scheduler time: 9.795125723350793 Scheduler overhead time: 0.03321805177256465 Adapter cache time: 0.007595569361001253 Engine time: 0.0340551882982254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.135449949651957,
    "estimated_duration": 3600.1588756998035,
    "input_throughput": 5380.306166692391,
    "output_throughput": 4736.930671340187,
    "total_throughput": 10117.236838032577,
    "itl": 179.0815312269623,
    "ttft": 1997568.005336701,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6667435044050245,
    "arrivals": 464531,
    "finished_requests": 78331,
    "scheduler_time": 95.70366261537255
}
#Debug simulation 
Total elapsed time: 10.135568690951914. Arrivals time: 0.3147470518015325 Scheduler time: 9.729628956411034 Scheduler overhead time: 0.033430445939302444 Adapter cache time: 0.007865960709750652 Engine time: 0.03468387294560671 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 10.397961650975049,
    "estimated_duration": 3600.0557843045726,
    "input_throughput": 5387.87428921657,
    "output_throughput": 4743.286777512702,
    "total_throughput": 10131.161066729272,
    "itl": 180.68251576424458,
    "ttft": 1995593.5675255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5976583020249389,
    "arrivals": 464531,
    "finished_requests": 78465,
    "scheduler_time": 95.61410312013724
}
#Debug simulation 
Total elapsed time: 10.398064752109349. Arrivals time: 0.31182284699752927 Scheduler time: 9.995822990313172 Scheduler overhead time: 0.0332709988579154 Adapter cache time: 0.00740757817402482 Engine time: 0.034624258521944284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 10.346917181275785,
    "estimated_duration": 3600.0529459534696,
    "input_throughput": 5383.926928570932,
    "output_throughput": 4735.412299744644,
    "total_throughput": 10119.339228315575,
    "itl": 179.010282648849,
    "ttft": 1997822.1863066775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6509606146812461,
    "arrivals": 464531,
    "finished_requests": 78338,
    "scheduler_time": 95.70611380462267
}
#Debug simulation 
Total elapsed time: 10.347021216060966. Arrivals time: 0.31249692803248763 Scheduler time: 9.94376295665279 Scheduler overhead time: 0.03346829768270254 Adapter cache time: 0.007615473587065935 Engine time: 0.034521559718996286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 10.342377269174904,
    "estimated_duration": 3600.1909401211647,
    "input_throughput": 5388.406704714143,
    "output_throughput": 4743.818392983683,
    "total_throughput": 10132.225097697827,
    "itl": 180.68263371913048,
    "ttft": 1995545.962401381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5710996063263163,
    "arrivals": 464531,
    "finished_requests": 78475,
    "scheduler_time": 95.6181963409769
}
#Debug simulation 
Total elapsed time: 10.3424799349159. Arrivals time: 0.2676918413490057 Scheduler time: 9.984943306539208 Scheduler overhead time: 0.03308209590613842 Adapter cache time: 0.007435809820890427 Engine time: 0.03432571375742555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_160_slots_96_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.430030604824424,
    "estimated_duration": 3600.132052704019,
    "input_throughput": 5383.951398516533,
    "output_throughput": 4735.3093582208,
    "total_throughput": 10119.260756737334,
    "itl": 179.02115869518354,
    "ttft": 1997867.3361825424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6597633797302858,
    "arrivals": 464531,
    "finished_requests": 78340,
    "scheduler_time": 95.70723346524807
}
#Debug simulation 
Total elapsed time: 10.430141082033515. Arrivals time: 0.269885141402483 Scheduler time: 10.068933143746108 Scheduler overhead time: 0.03374679991975427 Adapter cache time: 0.007658806629478931 Engine time: 0.034749210346490145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_160_slots_96_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_160_slots_96_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 9.427868924569339,
    "estimated_duration": 3600.166749524872,
    "input_throughput": 5393.270187432983,
    "output_throughput": 4747.27233183173,
    "total_throughput": 10140.542519264713,
    "itl": 180.52581283772702,
    "ttft": 1963123.2095501844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7199941120902313,
    "arrivals": 406769,
    "finished_requests": 78594,
    "scheduler_time": 95.40136702556256
}
#Debug simulation 
Total elapsed time: 9.42800136981532. Arrivals time: 0.28717072820290923 Scheduler time: 9.045097780879587 Scheduler overhead time: 0.03317007748410106 Adapter cache time: 0.01359829306602478 Engine time: 0.03388018626719713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_160_slots_96_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_160_slots_96_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 9.334874703083187,
    "estimated_duration": 3600.19345733848,
    "input_throughput": 5393.80236926371,
    "output_throughput": 4747.419604677398,
    "total_throughput": 10141.221973941107,
    "itl": 180.495666047766,
    "ttft": 1963347.0092550765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8487116446904885,
    "arrivals": 406769,
    "finished_requests": 78606,
    "scheduler_time": 95.39994711024859
}
#Debug simulation 
Total elapsed time: 9.334975000936538. Arrivals time: 0.29995168512687087 Scheduler time: 8.939528850838542 Scheduler overhead time: 0.033111173659563065 Adapter cache time: 0.013625430408865213 Engine time: 0.03379803290590644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_160_slots_96_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_160_slots_96_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 9.25237845396623,
    "estimated_duration": 3600.1149469307647,
    "input_throughput": 5381.968710893114,
    "output_throughput": 4737.605396333477,
    "total_throughput": 10119.574107226592,
    "itl": 178.89795607508037,
    "ttft": 1963904.5910897932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8724358871579285,
    "arrivals": 406769,
    "finished_requests": 78433,
    "scheduler_time": 95.48682570669268
}
#Debug simulation 
Total elapsed time: 9.252497659996152. Arrivals time: 0.3005071529187262 Scheduler time: 8.855949935503304 Scheduler overhead time: 0.03329370077699423 Adapter cache time: 0.013433333951979876 Engine time: 0.03417847724631429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_160_slots_96_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_160_slots_96_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 9.583005463704467,
    "estimated_duration": 3600.106459829399,
    "input_throughput": 5392.669693695668,
    "output_throughput": 4748.103754912301,
    "total_throughput": 10140.77344860797,
    "itl": 180.54126739533993,
    "ttft": 1963039.0919568809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.73031373294768,
    "arrivals": 406769,
    "finished_requests": 78600,
    "scheduler_time": 95.39810857577491
}
#Debug simulation 
Total elapsed time: 9.583100564777851. Arrivals time: 0.2866653762757778 Scheduler time: 9.201148844789714 Scheduler overhead time: 0.03303202893584967 Adapter cache time: 0.013246935326606035 Engine time: 0.03396457713097334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_160_slots_96_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_160_slots_96_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 9.257145300041884,
    "estimated_duration": 3600.1364585563133,
    "input_throughput": 5381.936552418858,
    "output_throughput": 4737.577088075039,
    "total_throughput": 10119.513640493897,
    "itl": 178.89862233323285,
    "ttft": 1963913.6648047408,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.894694307353352,
    "arrivals": 406769,
    "finished_requests": 78433,
    "scheduler_time": 95.4868686099524
}
#Debug simulation 
Total elapsed time: 9.25725819915533. Arrivals time: 0.30306957149878144 Scheduler time: 8.85805991012603 Scheduler overhead time: 0.03327312506735325 Adapter cache time: 0.013776164967566729 Engine time: 0.033873352222144604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_160_slots_96_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_160_slots_96_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 9.315551650710404,
    "estimated_duration": 3600.1526537808604,
    "input_throughput": 5391.908306891917,
    "output_throughput": 4747.331194984469,
    "total_throughput": 10139.239501876385,
    "itl": 180.53385508962793,
    "ttft": 1962941.3544275502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 567,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6953585172095103,
    "arrivals": 406769,
    "finished_requests": 78580,
    "scheduler_time": 95.40070384145439
}
#Debug simulation 
Total elapsed time: 9.315656684804708. Arrivals time: 0.29923107381910086 Scheduler time: 8.920908716972917 Scheduler overhead time: 0.03308577882125974 Adapter cache time: 0.013493143022060394 Engine time: 0.034014913253486156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_160_slots_96_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_160_slots_96_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 9.327797895297408,
    "estimated_duration": 3600.082341568914,
    "input_throughput": 5380.280827565405,
    "output_throughput": 4736.337222933932,
    "total_throughput": 10116.618050499337,
    "itl": 178.81180665186747,
    "ttft": 1964005.8731264337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 577,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9297358560934619,
    "arrivals": 406769,
    "finished_requests": 78414,
    "scheduler_time": 95.48952536571414
}
#Debug simulation 
Total elapsed time: 9.327917996328324. Arrivals time: 0.28405888052657247 Scheduler time: 8.947154697962105 Scheduler overhead time: 0.03343683574348688 Adapter cache time: 0.01366429403424263 Engine time: 0.03443060163408518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_160_slots_96_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_160_slots_96_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 8.399031839799136,
    "estimated_duration": 3600.101715405368,
    "input_throughput": 5365.812837270036,
    "output_throughput": 4741.52797598885,
    "total_throughput": 10107.340813258885,
    "itl": 181.06493395595317,
    "ttft": 1959856.9553868398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.79344581794462,
    "arrivals": 397242,
    "finished_requests": 78193,
    "scheduler_time": 95.32302946733218
}
#Debug simulation 
Total elapsed time: 8.399106786120683. Arrivals time: 0.5330162770114839 Scheduler time: 7.7713760496117175 Scheduler overhead time: 0.032827562186867 Adapter cache time: 0.013616382610052824 Engine time: 0.033344816416502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_160_slots_96_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_160_slots_96_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.121165312826633,
    "estimated_duration": 3600.088949073625,
    "input_throughput": 5365.989638942371,
    "output_throughput": 4741.81256115705,
    "total_throughput": 10107.80220009942,
    "itl": 181.0623597877586,
    "ttft": 1959918.0738935396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 588,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9162759616388982,
    "arrivals": 397242,
    "finished_requests": 78193,
    "scheduler_time": 95.3203169843014
}
#Debug simulation 
Total elapsed time: 8.121258711908013. Arrivals time: 0.28676594281569123 Scheduler time: 7.740133516956121 Scheduler overhead time: 0.032647992949932814 Adapter cache time: 0.013534750323742628 Engine time: 0.033344355411827564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_160_slots_96_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_160_slots_96_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.046000372152776,
    "estimated_duration": 3600.0428957951017,
    "input_throughput": 5354.700640516248,
    "output_throughput": 4731.373901098497,
    "total_throughput": 10086.074541614746,
    "itl": 179.1583623961179,
    "ttft": 1961562.742107367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 596,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9468127467110874,
    "arrivals": 397242,
    "finished_requests": 78023,
    "scheduler_time": 95.41773413655189
}
#Debug simulation 
Total elapsed time: 8.046095377299935. Arrivals time: 0.28549381205812097 Scheduler time: 7.665163110475987 Scheduler overhead time: 0.033007722813636065 Adapter cache time: 0.013757385779172182 Engine time: 0.033752364572137594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_160_slots_96_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_160_slots_96_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 8.218178950250149,
    "estimated_duration": 3600.1402494334097,
    "input_throughput": 5365.75540440131,
    "output_throughput": 4741.477225140458,
    "total_throughput": 10107.232629541768,
    "itl": 181.06667475014618,
    "ttft": 1959871.780403476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8334799411822946,
    "arrivals": 397242,
    "finished_requests": 78193,
    "scheduler_time": 95.32306514349511
}
#Debug simulation 
Total elapsed time: 8.218282922171056. Arrivals time: 0.28510872833430767 Scheduler time: 7.8385700159706175 Scheduler overhead time: 0.03269496886059642 Adapter cache time: 0.013660718221217394 Engine time: 0.033353451639413834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_160_slots_96_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_160_slots_96_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 8.18499748595059,
    "estimated_duration": 3600.00168114961,
    "input_throughput": 5353.532499975257,
    "output_throughput": 4730.907790732282,
    "total_throughput": 10084.440290707538,
    "itl": 179.10073762707918,
    "ttft": 1961716.889078826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 590,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9501698604971218,
    "arrivals": 397242,
    "finished_requests": 78013,
    "scheduler_time": 95.42101689146742
}
#Debug simulation 
Total elapsed time: 8.18509822897613. Arrivals time: 0.28357447357848287 Scheduler time: 7.805573289748281 Scheduler overhead time: 0.033146983943879604 Adapter cache time: 0.013808521442115307 Engine time: 0.03386598452925682 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_160_slots_96_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_160_slots_96_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 8.209120638202876,
    "estimated_duration": 3600.199884317411,
    "input_throughput": 5366.588695300504,
    "output_throughput": 4741.642838877151,
    "total_throughput": 10108.231534177656,
    "itl": 181.03323915259227,
    "ttft": 1960205.3785055662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 577,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.725259020158531,
    "arrivals": 397242,
    "finished_requests": 78201,
    "scheduler_time": 95.32902607138342
}
#Debug simulation 
Total elapsed time: 8.20923586608842. Arrivals time: 0.2834391281940043 Scheduler time: 7.831410611514002 Scheduler overhead time: 0.03269711975008249 Adapter cache time: 0.013434372842311859 Engine time: 0.0333227445371449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_160_slots_96_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_160_slots_96_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.678014338947833,
    "estimated_duration": 3600.1203679821915,
    "input_throughput": 5353.4023949321845,
    "output_throughput": 4730.176288395765,
    "total_throughput": 10083.578683327949,
    "itl": 179.04005486584586,
    "ttft": 1961388.6444201549,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 593,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9863031794503283,
    "arrivals": 397242,
    "finished_requests": 78008,
    "scheduler_time": 95.42766049318139
}
#Debug simulation 
Total elapsed time: 8.678111525718123. Arrivals time: 0.28456015093252063 Scheduler time: 8.296646539587528 Scheduler overhead time: 0.03354573808610439 Adapter cache time: 0.014096623286604881 Engine time: 0.033870633225888014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_160_slots_96_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_160_slots_96_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.4199796509929,
    "estimated_duration": 3600.1817702777953,
    "input_throughput": 5400.034565060279,
    "output_throughput": 4742.867191031428,
    "total_throughput": 10142.901756091707,
    "itl": 180.50826553664112,
    "ttft": 1958558.3720947192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 601,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.839353134103613,
    "arrivals": 392500,
    "finished_requests": 78535,
    "scheduler_time": 95.34272222061304
}
#Debug simulation 
Total elapsed time: 7.42007057601586. Arrivals time: 0.2469397010281682 Scheduler time: 7.078381471335888 Scheduler overhead time: 0.032651455607265234 Adapter cache time: 0.013632598333060741 Engine time: 0.033550186548382044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_160_slots_96_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_160_slots_96_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.466610599774867,
    "estimated_duration": 3600.129313081844,
    "input_throughput": 5399.055230980288,
    "output_throughput": 4742.229102149998,
    "total_throughput": 10141.284333130285,
    "itl": 180.50521302993315,
    "ttft": 1958546.942978524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 605,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9724185557919598,
    "arrivals": 392500,
    "finished_requests": 78530,
    "scheduler_time": 95.33859626054446
}
#Debug simulation 
Total elapsed time: 7.466707699932158. Arrivals time: 0.2517933789640665 Scheduler time: 7.119762382935733 Scheduler overhead time: 0.032889924477785826 Adapter cache time: 0.013793272897601128 Engine time: 0.03354330360889435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_160_slots_96_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_160_slots_96_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.44405778311193,
    "estimated_duration": 3600.1470818972784,
    "input_throughput": 5388.419294740777,
    "output_throughput": 4733.071069701976,
    "total_throughput": 10121.490364442754,
    "itl": 178.88313742099564,
    "ttft": 1960149.7998442415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9860329528525595,
    "arrivals": 392500,
    "finished_requests": 78358,
    "scheduler_time": 95.42674192085533
}
#Debug simulation 
Total elapsed time: 7.444147203117609. Arrivals time: 0.25185709074139595 Scheduler time: 7.096304769162089 Scheduler overhead time: 0.03296565730124712 Adapter cache time: 0.014200777746737003 Engine time: 0.033676006365567446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_160_slots_96_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_160_slots_96_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 7.4890770320780575,
    "estimated_duration": 3600.071661468039,
    "input_throughput": 5400.084172789067,
    "output_throughput": 4742.820034042698,
    "total_throughput": 10142.904206831767,
    "itl": 180.51072820560756,
    "ttft": 1958512.9590156067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 601,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8811908628349086,
    "arrivals": 392500,
    "finished_requests": 78532,
    "scheduler_time": 95.33886193918383
}
#Debug simulation 
Total elapsed time: 7.489179133903235. Arrivals time: 0.2523334613069892 Scheduler time: 7.14178506238386 Scheduler overhead time: 0.03269004635512829 Adapter cache time: 0.013634639792144299 Engine time: 0.03372488636523485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_160_slots_96_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_160_slots_96_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 7.3600740362890065,
    "estimated_duration": 3600.0280913755337,
    "input_throughput": 5388.139622151791,
    "output_throughput": 4732.302795306955,
    "total_throughput": 10120.442417458746,
    "itl": 178.90432926167122,
    "ttft": 1960448.4401583325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.007902687303727,
    "arrivals": 392500,
    "finished_requests": 78348,
    "scheduler_time": 95.42072487010468
}
#Debug simulation 
Total elapsed time: 7.360167645383626. Arrivals time: 0.2365056900307536 Scheduler time: 7.0280198426917195 Scheduler overhead time: 0.03304493287578225 Adapter cache time: 0.013821224216371775 Engine time: 0.033661140128970146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_160_slots_96_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_160_slots_96_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.395426936913282,
    "estimated_duration": 3600.1552267180537,
    "input_throughput": 5399.5857888942055,
    "output_throughput": 4742.337739576884,
    "total_throughput": 10141.92352847109,
    "itl": 180.5043988528067,
    "ttft": 1958386.7140354197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 604,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8059903781208868,
    "arrivals": 392500,
    "finished_requests": 78531,
    "scheduler_time": 95.34318141599996
}
#Debug simulation 
Total elapsed time: 7.395511373877525. Arrivals time: 0.23332012724131346 Scheduler time: 7.06747975340113 Scheduler overhead time: 0.032694277353584766 Adapter cache time: 0.013733004219830036 Engine time: 0.03334547812119126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_160_slots_96_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_160_slots_96_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.331275889649987,
    "estimated_duration": 3600.1114289289185,
    "input_throughput": 5387.981839709602,
    "output_throughput": 4732.9612808871825,
    "total_throughput": 10120.943120596785,
    "itl": 178.895567462883,
    "ttft": 1960366.2334487846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0592796107754086,
    "arrivals": 392500,
    "finished_requests": 78354,
    "scheduler_time": 95.42342647798799
}
#Debug simulation 
Total elapsed time: 7.331370356027037. Arrivals time: 0.23535291524603963 Scheduler time: 6.999910816550255 Scheduler overhead time: 0.032866889610886574 Adapter cache time: 0.014034806750714779 Engine time: 0.03416336793452501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.883259943686426,
    "estimated_duration": 3600.046052354585,
    "input_throughput": 5369.241314943087,
    "output_throughput": 4748.901750525749,
    "total_throughput": 10118.143065468836,
    "itl": 181.44659684221074,
    "ttft": 1954652.2677430392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6557238694676413,
    "arrivals": 390109,
    "finished_requests": 78282,
    "scheduler_time": 95.28370577669527
}
#Debug simulation 
Total elapsed time: 6.883349733892828. Arrivals time: 0.2383615905418992 Scheduler time: 6.551742864307016 Scheduler overhead time: 0.03243076242506504 Adapter cache time: 0.012842501979321241 Engine time: 0.03326410846784711 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.942857101093978,
    "estimated_duration": 3600.0904851058635,
    "input_throughput": 5369.547259985484,
    "output_throughput": 4748.700920359585,
    "total_throughput": 10118.248180345068,
    "itl": 181.46313872963785,
    "ttft": 1954458.8786059732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 537,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7519341323641182,
    "arrivals": 390109,
    "finished_requests": 78280,
    "scheduler_time": 95.28296466646262
}
#Debug simulation 
Total elapsed time: 6.942937286105007. Arrivals time: 0.24802325386554003 Scheduler time: 6.601305252872407 Scheduler overhead time: 0.03242849186062813 Adapter cache time: 0.012910486198961735 Engine time: 0.0333984587341547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.846820824313909,
    "estimated_duration": 3600.0622962714006,
    "input_throughput": 5359.465034808786,
    "output_throughput": 4738.254395671723,
    "total_throughput": 10097.719430480509,
    "itl": 180.02606454290452,
    "ttft": 1956167.2289362405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 540,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7662582663446773,
    "arrivals": 390109,
    "finished_requests": 78112,
    "scheduler_time": 95.35990898209339
}
#Debug simulation 
Total elapsed time: 6.846909714862704. Arrivals time: 0.2316691647283733 Scheduler time: 6.52139310631901 Scheduler overhead time: 0.032598202116787434 Adapter cache time: 0.012918857391923666 Engine time: 0.03335814783349633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.888735769782215,
    "estimated_duration": 3600.0986058201825,
    "input_throughput": 5369.234878386407,
    "output_throughput": 4749.089086715412,
    "total_throughput": 10118.32396510182,
    "itl": 181.44929133497843,
    "ttft": 1954649.7290904857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6964761060010571,
    "arrivals": 390109,
    "finished_requests": 78284,
    "scheduler_time": 95.28418638598603
}
#Debug simulation 
Total elapsed time: 6.8888283027336. Arrivals time: 0.23378896666690707 Scheduler time: 6.561292160302401 Scheduler overhead time: 0.03259423654526472 Adapter cache time: 0.012891715858131647 Engine time: 0.03342083981260657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.824923385400325,
    "estimated_duration": 3600.0003955388406,
    "input_throughput": 5359.639411126233,
    "output_throughput": 4738.397812716562,
    "total_throughput": 10098.037223842795,
    "itl": 180.0607361430402,
    "ttft": 1956057.9818946323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 544,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.799598991721873,
    "arrivals": 390109,
    "finished_requests": 78113,
    "scheduler_time": 95.35590481950042
}
#Debug simulation 
Total elapsed time: 6.825009311083704. Arrivals time: 0.24769515171647072 Scheduler time: 6.48350170860067 Scheduler overhead time: 0.03252025134861469 Adapter cache time: 0.012939156498759985 Engine time: 0.03346803039312363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.968658616766334,
    "estimated_duration": 3600.0029379622515,
    "input_throughput": 5369.305618106327,
    "output_throughput": 4748.958624371897,
    "total_throughput": 10118.264242478224,
    "itl": 181.4451533884435,
    "ttft": 1954633.989395315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6176172095420565,
    "arrivals": 390109,
    "finished_requests": 78282,
    "scheduler_time": 95.28353536015241
}
#Debug simulation 
Total elapsed time: 6.968776579014957. Arrivals time: 0.23838653322309256 Scheduler time: 6.636441000271589 Scheduler overhead time: 0.03261942882090807 Adapter cache time: 0.012950531672686338 Engine time: 0.03339810436591506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_160_slots_96_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.878275778144598,
    "estimated_duration": 3600.0254216102644,
    "input_throughput": 5359.602152856361,
    "output_throughput": 4738.364873093029,
    "total_throughput": 10097.96702594939,
    "itl": 180.06187489661818,
    "ttft": 1956068.5600332248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 544,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8237437187135188,
    "arrivals": 390109,
    "finished_requests": 78113,
    "scheduler_time": 95.35598313162028
}
#Debug simulation 
Total elapsed time: 6.87837615609169. Arrivals time: 0.22958038421347737 Scheduler time: 6.554832652676851 Scheduler overhead time: 0.03262518672272563 Adapter cache time: 0.012846629600971937 Engine time: 0.03357515297830105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.499866297002882,
    "estimated_duration": 3600.014091466638,
    "input_throughput": 5376.97200849398,
    "output_throughput": 4743.661154126973,
    "total_throughput": 10120.633162620954,
    "itl": 181.22920914240953,
    "ttft": 1957013.0334132363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 491,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5026994822709983,
    "arrivals": 388923,
    "finished_requests": 78116,
    "scheduler_time": 95.32282566259767
}
#Debug simulation 
Total elapsed time: 6.499957161955535. Arrivals time: 0.23678455175831914 Scheduler time: 6.1701990640722215 Scheduler overhead time: 0.03254246013239026 Adapter cache time: 0.012289424426853657 Engine time: 0.033233897760510445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.467456588987261,
    "estimated_duration": 3600.1358329074424,
    "input_throughput": 5376.588245107374,
    "output_throughput": 4743.609350486154,
    "total_throughput": 10120.197595593527,
    "itl": 181.2319576859354,
    "ttft": 1957022.326023237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 489,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5920679054712,
    "arrivals": 388923,
    "finished_requests": 78114,
    "scheduler_time": 95.32404270431931
}
#Debug simulation 
Total elapsed time: 6.467537153046578. Arrivals time: 0.2281660120934248 Scheduler time: 6.147436359431595 Scheduler overhead time: 0.03222704026848078 Adapter cache time: 0.012002602219581604 Engine time: 0.03288215585052967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.489189566113055,
    "estimated_duration": 3600.024977211379,
    "input_throughput": 5362.9269580666,
    "output_throughput": 4732.283277988961,
    "total_throughput": 10095.210236055562,
    "itl": 179.30696793888478,
    "ttft": 1957937.0246105613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6344083789549866,
    "arrivals": 388923,
    "finished_requests": 77912,
    "scheduler_time": 95.42456695330553
}
#Debug simulation 
Total elapsed time: 6.489270742051303. Arrivals time: 0.22768444754183292 Scheduler time: 6.16748803248629 Scheduler overhead time: 0.03305088961496949 Adapter cache time: 0.012433183379471302 Engine time: 0.033674261532723904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.489410468842834,
    "estimated_duration": 3600.0844944484707,
    "input_throughput": 5376.246038071158,
    "output_throughput": 4743.559498765657,
    "total_throughput": 10119.805536836815,
    "itl": 181.23331554785176,
    "ttft": 1956954.1801768402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 483,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5124297106172837,
    "arrivals": 388923,
    "finished_requests": 78114,
    "scheduler_time": 95.32451255618236
}
#Debug simulation 
Total elapsed time: 6.489497043192387. Arrivals time: 0.2491623917594552 Scheduler time: 6.148630162235349 Scheduler overhead time: 0.0321318618953228 Adapter cache time: 0.01190625037997961 Engine time: 0.03287569945678115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.855600764043629,
    "estimated_duration": 3600.0553508776165,
    "input_throughput": 5362.592548832256,
    "output_throughput": 4732.3502945161135,
    "total_throughput": 10094.94284334837,
    "itl": 179.3193894457638,
    "ttft": 1957991.9610200012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.657578215822582,
    "arrivals": 388923,
    "finished_requests": 77918,
    "scheduler_time": 95.42315791807243
}
#Debug simulation 
Total elapsed time: 6.855682848952711. Arrivals time: 0.24444134766235948 Scheduler time: 6.516822123434395 Scheduler overhead time: 0.03298998763784766 Adapter cache time: 0.012464412022382021 Engine time: 0.033747846726328135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.705065215006471,
    "estimated_duration": 3600.148806717226,
    "input_throughput": 5376.2513826891,
    "output_throughput": 4743.415596637952,
    "total_throughput": 10119.666979327052,
    "itl": 181.2276729105774,
    "ttft": 1957006.940088388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 490,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.465124644502051,
    "arrivals": 388923,
    "finished_requests": 78113,
    "scheduler_time": 95.3271510301562
}
#Debug simulation 
Total elapsed time: 6.705134527757764. Arrivals time: 0.45165573339909315 Scheduler time: 6.161167660728097 Scheduler overhead time: 0.03233043197542429 Adapter cache time: 0.012086025439202785 Engine time: 0.03315785015001893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_160_slots_96_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.463143971282989,
    "estimated_duration": 3600.066983364577,
    "input_throughput": 5362.554110578596,
    "output_throughput": 4732.251671628058,
    "total_throughput": 10094.805782206655,
    "itl": 179.28810151680224,
    "ttft": 1958059.3299464488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 495,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6565542451664792,
    "arrivals": 388923,
    "finished_requests": 77917,
    "scheduler_time": 95.42593400456735
}
#Debug simulation 
Total elapsed time: 6.463221985846758. Arrivals time: 0.2309350585564971 Scheduler time: 6.138492843136191 Scheduler overhead time: 0.0328186503611505 Adapter cache time: 0.012458746321499348 Engine time: 0.033590289764106274 
