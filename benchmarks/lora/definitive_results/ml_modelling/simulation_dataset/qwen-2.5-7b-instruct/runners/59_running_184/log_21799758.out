INFO 06-01 00:47:08 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:09 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_128_slots_64_rate_0.05-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_128_slots_64_rate_0.05-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 135, 135, 540, 135, 270, 270, 270, 135, 540, 270, 135, 540, 270, 135, 135, 135, 135, 270, 270, 540, 270, 135, 270, 270, 270, 270, 540, 270, 135, 270, 135, 540, 540, 135, 270, 270, 135, 270, 135, 270, 270, 540, 540, 540, 540, 270, 270, 135, 270, 540, 540, 270, 135, 135, 135, 540, 270, 270, 270, 270, 540, 270, 540, 135, 135, 270, 135, 540, 540, 135, 135, 270, 270, 270, 135, 540, 135, 540, 270, 135, 540, 540, 270, 270, 135, 135, 135, 540, 540, 540, 540, 540, 540, 540, 540, 135, 540, 135, 135, 540, 270, 540, 540, 270, 540, 135, 270, 540, 135, 540, 270, 135, 270, 270, 540, 540, 135, 135, 540, 135, 135, 135, 270, 135]
Prompts retrieved: 40500 . Total input tokens: 8939661 . Total output tokens: 8106807
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4954982749186456,
    "estimated_duration": 3599.5438160521503,
    "input_throughput": 922.9038927614682,
    "output_throughput": 839.2674612065986,
    "total_throughput": 1762.1713539680668,
    "itl": 22.412176431586712,
    "ttft": 5538.85795212185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 5583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.740721570810777,
    "arrivals": 13744,
    "finished_requests": 13723,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4956012959592044. Arrivals time: 0.047494746278971434 Scheduler time: 1.015344153624028 Scheduler overhead time: 0.1388271115720272 Adapter cache time: 0.0883878143504262 Engine time: 0.1370347272604704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 66, 66, 540, 66, 270, 270, 270, 66, 540, 270, 66, 540, 270, 66, 66, 66, 66, 270, 270, 540, 270, 66, 270, 270, 270, 270, 540, 270, 66, 270, 66, 540, 540, 66, 270, 270, 66, 270, 66, 270, 270, 540, 540, 540, 540, 270, 270, 66, 270, 540, 540, 270, 66, 66, 66, 540, 270, 270, 270, 270, 540, 270, 540, 66, 66, 270, 66, 540, 540, 66, 66, 270, 270, 270, 66, 540, 66, 540, 270, 66, 540, 540, 270, 270, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 270, 540, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 270, 270, 540, 540, 66, 66, 540, 66, 66, 66, 270, 66]
Prompts retrieved: 37602 . Total input tokens: 8311855 . Total output tokens: 7507822
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.4067940646782517,
    "estimated_duration": 3599.9347843560026,
    "input_throughput": 863.8289819898233,
    "output_throughput": 768.6772582728956,
    "total_throughput": 1632.506240262719,
    "itl": 21.802360068452405,
    "ttft": 5693.7467408349075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.248851443483792,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4068961278535426. Arrivals time: 0.044925079215317965 Scheduler time: 0.9317177752964199 Scheduler overhead time: 0.1411551064811647 Adapter cache time: 0.081558748614043 Engine time: 0.1379989432170987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 66, 66, 540, 66, 270, 270, 270, 66, 540, 270, 66, 540, 270, 66, 66, 66, 66, 270, 270, 540, 270, 66, 270, 270, 270, 270, 540, 270, 66, 270, 66, 540, 540, 66, 270, 270, 66, 270, 66, 270, 270, 540, 540, 540, 540, 270, 270, 66, 270, 540, 540, 270, 66, 66, 66, 540, 270, 270, 270, 270, 540, 270, 540, 66, 66, 270, 66, 540, 540, 66, 66, 270, 270, 270, 66, 540, 66, 540, 270, 66, 540, 540, 270, 270, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 270, 540, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 270, 270, 540, 540, 66, 66, 540, 66, 66, 66, 270, 66]
Prompts retrieved: 37602 . Total input tokens: 8311855 . Total output tokens: 7507822
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4438217030838132,
    "estimated_duration": 3599.9189199458506,
    "input_throughput": 863.8327887803584,
    "output_throughput": 768.6806457412167,
    "total_throughput": 1632.5134345215752,
    "itl": 21.809837685489395,
    "ttft": 5693.81587084948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.14519796284689,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4439525492489338. Arrivals time: 0.0459559834562242 Scheduler time: 0.968532778788358 Scheduler overhead time: 0.14031670661643147 Adapter cache time: 0.08093278110027313 Engine time: 0.13859892496839166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 66, 66, 540, 66, 270, 270, 270, 66, 540, 270, 66, 540, 270, 66, 66, 66, 66, 270, 270, 540, 270, 66, 270, 270, 270, 270, 540, 270, 66, 270, 66, 540, 540, 66, 270, 270, 66, 270, 66, 270, 270, 540, 540, 540, 540, 270, 270, 66, 270, 540, 540, 270, 66, 66, 66, 540, 270, 270, 270, 270, 540, 270, 540, 66, 66, 270, 66, 540, 540, 66, 66, 270, 270, 270, 66, 540, 66, 540, 270, 66, 540, 540, 270, 270, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 270, 540, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 270, 270, 540, 540, 66, 66, 540, 66, 66, 66, 270, 66]
Prompts retrieved: 37602 . Total input tokens: 8311855 . Total output tokens: 7507822
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4118765047751367,
    "estimated_duration": 3599.934099191805,
    "input_throughput": 863.8291463996917,
    "output_throughput": 768.677404572834,
    "total_throughput": 1632.5065509725257,
    "itl": 21.809260315496324,
    "ttft": 5693.765735683885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4331,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.17332501543575,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.41196499299258. Arrivals time: 0.04479382233694196 Scheduler time: 0.9393715690821409 Scheduler overhead time: 0.14001443842425942 Adapter cache time: 0.08077777083963156 Engine time: 0.13785530114546418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 66, 66, 540, 66, 270, 270, 270, 66, 540, 270, 66, 540, 270, 66, 66, 66, 66, 270, 270, 540, 270, 66, 270, 270, 270, 270, 540, 270, 66, 270, 66, 540, 540, 66, 270, 270, 66, 270, 66, 270, 270, 540, 540, 540, 540, 270, 270, 66, 270, 540, 540, 270, 66, 66, 66, 540, 270, 270, 270, 270, 540, 270, 540, 66, 66, 270, 66, 540, 540, 66, 66, 270, 270, 270, 66, 540, 66, 540, 270, 66, 540, 540, 270, 270, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 270, 540, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 270, 270, 540, 540, 66, 66, 540, 66, 66, 66, 270, 66]
Prompts retrieved: 37602 . Total input tokens: 8311855 . Total output tokens: 7507822
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 1.4157699989154935,
    "estimated_duration": 3599.9213677388893,
    "input_throughput": 863.8322014108936,
    "output_throughput": 768.6801230711523,
    "total_throughput": 1632.5123244820459,
    "itl": 21.805903248964878,
    "ttft": 5693.75629257597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4333,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.596156791808054,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4158602156676352. Arrivals time: 0.04510404588654637 Scheduler time: 0.9412497445009649 Scheduler overhead time: 0.13992543192580342 Adapter cache time: 0.08112537395209074 Engine time: 0.13926823996007442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 66, 66, 540, 66, 270, 270, 270, 66, 540, 270, 66, 540, 270, 66, 66, 66, 66, 270, 270, 540, 270, 66, 270, 270, 270, 270, 540, 270, 66, 270, 66, 540, 540, 66, 270, 270, 66, 270, 66, 270, 270, 540, 540, 540, 540, 270, 270, 66, 270, 540, 540, 270, 66, 66, 66, 540, 270, 270, 270, 270, 540, 270, 540, 66, 66, 270, 66, 540, 540, 66, 66, 270, 270, 270, 66, 540, 66, 540, 270, 66, 540, 540, 270, 270, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 270, 540, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 270, 270, 540, 540, 66, 66, 540, 66, 66, 66, 270, 66]
Prompts retrieved: 37602 . Total input tokens: 8311855 . Total output tokens: 7507822
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4210155648179352,
    "estimated_duration": 3599.923737470217,
    "input_throughput": 863.8316327737839,
    "output_throughput": 768.6796170700529,
    "total_throughput": 1632.5112498438368,
    "itl": 21.81179885339455,
    "ttft": 5693.820458574194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.336642913929063,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.421177248004824. Arrivals time: 0.04547133203595877 Scheduler time: 0.9433754836209118 Scheduler overhead time: 0.14013768173754215 Adapter cache time: 0.08175586117431521 Engine time: 0.14093478117138147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 66, 66, 540, 66, 270, 270, 270, 66, 540, 270, 66, 540, 270, 66, 66, 66, 66, 270, 270, 540, 270, 66, 270, 270, 270, 270, 540, 270, 66, 270, 66, 540, 540, 66, 270, 270, 66, 270, 66, 270, 270, 540, 540, 540, 540, 270, 270, 66, 270, 540, 540, 270, 66, 66, 66, 540, 270, 270, 270, 270, 540, 270, 540, 66, 66, 270, 66, 540, 540, 66, 66, 270, 270, 270, 66, 540, 66, 540, 270, 66, 540, 540, 270, 270, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 270, 540, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 270, 270, 540, 540, 66, 66, 540, 66, 66, 66, 270, 66]
Prompts retrieved: 37602 . Total input tokens: 8311855 . Total output tokens: 7507822
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.45333337970078,
    "estimated_duration": 3599.917300475623,
    "input_throughput": 863.8331773869197,
    "output_throughput": 768.6809915423328,
    "total_throughput": 1632.5141689292525,
    "itl": 21.79928639456742,
    "ttft": 5693.616004677646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4331,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.949907827222368,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.453425734769553. Arrivals time: 0.046111723873764277 Scheduler time: 0.9733536783605814 Scheduler overhead time: 0.139770052395761 Adapter cache time: 0.08327609160915017 Engine time: 0.14166572457179427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_128_slots_64_rate_0.05-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 66, 66, 540, 66, 270, 270, 270, 66, 540, 270, 66, 540, 270, 66, 66, 66, 66, 270, 270, 540, 270, 66, 270, 270, 270, 270, 540, 270, 66, 270, 66, 540, 540, 66, 270, 270, 66, 270, 66, 270, 270, 540, 540, 540, 540, 270, 270, 66, 270, 540, 540, 270, 66, 66, 66, 540, 270, 270, 270, 270, 540, 270, 540, 66, 66, 270, 66, 540, 540, 66, 66, 270, 270, 270, 66, 540, 66, 540, 270, 66, 540, 540, 270, 270, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 270, 540, 540, 270, 540, 66, 270, 540, 66, 540, 270, 66, 270, 270, 540, 540, 66, 66, 540, 66, 66, 66, 270, 66]
Prompts retrieved: 37602 . Total input tokens: 8311855 . Total output tokens: 7507822
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4512804988771677,
    "estimated_duration": 3599.923942895911,
    "input_throughput": 863.8315834801834,
    "output_throughput": 768.6795732061973,
    "total_throughput": 1632.5111566863807,
    "itl": 21.812798423248147,
    "ttft": 5693.790389054844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4331,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.542703900000971,
    "arrivals": 12728,
    "finished_requests": 12708,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.451371646951884. Arrivals time: 0.04578927345573902 Scheduler time: 0.9718972449190915 Scheduler overhead time: 0.14016013592481613 Adapter cache time: 0.08315907046198845 Engine time: 0.14081374928355217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 33, 33, 540, 33, 270, 270, 270, 33, 540, 270, 33, 540, 270, 33, 33, 33, 33, 270, 270, 540, 270, 33, 270, 270, 270, 270, 540, 270, 33, 270, 33, 540, 540, 33, 270, 270, 33, 270, 33, 270, 270, 540, 540, 540, 540, 270, 270, 33, 270, 540, 540, 270, 33, 33, 33, 540, 270, 270, 270, 270, 540, 270, 540, 33, 33, 270, 33, 540, 540, 33, 33, 270, 270, 270, 33, 540, 33, 540, 270, 33, 540, 540, 270, 270, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 270, 540, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 270, 270, 540, 540, 33, 33, 540, 33, 33, 33, 270, 33]
Prompts retrieved: 36216 . Total input tokens: 7995821 . Total output tokens: 7231656
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.406946374103427,
    "estimated_duration": 3599.9366599680716,
    "input_throughput": 839.4994927568542,
    "output_throughput": 733.145677075165,
    "total_throughput": 1572.6451698320193,
    "itl": 21.42579644220525,
    "ttft": 6491.541958642959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.760674907667099,
    "arrivals": 12269,
    "finished_requests": 12247,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4071168331429362. Arrivals time: 0.04556093644350767 Scheduler time: 0.9292083457112312 Scheduler overhead time: 0.14070825325325131 Adapter cache time: 0.08036528248339891 Engine time: 0.14126758929342031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 33, 33, 540, 33, 270, 270, 270, 33, 540, 270, 33, 540, 270, 33, 33, 33, 33, 270, 270, 540, 270, 33, 270, 270, 270, 270, 540, 270, 33, 270, 33, 540, 540, 33, 270, 270, 33, 270, 33, 270, 270, 540, 540, 540, 540, 270, 270, 33, 270, 540, 540, 270, 33, 33, 33, 540, 270, 270, 270, 270, 540, 270, 540, 33, 33, 270, 33, 540, 540, 33, 33, 270, 270, 270, 33, 540, 33, 540, 270, 33, 540, 540, 270, 270, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 270, 540, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 270, 270, 540, 540, 33, 33, 540, 33, 33, 33, 270, 33]
Prompts retrieved: 36216 . Total input tokens: 7995821 . Total output tokens: 7231656
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4053302579559386,
    "estimated_duration": 3599.943496127704,
    "input_throughput": 839.4978985783483,
    "output_throughput": 733.144284858624,
    "total_throughput": 1572.6421834369723,
    "itl": 21.432232291794726,
    "ttft": 6491.636461275719,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.46901894508189,
    "arrivals": 12269,
    "finished_requests": 12247,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4054208397865295. Arrivals time: 0.045018861535936594 Scheduler time: 0.9256914108991623 Scheduler overhead time: 0.1411222293972969 Adapter cache time: 0.08056054171174765 Engine time: 0.14313744148239493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 33, 33, 540, 33, 270, 270, 270, 33, 540, 270, 33, 540, 270, 33, 33, 33, 33, 270, 270, 540, 270, 33, 270, 270, 270, 270, 540, 270, 33, 270, 33, 540, 540, 33, 270, 270, 33, 270, 33, 270, 270, 540, 540, 540, 540, 270, 270, 33, 270, 540, 540, 270, 33, 33, 33, 540, 270, 270, 270, 270, 540, 270, 540, 33, 33, 270, 33, 540, 540, 33, 33, 270, 270, 270, 33, 540, 33, 540, 270, 33, 540, 540, 270, 270, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 270, 540, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 270, 270, 540, 540, 33, 33, 540, 33, 33, 33, 270, 33]
Prompts retrieved: 36216 . Total input tokens: 7995821 . Total output tokens: 7231656
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4201696426607668,
    "estimated_duration": 3599.9322615524256,
    "input_throughput": 839.5005184616274,
    "output_throughput": 733.1465728363023,
    "total_throughput": 1572.6470912979296,
    "itl": 21.432236624465734,
    "ttft": 6491.6348094003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.500963329299635,
    "arrivals": 12269,
    "finished_requests": 12247,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.420255655888468. Arrivals time: 0.044596103485673666 Scheduler time: 0.9403739511035383 Scheduler overhead time: 0.14210296142846346 Adapter cache time: 0.08050711220130324 Engine time: 0.14245783211663365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 33, 33, 540, 33, 270, 270, 270, 33, 540, 270, 33, 540, 270, 33, 33, 33, 33, 270, 270, 540, 270, 33, 270, 270, 270, 270, 540, 270, 33, 270, 33, 540, 540, 33, 270, 270, 33, 270, 33, 270, 270, 540, 540, 540, 540, 270, 270, 33, 270, 540, 540, 270, 33, 33, 33, 540, 270, 270, 270, 270, 540, 270, 540, 33, 33, 270, 33, 540, 540, 33, 33, 270, 270, 270, 33, 540, 33, 540, 270, 33, 540, 540, 270, 270, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 270, 540, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 270, 270, 540, 540, 33, 33, 540, 33, 33, 33, 270, 33]
Prompts retrieved: 36216 . Total input tokens: 7995821 . Total output tokens: 7231656
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 1.411874364130199,
    "estimated_duration": 3599.944929507685,
    "input_throughput": 839.4975643178234,
    "output_throughput": 733.1439929446192,
    "total_throughput": 1572.6415572624426,
    "itl": 21.4296104423226,
    "ttft": 6491.753578533513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.030855934527109,
    "arrivals": 12269,
    "finished_requests": 12247,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4120092610828578. Arrivals time: 0.04449279000982642 Scheduler time: 0.9342876202426851 Scheduler overhead time: 0.14086945075541735 Adapter cache time: 0.07999331783503294 Engine time: 0.14235564274713397 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 33, 33, 540, 33, 270, 270, 270, 33, 540, 270, 33, 540, 270, 33, 33, 33, 33, 270, 270, 540, 270, 33, 270, 270, 270, 270, 540, 270, 33, 270, 33, 540, 540, 33, 270, 270, 33, 270, 33, 270, 270, 540, 540, 540, 540, 270, 270, 33, 270, 540, 540, 270, 33, 33, 33, 540, 270, 270, 270, 270, 540, 270, 540, 33, 33, 270, 33, 540, 540, 33, 33, 270, 270, 270, 33, 540, 33, 540, 270, 33, 540, 540, 270, 270, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 270, 540, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 270, 270, 540, 540, 33, 33, 540, 33, 33, 33, 270, 33]
Prompts retrieved: 36216 . Total input tokens: 7995821 . Total output tokens: 7231656
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4231289369054139,
    "estimated_duration": 3599.9315897559013,
    "input_throughput": 839.5006751239184,
    "output_throughput": 733.1467096514909,
    "total_throughput": 1572.6473847754094,
    "itl": 21.43417160658369,
    "ttft": 6491.564638733938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3515,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.635117191429494,
    "arrivals": 12269,
    "finished_requests": 12247,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4232284501194954. Arrivals time: 0.045598171185702085 Scheduler time: 0.9446359816938639 Scheduler overhead time: 0.1406619600020349 Adapter cache time: 0.08087815484032035 Engine time: 0.141311377286911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 33, 33, 540, 33, 270, 270, 270, 33, 540, 270, 33, 540, 270, 33, 33, 33, 33, 270, 270, 540, 270, 33, 270, 270, 270, 270, 540, 270, 33, 270, 33, 540, 540, 33, 270, 270, 33, 270, 33, 270, 270, 540, 540, 540, 540, 270, 270, 33, 270, 540, 540, 270, 33, 33, 33, 540, 270, 270, 270, 270, 540, 270, 540, 33, 33, 270, 33, 540, 540, 33, 33, 270, 270, 270, 33, 540, 33, 540, 270, 33, 540, 540, 270, 270, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 270, 540, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 270, 270, 540, 540, 33, 33, 540, 33, 33, 33, 270, 33]
Prompts retrieved: 36216 . Total input tokens: 7995821 . Total output tokens: 7231656
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.4008428696542978,
    "estimated_duration": 3599.952570090234,
    "input_throughput": 839.4957825581155,
    "output_throughput": 733.1424369110078,
    "total_throughput": 1572.6382194691232,
    "itl": 21.424577641094753,
    "ttft": 6491.4573515719585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3515,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.510026786581555,
    "arrivals": 12269,
    "finished_requests": 12247,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4009323567152023. Arrivals time: 0.04484582552686334 Scheduler time: 0.9238846506923437 Scheduler overhead time: 0.14141467213630676 Adapter cache time: 0.07942453399300575 Engine time: 0.14120584912598133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.05-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 270, 540, 540, 33, 33, 540, 33, 270, 270, 270, 33, 540, 270, 33, 540, 270, 33, 33, 33, 33, 270, 270, 540, 270, 33, 270, 270, 270, 270, 540, 270, 33, 270, 33, 540, 540, 33, 270, 270, 33, 270, 33, 270, 270, 540, 540, 540, 540, 270, 270, 33, 270, 540, 540, 270, 33, 33, 33, 540, 270, 270, 270, 270, 540, 270, 540, 33, 33, 270, 33, 540, 540, 33, 33, 270, 270, 270, 33, 540, 33, 540, 270, 33, 540, 540, 270, 270, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 270, 540, 540, 270, 540, 33, 270, 540, 33, 540, 270, 33, 270, 270, 540, 540, 33, 33, 540, 33, 33, 33, 270, 33]
Prompts retrieved: 36216 . Total input tokens: 7995821 . Total output tokens: 7231656
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.4172912668436766,
    "estimated_duration": 3599.9384970538267,
    "input_throughput": 839.499064351602,
    "output_throughput": 733.1453029433623,
    "total_throughput": 1572.6443672949642,
    "itl": 21.433242578138806,
    "ttft": 6491.5739432152295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.792701271697174,
    "arrivals": 12269,
    "finished_requests": 12247,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4174128421582282. Arrivals time: 0.044824741780757904 Scheduler time: 0.9364446355029941 Scheduler overhead time: 0.14201974123716354 Adapter cache time: 0.08068532403558493 Engine time: 0.14294652128592134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 66, 66, 540, 66, 135, 135, 135, 66, 540, 135, 66, 540, 135, 66, 66, 66, 66, 135, 135, 540, 135, 66, 135, 135, 135, 135, 540, 135, 66, 135, 66, 540, 540, 66, 135, 135, 66, 135, 66, 135, 135, 540, 540, 540, 540, 135, 135, 66, 135, 540, 540, 135, 66, 66, 66, 540, 135, 135, 135, 135, 540, 135, 540, 66, 66, 135, 66, 540, 540, 66, 66, 135, 135, 135, 66, 540, 66, 540, 135, 66, 540, 540, 135, 135, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 135, 540, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 135, 135, 540, 540, 66, 66, 540, 66, 66, 66, 135, 66]
Prompts retrieved: 31797 . Total input tokens: 7021629 . Total output tokens: 6368784
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.3002114281989634,
    "estimated_duration": 3599.9946916804306,
    "input_throughput": 728.1719070469956,
    "output_throughput": 658.6529156500449,
    "total_throughput": 1386.8248226970404,
    "itl": 21.07776037887735,
    "ttft": 5388.310691816681,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3017,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.233491523445045,
    "arrivals": 10761,
    "finished_requests": 10745,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3003011322580278. Arrivals time: 0.04077125806361437 Scheduler time: 0.8330623535439372 Scheduler overhead time: 0.1414597723633051 Adapter cache time: 0.07149957679212093 Engine time: 0.14314566180109978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 66, 66, 540, 66, 135, 135, 135, 66, 540, 135, 66, 540, 135, 66, 66, 66, 66, 135, 135, 540, 135, 66, 135, 135, 135, 135, 540, 135, 66, 135, 66, 540, 540, 66, 135, 135, 66, 135, 66, 135, 135, 540, 540, 540, 540, 135, 135, 66, 135, 540, 540, 135, 66, 66, 66, 540, 135, 135, 135, 135, 540, 135, 540, 66, 66, 135, 66, 540, 540, 66, 66, 135, 135, 135, 66, 540, 66, 540, 135, 66, 540, 540, 135, 135, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 135, 540, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 135, 135, 540, 540, 66, 66, 540, 66, 66, 66, 135, 66]
Prompts retrieved: 31797 . Total input tokens: 7021629 . Total output tokens: 6368784
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.304609898943454,
    "estimated_duration": 3600.0065027362116,
    "input_throughput": 728.1695180293631,
    "output_throughput": 658.6507547133018,
    "total_throughput": 1386.8202727426649,
    "itl": 20.922096151664814,
    "ttft": 5388.094383228719,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3035,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.91078866655704,
    "arrivals": 10761,
    "finished_requests": 10745,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3046971042640507. Arrivals time: 0.040616219863295555 Scheduler time: 0.8318326584994793 Scheduler overhead time: 0.1435378873720765 Adapter cache time: 0.07169132865965366 Engine time: 0.14492979599162936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 66, 66, 540, 66, 135, 135, 135, 66, 540, 135, 66, 540, 135, 66, 66, 66, 66, 135, 135, 540, 135, 66, 135, 135, 135, 135, 540, 135, 66, 135, 66, 540, 540, 66, 135, 135, 66, 135, 66, 135, 135, 540, 540, 540, 540, 135, 135, 66, 135, 540, 540, 135, 66, 66, 66, 540, 135, 135, 135, 135, 540, 135, 540, 66, 66, 135, 66, 540, 540, 66, 66, 135, 135, 135, 66, 540, 66, 540, 135, 66, 540, 540, 135, 135, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 135, 540, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 135, 135, 540, 540, 66, 66, 540, 66, 66, 66, 135, 66]
Prompts retrieved: 31797 . Total input tokens: 7021629 . Total output tokens: 6368784
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.3037965609692037,
    "estimated_duration": 3600.0016522933574,
    "input_throughput": 728.1704991246448,
    "output_throughput": 658.6516421428519,
    "total_throughput": 1386.8221412674966,
    "itl": 20.92241088032354,
    "ttft": 5388.154039857093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3036,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.930131598673404,
    "arrivals": 10761,
    "finished_requests": 10745,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.303926601074636. Arrivals time: 0.041040547657758 Scheduler time: 0.8317082393914461 Scheduler overhead time: 0.1441183048300445 Adapter cache time: 0.07171899126842618 Engine time: 0.14394700108096004 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 66, 66, 540, 66, 135, 135, 135, 66, 540, 135, 66, 540, 135, 66, 66, 66, 66, 135, 135, 540, 135, 66, 135, 135, 135, 135, 540, 135, 66, 135, 66, 540, 540, 66, 135, 135, 66, 135, 66, 135, 135, 540, 540, 540, 540, 135, 135, 66, 135, 540, 540, 135, 66, 66, 66, 540, 135, 135, 135, 135, 540, 135, 540, 66, 66, 135, 66, 540, 540, 66, 66, 135, 135, 135, 66, 540, 66, 540, 135, 66, 540, 540, 135, 135, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 135, 540, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 135, 135, 540, 540, 66, 66, 540, 66, 66, 66, 135, 66]
Prompts retrieved: 31797 . Total input tokens: 7021629 . Total output tokens: 6368784
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 1.2935310420580208,
    "estimated_duration": 3599.998316759955,
    "input_throughput": 728.171173801911,
    "output_throughput": 658.6522524082909,
    "total_throughput": 1386.823426210202,
    "itl": 21.07293873072178,
    "ttft": 5388.394719662609,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3013,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.431917693126028,
    "arrivals": 10761,
    "finished_requests": 10745,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2936113253235817. Arrivals time: 0.0408955872990191 Scheduler time: 0.8241889788769186 Scheduler overhead time: 0.14252371760085225 Adapter cache time: 0.07118255645036697 Engine time: 0.14385258313268423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 66, 66, 540, 66, 135, 135, 135, 66, 540, 135, 66, 540, 135, 66, 66, 66, 66, 135, 135, 540, 135, 66, 135, 135, 135, 135, 540, 135, 66, 135, 66, 540, 540, 66, 135, 135, 66, 135, 66, 135, 135, 540, 540, 540, 540, 135, 135, 66, 135, 540, 540, 135, 66, 66, 66, 540, 135, 135, 135, 135, 540, 135, 540, 66, 66, 135, 66, 540, 540, 66, 66, 135, 135, 135, 66, 540, 66, 540, 135, 66, 540, 540, 135, 135, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 135, 540, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 135, 135, 540, 540, 66, 66, 540, 66, 66, 66, 135, 66]
Prompts retrieved: 31797 . Total input tokens: 7021629 . Total output tokens: 6368784
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 1.288600703701377,
    "estimated_duration": 3599.995836565053,
    "input_throughput": 728.1716754709447,
    "output_throughput": 658.652706182693,
    "total_throughput": 1386.8243816536376,
    "itl": 20.922447642423794,
    "ttft": 5387.990113014559,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3036,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.05387332450553,
    "arrivals": 10761,
    "finished_requests": 10745,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2886790470220149. Arrivals time: 0.0413613636046648 Scheduler time: 0.819816071074456 Scheduler overhead time: 0.14267278788611293 Adapter cache time: 0.07180362194776535 Engine time: 0.14227611990645528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 66, 66, 540, 66, 135, 135, 135, 66, 540, 135, 66, 540, 135, 66, 66, 66, 66, 135, 135, 540, 135, 66, 135, 135, 135, 135, 540, 135, 66, 135, 66, 540, 540, 66, 135, 135, 66, 135, 66, 135, 135, 540, 540, 540, 540, 135, 135, 66, 135, 540, 540, 135, 66, 66, 66, 540, 135, 135, 135, 135, 540, 135, 540, 66, 66, 135, 66, 540, 540, 66, 66, 135, 135, 135, 66, 540, 66, 540, 135, 66, 540, 540, 135, 135, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 135, 540, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 135, 135, 540, 540, 66, 66, 540, 66, 66, 66, 135, 66]
Prompts retrieved: 31797 . Total input tokens: 7021629 . Total output tokens: 6368784
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.2974808160215616,
    "estimated_duration": 3600.013531814651,
    "input_throughput": 728.1680962678574,
    "output_throughput": 658.6494686881861,
    "total_throughput": 1386.8175649560435,
    "itl": 21.076367405188247,
    "ttft": 5388.311980612152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3017,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.020981739719883,
    "arrivals": 10761,
    "finished_requests": 10745,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.29759885603562. Arrivals time: 0.04037846531718969 Scheduler time: 0.8299508048221469 Scheduler overhead time: 0.14175031101331115 Adapter cache time: 0.0710681276395917 Engine time: 0.14303626911714673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_128_slots_64_rate_0.05-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 66, 66, 540, 66, 135, 135, 135, 66, 540, 135, 66, 540, 135, 66, 66, 66, 66, 135, 135, 540, 135, 66, 135, 135, 135, 135, 540, 135, 66, 135, 66, 540, 540, 66, 135, 135, 66, 135, 66, 135, 135, 540, 540, 540, 540, 135, 135, 66, 135, 540, 540, 135, 66, 66, 66, 540, 135, 135, 135, 135, 540, 135, 540, 66, 66, 135, 66, 540, 540, 66, 66, 135, 135, 135, 66, 540, 66, 540, 135, 66, 540, 540, 135, 135, 66, 66, 66, 540, 540, 540, 540, 540, 540, 540, 540, 66, 540, 66, 66, 540, 135, 540, 540, 135, 540, 66, 135, 540, 66, 540, 135, 66, 135, 135, 540, 540, 66, 66, 540, 66, 66, 66, 135, 66]
Prompts retrieved: 31797 . Total input tokens: 7021629 . Total output tokens: 6368784
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.3023187620565295,
    "estimated_duration": 3599.9976049988995,
    "input_throughput": 728.171317769752,
    "output_throughput": 658.6523826314392,
    "total_throughput": 1386.8237004011912,
    "itl": 20.92454520461924,
    "ttft": 5388.112974758514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.190467364526738,
    "arrivals": 10761,
    "finished_requests": 10745,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.30240587471053. Arrivals time: 0.040886415634304285 Scheduler time: 0.8280530441552401 Scheduler overhead time: 0.1437775525264442 Adapter cache time: 0.0716747916303575 Engine time: 0.1458928263746202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 33, 33, 540, 33, 135, 135, 135, 33, 540, 135, 33, 540, 135, 33, 33, 33, 33, 135, 135, 540, 135, 33, 135, 135, 135, 135, 540, 135, 33, 135, 33, 540, 540, 33, 135, 135, 33, 135, 33, 135, 135, 540, 540, 540, 540, 135, 135, 33, 135, 540, 540, 135, 33, 33, 33, 540, 135, 135, 135, 135, 540, 135, 540, 33, 33, 135, 33, 540, 540, 33, 33, 135, 135, 135, 33, 540, 33, 540, 135, 33, 540, 540, 135, 135, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 135, 540, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 135, 135, 540, 540, 33, 33, 540, 33, 33, 33, 135, 33]
Prompts retrieved: 30411 . Total input tokens: 6723561 . Total output tokens: 6097076
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.2636677236296237,
    "estimated_duration": 3600.02099860392,
    "input_throughput": 705.6069953439394,
    "output_throughput": 630.8021538987274,
    "total_throughput": 1336.4091492426667,
    "itl": 20.646026948195907,
    "ttft": 4594.841624595555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.766738401835518,
    "arrivals": 10263,
    "finished_requests": 10250,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.263778980821371. Arrivals time: 0.039634350687265396 Scheduler time: 0.7958476566709578 Scheduler overhead time: 0.144166040699929 Adapter cache time: 0.06821028469130397 Engine time: 0.14358560740947723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 33, 33, 540, 33, 135, 135, 135, 33, 540, 135, 33, 540, 135, 33, 33, 33, 33, 135, 135, 540, 135, 33, 135, 135, 135, 135, 540, 135, 33, 135, 33, 540, 540, 33, 135, 135, 33, 135, 33, 135, 135, 540, 540, 540, 540, 135, 135, 33, 135, 540, 540, 135, 33, 33, 33, 540, 135, 135, 135, 135, 540, 135, 540, 33, 33, 135, 33, 540, 540, 33, 33, 135, 135, 135, 33, 540, 33, 540, 135, 33, 540, 540, 135, 135, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 135, 540, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 135, 135, 540, 540, 33, 33, 540, 33, 33, 33, 135, 33]
Prompts retrieved: 30411 . Total input tokens: 6723561 . Total output tokens: 6097076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.2753445389680564,
    "estimated_duration": 3600.016734161187,
    "input_throughput": 705.6078311791163,
    "output_throughput": 630.8029011229377,
    "total_throughput": 1336.410732302054,
    "itl": 20.648423846686775,
    "ttft": 4595.035008874652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.215313678001699,
    "arrivals": 10263,
    "finished_requests": 10250,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2754802349954844. Arrivals time: 0.039982274640351534 Scheduler time: 0.8025110592134297 Scheduler overhead time: 0.14731338806450367 Adapter cache time: 0.06834509503096342 Engine time: 0.14454573020339012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 33, 33, 540, 33, 135, 135, 135, 33, 540, 135, 33, 540, 135, 33, 33, 33, 33, 135, 135, 540, 135, 33, 135, 135, 135, 135, 540, 135, 33, 135, 33, 540, 540, 33, 135, 135, 33, 135, 33, 135, 135, 540, 540, 540, 540, 135, 135, 33, 135, 540, 540, 135, 33, 33, 33, 540, 135, 135, 135, 135, 540, 135, 540, 33, 33, 135, 33, 540, 540, 33, 33, 135, 135, 135, 33, 540, 33, 540, 135, 33, 540, 540, 135, 135, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 135, 540, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 135, 135, 540, 540, 33, 33, 540, 33, 33, 33, 135, 33]
Prompts retrieved: 30411 . Total input tokens: 6723561 . Total output tokens: 6097076
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.2569888401776552,
    "estimated_duration": 3600.0027249542995,
    "input_throughput": 705.6105770120623,
    "output_throughput": 630.8053558567315,
    "total_throughput": 1336.4159328687938,
    "itl": 20.64900327216303,
    "ttft": 4244.544516457298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.22800921903919,
    "arrivals": 10263,
    "finished_requests": 10250,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.257052366156131. Arrivals time: 0.039112390484660864 Scheduler time: 0.7918019122444093 Scheduler overhead time: 0.14354795962572098 Adapter cache time: 0.06750003108754754 Engine time: 0.14370186114683747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 33, 33, 540, 33, 135, 135, 135, 33, 540, 135, 33, 540, 135, 33, 33, 33, 33, 135, 135, 540, 135, 33, 135, 135, 135, 135, 540, 135, 33, 135, 33, 540, 540, 33, 135, 135, 33, 135, 33, 135, 135, 540, 540, 540, 540, 135, 135, 33, 135, 540, 540, 135, 33, 33, 33, 540, 135, 135, 135, 135, 540, 135, 540, 33, 33, 135, 33, 540, 540, 33, 33, 135, 135, 135, 33, 540, 33, 540, 135, 33, 540, 540, 135, 135, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 135, 540, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 135, 135, 540, 540, 33, 33, 540, 33, 33, 33, 135, 33]
Prompts retrieved: 30411 . Total input tokens: 6723561 . Total output tokens: 6097076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 1.2542852847836912,
    "estimated_duration": 3600.009925784775,
    "input_throughput": 705.6091656320241,
    "output_throughput": 630.8040941039797,
    "total_throughput": 1336.4132597360037,
    "itl": 20.647141923584485,
    "ttft": 4244.524626734241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.931339931685705,
    "arrivals": 10263,
    "finished_requests": 10250,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2543591219000518. Arrivals time: 0.03984408359974623 Scheduler time: 0.7868549842387438 Scheduler overhead time: 0.1439888677559793 Adapter cache time: 0.068004768807441 Engine time: 0.144224114716053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 33, 33, 540, 33, 135, 135, 135, 33, 540, 135, 33, 540, 135, 33, 33, 33, 33, 135, 135, 540, 135, 33, 135, 135, 135, 135, 540, 135, 33, 135, 33, 540, 540, 33, 135, 135, 33, 135, 33, 135, 135, 540, 540, 540, 540, 135, 135, 33, 135, 540, 540, 135, 33, 33, 33, 540, 135, 135, 135, 135, 540, 135, 540, 33, 33, 135, 33, 540, 540, 33, 33, 135, 135, 135, 33, 540, 33, 540, 135, 33, 540, 540, 135, 135, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 135, 540, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 135, 135, 540, 540, 33, 33, 540, 33, 33, 33, 135, 33]
Prompts retrieved: 30411 . Total input tokens: 6723561 . Total output tokens: 6097076
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 1.2643450498580933,
    "estimated_duration": 3600.01484318446,
    "input_throughput": 705.60820181314,
    "output_throughput": 630.8032324642396,
    "total_throughput": 1336.4114342773796,
    "itl": 20.649517673788527,
    "ttft": 4594.862053666498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.315408100597414,
    "arrivals": 10263,
    "finished_requests": 10250,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2644730750471354. Arrivals time: 0.039601429365575314 Scheduler time: 0.7936530029401183 Scheduler overhead time: 0.14464080845937133 Adapter cache time: 0.06801624037325382 Engine time: 0.1459339177235961 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 33, 33, 540, 33, 135, 135, 135, 33, 540, 135, 33, 540, 135, 33, 33, 33, 33, 135, 135, 540, 135, 33, 135, 135, 135, 135, 540, 135, 33, 135, 33, 540, 540, 33, 135, 135, 33, 135, 33, 135, 135, 540, 540, 540, 540, 135, 135, 33, 135, 540, 540, 135, 33, 33, 33, 540, 135, 135, 135, 135, 540, 135, 540, 33, 33, 135, 33, 540, 540, 33, 33, 135, 135, 135, 33, 540, 33, 540, 135, 33, 540, 540, 135, 135, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 135, 540, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 135, 135, 540, 540, 33, 33, 540, 33, 33, 33, 135, 33]
Prompts retrieved: 30411 . Total input tokens: 6723561 . Total output tokens: 6097076
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.273706772364676,
    "estimated_duration": 3600.014558000043,
    "input_throughput": 705.6082577097094,
    "output_throughput": 630.8032824349409,
    "total_throughput": 1336.4115401446502,
    "itl": 20.645045281272427,
    "ttft": 4594.836067651946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.611001202028512,
    "arrivals": 10263,
    "finished_requests": 10250,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2737951623275876. Arrivals time: 0.03976424038410187 Scheduler time: 0.8020571302622557 Scheduler overhead time: 0.14469479862600565 Adapter cache time: 0.06860509933903813 Engine time: 0.146828789729625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.05-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 135, 540, 540, 33, 33, 540, 33, 135, 135, 135, 33, 540, 135, 33, 540, 135, 33, 33, 33, 33, 135, 135, 540, 135, 33, 135, 135, 135, 135, 540, 135, 33, 135, 33, 540, 540, 33, 135, 135, 33, 135, 33, 135, 135, 540, 540, 540, 540, 135, 135, 33, 135, 540, 540, 135, 33, 33, 33, 540, 135, 135, 135, 135, 540, 135, 540, 33, 33, 135, 33, 540, 540, 33, 33, 135, 135, 135, 33, 540, 33, 540, 135, 33, 540, 540, 135, 135, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 135, 540, 540, 135, 540, 33, 135, 540, 33, 540, 135, 33, 135, 135, 540, 540, 33, 33, 540, 33, 33, 33, 135, 33]
Prompts retrieved: 30411 . Total input tokens: 6723561 . Total output tokens: 6097076
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.2559397537261248,
    "estimated_duration": 3600.0039305879927,
    "input_throughput": 705.6103407045742,
    "output_throughput": 630.8051446013536,
    "total_throughput": 1336.4154853059279,
    "itl": 20.650412055598306,
    "ttft": 4244.507265596285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.413999069146537,
    "arrivals": 10263,
    "finished_requests": 10250,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2560292310081422. Arrivals time: 0.03937418386340141 Scheduler time: 0.7875052439048886 Scheduler overhead time: 0.1448503565043211 Adapter cache time: 0.06839317874982953 Engine time: 0.1437019184231758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 66, 540, 540, 33, 33, 540, 33, 66, 66, 66, 33, 540, 66, 33, 540, 66, 33, 33, 33, 33, 66, 66, 540, 66, 33, 66, 66, 66, 66, 540, 66, 33, 66, 33, 540, 540, 33, 66, 66, 33, 66, 33, 66, 66, 540, 540, 540, 540, 66, 66, 33, 66, 540, 540, 66, 33, 33, 33, 540, 66, 66, 66, 66, 540, 66, 540, 33, 33, 66, 33, 540, 540, 33, 33, 66, 66, 66, 33, 540, 33, 540, 66, 33, 540, 540, 66, 66, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 66, 540, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 66, 66, 540, 540, 33, 33, 540, 33, 33, 33, 66, 33]
Prompts retrieved: 27444 . Total input tokens: 6052346 . Total output tokens: 5499823
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.185600059106946,
    "estimated_duration": 3599.9347298658536,
    "input_throughput": 644.1133448234506,
    "output_throughput": 556.9798205965576,
    "total_throughput": 1201.093165420008,
    "itl": 20.103587954458796,
    "ttft": 3127.0491562919406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.33977162089676,
    "arrivals": 9311,
    "finished_requests": 9303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.185740646906197. Arrivals time: 0.03726354939863086 Scheduler time: 0.7195607083849609 Scheduler overhead time: 0.14675082685425878 Adapter cache time: 0.06275219144299626 Engine time: 0.14666183525696397 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 66, 540, 540, 33, 33, 540, 33, 66, 66, 66, 33, 540, 66, 33, 540, 66, 33, 33, 33, 33, 66, 66, 540, 66, 33, 66, 66, 66, 66, 540, 66, 33, 66, 33, 540, 540, 33, 66, 66, 33, 66, 33, 66, 66, 540, 540, 540, 540, 66, 66, 33, 66, 540, 540, 66, 33, 33, 33, 540, 66, 66, 66, 66, 540, 66, 540, 33, 33, 66, 33, 540, 540, 33, 33, 66, 66, 66, 33, 540, 33, 540, 66, 33, 540, 540, 66, 66, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 66, 540, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 66, 66, 540, 540, 33, 33, 540, 33, 33, 33, 66, 33]
Prompts retrieved: 27444 . Total input tokens: 6052346 . Total output tokens: 5499823
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.2248402386903763,
    "estimated_duration": 3599.939424939469,
    "input_throughput": 644.1125047649903,
    "output_throughput": 556.979094178429,
    "total_throughput": 1201.0915989434193,
    "itl": 20.10534152709274,
    "ttft": 3126.9049250242315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.627239680052686,
    "arrivals": 9311,
    "finished_requests": 9303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2249191887676716. Arrivals time: 0.038075636606663465 Scheduler time: 0.7388403150252998 Scheduler overhead time: 0.15192110743373632 Adapter cache time: 0.06400059163570404 Engine time: 0.15734117338433862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 66, 540, 540, 33, 33, 540, 33, 66, 66, 66, 33, 540, 66, 33, 540, 66, 33, 33, 33, 33, 66, 66, 540, 66, 33, 66, 66, 66, 66, 540, 66, 33, 66, 33, 540, 540, 33, 66, 66, 33, 66, 33, 66, 66, 540, 540, 540, 540, 66, 66, 33, 66, 540, 540, 66, 33, 33, 33, 540, 66, 66, 66, 66, 540, 66, 540, 33, 33, 66, 33, 540, 540, 33, 33, 66, 66, 66, 33, 540, 33, 540, 66, 33, 540, 540, 66, 66, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 66, 540, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 66, 66, 540, 540, 33, 33, 540, 33, 33, 33, 66, 33]
Prompts retrieved: 27444 . Total input tokens: 6052346 . Total output tokens: 5499823
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.1817695312201977,
    "estimated_duration": 3599.928103366517,
    "input_throughput": 644.114530462866,
    "output_throughput": 556.9808458465919,
    "total_throughput": 1201.095376309458,
    "itl": 20.10480046418768,
    "ttft": 3127.022687065848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.635420352369487,
    "arrivals": 9311,
    "finished_requests": 9303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1818438619375229. Arrivals time: 0.03699758555740118 Scheduler time: 0.7168951723724604 Scheduler overhead time: 0.14700061082839966 Adapter cache time: 0.06288182595744729 Engine time: 0.14467733120545745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 66, 540, 540, 33, 33, 540, 33, 66, 66, 66, 33, 540, 66, 33, 540, 66, 33, 33, 33, 33, 66, 66, 540, 66, 33, 66, 66, 66, 66, 540, 66, 33, 66, 33, 540, 540, 33, 66, 66, 33, 66, 33, 66, 66, 540, 540, 540, 540, 66, 66, 33, 66, 540, 540, 66, 33, 33, 33, 540, 66, 66, 66, 66, 540, 66, 540, 33, 33, 66, 33, 540, 540, 33, 33, 66, 66, 66, 33, 540, 33, 540, 66, 33, 540, 540, 66, 66, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 66, 540, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 66, 66, 540, 540, 33, 33, 540, 33, 33, 33, 66, 33]
Prompts retrieved: 27444 . Total input tokens: 6052346 . Total output tokens: 5499823
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 1.195901261176914,
    "estimated_duration": 3599.930779947679,
    "input_throughput": 644.1140515578749,
    "output_throughput": 556.9804317262849,
    "total_throughput": 1201.0944832841599,
    "itl": 20.104124997203026,
    "ttft": 3127.0110664793388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.428662355290651,
    "arrivals": 9311,
    "finished_requests": 9303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1960649648681283. Arrivals time: 0.037277805618941784 Scheduler time: 0.7286505554802716 Scheduler overhead time: 0.14626725064590573 Adapter cache time: 0.0635480135679245 Engine time: 0.14710450544953346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 66, 540, 540, 33, 33, 540, 33, 66, 66, 66, 33, 540, 66, 33, 540, 66, 33, 33, 33, 33, 66, 66, 540, 66, 33, 66, 66, 66, 66, 540, 66, 33, 66, 33, 540, 540, 33, 66, 66, 33, 66, 33, 66, 66, 540, 540, 540, 540, 66, 66, 33, 66, 540, 540, 66, 33, 33, 33, 540, 66, 66, 66, 66, 540, 66, 540, 33, 33, 66, 33, 540, 540, 33, 33, 66, 66, 66, 33, 540, 33, 540, 66, 33, 540, 540, 66, 66, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 66, 540, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 66, 66, 540, 540, 33, 33, 540, 33, 33, 33, 66, 33]
Prompts retrieved: 27444 . Total input tokens: 6052346 . Total output tokens: 5499823
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 1.2007857621647418,
    "estimated_duration": 3599.933566925206,
    "input_throughput": 644.1135529010655,
    "output_throughput": 556.9800005261205,
    "total_throughput": 1201.093553427186,
    "itl": 20.10614624121868,
    "ttft": 3126.8671533056317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.696536692567138,
    "arrivals": 9311,
    "finished_requests": 9303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2008744389750063. Arrivals time: 0.03722990117967129 Scheduler time: 0.7302003740333021 Scheduler overhead time: 0.14980847295373678 Adapter cache time: 0.06299600889906287 Engine time: 0.14589166641235352 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 66, 540, 540, 33, 33, 540, 33, 66, 66, 66, 33, 540, 66, 33, 540, 66, 33, 33, 33, 33, 66, 66, 540, 66, 33, 66, 66, 66, 66, 540, 66, 33, 66, 33, 540, 540, 33, 66, 66, 33, 66, 33, 66, 66, 540, 540, 540, 540, 66, 66, 33, 66, 540, 540, 66, 33, 33, 33, 540, 66, 66, 66, 66, 540, 66, 540, 33, 33, 66, 33, 540, 540, 33, 33, 66, 66, 66, 33, 540, 33, 540, 66, 33, 540, 540, 66, 66, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 66, 540, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 66, 66, 540, 540, 33, 33, 540, 33, 33, 33, 66, 33]
Prompts retrieved: 27444 . Total input tokens: 6052346 . Total output tokens: 5499823
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.1918546888045967,
    "estimated_duration": 3599.9256925093964,
    "input_throughput": 644.1149618240204,
    "output_throughput": 556.9812188546351,
    "total_throughput": 1201.0961806786554,
    "itl": 20.10359031349108,
    "ttft": 3126.894601680956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.239891318171171,
    "arrivals": 9311,
    "finished_requests": 9303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1919411947019398. Arrivals time: 0.03755825897678733 Scheduler time: 0.7214021566323936 Scheduler overhead time: 0.14765928173437715 Adapter cache time: 0.06331483414396644 Engine time: 0.14790192479267716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.05-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [42 43 43]
Adapter prompts. [540, 66, 540, 540, 33, 33, 540, 33, 66, 66, 66, 33, 540, 66, 33, 540, 66, 33, 33, 33, 33, 66, 66, 540, 66, 33, 66, 66, 66, 66, 540, 66, 33, 66, 33, 540, 540, 33, 66, 66, 33, 66, 33, 66, 66, 540, 540, 540, 540, 66, 66, 33, 66, 540, 540, 66, 33, 33, 33, 540, 66, 66, 66, 66, 540, 66, 540, 33, 33, 66, 33, 540, 540, 33, 33, 66, 66, 66, 33, 540, 33, 540, 66, 33, 540, 540, 66, 66, 33, 33, 33, 540, 540, 540, 540, 540, 540, 540, 540, 33, 540, 33, 33, 540, 66, 540, 540, 66, 540, 33, 66, 540, 33, 540, 66, 33, 66, 66, 540, 540, 33, 33, 540, 33, 33, 33, 66, 33]
Prompts retrieved: 27444 . Total input tokens: 6052346 . Total output tokens: 5499823
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.1969450111500919,
    "estimated_duration": 3599.9294449018967,
    "input_throughput": 644.1142904297086,
    "output_throughput": 556.9806382843266,
    "total_throughput": 1201.0949287140352,
    "itl": 20.106669111230058,
    "ttft": 3127.0013538062503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.754634941890823,
    "arrivals": 9311,
    "finished_requests": 9303,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1970873661339283. Arrivals time: 0.03723305417224765 Scheduler time: 0.7241592737846076 Scheduler overhead time: 0.15175841515883803 Adapter cache time: 0.06287784734740853 Engine time: 0.14659291738644242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 66, 66, 270, 66, 135, 135, 135, 66, 270, 135, 66, 270, 135, 66, 66, 66, 66, 135, 135, 270, 135, 66, 135, 135, 135, 135, 270, 135, 66, 135, 66, 270, 270, 66, 135, 135, 66, 135, 66, 135, 135, 270, 270, 270, 270, 135, 135, 66, 135, 270, 270, 135, 66, 66, 66, 270, 135, 135, 135, 135, 270, 135, 270, 66, 66, 135, 66, 270, 270, 66, 66, 135, 135, 135, 66, 270, 66, 270, 135, 66, 270, 270, 135, 135, 66, 66, 66, 270, 270, 270, 270, 270, 270, 270, 270, 66, 270, 66, 66, 270, 135, 270, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 135, 135, 270, 270, 66, 66, 270, 66, 66, 66, 135, 66]
Prompts retrieved: 20187 . Total input tokens: 4469012 . Total output tokens: 4045314
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.0307448119856417,
    "estimated_duration": 3598.554888640076,
    "input_throughput": 458.9775760302977,
    "output_throughput": 415.0360481410963,
    "total_throughput": 874.013624171394,
    "itl": 19.57887051051886,
    "ttft": 3197.589761968537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2759,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.443885685510596,
    "arrivals": 6826,
    "finished_requests": 6820,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0308126667514443. Arrivals time: 0.031002433504909277 Scheduler time: 0.5652673081494868 Scheduler overhead time: 0.14844889054074883 Adapter cache time: 0.0576291480101645 Engine time: 0.15174181992188096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 66, 66, 270, 66, 135, 135, 135, 66, 270, 135, 66, 270, 135, 66, 66, 66, 66, 135, 135, 270, 135, 66, 135, 135, 135, 135, 270, 135, 66, 135, 66, 270, 270, 66, 135, 135, 66, 135, 66, 135, 135, 270, 270, 270, 270, 135, 135, 66, 135, 270, 270, 135, 66, 66, 66, 270, 135, 135, 135, 135, 270, 135, 270, 66, 66, 135, 66, 270, 270, 66, 66, 135, 135, 135, 66, 270, 66, 270, 135, 66, 270, 270, 135, 135, 66, 66, 66, 270, 270, 270, 270, 270, 270, 270, 270, 66, 270, 66, 66, 270, 135, 270, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 135, 135, 270, 270, 66, 66, 270, 66, 66, 66, 135, 66]
Prompts retrieved: 20187 . Total input tokens: 4469012 . Total output tokens: 4045314
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.024889016058296,
    "estimated_duration": 3598.571548437172,
    "input_throughput": 458.9754511668108,
    "output_throughput": 415.0341267074784,
    "total_throughput": 874.0095778742892,
    "itl": 19.583517288984705,
    "ttft": 3197.9655121725646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2760,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.013343296865763,
    "arrivals": 6826,
    "finished_requests": 6820,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.024975209031254. Arrivals time: 0.03119492344558239 Scheduler time: 0.5630138679407537 Scheduler overhead time: 0.1483874712139368 Adapter cache time: 0.05765100056305528 Engine time: 0.15025101788342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 66, 66, 270, 66, 135, 135, 135, 66, 270, 135, 66, 270, 135, 66, 66, 66, 66, 135, 135, 270, 135, 66, 135, 135, 135, 135, 270, 135, 66, 135, 66, 270, 270, 66, 135, 135, 66, 135, 66, 135, 135, 270, 270, 270, 270, 135, 135, 66, 135, 270, 270, 135, 66, 66, 66, 270, 135, 135, 135, 135, 270, 135, 270, 66, 66, 135, 66, 270, 270, 66, 66, 135, 135, 135, 66, 270, 66, 270, 135, 66, 270, 270, 135, 135, 66, 66, 66, 270, 270, 270, 270, 270, 270, 270, 270, 66, 270, 66, 66, 270, 135, 270, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 135, 135, 270, 270, 66, 66, 270, 66, 66, 66, 135, 66]
Prompts retrieved: 20187 . Total input tokens: 4469012 . Total output tokens: 4045314
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.0287144440226257,
    "estimated_duration": 3598.5613696112205,
    "input_throughput": 458.97674941651496,
    "output_throughput": 415.0353006655427,
    "total_throughput": 874.0120500820576,
    "itl": 19.58436717454139,
    "ttft": 3197.8510340662674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2759,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.024686091076365,
    "arrivals": 6826,
    "finished_requests": 6820,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0288193668238819. Arrivals time: 0.031128005124628544 Scheduler time: 0.5683101709000766 Scheduler overhead time: 0.14796543307602406 Adapter cache time: 0.0576643836684525 Engine time: 0.14881543722003698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 66, 66, 270, 66, 135, 135, 135, 66, 270, 135, 66, 270, 135, 66, 66, 66, 66, 135, 135, 270, 135, 66, 135, 135, 135, 135, 270, 135, 66, 135, 66, 270, 270, 66, 135, 135, 66, 135, 66, 135, 135, 270, 270, 270, 270, 135, 135, 66, 135, 270, 270, 135, 66, 66, 66, 270, 135, 135, 135, 135, 270, 135, 270, 66, 66, 135, 66, 270, 270, 66, 66, 135, 135, 135, 66, 270, 66, 270, 135, 66, 270, 270, 135, 135, 66, 66, 66, 270, 270, 270, 270, 270, 270, 270, 270, 66, 270, 66, 66, 270, 135, 270, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 135, 135, 270, 270, 66, 66, 270, 66, 66, 66, 135, 66]
Prompts retrieved: 20187 . Total input tokens: 4469012 . Total output tokens: 4045314
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 1.0370241138152778,
    "estimated_duration": 3598.558665536787,
    "input_throughput": 458.97709430662485,
    "output_throughput": 415.0356125366138,
    "total_throughput": 874.0127068432387,
    "itl": 19.581309317151185,
    "ttft": 3197.764293335611,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2760,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.650102058772301,
    "arrivals": 6826,
    "finished_requests": 6820,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0371127859689295. Arrivals time: 0.03172233887016773 Scheduler time: 0.5698611470870674 Scheduler overhead time: 0.15040383394807577 Adapter cache time: 0.05795995984226465 Engine time: 0.15176026802510023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 66, 66, 270, 66, 135, 135, 135, 66, 270, 135, 66, 270, 135, 66, 66, 66, 66, 135, 135, 270, 135, 66, 135, 135, 135, 135, 270, 135, 66, 135, 66, 270, 270, 66, 135, 135, 66, 135, 66, 135, 135, 270, 270, 270, 270, 135, 135, 66, 135, 270, 270, 135, 66, 66, 66, 270, 135, 135, 135, 135, 270, 135, 270, 66, 66, 135, 66, 270, 270, 66, 66, 135, 135, 135, 66, 270, 66, 270, 135, 66, 270, 270, 135, 135, 66, 66, 66, 270, 270, 270, 270, 270, 270, 270, 270, 66, 270, 66, 66, 270, 135, 270, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 135, 135, 270, 270, 66, 66, 270, 66, 66, 66, 135, 66]
Prompts retrieved: 20187 . Total input tokens: 4469012 . Total output tokens: 4045314
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 1.038204797077924,
    "estimated_duration": 3598.5583908477906,
    "input_throughput": 458.9771293417538,
    "output_throughput": 415.03564421755476,
    "total_throughput": 874.0127735593086,
    "itl": 19.583626448172623,
    "ttft": 3197.975287830402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2760,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.139861973542386,
    "arrivals": 6826,
    "finished_requests": 6820,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0382822919636965. Arrivals time: 0.031530413776636124 Scheduler time: 0.5736494618467987 Scheduler overhead time: 0.14849685365334153 Adapter cache time: 0.0579422558657825 Engine time: 0.15201167622581124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 66, 66, 270, 66, 135, 135, 135, 66, 270, 135, 66, 270, 135, 66, 66, 66, 66, 135, 135, 270, 135, 66, 135, 135, 135, 135, 270, 135, 66, 135, 66, 270, 270, 66, 135, 135, 66, 135, 66, 135, 135, 270, 270, 270, 270, 135, 135, 66, 135, 270, 270, 135, 66, 66, 66, 270, 135, 135, 135, 135, 270, 135, 270, 66, 66, 135, 66, 270, 270, 66, 66, 135, 135, 135, 66, 270, 66, 270, 135, 66, 270, 270, 135, 135, 66, 66, 66, 270, 270, 270, 270, 270, 270, 270, 270, 66, 270, 66, 66, 270, 135, 270, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 135, 135, 270, 270, 66, 66, 270, 66, 66, 66, 135, 66]
Prompts retrieved: 20187 . Total input tokens: 4469012 . Total output tokens: 4045314
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.0336629240773618,
    "estimated_duration": 3598.5654783899404,
    "input_throughput": 458.97622536494157,
    "output_throughput": 415.034826785542,
    "total_throughput": 874.0110521504836,
    "itl": 19.578776993630612,
    "ttft": 3197.754254815675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2759,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.24954876363492,
    "arrivals": 6826,
    "finished_requests": 6820,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0337729481980205. Arrivals time: 0.031698581762611866 Scheduler time: 0.5693439696915448 Scheduler overhead time: 0.1485643577761948 Adapter cache time: 0.05768662365153432 Engine time: 0.1515447893179953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_128_slots_64_rate_0.025-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 66, 66, 270, 66, 135, 135, 135, 66, 270, 135, 66, 270, 135, 66, 66, 66, 66, 135, 135, 270, 135, 66, 135, 135, 135, 135, 270, 135, 66, 135, 66, 270, 270, 66, 135, 135, 66, 135, 66, 135, 135, 270, 270, 270, 270, 135, 135, 66, 135, 270, 270, 135, 66, 66, 66, 270, 135, 135, 135, 135, 270, 135, 270, 66, 66, 135, 66, 270, 270, 66, 66, 135, 135, 135, 66, 270, 66, 270, 135, 66, 270, 270, 135, 135, 66, 66, 66, 270, 270, 270, 270, 270, 270, 270, 270, 66, 270, 66, 66, 270, 135, 270, 270, 135, 270, 66, 135, 270, 66, 270, 135, 66, 135, 135, 270, 270, 66, 66, 270, 66, 66, 66, 135, 66]
Prompts retrieved: 20187 . Total input tokens: 4469012 . Total output tokens: 4045314
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.0360478344373405,
    "estimated_duration": 3598.5724912003225,
    "input_throughput": 458.97533092325773,
    "output_throughput": 415.034017975785,
    "total_throughput": 874.0093488990427,
    "itl": 19.583349971901118,
    "ttft": 3197.9324588876602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2760,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.262220407723923,
    "arrivals": 6826,
    "finished_requests": 6820,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0361321731470525. Arrivals time: 0.03138198284432292 Scheduler time: 0.5730802728794515 Scheduler overhead time: 0.1492587854154408 Adapter cache time: 0.0574803133495152 Engine time: 0.14939909055829048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 33, 33, 270, 33, 135, 135, 135, 33, 270, 135, 33, 270, 135, 33, 33, 33, 33, 135, 135, 270, 135, 33, 135, 135, 135, 135, 270, 135, 33, 135, 33, 270, 270, 33, 135, 135, 33, 135, 33, 135, 135, 270, 270, 270, 270, 135, 135, 33, 135, 270, 270, 135, 33, 33, 33, 270, 135, 135, 135, 135, 270, 135, 270, 33, 33, 135, 33, 270, 270, 33, 33, 135, 135, 135, 33, 270, 33, 270, 135, 33, 270, 270, 135, 135, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 135, 270, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 135, 135, 270, 270, 33, 33, 270, 33, 33, 33, 135, 33]
Prompts retrieved: 18801 . Total input tokens: 4147914 . Total output tokens: 3764709
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.0095179891213775,
    "estimated_duration": 3600.009638099397,
    "input_throughput": 426.8121906504674,
    "output_throughput": 387.7289619527016,
    "total_throughput": 814.5411526031689,
    "itl": 19.225722498625075,
    "ttft": 5717.396608664803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.5769881617116805,
    "arrivals": 6333,
    "finished_requests": 6323,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0095745860598981. Arrivals time: 0.030463270377367735 Scheduler time: 0.5481462362222373 Scheduler overhead time: 0.14931336464360356 Adapter cache time: 0.05455306079238653 Engine time: 0.15200721798464656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 33, 33, 270, 33, 135, 135, 135, 33, 270, 135, 33, 270, 135, 33, 33, 33, 33, 135, 135, 270, 135, 33, 135, 135, 135, 135, 270, 135, 33, 135, 33, 270, 270, 33, 135, 135, 33, 135, 33, 135, 135, 270, 270, 270, 270, 135, 135, 33, 135, 270, 270, 135, 33, 33, 33, 270, 135, 135, 135, 135, 270, 135, 270, 33, 33, 135, 33, 270, 270, 33, 33, 135, 135, 135, 33, 270, 33, 270, 135, 33, 270, 270, 135, 135, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 135, 270, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 135, 135, 270, 270, 33, 33, 270, 33, 33, 33, 135, 33]
Prompts retrieved: 18801 . Total input tokens: 4147914 . Total output tokens: 3764709
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.0044237882830203,
    "estimated_duration": 3600.011165439442,
    "input_throughput": 426.81200957120944,
    "output_throughput": 387.7287974548867,
    "total_throughput": 814.5408070260962,
    "itl": 19.228104192722803,
    "ttft": 5717.486015869751,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.012769556343291,
    "arrivals": 6333,
    "finished_requests": 6323,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0045613232068717. Arrivals time: 0.03010360524058342 Scheduler time: 0.5421290048398077 Scheduler overhead time: 0.14944729395210743 Adapter cache time: 0.05519474809989333 Engine time: 0.1520491261035204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 33, 33, 270, 33, 135, 135, 135, 33, 270, 135, 33, 270, 135, 33, 33, 33, 33, 135, 135, 270, 135, 33, 135, 135, 135, 135, 270, 135, 33, 135, 33, 270, 270, 33, 135, 135, 33, 135, 33, 135, 135, 270, 270, 270, 270, 135, 135, 33, 135, 270, 270, 135, 33, 33, 33, 270, 135, 135, 135, 135, 270, 135, 270, 33, 33, 135, 33, 270, 270, 33, 33, 135, 135, 135, 33, 270, 33, 270, 135, 33, 270, 270, 135, 135, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 135, 270, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 135, 135, 270, 270, 33, 33, 270, 33, 33, 33, 135, 33]
Prompts retrieved: 18801 . Total input tokens: 4147914 . Total output tokens: 3764709
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 1.003110766876489,
    "estimated_duration": 3600.000605712313,
    "input_throughput": 426.8132615205423,
    "output_throughput": 387.72993476311234,
    "total_throughput": 814.5431962836547,
    "itl": 19.227683834210854,
    "ttft": 5717.503620178268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.025146655645175,
    "arrivals": 6333,
    "finished_requests": 6323,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.003182533197105. Arrivals time: 0.030013118870556355 Scheduler time: 0.5431995978578925 Scheduler overhead time: 0.1493113823235035 Adapter cache time: 0.05473085353150964 Engine time: 0.15036390908062458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 33, 33, 270, 33, 135, 135, 135, 33, 270, 135, 33, 270, 135, 33, 33, 33, 33, 135, 135, 270, 135, 33, 135, 135, 135, 135, 270, 135, 33, 135, 33, 270, 270, 33, 135, 135, 33, 135, 33, 135, 135, 270, 270, 270, 270, 135, 135, 33, 135, 270, 270, 135, 33, 33, 33, 270, 135, 135, 135, 135, 270, 135, 270, 33, 33, 135, 33, 270, 270, 33, 33, 135, 135, 135, 33, 270, 33, 270, 135, 33, 270, 270, 135, 135, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 135, 270, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 135, 135, 270, 270, 33, 33, 270, 33, 33, 33, 135, 33]
Prompts retrieved: 18801 . Total input tokens: 4147914 . Total output tokens: 3764709
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 1.0079595712013543,
    "estimated_duration": 3600.006096938799,
    "input_throughput": 426.81261048600976,
    "output_throughput": 387.7293433438675,
    "total_throughput": 814.5419538298772,
    "itl": 19.226959834411414,
    "ttft": 5717.427220834522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.745139622764903,
    "arrivals": 6333,
    "finished_requests": 6323,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0080430721864104. Arrivals time: 0.030698814429342747 Scheduler time: 0.5459506846964359 Scheduler overhead time: 0.14927877578884363 Adapter cache time: 0.05499274609610438 Engine time: 0.1513347583822906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 33, 33, 270, 33, 135, 135, 135, 33, 270, 135, 33, 270, 135, 33, 33, 33, 33, 135, 135, 270, 135, 33, 135, 135, 135, 135, 270, 135, 33, 135, 33, 270, 270, 33, 135, 135, 33, 135, 33, 135, 135, 270, 270, 270, 270, 135, 135, 33, 135, 270, 270, 135, 33, 33, 33, 270, 135, 135, 135, 135, 270, 135, 270, 33, 33, 135, 33, 270, 270, 33, 33, 135, 135, 135, 33, 270, 33, 270, 135, 33, 270, 270, 135, 135, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 135, 270, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 135, 135, 270, 270, 33, 33, 270, 33, 33, 33, 135, 33]
Prompts retrieved: 18801 . Total input tokens: 4147914 . Total output tokens: 3764709
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 1.001130148768425,
    "estimated_duration": 3600.0123992009926,
    "input_throughput": 426.8118632983114,
    "output_throughput": 387.72866457621035,
    "total_throughput": 814.5405278745218,
    "itl": 19.228836131145552,
    "ttft": 5717.4172280834555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.107515385746816,
    "arrivals": 6333,
    "finished_requests": 6323,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.001299420837313. Arrivals time: 0.030297466088086367 Scheduler time: 0.5405428796075284 Scheduler overhead time: 0.1492538400925696 Adapter cache time: 0.05453204922378063 Engine time: 0.1510772407054901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 33, 33, 270, 33, 135, 135, 135, 33, 270, 135, 33, 270, 135, 33, 33, 33, 33, 135, 135, 270, 135, 33, 135, 135, 135, 135, 270, 135, 33, 135, 33, 270, 270, 33, 135, 135, 33, 135, 33, 135, 135, 270, 270, 270, 270, 135, 135, 33, 135, 270, 270, 135, 33, 33, 33, 270, 135, 135, 135, 135, 270, 135, 270, 33, 33, 135, 33, 270, 270, 33, 33, 135, 135, 135, 33, 270, 33, 270, 135, 33, 270, 270, 135, 135, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 135, 270, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 135, 135, 270, 270, 33, 33, 270, 33, 33, 33, 135, 33]
Prompts retrieved: 18801 . Total input tokens: 4147914 . Total output tokens: 3764709
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 1.0171745670959353,
    "estimated_duration": 3600.01175949623,
    "input_throughput": 426.8119391407252,
    "output_throughput": 387.7287334737279,
    "total_throughput": 814.5406726144531,
    "itl": 19.225142064129916,
    "ttft": 5717.261211773482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.428608134039486,
    "arrivals": 6333,
    "finished_requests": 6323,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0172362141311169. Arrivals time: 0.030593162402510643 Scheduler time: 0.554239752702415 Scheduler overhead time: 0.1501068607904017 Adapter cache time: 0.054771100636571646 Engine time: 0.15184544026851654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 135, 270, 270, 33, 33, 270, 33, 135, 135, 135, 33, 270, 135, 33, 270, 135, 33, 33, 33, 33, 135, 135, 270, 135, 33, 135, 135, 135, 135, 270, 135, 33, 135, 33, 270, 270, 33, 135, 135, 33, 135, 33, 135, 135, 270, 270, 270, 270, 135, 135, 33, 135, 270, 270, 135, 33, 33, 33, 270, 135, 135, 135, 135, 270, 135, 270, 33, 33, 135, 33, 270, 270, 33, 33, 135, 135, 135, 33, 270, 33, 270, 135, 33, 270, 270, 135, 135, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 135, 270, 270, 135, 270, 33, 135, 270, 33, 270, 135, 33, 135, 135, 270, 270, 33, 33, 270, 33, 33, 33, 135, 33]
Prompts retrieved: 18801 . Total input tokens: 4147914 . Total output tokens: 3764709
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 0.9967884961515665,
    "estimated_duration": 3599.995117964499,
    "input_throughput": 426.81391214463093,
    "output_throughput": 387.73052580949775,
    "total_throughput": 814.5444379541286,
    "itl": 19.2297495068156,
    "ttft": 5717.472730468139,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.205854846723122,
    "arrivals": 6333,
    "finished_requests": 6323,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9968724180944264. Arrivals time: 0.029925540555268526 Scheduler time: 0.5384139101952314 Scheduler overhead time: 0.1487661716528237 Adapter cache time: 0.054719173815101385 Engine time: 0.14975760132074356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 66, 270, 270, 33, 33, 270, 33, 66, 66, 66, 33, 270, 66, 33, 270, 66, 33, 33, 33, 33, 66, 66, 270, 66, 33, 66, 66, 66, 66, 270, 66, 33, 66, 33, 270, 270, 33, 66, 66, 33, 66, 33, 66, 66, 270, 270, 270, 270, 66, 66, 33, 66, 270, 270, 66, 33, 33, 33, 270, 66, 66, 66, 66, 270, 66, 270, 33, 33, 66, 33, 270, 270, 33, 33, 66, 66, 66, 33, 270, 33, 270, 66, 33, 270, 270, 66, 66, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 66, 270, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 66, 66, 270, 270, 33, 33, 270, 33, 33, 33, 66, 33]
Prompts retrieved: 15834 . Total input tokens: 3478100 . Total output tokens: 3189536
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 0.9298873362131417,
    "estimated_duration": 3599.426972721223,
    "input_throughput": 359.52945005066744,
    "output_throughput": 326.2883255864743,
    "total_throughput": 685.8177756371417,
    "itl": 18.944996690233715,
    "ttft": 6749.86203145076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.336711133152827,
    "arrivals": 5359,
    "finished_requests": 5349,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9299971968866885. Arrivals time: 0.02766779577359557 Scheduler time: 0.4748011967167258 Scheduler overhead time: 0.14959826366975904 Adapter cache time: 0.04922789754346013 Engine time: 0.1519785071723163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 66, 270, 270, 33, 33, 270, 33, 66, 66, 66, 33, 270, 66, 33, 270, 66, 33, 33, 33, 33, 66, 66, 270, 66, 33, 66, 66, 66, 66, 270, 66, 33, 66, 33, 270, 270, 33, 66, 66, 33, 66, 33, 66, 66, 270, 270, 270, 270, 66, 66, 33, 66, 270, 270, 66, 33, 33, 33, 270, 66, 66, 66, 66, 270, 66, 270, 33, 33, 66, 33, 270, 270, 33, 33, 66, 66, 66, 33, 270, 33, 270, 66, 33, 270, 270, 66, 66, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 66, 270, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 66, 66, 270, 270, 33, 33, 270, 33, 33, 33, 66, 33]
Prompts retrieved: 15834 . Total input tokens: 3478100 . Total output tokens: 3189536
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 0.960859689861536,
    "estimated_duration": 3599.4459100323343,
    "input_throughput": 359.52755850368504,
    "output_throughput": 326.2866089268306,
    "total_throughput": 685.8141674305157,
    "itl": 18.94704035318015,
    "ttft": 6749.812007063083,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.623841034439345,
    "arrivals": 5359,
    "finished_requests": 5349,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9609336359426379. Arrivals time: 0.02865826804190874 Scheduler time: 0.5019271648488939 Scheduler overhead time: 0.15104341879487038 Adapter cache time: 0.04959863796830177 Engine time: 0.15319211641326547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 66, 270, 270, 33, 33, 270, 33, 66, 66, 66, 33, 270, 66, 33, 270, 66, 33, 33, 33, 33, 66, 66, 270, 66, 33, 66, 66, 66, 66, 270, 66, 33, 66, 33, 270, 270, 33, 66, 66, 33, 66, 33, 66, 66, 270, 270, 270, 270, 66, 66, 33, 66, 270, 270, 66, 33, 33, 33, 270, 66, 66, 66, 66, 270, 66, 270, 33, 33, 66, 33, 270, 270, 33, 33, 66, 66, 66, 33, 270, 33, 270, 66, 33, 270, 270, 66, 66, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 66, 270, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 66, 66, 270, 270, 33, 33, 270, 33, 33, 33, 66, 33]
Prompts retrieved: 15834 . Total input tokens: 3478100 . Total output tokens: 3189536
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 0.9556654477491975,
    "estimated_duration": 3599.447171422713,
    "input_throughput": 359.52743251083626,
    "output_throughput": 326.2864945829412,
    "total_throughput": 685.8139270937775,
    "itl": 18.947125166366153,
    "ttft": 6749.815090292246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.6320395860261545,
    "arrivals": 5359,
    "finished_requests": 5349,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9557263506576419. Arrivals time: 0.028421519324183464 Scheduler time: 0.4959670789539814 Scheduler overhead time: 0.15066637890413404 Adapter cache time: 0.049709261395037174 Engine time: 0.15416326373815536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 66, 270, 270, 33, 33, 270, 33, 66, 66, 66, 33, 270, 66, 33, 270, 66, 33, 33, 33, 33, 66, 66, 270, 66, 33, 66, 66, 66, 66, 270, 66, 33, 66, 33, 270, 270, 33, 66, 66, 33, 66, 33, 66, 66, 270, 270, 270, 270, 66, 66, 33, 66, 270, 270, 66, 33, 33, 33, 270, 66, 66, 66, 66, 270, 66, 270, 33, 33, 66, 33, 270, 270, 33, 33, 66, 66, 66, 33, 270, 33, 270, 66, 33, 270, 270, 66, 66, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 66, 270, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 66, 66, 270, 270, 33, 33, 270, 33, 33, 33, 66, 33]
Prompts retrieved: 15834 . Total input tokens: 3478100 . Total output tokens: 3189536
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 0.9308114559389651,
    "estimated_duration": 3599.4326691443384,
    "input_throughput": 359.528881063258,
    "output_throughput": 326.287809206108,
    "total_throughput": 685.816690269366,
    "itl": 18.946338223267794,
    "ttft": 6749.68194517613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.439564545822724,
    "arrivals": 5359,
    "finished_requests": 5349,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9309441987425089. Arrivals time: 0.027841510251164436 Scheduler time: 0.4739690492860973 Scheduler overhead time: 0.15058905771002173 Adapter cache time: 0.049418157897889614 Engine time: 0.15260460507124662 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 66, 270, 270, 33, 33, 270, 33, 66, 66, 66, 33, 270, 66, 33, 270, 66, 33, 33, 33, 33, 66, 66, 270, 66, 33, 66, 66, 66, 66, 270, 66, 33, 66, 33, 270, 270, 33, 66, 66, 33, 66, 33, 66, 66, 270, 270, 270, 270, 66, 66, 33, 66, 270, 270, 66, 33, 33, 33, 270, 66, 66, 66, 66, 270, 66, 270, 33, 33, 66, 33, 270, 270, 33, 33, 66, 66, 66, 33, 270, 33, 270, 66, 33, 270, 270, 66, 66, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 66, 270, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 66, 66, 270, 270, 33, 33, 270, 33, 33, 33, 66, 33]
Prompts retrieved: 15834 . Total input tokens: 3478100 . Total output tokens: 3189536
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 0.9362588529475033,
    "estimated_duration": 3599.4299583710717,
    "input_throughput": 359.52915182870987,
    "output_throughput": 326.2880549373156,
    "total_throughput": 685.8172067660255,
    "itl": 18.947331670032813,
    "ttft": 6749.830891576266,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.688754543699284,
    "arrivals": 5359,
    "finished_requests": 5349,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9363362481817603. Arrivals time: 0.0277616991661489 Scheduler time: 0.47835593810305 Scheduler overhead time: 0.15051517030224204 Adapter cache time: 0.049694742541760206 Engine time: 0.1533851926214993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 66, 270, 270, 33, 33, 270, 33, 66, 66, 66, 33, 270, 66, 33, 270, 66, 33, 33, 33, 33, 66, 66, 270, 66, 33, 66, 66, 66, 66, 270, 66, 33, 66, 33, 270, 270, 33, 66, 66, 33, 66, 33, 66, 66, 270, 270, 270, 270, 66, 66, 33, 66, 270, 270, 66, 33, 33, 33, 270, 66, 66, 66, 66, 270, 66, 270, 33, 33, 66, 33, 270, 270, 33, 33, 66, 66, 66, 33, 270, 33, 270, 66, 33, 270, 270, 66, 66, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 66, 270, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 66, 66, 270, 270, 33, 33, 270, 33, 33, 33, 66, 33]
Prompts retrieved: 15834 . Total input tokens: 3478100 . Total output tokens: 3189536
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 0.9382151649333537,
    "estimated_duration": 3599.440469461914,
    "input_throughput": 359.52810193120297,
    "output_throughput": 326.287102110504,
    "total_throughput": 685.815204041707,
    "itl": 18.94461568810954,
    "ttft": 6749.729166553515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.236901267876269,
    "arrivals": 5359,
    "finished_requests": 5349,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.938288654666394. Arrivals time: 0.02811378799378872 Scheduler time: 0.47884315624833107 Scheduler overhead time: 0.15105542819947004 Adapter cache time: 0.05036068148910999 Engine time: 0.15346141718328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [42 43 43]
Adapter prompts. [270, 66, 270, 270, 33, 33, 270, 33, 66, 66, 66, 33, 270, 66, 33, 270, 66, 33, 33, 33, 33, 66, 66, 270, 66, 33, 66, 66, 66, 66, 270, 66, 33, 66, 33, 270, 270, 33, 66, 66, 33, 66, 33, 66, 66, 270, 270, 270, 270, 66, 66, 33, 66, 270, 270, 66, 33, 33, 33, 270, 66, 66, 66, 66, 270, 66, 270, 33, 33, 66, 33, 270, 270, 33, 33, 66, 66, 66, 33, 270, 33, 270, 66, 33, 270, 270, 66, 66, 33, 33, 33, 270, 270, 270, 270, 270, 270, 270, 270, 33, 270, 33, 33, 270, 66, 270, 270, 66, 270, 33, 66, 270, 33, 270, 66, 33, 66, 66, 270, 270, 33, 33, 270, 33, 33, 33, 66, 33]
Prompts retrieved: 15834 . Total input tokens: 3478100 . Total output tokens: 3189536
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 0.9336306741461158,
    "estimated_duration": 3599.4336052308686,
    "input_throughput": 359.52878756239653,
    "output_throughput": 326.2877243500844,
    "total_throughput": 685.8165119124809,
    "itl": 18.9482526802302,
    "ttft": 6749.880837857953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.751128421761083,
    "arrivals": 5359,
    "finished_requests": 5349,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.933787533082068. Arrivals time: 0.027931755408644676 Scheduler time: 0.4765502861700952 Scheduler overhead time: 0.1498893778771162 Adapter cache time: 0.04968000343069434 Engine time: 0.15359404915943742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [42 43 43]
Adapter prompts. [135, 66, 135, 135, 33, 33, 135, 33, 66, 66, 66, 33, 135, 66, 33, 135, 66, 33, 33, 33, 33, 66, 66, 135, 66, 33, 66, 66, 66, 66, 135, 66, 33, 66, 33, 135, 135, 33, 66, 66, 33, 66, 33, 66, 66, 135, 135, 135, 135, 66, 66, 33, 66, 135, 135, 66, 33, 33, 33, 135, 66, 66, 66, 66, 135, 66, 135, 33, 33, 66, 33, 135, 135, 33, 33, 66, 66, 66, 33, 135, 33, 135, 66, 33, 135, 135, 66, 66, 33, 33, 33, 135, 135, 135, 135, 135, 135, 135, 135, 33, 135, 33, 33, 135, 66, 135, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 66, 66, 135, 135, 33, 33, 135, 33, 33, 33, 66, 33]
Prompts retrieved: 10029 . Total input tokens: 2205085 . Total output tokens: 2017823
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 0.7859448762610555,
    "estimated_duration": 3598.3614066291416,
    "input_throughput": 219.7066694144544,
    "output_throughput": 202.52968438840026,
    "total_throughput": 422.23635380285464,
    "itl": 18.385966593120326,
    "ttft": 3306.819517814995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1368,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.186747233700117,
    "arrivals": 3297,
    "finished_requests": 3294,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7860017903149128. Arrivals time: 0.023060997016727924 Scheduler time: 0.3429372669197619 Scheduler overhead time: 0.1492401254363358 Adapter cache time: 0.040304495487362146 Engine time: 0.15362728480249643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [42 43 43]
Adapter prompts. [135, 66, 135, 135, 33, 33, 135, 33, 66, 66, 66, 33, 135, 66, 33, 135, 66, 33, 33, 33, 33, 66, 66, 135, 66, 33, 66, 66, 66, 66, 135, 66, 33, 66, 33, 135, 135, 33, 66, 66, 33, 66, 33, 66, 66, 135, 135, 135, 135, 66, 66, 33, 66, 135, 135, 66, 33, 33, 33, 135, 66, 66, 66, 66, 135, 66, 135, 33, 33, 66, 33, 135, 135, 33, 33, 66, 66, 66, 33, 135, 33, 135, 66, 33, 135, 135, 66, 66, 33, 33, 33, 135, 135, 135, 135, 135, 135, 135, 135, 33, 135, 33, 33, 135, 66, 135, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 66, 66, 135, 135, 33, 33, 135, 33, 33, 33, 66, 33]
Prompts retrieved: 10029 . Total input tokens: 2205085 . Total output tokens: 2017823
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 0.7931428379379213,
    "estimated_duration": 3598.367581155984,
    "input_throughput": 219.70629241441281,
    "output_throughput": 202.52933686276688,
    "total_throughput": 422.2356292771797,
    "itl": 18.388318667644103,
    "ttft": 3307.0687947322194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.470661161828288,
    "arrivals": 3297,
    "finished_requests": 3294,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7932256460189819. Arrivals time: 0.022760718129575253 Scheduler time: 0.34779100539162755 Scheduler overhead time: 0.1497888877056539 Adapter cache time: 0.04053637199103832 Engine time: 0.15560446912422776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [42 43 43]
Adapter prompts. [135, 66, 135, 135, 33, 33, 135, 33, 66, 66, 66, 33, 135, 66, 33, 135, 66, 33, 33, 33, 33, 66, 66, 135, 66, 33, 66, 66, 66, 66, 135, 66, 33, 66, 33, 135, 135, 33, 66, 66, 33, 66, 33, 66, 66, 135, 135, 135, 135, 66, 66, 33, 66, 135, 135, 66, 33, 33, 33, 135, 66, 66, 66, 66, 135, 66, 135, 33, 33, 66, 33, 135, 135, 33, 33, 66, 66, 66, 33, 135, 33, 135, 66, 33, 135, 135, 66, 66, 33, 33, 33, 135, 135, 135, 135, 135, 135, 135, 135, 33, 135, 33, 33, 135, 66, 135, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 66, 66, 135, 135, 33, 33, 135, 33, 33, 33, 66, 33]
Prompts retrieved: 10029 . Total input tokens: 2205085 . Total output tokens: 2017823
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 0.7986653749831021,
    "estimated_duration": 3598.368114656732,
    "input_throughput": 219.70625984034936,
    "output_throughput": 202.5293068353908,
    "total_throughput": 422.2355666757402,
    "itl": 18.388357043482202,
    "ttft": 3307.1114874714385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1367,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.476828416138819,
    "arrivals": 3297,
    "finished_requests": 3294,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7987795150838792. Arrivals time: 0.022975991014391184 Scheduler time: 0.3512463625520468 Scheduler overhead time: 0.15431574126705527 Adapter cache time: 0.04028909234330058 Engine time: 0.1527724675834179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [42 43 43]
Adapter prompts. [135, 66, 135, 135, 33, 33, 135, 33, 66, 66, 66, 33, 135, 66, 33, 135, 66, 33, 33, 33, 33, 66, 66, 135, 66, 33, 66, 66, 66, 66, 135, 66, 33, 66, 33, 135, 135, 33, 66, 66, 33, 66, 33, 66, 66, 135, 135, 135, 135, 66, 66, 33, 66, 135, 135, 66, 33, 33, 33, 135, 66, 66, 66, 66, 135, 66, 135, 33, 33, 66, 33, 135, 135, 33, 33, 66, 66, 66, 33, 135, 33, 135, 66, 33, 135, 135, 66, 66, 33, 33, 33, 135, 135, 135, 135, 135, 135, 135, 135, 33, 135, 33, 33, 135, 66, 135, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 66, 66, 135, 135, 33, 33, 135, 33, 33, 33, 66, 33]
Prompts retrieved: 10029 . Total input tokens: 2205085 . Total output tokens: 2017823
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 0.7871249173767865,
    "estimated_duration": 3598.362819225882,
    "input_throughput": 219.70658316497358,
    "output_throughput": 202.52960488202848,
    "total_throughput": 422.23618804700203,
    "itl": 18.38641899074957,
    "ttft": 3306.9742353963375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1366,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.285437599508964,
    "arrivals": 3297,
    "finished_requests": 3294,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.787182402331382. Arrivals time: 0.023305398877710104 Scheduler time: 0.3438906278461218 Scheduler overhead time: 0.14998154202476144 Adapter cache time: 0.0403387900441885 Engine time: 0.15318711614236236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [42 43 43]
Adapter prompts. [135, 66, 135, 135, 33, 33, 135, 33, 66, 66, 66, 33, 135, 66, 33, 135, 66, 33, 33, 33, 33, 66, 66, 135, 66, 33, 66, 66, 66, 66, 135, 66, 33, 66, 33, 135, 135, 33, 66, 66, 33, 66, 33, 66, 66, 135, 135, 135, 135, 66, 66, 33, 66, 135, 135, 66, 33, 33, 33, 135, 66, 66, 66, 66, 135, 66, 135, 33, 33, 66, 33, 135, 135, 33, 33, 66, 66, 66, 33, 135, 33, 135, 66, 33, 135, 135, 66, 66, 33, 33, 33, 135, 135, 135, 135, 135, 135, 135, 135, 33, 135, 33, 33, 135, 66, 135, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 66, 66, 135, 135, 33, 33, 135, 33, 33, 33, 66, 33]
Prompts retrieved: 10029 . Total input tokens: 2205085 . Total output tokens: 2017823
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 0.7905450682155788,
    "estimated_duration": 3598.3694979483785,
    "input_throughput": 219.70617538047549,
    "output_throughput": 202.52922897871198,
    "total_throughput": 422.2354043591875,
    "itl": 18.388775696243755,
    "ttft": 3307.1004028170005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1368,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.535832369942185,
    "arrivals": 3297,
    "finished_requests": 3294,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7906284052878618. Arrivals time: 0.02276220778003335 Scheduler time: 0.34734492702409625 Scheduler overhead time: 0.15042269928380847 Adapter cache time: 0.040422065649181604 Engine time: 0.1529914876446128 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [42 43 43]
Adapter prompts. [135, 66, 135, 135, 33, 33, 135, 33, 66, 66, 66, 33, 135, 66, 33, 135, 66, 33, 33, 33, 33, 66, 66, 135, 66, 33, 66, 66, 66, 66, 135, 66, 33, 66, 33, 135, 135, 33, 66, 66, 33, 66, 33, 66, 66, 135, 135, 135, 135, 66, 66, 33, 66, 135, 135, 66, 33, 33, 33, 135, 66, 66, 66, 66, 135, 66, 135, 33, 33, 66, 33, 135, 135, 33, 33, 66, 66, 66, 33, 135, 33, 135, 66, 33, 135, 135, 66, 66, 33, 33, 33, 135, 135, 135, 135, 135, 135, 135, 135, 33, 135, 33, 33, 135, 66, 135, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 66, 66, 135, 135, 33, 33, 135, 33, 33, 33, 66, 33]
Prompts retrieved: 10029 . Total input tokens: 2205085 . Total output tokens: 2017823
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 0.7904609660618007,
    "estimated_duration": 3598.3587267394737,
    "input_throughput": 219.70683304172954,
    "output_throughput": 202.52983522305846,
    "total_throughput": 422.23666826478797,
    "itl": 18.38531482390115,
    "ttft": 3306.985486486791,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1366,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.084408702836264,
    "arrivals": 3297,
    "finished_requests": 3294,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7905965847894549. Arrivals time: 0.02330971322953701 Scheduler time: 0.3460970507003367 Scheduler overhead time: 0.14941270649433136 Adapter cache time: 0.0406125015579164 Engine time: 0.1547365142032504 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_128_slots_64_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [42 43 43]
Adapter prompts. [135, 66, 135, 135, 33, 33, 135, 33, 66, 66, 66, 33, 135, 66, 33, 135, 66, 33, 33, 33, 33, 66, 66, 135, 66, 33, 66, 66, 66, 66, 135, 66, 33, 66, 33, 135, 135, 33, 66, 66, 33, 66, 33, 66, 66, 135, 135, 135, 135, 66, 66, 33, 66, 135, 135, 66, 33, 33, 33, 135, 66, 66, 66, 66, 135, 66, 135, 33, 33, 66, 33, 135, 135, 33, 33, 66, 66, 66, 33, 135, 33, 135, 66, 33, 135, 135, 66, 66, 33, 33, 33, 135, 135, 135, 135, 135, 135, 135, 135, 33, 135, 33, 33, 135, 66, 135, 135, 66, 135, 33, 66, 135, 33, 135, 66, 33, 66, 66, 135, 135, 33, 33, 135, 33, 33, 33, 66, 33]
Prompts retrieved: 10029 . Total input tokens: 2205085 . Total output tokens: 2017823
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 0.7943550879135728,
    "estimated_duration": 3598.351348786439,
    "input_throughput": 219.70728352211304,
    "output_throughput": 202.53025048423436,
    "total_throughput": 422.2375340063474,
    "itl": 18.389626975261177,
    "ttft": 3307.1988231920523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1368,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5978289866447515,
    "arrivals": 3297,
    "finished_requests": 3294,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.7944187470711768. Arrivals time: 0.02291892096400261 Scheduler time: 0.34715117421001196 Scheduler overhead time: 0.15076906699687243 Adapter cache time: 0.0407569301314652 Engine time: 0.15581847913563251 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577126872 . Total output tokens: 517998278
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 12.60223557986319,
    "estimated_duration": 3600.1597769827767,
    "input_throughput": 5418.3817409205285,
    "output_throughput": 4765.605157218576,
    "total_throughput": 10183.986898139105,
    "itl": 179.87833564762502,
    "ttft": 2102092.665547789,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8569365683011699,
    "arrivals": 863203,
    "finished_requests": 78993,
    "scheduler_time": 78.43938874896291
}
#Debug simulation 
Total elapsed time: 12.602357483003289. Arrivals time: 0.4137813989073038 Scheduler time: 12.085139910690486 Scheduler overhead time: 0.03898886777460575 Adapter cache time: 0.009711478371173143 Engine time: 0.038521697744727135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577126872 . Total output tokens: 517998278
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 13.651626891922206,
    "estimated_duration": 3600.0103732267935,
    "input_throughput": 5423.615205447065,
    "output_throughput": 4764.757381691962,
    "total_throughput": 10188.372587139027,
    "itl": 179.77812052494065,
    "ttft": 2102674.762213296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8081335100415198,
    "arrivals": 863203,
    "finished_requests": 79035,
    "scheduler_time": 78.43343393412523
}
#Debug simulation 
Total elapsed time: 13.651765223126858. Arrivals time: 0.3459507576189935 Scheduler time: 13.201435037422925 Scheduler overhead time: 0.03923703217878938 Adapter cache time: 0.009315181989222765 Engine time: 0.039357373025268316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577126872 . Total output tokens: 517998278
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 13.010906100273132,
    "estimated_duration": 3600.100424187117,
    "input_throughput": 5408.254689005315,
    "output_throughput": 4754.980968028202,
    "total_throughput": 10163.235657033518,
    "itl": 178.0352654831641,
    "ttft": 2103614.4679532982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8317431430332403,
    "arrivals": 863203,
    "finished_requests": 78821,
    "scheduler_time": 78.36250607188292
}
#Debug simulation 
Total elapsed time: 13.010975174140185. Arrivals time: 0.6762111028656363 Scheduler time: 12.230923870578408 Scheduler overhead time: 0.03874128544703126 Adapter cache time: 0.009410577826201916 Engine time: 0.039195242803543806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577126872 . Total output tokens: 517998278
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 12.6376887219958,
    "estimated_duration": 3600.1731840981515,
    "input_throughput": 5418.361562760915,
    "output_throughput": 4765.587410011732,
    "total_throughput": 10183.948972772647,
    "itl": 179.87870822252265,
    "ttft": 2102101.100488915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8703103033662796,
    "arrivals": 863203,
    "finished_requests": 78993,
    "scheduler_time": 78.43942212925539
}
#Debug simulation 
Total elapsed time: 12.637843667995185. Arrivals time: 0.41940168337896466 Scheduler time: 12.115098009817302 Scheduler overhead time: 0.03851165296509862 Adapter cache time: 0.009769643191248178 Engine time: 0.03872111393138766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577126872 . Total output tokens: 517998278
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 12.79006567504257,
    "estimated_duration": 3600.112138482914,
    "input_throughput": 5408.237091249263,
    "output_throughput": 4754.965495939716,
    "total_throughput": 10163.20258718898,
    "itl": 178.03570742133257,
    "ttft": 2103620.3627722417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8434382451698225,
    "arrivals": 863203,
    "finished_requests": 78821,
    "scheduler_time": 78.36252526555108
}
#Debug simulation 
Total elapsed time: 12.790181073360145. Arrivals time: 0.45095986407250166 Scheduler time: 12.23359453259036 Scheduler overhead time: 0.03986264066770673 Adapter cache time: 0.009822640102356672 Engine time: 0.03948185034096241 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577126872 . Total output tokens: 517998278
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 12.621272162999958,
    "estimated_duration": 3600.140022629322,
    "input_throughput": 5418.411472160811,
    "output_throughput": 4765.631306603908,
    "total_throughput": 10184.042778764719,
    "itl": 179.8775916419572,
    "ttft": 2102082.159920406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8372140825726103,
    "arrivals": 863203,
    "finished_requests": 78993,
    "scheduler_time": 78.43935688121495
}
#Debug simulation 
Total elapsed time: 12.621372831985354. Arrivals time: 0.42501475289463997 Scheduler time: 12.092469931580126 Scheduler overhead time: 0.03875271650031209 Adapter cache time: 0.009698718786239624 Engine time: 0.039207416120916605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 17280, 17280, 17280, 8640, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 8640, 34560, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 17280, 8640, 8640, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 17280, 8640, 34560, 34560, 8640, 8640, 17280, 17280, 17280, 8640, 34560, 8640, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 34560, 8640, 8640, 34560, 8640, 8640, 8640, 17280, 8640]
Prompts retrieved: 2592000 . Total input tokens: 577126872 . Total output tokens: 517998278
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 12.612219032365829,
    "estimated_duration": 3600.1212119231213,
    "input_throughput": 5408.223460787124,
    "output_throughput": 4754.953511927907,
    "total_throughput": 10163.176972715031,
    "itl": 178.03599326399933,
    "ttft": 2103625.606917772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8524925177916923,
    "arrivals": 863203,
    "finished_requests": 78821,
    "scheduler_time": 78.3625444331409
}
#Debug simulation 
Total elapsed time: 12.612353906035423. Arrivals time: 0.4246082562021911 Scheduler time: 12.084191292058676 Scheduler overhead time: 0.03885463112965226 Adapter cache time: 0.009361637756228447 Engine time: 0.0389963467605412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 536675938 . Total output tokens: 481710298
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.828224617056549,
    "estimated_duration": 3600.0948156585246,
    "input_throughput": 5407.282584705807,
    "output_throughput": 4769.732154084673,
    "total_throughput": 10177.01473879048,
    "itl": 180.045310334346,
    "ttft": 2089750.9934792228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 307,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9395697373873542,
    "arrivals": 802767,
    "finished_requests": 78884,
    "scheduler_time": 78.43054507009762
}
#Debug simulation 
Total elapsed time: 8.828320357017219. Arrivals time: 0.323849817737937 Scheduler time: 8.407752931118011 Scheduler overhead time: 0.03528077341616154 Adapter cache time: 0.00985944177955389 Engine time: 0.035753590520471334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 536675938 . Total output tokens: 481710298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.85099233686924,
    "estimated_duration": 3600.167118853637,
    "input_throughput": 5407.173988689332,
    "output_throughput": 4769.636362177469,
    "total_throughput": 10176.810350866801,
    "itl": 180.04722854122602,
    "ttft": 2089779.124450221,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 307,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0033418620890062,
    "arrivals": 802767,
    "finished_requests": 78884,
    "scheduler_time": 78.43084135647847
}
#Debug simulation 
Total elapsed time: 8.851114469114691. Arrivals time: 0.3123752558603883 Scheduler time: 8.440806975588202 Scheduler overhead time: 0.0358096226118505 Adapter cache time: 0.009688780177384615 Engine time: 0.036411937326192856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 536675938 . Total output tokens: 481710298
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.758427241817117,
    "estimated_duration": 3600.1157811126263,
    "input_throughput": 5398.700259021465,
    "output_throughput": 4761.161874272445,
    "total_throughput": 10159.862133293911,
    "itl": 178.59808615182055,
    "ttft": 2090458.926585211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0112692981213385,
    "arrivals": 802767,
    "finished_requests": 78747,
    "scheduler_time": 78.37034393148109
}
#Debug simulation 
Total elapsed time: 8.758587011136115. Arrivals time: 0.31888673501089215 Scheduler time: 8.341998847201467 Scheduler overhead time: 0.03571586590260267 Adapter cache time: 0.00982319051399827 Engine time: 0.03610882721841335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 536675938 . Total output tokens: 481710298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 8.798111218959093,
    "estimated_duration": 3600.117620613231,
    "input_throughput": 5407.248332259796,
    "output_throughput": 4769.701940203574,
    "total_throughput": 10176.95027246337,
    "itl": 180.04525917070566,
    "ttft": 2089755.2371986276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 307,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9539018285577212,
    "arrivals": 802767,
    "finished_requests": 78884,
    "scheduler_time": 78.43078314959347
}
#Debug simulation 
Total elapsed time: 8.798210044857115. Arrivals time: 0.31249565724283457 Scheduler time: 8.388878223951906 Scheduler overhead time: 0.03530071210116148 Adapter cache time: 0.009750797413289547 Engine time: 0.03614067239686847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 536675938 . Total output tokens: 481710298
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 8.727975165005773,
    "estimated_duration": 3600.1310169838503,
    "input_throughput": 5398.677411546877,
    "output_throughput": 4761.1417248809785,
    "total_throughput": 10159.819136427855,
    "itl": 178.598694887988,
    "ttft": 2090466.2498421427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0264855062775367,
    "arrivals": 802767,
    "finished_requests": 78747,
    "scheduler_time": 78.37036359455962
}
#Debug simulation 
Total elapsed time: 8.728087022900581. Arrivals time: 0.32001994317397475 Scheduler time: 8.31051793275401 Scheduler overhead time: 0.035599942319095135 Adapter cache time: 0.009746932424604893 Engine time: 0.036261618603020906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 536675938 . Total output tokens: 481710298
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.798602710943669,
    "estimated_duration": 3600.0731582943495,
    "input_throughput": 5407.315113902571,
    "output_throughput": 4769.760847897754,
    "total_throughput": 10177.075961800325,
    "itl": 180.04449001315817,
    "ttft": 2089740.0795534982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 307,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9179454405349692,
    "arrivals": 802767,
    "finished_requests": 78884,
    "scheduler_time": 78.43051200274539
}
#Debug simulation 
Total elapsed time: 8.798699161037803. Arrivals time: 0.3095750682987273 Scheduler time: 8.392168851569295 Scheduler overhead time: 0.03544805012643337 Adapter cache time: 0.00974754523485899 Engine time: 0.03593197138980031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 17280, 17280, 17280, 4320, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 4320, 34560, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 17280, 4320, 4320, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 17280, 4320, 34560, 34560, 4320, 4320, 17280, 17280, 17280, 4320, 34560, 4320, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 17280, 4320]
Prompts retrieved: 2410560 . Total input tokens: 536675938 . Total output tokens: 481710298
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.769971909932792,
    "estimated_duration": 3600.1422306376326,
    "input_throughput": 5398.660595850303,
    "output_throughput": 4761.1268949683,
    "total_throughput": 10159.787490818602,
    "itl": 178.59907380731735,
    "ttft": 2090472.1845911995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0376775932684594,
    "arrivals": 802767,
    "finished_requests": 78747,
    "scheduler_time": 78.37038516135797
}
#Debug simulation 
Total elapsed time: 8.770097095053643. Arrivals time: 0.3097867267206311 Scheduler time: 8.362652148120105 Scheduler overhead time: 0.03572206711396575 Adapter cache time: 0.009918254800140858 Engine time: 0.0360338194295764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.18694030912593,
    "estimated_duration": 3600.1671522396814,
    "input_throughput": 5468.696082000488,
    "output_throughput": 4759.492900028336,
    "total_throughput": 10228.188982028825,
    "itl": 179.29689510801578,
    "ttft": 2080919.7777699146,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 588,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7995667934324857,
    "arrivals": 757465,
    "finished_requests": 79126,
    "scheduler_time": 78.3568277002105
}
#Debug simulation 
Total elapsed time: 6.187036680988967. Arrivals time: 0.2933103162795305 Scheduler time: 5.795816235244274 Scheduler overhead time: 0.03385192155838013 Adapter cache time: 0.013766617979854345 Engine time: 0.034780402667820454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.236971018835902,
    "estimated_duration": 3600.12450536345,
    "input_throughput": 5468.61086906005,
    "output_throughput": 4759.315955454776,
    "total_throughput": 10227.926824514825,
    "itl": 179.30634066267442,
    "ttft": 2080944.5344178677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 592,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9310963300475903,
    "arrivals": 757465,
    "finished_requests": 79121,
    "scheduler_time": 78.35359871555202
}
#Debug simulation 
Total elapsed time: 6.2371023166924715. Arrivals time: 0.3368802275508642 Scheduler time: 5.801824964117259 Scheduler overhead time: 0.034042655024677515 Adapter cache time: 0.013982343021780252 Engine time: 0.03477860055863857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.156179670244455,
    "estimated_duration": 3600.1143764005965,
    "input_throughput": 5460.561789054815,
    "output_throughput": 4751.793196388283,
    "total_throughput": 10212.354985443098,
    "itl": 178.08376079598855,
    "ttft": 2081132.600556435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.95762954073028,
    "arrivals": 757465,
    "finished_requests": 78988,
    "scheduler_time": 78.30218982395715
}
#Debug simulation 
Total elapsed time: 6.156276499386877. Arrivals time: 0.2893640170805156 Scheduler time: 5.768540871795267 Scheduler overhead time: 0.034031305462121964 Adapter cache time: 0.014183943625539541 Engine time: 0.03449859609827399 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.064617405645549,
    "estimated_duration": 3600.1926574966824,
    "input_throughput": 5468.546234325557,
    "output_throughput": 4759.367797809523,
    "total_throughput": 10227.914032135079,
    "itl": 179.3013439799736,
    "ttft": 2080974.487439807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 593,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.837249289872109,
    "arrivals": 757465,
    "finished_requests": 79124,
    "scheduler_time": 78.35696598605148
}
#Debug simulation 
Total elapsed time: 6.0647189216688275. Arrivals time: 0.27663806453347206 Scheduler time: 5.691886697430164 Scheduler overhead time: 0.0332033121958375 Adapter cache time: 0.013199919369071722 Engine time: 0.03439342603087425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.069857731927186,
    "estimated_duration": 3600.144703273818,
    "input_throughput": 5460.515790413442,
    "output_throughput": 4751.753168266716,
    "total_throughput": 10212.268958680159,
    "itl": 178.0851442551876,
    "ttft": 2081145.357187369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9879362032562506,
    "arrivals": 757465,
    "finished_requests": 78988,
    "scheduler_time": 78.30221003468112
}
#Debug simulation 
Total elapsed time: 6.069953997153789. Arrivals time: 0.2837007949128747 Scheduler time: 5.689335109200329 Scheduler overhead time: 0.033714499324560165 Adapter cache time: 0.013722435105592012 Engine time: 0.034042634069919586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.0492475922219455,
    "estimated_duration": 3600.125572066926,
    "input_throughput": 5468.75924349952,
    "output_throughput": 4759.547870482297,
    "total_throughput": 10228.307113981818,
    "itl": 179.29508647165846,
    "ttft": 2080901.3327089024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 588,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7581495734024537,
    "arrivals": 757465,
    "finished_requests": 79126,
    "scheduler_time": 78.3567977910023
}
#Debug simulation 
Total elapsed time: 6.049369410146028. Arrivals time: 0.27635382302105427 Scheduler time: 5.677772840950638 Scheduler overhead time: 0.033247777726501226 Adapter cache time: 0.013232823926955462 Engine time: 0.03352578077465296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 17280, 17280, 17280, 1080, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 1080, 34560, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 17280, 1080, 1080, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 17280, 1080, 34560, 34560, 1080, 1080, 17280, 17280, 17280, 1080, 34560, 1080, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 17280, 1080]
Prompts retrieved: 2274480 . Total input tokens: 506350051 . Total output tokens: 454613738
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.09779418213293,
    "estimated_duration": 3600.16539518909,
    "input_throughput": 5460.484406152534,
    "output_throughput": 4751.725857612021,
    "total_throughput": 10212.210263764555,
    "itl": 178.08590362908328,
    "ttft": 2081155.3270411063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.007805301509793,
    "arrivals": 757465,
    "finished_requests": 78988,
    "scheduler_time": 78.30224819263877
}
#Debug simulation 
Total elapsed time: 6.097953832242638. Arrivals time: 0.3247442743740976 Scheduler time: 5.677005806006491 Scheduler overhead time: 0.03342275181785226 Adapter cache time: 0.013639437966048717 Engine time: 0.03364811325445771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.867928749881685,
    "estimated_duration": 3600.0768370487785,
    "input_throughput": 5460.388177746455,
    "output_throughput": 4803.57136326468,
    "total_throughput": 10263.959541011134,
    "itl": 178.42455883921286,
    "ttft": 2077289.1474397352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.096434104593973,
    "arrivals": 749804,
    "finished_requests": 79455,
    "scheduler_time": 78.92236347482417
}
#Debug simulation 
Total elapsed time: 5.868021247908473. Arrivals time: 0.27613182831555605 Scheduler time: 5.495605473872274 Scheduler overhead time: 0.03271398553624749 Adapter cache time: 0.015065131708979607 Engine time: 0.033536669332534075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.852262119296938,
    "estimated_duration": 3600.013214723941,
    "input_throughput": 5460.319123163933,
    "output_throughput": 4803.615422652298,
    "total_throughput": 10263.93454581623,
    "itl": 178.43099210068294,
    "ttft": 2077264.7392403046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2246976295742225,
    "arrivals": 749804,
    "finished_requests": 79453,
    "scheduler_time": 78.91818024417292
}
#Debug simulation 
Total elapsed time: 5.852358586154878. Arrivals time: 0.27928185230121017 Scheduler time: 5.476470573339611 Scheduler overhead time: 0.03272848716005683 Adapter cache time: 0.01511775329709053 Engine time: 0.03375650476664305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.841947036329657,
    "estimated_duration": 3600.1346256621437,
    "input_throughput": 5450.629223731015,
    "output_throughput": 4794.697919616054,
    "total_throughput": 10245.327143347069,
    "itl": 176.76902239554354,
    "ttft": 2078486.556646761,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 692,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2528177034668664,
    "arrivals": 749804,
    "finished_requests": 79317,
    "scheduler_time": 78.88239135895888
}
#Debug simulation 
Total elapsed time: 5.842039030045271. Arrivals time: 0.2782532977871597 Scheduler time: 5.467271379195154 Scheduler overhead time: 0.032826648093760014 Adapter cache time: 0.015192436054348946 Engine time: 0.03341687470674515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.890957051888108,
    "estimated_duration": 3600.102171193671,
    "input_throughput": 5460.349752652197,
    "output_throughput": 4803.5375602315635,
    "total_throughput": 10263.88731288376,
    "itl": 178.42550333067345,
    "ttft": 2077302.2325588872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1217316093272305,
    "arrivals": 749804,
    "finished_requests": 79455,
    "scheduler_time": 78.92240011491083
}
#Debug simulation 
Total elapsed time: 5.891081633046269. Arrivals time: 0.323004596401006 Scheduler time: 5.4726540092378855 Scheduler overhead time: 0.03246103087440133 Adapter cache time: 0.015023066196590662 Engine time: 0.03296130150556564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.797254918143153,
    "estimated_duration": 3600.1665828751866,
    "input_throughput": 5450.580840714477,
    "output_throughput": 4794.655359034657,
    "total_throughput": 10245.236199749133,
    "itl": 176.7704791190925,
    "ttft": 2078499.33100809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 692,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.284759165216242,
    "arrivals": 749804,
    "finished_requests": 79317,
    "scheduler_time": 78.88240711028439
}
#Debug simulation 
Total elapsed time: 5.797350814100355. Arrivals time: 0.2719360659830272 Scheduler time: 5.429013791959733 Scheduler overhead time: 0.03265970107167959 Adapter cache time: 0.015358471777290106 Engine time: 0.03334803879261017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.894590322859585,
    "estimated_duration": 3600.0285572236266,
    "input_throughput": 5460.461406772917,
    "output_throughput": 4803.63578374964,
    "total_throughput": 10264.097190522556,
    "itl": 178.42241127735548,
    "ttft": 2077269.2322783724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 685,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0481844520079546,
    "arrivals": 749804,
    "finished_requests": 79455,
    "scheduler_time": 78.92233330216918
}
#Debug simulation 
Total elapsed time: 5.894699726719409. Arrivals time: 0.28605590714141726 Scheduler time: 5.513013216201216 Scheduler overhead time: 0.03238771576434374 Adapter cache time: 0.015034074895083904 Engine time: 0.0332329198718071 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 540, 540, 34560, 540, 17280, 17280, 17280, 540, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 17280, 540, 34560, 34560, 540, 17280, 17280, 540, 17280, 540, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 540, 17280, 34560, 34560, 17280, 540, 540, 540, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 540, 540, 17280, 540, 34560, 34560, 540, 540, 17280, 17280, 17280, 540, 34560, 540, 34560, 17280, 540, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 17280, 34560, 34560, 17280, 34560, 540, 17280, 34560, 540, 34560, 17280, 540, 17280, 17280, 34560, 34560, 540, 540, 34560, 540, 540, 540, 17280, 540]
Prompts retrieved: 2251800 . Total input tokens: 501261240 . Total output tokens: 450084141
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.820403462741524,
    "estimated_duration": 3600.189364051819,
    "input_throughput": 5450.546350682891,
    "output_throughput": 4794.625019549818,
    "total_throughput": 10245.171370232709,
    "itl": 176.7714188467081,
    "ttft": 2078509.4712075256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 692,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.307520600557333,
    "arrivals": 749804,
    "finished_requests": 79317,
    "scheduler_time": 78.88242685159798
}
#Debug simulation 
Total elapsed time: 5.820497798733413. Arrivals time: 0.27267982717603445 Scheduler time: 5.451393582392484 Scheduler overhead time: 0.03282689955085516 Adapter cache time: 0.015177981462329626 Engine time: 0.03335818601772189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.36322697577998,
    "estimated_duration": 3600.0794892538315,
    "input_throughput": 5532.677558774767,
    "output_throughput": 4903.705335589407,
    "total_throughput": 10436.382894364175,
    "itl": 175.82098707057702,
    "ttft": 2066610.7528711108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1844087569019808,
    "arrivals": 746017,
    "finished_requests": 80695,
    "scheduler_time": 80.56346919936247
}
#Debug simulation 
Total elapsed time: 6.363300216849893. Arrivals time: 0.6046398896723986 Scheduler time: 5.666351984720677 Scheduler overhead time: 0.03258567536249757 Adapter cache time: 0.011140421032905579 Engine time: 0.033465941436588764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.019503023009747,
    "estimated_duration": 3600.156555708299,
    "input_throughput": 5532.559123968789,
    "output_throughput": 4903.600364825464,
    "total_throughput": 10436.159488794252,
    "itl": 175.82406491944982,
    "ttft": 2066644.7410040323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2613412703294362,
    "arrivals": 746017,
    "finished_requests": 80695,
    "scheduler_time": 80.5635435955785
}
#Debug simulation 
Total elapsed time: 6.019630649127066. Arrivals time: 0.35839638160541654 Scheduler time: 5.568333983886987 Scheduler overhead time: 0.032835000194609165 Adapter cache time: 0.01135359751060605 Engine time: 0.033429221250116825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.022259275894612,
    "estimated_duration": 3600.09994594837,
    "input_throughput": 5520.188966521822,
    "output_throughput": 4891.542808365285,
    "total_throughput": 10411.731774887106,
    "itl": 173.82186198029865,
    "ttft": 2068190.8360141038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 384,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2540348540432822,
    "arrivals": 746017,
    "finished_requests": 80509,
    "scheduler_time": 80.50310942359833
}
#Debug simulation 
Total elapsed time: 6.0223567257635295. Arrivals time: 0.3552000089548528 Scheduler time: 5.573872436303645 Scheduler overhead time: 0.032994160894304514 Adapter cache time: 0.01142326695844531 Engine time: 0.033639737870544195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.054137935861945,
    "estimated_duration": 3600.0972080701436,
    "input_throughput": 5532.650328260781,
    "output_throughput": 4903.681200726077,
    "total_throughput": 10436.331528986859,
    "itl": 175.8215799783337,
    "ttft": 2066620.6091603944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2020949491555786,
    "arrivals": 746017,
    "finished_requests": 80695,
    "scheduler_time": 80.5635018233827
}
#Debug simulation 
Total elapsed time: 6.054247668944299. Arrivals time: 0.3679778897203505 Scheduler time: 5.593539064284414 Scheduler overhead time: 0.032744110096246004 Adapter cache time: 0.011309169232845306 Engine time: 0.0335175059735775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.020517075899988,
    "estimated_duration": 3600.094534187989,
    "input_throughput": 5520.1972646205695,
    "output_throughput": 4891.550161465966,
    "total_throughput": 10411.747426086535,
    "itl": 173.82908424838834,
    "ttft": 2068185.8894109942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 384,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2722691530734338,
    "arrivals": 746017,
    "finished_requests": 80509,
    "scheduler_time": 80.50279970493862
}
#Debug simulation 
Total elapsed time: 6.020615508314222. Arrivals time: 0.2770351921208203 Scheduler time: 5.650085440836847 Scheduler overhead time: 0.032990286592394114 Adapter cache time: 0.011581426486372948 Engine time: 0.033656186424195766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.4043042827397585,
    "estimated_duration": 3600.0191759390996,
    "input_throughput": 5532.752473409866,
    "output_throughput": 4903.703046358061,
    "total_throughput": 10436.455519767927,
    "itl": 175.8194743287699,
    "ttft": 2066602.0375812072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1571494641271378,
    "arrivals": 746017,
    "finished_requests": 80694,
    "scheduler_time": 80.56277520189691
}
#Debug simulation 
Total elapsed time: 6.404376337770373. Arrivals time: 0.6070518721826375 Scheduler time: 5.704406259581447 Scheduler overhead time: 0.03275578701868653 Adapter cache time: 0.011487720534205437 Engine time: 0.033567295875400305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 270, 270, 34560, 270, 17280, 17280, 17280, 270, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 17280, 270, 34560, 34560, 270, 17280, 17280, 270, 17280, 270, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 270, 17280, 34560, 34560, 17280, 270, 270, 270, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 270, 270, 17280, 270, 34560, 34560, 270, 270, 17280, 17280, 17280, 270, 34560, 270, 34560, 17280, 270, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 17280, 34560, 34560, 17280, 34560, 270, 17280, 34560, 270, 34560, 17280, 270, 17280, 17280, 34560, 34560, 270, 270, 34560, 270, 270, 270, 17280, 270]
Prompts retrieved: 2240460 . Total input tokens: 498739200 . Total output tokens: 447846647
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.97440174780786,
    "estimated_duration": 3600.108134660188,
    "input_throughput": 5520.176410444355,
    "output_throughput": 4891.53168218993,
    "total_throughput": 10411.708092634284,
    "itl": 173.82960713688573,
    "ttft": 2068192.5866628098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 384,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2858505620062375,
    "arrivals": 746017,
    "finished_requests": 80509,
    "scheduler_time": 80.50281876821504
}
#Debug simulation 
Total elapsed time: 5.97452801419422. Arrivals time: 0.3581285113468766 Scheduler time: 5.522591186221689 Scheduler overhead time: 0.033245029393583536 Adapter cache time: 0.011514073237776756 Engine time: 0.033665535505861044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.325193557888269,
    "estimated_duration": 3600.1837200945515,
    "input_throughput": 5630.124064742913,
    "output_throughput": 4950.380698774988,
    "total_throughput": 10580.504763517902,
    "itl": 173.0568117903775,
    "ttft": 2060255.8140663968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7620614482392547,
    "arrivals": 744176,
    "finished_requests": 81808,
    "scheduler_time": 81.36656507618594
}
#Debug simulation 
Total elapsed time: 6.325259878765792. Arrivals time: 0.6022704299539328 Scheduler time: 5.631502757314593 Scheduler overhead time: 0.033209413290023804 Adapter cache time: 0.009134159423410892 Engine time: 0.03391841892153025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.9936642539687455,
    "estimated_duration": 3600.0460255872845,
    "input_throughput": 5630.016354219688,
    "output_throughput": 4950.218378692204,
    "total_throughput": 10580.234732911891,
    "itl": 173.05773887648098,
    "ttft": 2060240.8415711785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8115321556548623,
    "arrivals": 744176,
    "finished_requests": 81804,
    "scheduler_time": 81.36237441320395
}
#Debug simulation 
Total elapsed time: 5.993762505240738. Arrivals time: 0.28045549243688583 Scheduler time: 5.6220063348300755 Scheduler overhead time: 0.033093944657593966 Adapter cache time: 0.009148271288722754 Engine time: 0.03385944850742817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.958153727930039,
    "estimated_duration": 3600.14854994285,
    "input_throughput": 5617.569030678204,
    "output_throughput": 4940.377529777859,
    "total_throughput": 10557.946560456065,
    "itl": 171.1451537211055,
    "ttft": 2061252.2168043547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8101012635976115,
    "arrivals": 744176,
    "finished_requests": 81628,
    "scheduler_time": 81.29341695180895
}
#Debug simulation 
Total elapsed time: 5.958315975032747. Arrivals time: 0.27756081661209464 Scheduler time: 5.5883619911037385 Scheduler overhead time: 0.033411319367587566 Adapter cache time: 0.009276214987039566 Engine time: 0.034209050703793764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.319122058805078,
    "estimated_duration": 3600.006747367273,
    "input_throughput": 5630.077781054843,
    "output_throughput": 4950.272388526137,
    "total_throughput": 10580.350169580981,
    "itl": 173.05619525648396,
    "ttft": 2060222.201836606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.772307005084587,
    "arrivals": 744176,
    "finished_requests": 81804,
    "scheduler_time": 81.36232134375459
}
#Debug simulation 
Total elapsed time: 6.319189211819321. Arrivals time: 0.6022046450525522 Scheduler time: 5.625514612998813 Scheduler overhead time: 0.03325410978868604 Adapter cache time: 0.009135713800787926 Engine time: 0.033885689452290535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.017533425241709,
    "estimated_duration": 3600.160637353544,
    "input_throughput": 5617.550169890918,
    "output_throughput": 4940.360942636839,
    "total_throughput": 10557.911112527758,
    "itl": 171.1456268999318,
    "ttft": 2061257.9441802842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.822173627093438,
    "arrivals": 744176,
    "finished_requests": 81628,
    "scheduler_time": 81.29343199901892
}
#Debug simulation 
Total elapsed time: 6.017625073902309. Arrivals time: 0.2851525959558785 Scheduler time: 5.6393188699148595 Scheduler overhead time: 0.03389920247718692 Adapter cache time: 0.009401957504451275 Engine time: 0.03437700727954507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.003375409170985,
    "estimated_duration": 3600.166152379265,
    "input_throughput": 5630.151538035093,
    "output_throughput": 4950.404855126388,
    "total_throughput": 10580.55639316148,
    "itl": 173.05617118591147,
    "ttft": 2060246.8185365538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7445225234306427,
    "arrivals": 744176,
    "finished_requests": 81808,
    "scheduler_time": 81.36653628568368
}
#Debug simulation 
Total elapsed time: 6.003470275085419. Arrivals time: 0.27880787570029497 Scheduler time: 5.633096543140709 Scheduler overhead time: 0.03325440641492605 Adapter cache time: 0.009103824384510517 Engine time: 0.03390727378427982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 135, 135, 34560, 135, 17280, 17280, 17280, 135, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 17280, 135, 34560, 34560, 135, 17280, 17280, 135, 17280, 135, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 135, 17280, 34560, 34560, 17280, 135, 135, 135, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 135, 135, 17280, 135, 34560, 34560, 135, 135, 17280, 17280, 17280, 135, 34560, 135, 34560, 17280, 135, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 17280, 34560, 34560, 17280, 34560, 135, 17280, 34560, 135, 34560, 17280, 135, 17280, 17280, 34560, 34560, 135, 135, 34560, 135, 135, 135, 17280, 135]
Prompts retrieved: 2234790 . Total input tokens: 497488897 . Total output tokens: 446728565
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.303924114909023,
    "estimated_duration": 3600.1692076221357,
    "input_throughput": 5617.536797210079,
    "output_throughput": 4940.349182017331,
    "total_throughput": 10557.88597922741,
    "itl": 171.14589641185472,
    "ttft": 2061262.7057675845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8307248845696484,
    "arrivals": 744176,
    "finished_requests": 81628,
    "scheduler_time": 81.29345101013861
}
#Debug simulation 
Total elapsed time: 6.303993397857994. Arrivals time: 0.5999136012978852 Scheduler time: 5.611621384508908 Scheduler overhead time: 0.03344882000237703 Adapter cache time: 0.009320131968706846 Engine time: 0.03436020947992802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.470849942881614,
    "estimated_duration": 3600.0791568456734,
    "input_throughput": 5627.963752261613,
    "output_throughput": 4986.4345248810705,
    "total_throughput": 10614.398277142684,
    "itl": 172.82568920383616,
    "ttft": 2057152.601846907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4590731615899125,
    "arrivals": 743225,
    "finished_requests": 82084,
    "scheduler_time": 81.92951256159391
}
#Debug simulation 
Total elapsed time: 6.47095451829955. Arrivals time: 0.6114700073376298 Scheduler time: 5.769534063991159 Scheduler overhead time: 0.033123370725661516 Adapter cache time: 0.007382363546639681 Engine time: 0.03406828036531806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.436138263903558,
    "estimated_duration": 3600.14491473109,
    "input_throughput": 5627.860955567503,
    "output_throughput": 4986.343445939003,
    "total_throughput": 10614.204401506506,
    "itl": 172.82673450204302,
    "ttft": 2057167.272652131,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48936707607936114,
    "arrivals": 743225,
    "finished_requests": 82084,
    "scheduler_time": 81.93056916380064
}
#Debug simulation 
Total elapsed time: 6.436209371313453. Arrivals time: 0.6918840715661645 Scheduler time: 5.654203921556473 Scheduler overhead time: 0.033170231617987156 Adapter cache time: 0.00734119676053524 Engine time: 0.03432087739929557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.4459054614417255,
    "estimated_duration": 3600.1314197985166,
    "input_throughput": 5614.380044251608,
    "output_throughput": 4975.080882186905,
    "total_throughput": 10589.460926438513,
    "itl": 170.87877999027606,
    "ttft": 2058732.6146196474,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4902525767683999,
    "arrivals": 743225,
    "finished_requests": 81897,
    "scheduler_time": 81.85209029929479
}
#Debug simulation 
Total elapsed time: 6.446009441278875. Arrivals time: 0.7052652821876109 Scheduler time: 5.649811612442136 Scheduler overhead time: 0.03363112173974514 Adapter cache time: 0.007413502316921949 Engine time: 0.03448466071859002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.445078152231872,
    "estimated_duration": 3600.0869094411164,
    "input_throughput": 5627.95163274138,
    "output_throughput": 4986.423786859865,
    "total_throughput": 10614.375419601245,
    "itl": 172.82580930446412,
    "ttft": 2057158.1621698637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46730292888358266,
    "arrivals": 743225,
    "finished_requests": 82084,
    "scheduler_time": 81.92951932332306
}
#Debug simulation 
Total elapsed time: 6.445149405859411. Arrivals time: 0.6932088788598776 Scheduler time: 5.662149631418288 Scheduler overhead time: 0.0332157788798213 Adapter cache time: 0.007331831380724907 Engine time: 0.03390711545944214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.448599544819444,
    "estimated_duration": 3600.138223985488,
    "input_throughput": 5614.369433189151,
    "output_throughput": 4975.0714793866755,
    "total_throughput": 10589.440912575827,
    "itl": 170.87900302569537,
    "ttft": 2058736.4612895113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4970432812348013,
    "arrivals": 743225,
    "finished_requests": 81897,
    "scheduler_time": 81.85210378180355
}
#Debug simulation 
Total elapsed time: 6.448699546046555. Arrivals time: 0.7030239296145737 Scheduler time: 5.655009028501809 Scheduler overhead time: 0.03339039022102952 Adapter cache time: 0.007470856420695782 Engine time: 0.03435830073431134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.443291782867163,
    "estimated_duration": 3600.015893458915,
    "input_throughput": 5628.0009865548745,
    "output_throughput": 4986.41048574717,
    "total_throughput": 10614.411472302045,
    "itl": 172.82474652155548,
    "ttft": 2057141.4864604275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4485075442353269,
    "arrivals": 743225,
    "finished_requests": 82083,
    "scheduler_time": 81.92810877646407
}
#Debug simulation 
Total elapsed time: 6.443364271894097. Arrivals time: 0.6025330722332001 Scheduler time: 5.75103111192584 Scheduler overhead time: 0.03317191544920206 Adapter cache time: 0.007392322178930044 Engine time: 0.033826840575784445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 66, 66, 34560, 66, 17280, 17280, 17280, 66, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 17280, 66, 34560, 34560, 66, 17280, 17280, 66, 17280, 66, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 66, 17280, 34560, 34560, 17280, 66, 66, 66, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 66, 66, 17280, 66, 34560, 34560, 66, 66, 17280, 17280, 17280, 66, 34560, 66, 34560, 17280, 66, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 17280, 34560, 34560, 17280, 34560, 66, 17280, 34560, 66, 34560, 17280, 66, 17280, 17280, 34560, 34560, 66, 66, 34560, 66, 66, 66, 17280, 66]
Prompts retrieved: 2231892 . Total input tokens: 496841360 . Total output tokens: 446163535
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.4785544578917325,
    "estimated_duration": 3600.152574555022,
    "input_throughput": 5614.3470537490375,
    "output_throughput": 4975.051648252376,
    "total_throughput": 10589.398702001414,
    "itl": 170.88018299402748,
    "ttft": 2058753.7866183775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5028279554098842,
    "arrivals": 743225,
    "finished_requests": 81897,
    "scheduler_time": 81.85237262631955
}
#Debug simulation 
Total elapsed time: 6.478659534826875. Arrivals time: 0.7041253568604589 Scheduler time: 5.683544715400785 Scheduler overhead time: 0.033546057529747486 Adapter cache time: 0.007410341873764992 Engine time: 0.03459056839346886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.10434167413041,
    "estimated_duration": 3600.0935413817638,
    "input_throughput": 5688.076369299121,
    "output_throughput": 4995.014377625886,
    "total_throughput": 10683.090746925007,
    "itl": 171.42873174778623,
    "ttft": 2059488.981147321,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 742746,
    "finished_requests": 82613,
    "scheduler_time": 82.14844157603507
}
#Debug simulation 
Total elapsed time: 6.1044374871999025. Arrivals time: 0.3389788852073252 Scheduler time: 5.675496622920036 Scheduler overhead time: 0.033423565328121185 Adapter cache time: 0.006934176664799452 Engine time: 0.034281347412616014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.1344140861183405,
    "estimated_duration": 3600.1185177910543,
    "input_throughput": 5688.036907341751,
    "output_throughput": 4994.9797238991005,
    "total_throughput": 10683.016631240853,
    "itl": 171.4293531050143,
    "ttft": 2059505.0766163804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4166398491780273,
    "arrivals": 742746,
    "finished_requests": 82613,
    "scheduler_time": 82.1485205673678
}
#Debug simulation 
Total elapsed time: 6.1345220459625125. Arrivals time: 0.3355099963955581 Scheduler time: 5.7085514036007226 Scheduler overhead time: 0.033490967471152544 Adapter cache time: 0.006856096442788839 Engine time: 0.034633913077414036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.3685705228708684,
    "estimated_duration": 3600.139115822849,
    "input_throughput": 5675.5142905998255,
    "output_throughput": 4984.973475364748,
    "total_throughput": 10660.487765964574,
    "itl": 169.7366748137378,
    "ttft": 2060801.784832615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41756195468828194,
    "arrivals": 742746,
    "finished_requests": 82431,
    "scheduler_time": 82.0742145160076
}
#Debug simulation 
Total elapsed time: 6.368637754116207. Arrivals time: 0.6027736156247556 Scheduler time: 5.675166219938546 Scheduler overhead time: 0.033834624104201794 Adapter cache time: 0.0069380453787744045 Engine time: 0.034514445811510086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.085165932774544,
    "estimated_duration": 3600.1009014338756,
    "input_throughput": 5688.064740586583,
    "output_throughput": 4995.004165810405,
    "total_throughput": 10683.068906396988,
    "itl": 171.4288566319853,
    "ttft": 2059494.7471069086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39907025048509265,
    "arrivals": 742746,
    "finished_requests": 82613,
    "scheduler_time": 82.14847380887998
}
#Debug simulation 
Total elapsed time: 6.085263599641621. Arrivals time: 0.3319758060388267 Scheduler time: 5.663245788775384 Scheduler overhead time: 0.03340153954923153 Adapter cache time: 0.006836865097284317 Engine time: 0.0343980579636991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.095199378672987,
    "estimated_duration": 3600.144536666014,
    "input_throughput": 5675.505744811584,
    "output_throughput": 4984.965969344054,
    "total_throughput": 10660.471714155638,
    "itl": 169.73682953338124,
    "ttft": 2060804.960318373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4229693675041202,
    "arrivals": 742746,
    "finished_requests": 82431,
    "scheduler_time": 82.07422794635966
}
#Debug simulation 
Total elapsed time: 6.095325559843332. Arrivals time: 0.32972339214757085 Scheduler time: 5.674367568921298 Scheduler overhead time: 0.033987822476774454 Adapter cache time: 0.0069670104421675205 Engine time: 0.03452283889055252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.381679089274257,
    "estimated_duration": 3600.0844993620967,
    "input_throughput": 5688.090655546683,
    "output_throughput": 4995.026923169816,
    "total_throughput": 10683.117578716498,
    "itl": 171.42850378573957,
    "ttft": 2059483.3079953277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 742746,
    "finished_requests": 82613,
    "scheduler_time": 82.14841554983828
}
#Debug simulation 
Total elapsed time: 6.381745390128344. Arrivals time: 0.2847452783025801 Scheduler time: 6.006562457885593 Scheduler overhead time: 0.033426650799810886 Adapter cache time: 0.006816165987402201 Engine time: 0.03483462939038873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 17280, 34560, 34560, 33, 33, 34560, 33, 17280, 17280, 17280, 33, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 17280, 33, 34560, 34560, 33, 17280, 17280, 33, 17280, 33, 17280, 17280, 34560, 34560, 34560, 34560, 17280, 17280, 33, 17280, 34560, 34560, 17280, 33, 33, 33, 34560, 17280, 17280, 17280, 17280, 34560, 17280, 34560, 33, 33, 17280, 33, 34560, 34560, 33, 33, 17280, 17280, 17280, 33, 34560, 33, 34560, 17280, 33, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 17280, 34560, 34560, 17280, 34560, 33, 17280, 34560, 33, 34560, 17280, 33, 17280, 17280, 34560, 34560, 33, 33, 34560, 33, 33, 33, 17280, 33]
Prompts retrieved: 2230506 . Total input tokens: 496524887 . Total output tokens: 445889420
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.074665171094239,
    "estimated_duration": 3600.1495856721194,
    "input_throughput": 5675.497785235884,
    "output_throughput": 4984.958978211322,
    "total_throughput": 10660.456763447206,
    "itl": 169.73693658772643,
    "ttft": 2060808.479984486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42799951896071414,
    "arrivals": 742746,
    "finished_requests": 82431,
    "scheduler_time": 82.07424680100922
}
#Debug simulation 
Total elapsed time: 6.074761925265193. Arrivals time: 0.33114605164155364 Scheduler time: 5.653001889120787 Scheduler overhead time: 0.03354403376579285 Adapter cache time: 0.006931733805686235 Engine time: 0.034643021412193775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.507077302318066,
    "estimated_duration": 3600.1525193483276,
    "input_throughput": 5409.273883631206,
    "output_throughput": 4766.76943761993,
    "total_throughput": 10176.043321251136,
    "itl": 180.04701965887702,
    "ttft": 2068484.7047320036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6771472836751713,
    "arrivals": 679344,
    "finished_requests": 78724,
    "scheduler_time": 78.37302113238339
}
#Debug simulation 
Total elapsed time: 7.507176190149039. Arrivals time: 0.36087741144001484 Scheduler time: 7.049868640024215 Scheduler overhead time: 0.03353877319023013 Adapter cache time: 0.013702884316444397 Engine time: 0.03383484948426485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.526997365057468,
    "estimated_duration": 3600.069254911124,
    "input_throughput": 5408.943167811566,
    "output_throughput": 4766.4538610170775,
    "total_throughput": 10175.397028828644,
    "itl": 180.02971120241298,
    "ttft": 2068744.9976681173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.761442761633084,
    "arrivals": 679344,
    "finished_requests": 78722,
    "scheduler_time": 78.36907830612947
}
#Debug simulation 
Total elapsed time: 7.527123969979584. Arrivals time: 0.40090652741491795 Scheduler time: 7.029468750581145 Scheduler overhead time: 0.033872179221361876 Adapter cache time: 0.013267927337437868 Engine time: 0.034254050347954035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.41288484679535,
    "estimated_duration": 3600.015922820701,
    "input_throughput": 5397.270572285668,
    "output_throughput": 4756.413962353694,
    "total_throughput": 10153.684534639362,
    "itl": 178.2026915828284,
    "ttft": 2069938.2675678977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8379454373382145,
    "arrivals": 679344,
    "finished_requests": 78538,
    "scheduler_time": 78.29206409526826
}
#Debug simulation 
Total elapsed time: 7.412982239853591. Arrivals time: 0.4007945586927235 Scheduler time: 6.913799114990979 Scheduler overhead time: 0.03417677525430918 Adapter cache time: 0.013703383971005678 Engine time: 0.0350151639431715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.507926059886813,
    "estimated_duration": 3600.1907225158957,
    "input_throughput": 5409.3309218881295,
    "output_throughput": 4766.5152550606945,
    "total_throughput": 10175.846176948824,
    "itl": 180.0270765088825,
    "ttft": 2068750.1622533128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 541,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.678906507308118,
    "arrivals": 679344,
    "finished_requests": 78726,
    "scheduler_time": 78.37349392109617
}
#Debug simulation 
Total elapsed time: 7.50804841471836. Arrivals time: 0.3288989355787635 Scheduler time: 7.082907022442669 Scheduler overhead time: 0.033539116848260164 Adapter cache time: 0.0133373006246984 Engine time: 0.034028429072350264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.418573065660894,
    "estimated_duration": 3600.042725614975,
    "input_throughput": 5397.230388892353,
    "output_throughput": 4756.378550222607,
    "total_throughput": 10153.608939114962,
    "itl": 178.2039021281213,
    "ttft": 2069949.7688149912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.864730993844571,
    "arrivals": 679344,
    "finished_requests": 78538,
    "scheduler_time": 78.29208133305993
}
#Debug simulation 
Total elapsed time: 7.4186735530383885. Arrivals time: 0.40288663981482387 Scheduler time: 6.918389875907451 Scheduler overhead time: 0.03416986111551523 Adapter cache time: 0.013743281830102205 Engine time: 0.03404863551259041 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.504233707673848,
    "estimated_duration": 3600.1138864285817,
    "input_throughput": 5409.331930695945,
    "output_throughput": 4766.8205899520335,
    "total_throughput": 10176.152520647978,
    "itl": 180.04530229379188,
    "ttft": 2068467.7680182445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.638547561606371,
    "arrivals": 679344,
    "finished_requests": 78724,
    "scheduler_time": 78.37298793463938
}
#Debug simulation 
Total elapsed time: 7.5043448456563056. Arrivals time: 0.36329990392550826 Scheduler time: 7.044959731865674 Scheduler overhead time: 0.033383538480848074 Adapter cache time: 0.013395870570093393 Engine time: 0.03403886780142784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 8640, 8640, 8640, 4320, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 4320, 34560, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 8640, 4320, 4320, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 8640, 4320, 34560, 34560, 4320, 4320, 8640, 8640, 8640, 4320, 34560, 4320, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 34560, 4320, 4320, 34560, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 2039040 . Total input tokens: 453806766 . Total output tokens: 407791679
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.378829385153949,
    "estimated_duration": 3600.0623631034246,
    "input_throughput": 5397.2009482775165,
    "output_throughput": 4756.352605302931,
    "total_throughput": 10153.553553580448,
    "itl": 178.2047031287857,
    "ttft": 2069958.8123218978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8843485845252832,
    "arrivals": 679344,
    "finished_requests": 78538,
    "scheduler_time": 78.29210123084364
}
#Debug simulation 
Total elapsed time: 7.378923753742129. Arrivals time: 0.39669751189649105 Scheduler time: 6.885208328720182 Scheduler overhead time: 0.0336224720813334 Adapter cache time: 0.013754456769675016 Engine time: 0.034226685762405396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.165329162962735,
    "estimated_duration": 3600.1444724736057,
    "input_throughput": 5385.467763375195,
    "output_throughput": 4771.196025974464,
    "total_throughput": 10156.663789349659,
    "itl": 180.4736616848642,
    "ttft": 2053564.4123737738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 848,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.595293606855029,
    "arrivals": 634153,
    "finished_requests": 78657,
    "scheduler_time": 78.34505309468805
}
#Debug simulation 
Total elapsed time: 6.165421755053103. Arrivals time: 0.2834939220920205 Scheduler time: 5.782665211707354 Scheduler overhead time: 0.033119280356913805 Adapter cache time: 0.01736781792715192 Engine time: 0.03357974113896489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.171815926209092,
    "estimated_duration": 3600.0385357624277,
    "input_throughput": 5385.379297306337,
    "output_throughput": 4770.855319848005,
    "total_throughput": 10156.234617154343,
    "itl": 180.4698156527684,
    "ttft": 2053516.4393913182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 797,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.599217008491983,
    "arrivals": 634153,
    "finished_requests": 78654,
    "scheduler_time": 78.34296302540409
}
#Debug simulation 
Total elapsed time: 6.17191106127575. Arrivals time: 0.3402100084349513 Scheduler time: 5.732946682721376 Scheduler overhead time: 0.033257477916777134 Adapter cache time: 0.016813461668789387 Engine time: 0.033495670184493065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.148536846041679,
    "estimated_duration": 3600.132548756882,
    "input_throughput": 5371.84860226267,
    "output_throughput": 4757.68843730889,
    "total_throughput": 10129.53703957156,
    "itl": 178.54435960386337,
    "ttft": 2054499.3097451525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 863,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.821148570831846,
    "arrivals": 634153,
    "finished_requests": 78445,
    "scheduler_time": 78.26037076716678
}
#Debug simulation 
Total elapsed time: 6.148633582051843. Arrivals time: 0.37138482742011547 Scheduler time: 5.677479687612504 Scheduler overhead time: 0.033274149522185326 Adapter cache time: 0.0174946878105402 Engine time: 0.03366113733500242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.132735348306596,
    "estimated_duration": 3600.027918711195,
    "input_throughput": 5385.38823525033,
    "output_throughput": 4771.162720912758,
    "total_throughput": 10156.550956163088,
    "itl": 180.47202951241988,
    "ttft": 2053548.1893703495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 829,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.56823406921231,
    "arrivals": 634153,
    "finished_requests": 78654,
    "scheduler_time": 78.3433529194179
}
#Debug simulation 
Total elapsed time: 6.132840686943382. Arrivals time: 0.3359247837215662 Scheduler time: 5.698876415379345 Scheduler overhead time: 0.032989750150591135 Adapter cache time: 0.016845167614519596 Engine time: 0.03313820715993643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.101559560280293,
    "estimated_duration": 3600.175429287862,
    "input_throughput": 5371.784619902662,
    "output_throughput": 4757.631770012966,
    "total_throughput": 10129.416389915628,
    "itl": 178.54637408698002,
    "ttft": 2054516.7891054351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 863,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8644078733585845,
    "arrivals": 634153,
    "finished_requests": 78445,
    "scheduler_time": 78.26037826974284
}
#Debug simulation 
Total elapsed time: 6.101658309344202. Arrivals time: 0.3366806674748659 Scheduler time: 5.664862963370979 Scheduler overhead time: 0.03336664056405425 Adapter cache time: 0.017577894032001495 Engine time: 0.0339139886200428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.205686822067946,
    "estimated_duration": 3600.087294736757,
    "input_throughput": 5385.553297095177,
    "output_throughput": 4771.271803634419,
    "total_throughput": 10156.825100729595,
    "itl": 180.47094547044082,
    "ttft": 2053541.6727324224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 848,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.535562650076992,
    "arrivals": 634153,
    "finished_requests": 78657,
    "scheduler_time": 78.3450845095592
}
#Debug simulation 
Total elapsed time: 6.205785533878952. Arrivals time: 0.38070825720205903 Scheduler time: 5.72638523299247 Scheduler overhead time: 0.03281141258776188 Adapter cache time: 0.01759290834888816 Engine time: 0.033160803373903036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 8640, 8640, 8640, 1080, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 1080, 34560, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 8640, 1080, 1080, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 8640, 1080, 34560, 34560, 1080, 1080, 8640, 8640, 8640, 1080, 34560, 1080, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1902960 . Total input tokens: 423443112 . Total output tokens: 380548868
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.128492403775454,
    "estimated_duration": 3600.0034131083858,
    "input_throughput": 5371.630185012214,
    "output_throughput": 4757.209378646882,
    "total_throughput": 10128.839563659096,
    "itl": 178.54586863683778,
    "ttft": 2054548.8963178797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 862,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8906649867445484,
    "arrivals": 634153,
    "finished_requests": 78438,
    "scheduler_time": 78.25605401157411
}
#Debug simulation 
Total elapsed time: 6.128617540933192. Arrivals time: 0.3400498158298433 Scheduler time: 5.688036779407412 Scheduler overhead time: 0.033608723897486925 Adapter cache time: 0.017794291023164988 Engine time: 0.03384983027353883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.924961551092565,
    "estimated_duration": 3600.19639287673,
    "input_throughput": 5419.078258786537,
    "output_throughput": 4793.75126149984,
    "total_throughput": 10212.829520286377,
    "itl": 179.39481901712014,
    "ttft": 2047483.5261632677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 863,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.641200923014022,
    "arrivals": 626696,
    "finished_requests": 79021,
    "scheduler_time": 78.7268709800659
}
#Debug simulation 
Total elapsed time: 5.925059742294252. Arrivals time: 0.33565910207107663 Scheduler time: 5.490735927596688 Scheduler overhead time: 0.03244313271716237 Adapter cache time: 0.018095246516168118 Engine time: 0.03309496585279703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.904187365900725,
    "estimated_duration": 3600.090578506129,
    "input_throughput": 5419.237259329081,
    "output_throughput": 4793.855216598856,
    "total_throughput": 10213.092475927937,
    "itl": 179.39499152581385,
    "ttft": 2047390.3020431953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 866,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8271860307175722,
    "arrivals": 626696,
    "finished_requests": 79020,
    "scheduler_time": 78.72510486584969
}
#Debug simulation 
Total elapsed time: 5.904281241353601. Arrivals time: 0.3353020390495658 Scheduler time: 5.469643355347216 Scheduler overhead time: 0.032748399302363396 Adapter cache time: 0.01836601458489895 Engine time: 0.03325932705774903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.927577776834369,
    "estimated_duration": 3600.1153645807954,
    "input_throughput": 5409.236657133508,
    "output_throughput": 4786.002184684522,
    "total_throughput": 10195.238841818029,
    "itl": 177.72203743057634,
    "ttft": 2048161.8146993255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 871,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.848194701578449,
    "arrivals": 626696,
    "finished_requests": 78889,
    "scheduler_time": 78.69095957103117
}
#Debug simulation 
Total elapsed time: 5.927673545200378. Arrivals time: 0.33433911856263876 Scheduler time: 5.492951970547438 Scheduler overhead time: 0.03308282466605306 Adapter cache time: 0.018509226851165295 Engine time: 0.033591069746762514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.929718032013625,
    "estimated_duration": 3600.038669619815,
    "input_throughput": 5419.315399203849,
    "output_throughput": 4793.924339102329,
    "total_throughput": 10213.239738306178,
    "itl": 179.39327259702006,
    "ttft": 2047309.1648731912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 866,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.679274525442125,
    "arrivals": 626696,
    "finished_requests": 79020,
    "scheduler_time": 78.7243166701533
}
#Debug simulation 
Total elapsed time: 5.929842726327479. Arrivals time: 0.3355492097325623 Scheduler time: 5.495382718741894 Scheduler overhead time: 0.032467983197420835 Adapter cache time: 0.018339086323976517 Engine time: 0.033194190822541714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.921145856380463,
    "estimated_duration": 3600.1607776247024,
    "input_throughput": 5409.168424096988,
    "output_throughput": 4785.941813234252,
    "total_throughput": 10195.110237331239,
    "itl": 177.724178736851,
    "ttft": 2048179.570644279,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 871,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.893591818474244,
    "arrivals": 626696,
    "finished_requests": 78889,
    "scheduler_time": 78.69097876469935
}
#Debug simulation 
Total elapsed time: 5.9212443539872766. Arrivals time: 0.3455533515661955 Scheduler time: 5.475688064005226 Scheduler overhead time: 0.03292508004233241 Adapter cache time: 0.018390033394098282 Engine time: 0.03346954705193639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.935395348351449,
    "estimated_duration": 3600.1186401765835,
    "input_throughput": 5419.195018262803,
    "output_throughput": 4793.817850167708,
    "total_throughput": 10213.012868430511,
    "itl": 179.39161299753536,
    "ttft": 2047440.9839736617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 862,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.577423354205621,
    "arrivals": 626696,
    "finished_requests": 79020,
    "scheduler_time": 78.72625339594767
}
#Debug simulation 
Total elapsed time: 5.935507645364851. Arrivals time: 0.3512560464441776 Scheduler time: 5.485071131493896 Scheduler overhead time: 0.03265610430389643 Adapter cache time: 0.018237529322504997 Engine time: 0.033314998261630535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 540, 540, 34560, 540, 8640, 8640, 8640, 540, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 8640, 540, 34560, 34560, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 540, 8640, 34560, 34560, 8640, 540, 540, 540, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 540, 540, 8640, 540, 34560, 34560, 540, 540, 8640, 8640, 8640, 540, 34560, 540, 34560, 8640, 540, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 8640, 34560, 34560, 8640, 34560, 540, 8640, 34560, 540, 34560, 8640, 540, 8640, 8640, 34560, 34560, 540, 540, 34560, 540, 540, 540, 8640, 540]
Prompts retrieved: 1880280 . Total input tokens: 418335861 . Total output tokens: 375974930
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.973610360175371,
    "estimated_duration": 3600.187764525108,
    "input_throughput": 5409.127877131362,
    "output_throughput": 4785.905937956763,
    "total_throughput": 10195.033815088125,
    "itl": 177.73480747153826,
    "ttft": 2048182.6031621417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 871,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.921760666631188,
    "arrivals": 626696,
    "finished_requests": 78889,
    "scheduler_time": 78.69079066704408
}
#Debug simulation 
Total elapsed time: 5.973709173034877. Arrivals time: 0.34561080299317837 Scheduler time: 5.528100864030421 Scheduler overhead time: 0.03300643293187022 Adapter cache time: 0.018476705998182297 Engine time: 0.03341750241816044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.97309077065438,
    "estimated_duration": 3600.159860793497,
    "input_throughput": 5545.538746048802,
    "output_throughput": 4903.39753860518,
    "total_throughput": 10448.936284653983,
    "itl": 175.60507590437498,
    "ttft": 2042417.4636554124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 447,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3680380215379524,
    "arrivals": 623022,
    "finished_requests": 80957,
    "scheduler_time": 80.47191219031502
}
#Debug simulation 
Total elapsed time: 5.973190242890269. Arrivals time: 0.2835098188370466 Scheduler time: 5.593583609908819 Scheduler overhead time: 0.03280270751565695 Adapter cache time: 0.014643730595707893 Engine time: 0.0334527101367712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.248461592011154,
    "estimated_duration": 3600.1228211654443,
    "input_throughput": 5545.432473200208,
    "output_throughput": 4903.379655887791,
    "total_throughput": 10448.812129087999,
    "itl": 175.6084848901692,
    "ttft": 2042426.1630418212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4613039370114043,
    "arrivals": 623022,
    "finished_requests": 80956,
    "scheduler_time": 80.46921563142025
}
#Debug simulation 
Total elapsed time: 6.248570536263287. Arrivals time: 0.5784472953528166 Scheduler time: 5.573899200651795 Scheduler overhead time: 0.03303137654438615 Adapter cache time: 0.014617408160120249 Engine time: 0.033534391317516565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.9310181410983205,
    "estimated_duration": 3600.1988462060017,
    "input_throughput": 5535.279536295847,
    "output_throughput": 4894.87157593285,
    "total_throughput": 10430.151112228697,
    "itl": 173.91803140569382,
    "ttft": 2043678.2323815364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 447,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4606154312752273,
    "arrivals": 623022,
    "finished_requests": 80814,
    "scheduler_time": 80.43081356411629
}
#Debug simulation 
Total elapsed time: 5.931116132996976. Arrivals time: 0.27579732006415725 Scheduler time: 5.558677447959781 Scheduler overhead time: 0.032661378383636475 Adapter cache time: 0.014668320305645466 Engine time: 0.03411063877865672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.953266816679388,
    "estimated_duration": 3600.0340293489926,
    "input_throughput": 5545.569246635762,
    "output_throughput": 4903.500593629726,
    "total_throughput": 10449.069840265489,
    "itl": 175.60512322741099,
    "ttft": 2042395.9142767433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3865309937368089,
    "arrivals": 623022,
    "finished_requests": 80956,
    "scheduler_time": 80.46891389943654
}
#Debug simulation 
Total elapsed time: 5.953361754771322. Arrivals time: 0.2750971750356257 Scheduler time: 5.582336673513055 Scheduler overhead time: 0.032703814562410116 Adapter cache time: 0.014461403712630272 Engine time: 0.03355177026242018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.997638446744531,
    "estimated_duration": 3600.021597210509,
    "input_throughput": 5535.2151263315845,
    "output_throughput": 4894.708413320005,
    "total_throughput": 10429.92353965159,
    "itl": 173.91795705652768,
    "ttft": 2043654.0128601603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 447,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.483628374189141,
    "arrivals": 623022,
    "finished_requests": 80809,
    "scheduler_time": 80.4263509506301
}
#Debug simulation 
Total elapsed time: 5.997739410027862. Arrivals time: 0.276622848585248 Scheduler time: 5.624213930219412 Scheduler overhead time: 0.033201290760189295 Adapter cache time: 0.014642064459621906 Engine time: 0.03386956173926592 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.992608210071921,
    "estimated_duration": 3600.098701778361,
    "input_throughput": 5545.469625635029,
    "output_throughput": 4903.412506795984,
    "total_throughput": 10448.882132431012,
    "itl": 175.60432068599013,
    "ttft": 2042418.9091942026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 448,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.339542532116164,
    "arrivals": 623022,
    "finished_requests": 80956,
    "scheduler_time": 80.47085417399606
}
#Debug simulation 
Total elapsed time: 5.992704867850989. Arrivals time: 0.28800736553967 Scheduler time: 5.608531718607992 Scheduler overhead time: 0.03277904400601983 Adapter cache time: 0.014495796523988247 Engine time: 0.03374256053939462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 270, 270, 34560, 270, 8640, 8640, 8640, 270, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 8640, 270, 34560, 34560, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 270, 8640, 34560, 34560, 8640, 270, 270, 270, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 270, 270, 8640, 270, 34560, 34560, 270, 270, 8640, 8640, 8640, 270, 34560, 270, 34560, 8640, 270, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 8640, 34560, 34560, 8640, 34560, 270, 8640, 34560, 270, 34560, 8640, 270, 8640, 8640, 34560, 34560, 270, 270, 34560, 270, 270, 270, 8640, 270]
Prompts retrieved: 1868940 . Total input tokens: 415796115 . Total output tokens: 373724270
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.947657451033592,
    "estimated_duration": 3600.0339319851173,
    "input_throughput": 5535.196161057289,
    "output_throughput": 4894.691642610008,
    "total_throughput": 10429.887803667298,
    "itl": 173.91792407601832,
    "ttft": 2043659.4339407978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 447,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4979643058404326,
    "arrivals": 623022,
    "finished_requests": 80809,
    "scheduler_time": 80.42640961416338
}
#Debug simulation 
Total elapsed time: 5.947755080182105. Arrivals time: 0.2793105011805892 Scheduler time: 5.571874972432852 Scheduler overhead time: 0.03305445332080126 Adapter cache time: 0.014474571216851473 Engine time: 0.03379901312291622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.085789121221751,
    "estimated_duration": 3600.013689220565,
    "input_throughput": 5648.749631394549,
    "output_throughput": 4959.645585088239,
    "total_throughput": 10608.39521648279,
    "itl": 172.7215850783601,
    "ttft": 2028299.2906880858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8202107153739769,
    "arrivals": 621138,
    "finished_requests": 82185,
    "scheduler_time": 81.43962651329669
}
#Debug simulation 
Total elapsed time: 6.085892074275762. Arrivals time: 0.34630586905404925 Scheduler time: 5.6451041656546295 Scheduler overhead time: 0.03315829439088702 Adapter cache time: 0.012116056401282549 Engine time: 0.033921878319233656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.101083010900766,
    "estimated_duration": 3600.127698115544,
    "input_throughput": 5648.570746711147,
    "output_throughput": 4959.488522961544,
    "total_throughput": 10608.05926967269,
    "itl": 172.724419435516,
    "ttft": 2028350.4412725782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8736548503977287,
    "arrivals": 621138,
    "finished_requests": 82185,
    "scheduler_time": 81.44119855226644
}
#Debug simulation 
Total elapsed time: 6.101180008612573. Arrivals time: 0.36451258650049567 Scheduler time: 5.642240605317056 Scheduler overhead time: 0.033192072063684464 Adapter cache time: 0.01214889483526349 Engine time: 0.03380533354356885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.087059872224927,
    "estimated_duration": 3600.1438694881786,
    "input_throughput": 5637.621088427629,
    "output_throughput": 4949.125547733477,
    "total_throughput": 10586.746636161106,
    "itl": 171.0323981641415,
    "ttft": 2029488.2367489883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8719750916585369,
    "arrivals": 621138,
    "finished_requests": 82013,
    "scheduler_time": 81.38960531274044
}
#Debug simulation 
Total elapsed time: 6.087155948393047. Arrivals time: 0.3607867560349405 Scheduler time: 5.630628735758364 Scheduler overhead time: 0.03350759716704488 Adapter cache time: 0.012453342787921429 Engine time: 0.034360661171376705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.076380569022149,
    "estimated_duration": 3600.037737531176,
    "input_throughput": 5648.711897655183,
    "output_throughput": 4959.6124545751045,
    "total_throughput": 10608.324352230287,
    "itl": 172.72178121344726,
    "ttft": 2028305.413008443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8319781279168111,
    "arrivals": 621138,
    "finished_requests": 82185,
    "scheduler_time": 81.43996016041324
}
#Debug simulation 
Total elapsed time: 6.076495986431837. Arrivals time: 0.35403731325641274 Scheduler time: 5.62800308316946 Scheduler overhead time: 0.03298903536051512 Adapter cache time: 0.012053004931658506 Engine time: 0.034034968353807926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.384071187116206,
    "estimated_duration": 3600.1477352928814,
    "input_throughput": 5637.615034803244,
    "output_throughput": 4949.120233409114,
    "total_throughput": 10586.735268212358,
    "itl": 171.03198300647827,
    "ttft": 2029489.0910847047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8848019778728528,
    "arrivals": 621138,
    "finished_requests": 82013,
    "scheduler_time": 81.38943236842815
}
#Debug simulation 
Total elapsed time: 6.384166525211185. Arrivals time: 0.5794688644818962 Scheduler time: 5.708820592146367 Scheduler overhead time: 0.03361152112483978 Adapter cache time: 0.012376263737678528 Engine time: 0.03429834917187691 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.430871766060591,
    "estimated_duration": 3600.1465969332976,
    "input_throughput": 5648.699699429707,
    "output_throughput": 4959.696645467136,
    "total_throughput": 10608.396344896842,
    "itl": 172.72172888808015,
    "ttft": 2028315.7056028747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8013334790337842,
    "arrivals": 621138,
    "finished_requests": 82187,
    "scheduler_time": 81.44287933433905
}
#Debug simulation 
Total elapsed time: 6.430941358208656. Arrivals time: 0.5946646076627076 Scheduler time: 5.740806110668927 Scheduler overhead time: 0.033523695543408394 Adapter cache time: 0.012067478150129318 Engine time: 0.034661928191781044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 135, 135, 34560, 135, 8640, 8640, 8640, 135, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 8640, 135, 34560, 34560, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 135, 8640, 34560, 34560, 8640, 135, 135, 135, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 135, 135, 8640, 135, 34560, 34560, 135, 135, 8640, 8640, 8640, 135, 34560, 135, 34560, 8640, 135, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 8640, 34560, 34560, 8640, 34560, 135, 8640, 34560, 135, 34560, 8640, 135, 8640, 8640, 34560, 34560, 135, 135, 34560, 135, 135, 135, 8640, 135]
Prompts retrieved: 1863270 . Total input tokens: 414529495 . Total output tokens: 372593566
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.3562998380512,
    "estimated_duration": 3600.1676414703365,
    "input_throughput": 5637.5838630977905,
    "output_throughput": 4949.092868554078,
    "total_throughput": 10586.676731651869,
    "itl": 171.03159210437977,
    "ttft": 2029500.9867914536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8941077580675523,
    "arrivals": 621138,
    "finished_requests": 82013,
    "scheduler_time": 81.38965658911074
}
#Debug simulation 
Total elapsed time: 6.3564001359045506. Arrivals time: 0.3567880536429584 Scheduler time: 5.903399353846908 Scheduler overhead time: 0.03337831096723676 Adapter cache time: 0.012338405009359121 Engine time: 0.03495797235518694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.053136740345508,
    "estimated_duration": 3600.088117215478,
    "input_throughput": 5676.865214012416,
    "output_throughput": 4989.1234367597435,
    "total_throughput": 10665.988650772159,
    "itl": 171.66708448392689,
    "ttft": 2027334.4885608638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5355853551882312,
    "arrivals": 620208,
    "finished_requests": 82613,
    "scheduler_time": 81.9180184821449
}
#Debug simulation 
Total elapsed time: 6.053231279365718. Arrivals time: 0.28110117139294744 Scheduler time: 5.679438838735223 Scheduler overhead time: 0.03320672595873475 Adapter cache time: 0.009859674144536257 Engine time: 0.034224487375468016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.064365872181952,
    "estimated_duration": 3600.0341210638394,
    "input_throughput": 5676.950360115097,
    "output_throughput": 4989.198267568723,
    "total_throughput": 10666.14862768382,
    "itl": 171.6684302198921,
    "ttft": 2027329.477891406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5714730491838425,
    "arrivals": 620208,
    "finished_requests": 82613,
    "scheduler_time": 81.9165072978116
}
#Debug simulation 
Total elapsed time: 6.064493856392801. Arrivals time: 0.2778052925132215 Scheduler time: 5.693665602710098 Scheduler overhead time: 0.033434789162129164 Adapter cache time: 0.009916632901877165 Engine time: 0.03421097109094262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.0227961842902005,
    "estimated_duration": 3600.177621523527,
    "input_throughput": 5665.089932804228,
    "output_throughput": 4977.649961730615,
    "total_throughput": 10642.739894534843,
    "itl": 169.74276603425457,
    "ttft": 2029504.9691047322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5724110028892777,
    "arrivals": 620208,
    "finished_requests": 82438,
    "scheduler_time": 81.85108341470442
}
#Debug simulation 
Total elapsed time: 6.022890276275575. Arrivals time: 0.2730259457603097 Scheduler time: 5.656452420167625 Scheduler overhead time: 0.03347975853830576 Adapter cache time: 0.00996649544686079 Engine time: 0.034498462453484535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.083370901178569,
    "estimated_duration": 3600.129761023425,
    "input_throughput": 5676.799825734676,
    "output_throughput": 4989.073503532289,
    "total_throughput": 10665.873329266964,
    "itl": 171.66709313775837,
    "ttft": 2027347.5950932999,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5424627815745775,
    "arrivals": 620208,
    "finished_requests": 82614,
    "scheduler_time": 81.9190560657967
}
#Debug simulation 
Total elapsed time: 6.083481164183468. Arrivals time: 0.2848911941982806 Scheduler time: 5.705605936702341 Scheduler overhead time: 0.033395064529031515 Adapter cache time: 0.010023123119026423 Engine time: 0.03420372912660241 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.0280527379363775,
    "estimated_duration": 3600.1870069607694,
    "input_throughput": 5664.850731522635,
    "output_throughput": 4977.290336683686,
    "total_throughput": 10642.141068206322,
    "itl": 169.66089863490618,
    "ttft": 2029509.6940562667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5813395217247318,
    "arrivals": 620208,
    "finished_requests": 82433,
    "scheduler_time": 81.84859590832752
}
#Debug simulation 
Total elapsed time: 6.028173545841128. Arrivals time: 0.28106240928173065 Scheduler time: 5.65303609194234 Scheduler overhead time: 0.03366213059052825 Adapter cache time: 0.010061993729323149 Engine time: 0.034641409292817116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.0372802540659904,
    "estimated_duration": 3600.075794260303,
    "input_throughput": 5676.884645757625,
    "output_throughput": 4989.140514384768,
    "total_throughput": 10666.025160142392,
    "itl": 171.66668846778148,
    "ttft": 2027327.4877381574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5232588016078814,
    "arrivals": 620208,
    "finished_requests": 82613,
    "scheduler_time": 81.91799104771702
}
#Debug simulation 
Total elapsed time: 6.037380216177553. Arrivals time: 0.27791427401825786 Scheduler time: 5.66688622860238 Scheduler overhead time: 0.03335297433659434 Adapter cache time: 0.009935602080076933 Engine time: 0.03390105115249753 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 66, 66, 34560, 66, 8640, 8640, 8640, 66, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 8640, 66, 34560, 34560, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 66, 8640, 34560, 34560, 8640, 66, 66, 66, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 66, 66, 8640, 66, 34560, 34560, 66, 66, 8640, 8640, 8640, 66, 34560, 66, 34560, 8640, 66, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 8640, 34560, 34560, 8640, 34560, 66, 8640, 34560, 66, 34560, 8640, 66, 8640, 8640, 34560, 34560, 66, 66, 34560, 66, 66, 66, 8640, 66]
Prompts retrieved: 1860372 . Total input tokens: 413901368 . Total output tokens: 372009832
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.013134197331965,
    "estimated_duration": 3600.002474012727,
    "input_throughput": 5664.422218374261,
    "output_throughput": 4977.010746336687,
    "total_throughput": 10641.432964710948,
    "itl": 169.6604895198288,
    "ttft": 2029431.9799819733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5872499496862299,
    "arrivals": 620208,
    "finished_requests": 82426,
    "scheduler_time": 81.84424469257688
}
#Debug simulation 
Total elapsed time: 6.013249573297799. Arrivals time: 0.2728875516913831 Scheduler time: 5.646579856518656 Scheduler overhead time: 0.03379005426540971 Adapter cache time: 0.010049724020063877 Engine time: 0.034443596843630075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.1274962038733065,
    "estimated_duration": 3600.0738014356552,
    "input_throughput": 5692.784128988446,
    "output_throughput": 4999.487508512314,
    "total_throughput": 10692.27163750076,
    "itl": 171.2218603131893,
    "ttft": 2020442.3555144658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4590731615899125,
    "arrivals": 619792,
    "finished_requests": 82940,
    "scheduler_time": 82.1215122301987
}
#Debug simulation 
Total elapsed time: 6.1275903750211. Arrivals time: 0.27694138465449214 Scheduler time: 5.757093215826899 Scheduler overhead time: 0.0335185881704092 Adapter cache time: 0.010055817663669586 Engine time: 0.03443556930869818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.116553810890764,
    "estimated_duration": 3600.1877605981936,
    "input_throughput": 5692.739202189455,
    "output_throughput": 4999.545356214784,
    "total_throughput": 10692.284558404239,
    "itl": 171.2235333990594,
    "ttft": 2020460.1118061568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48854988544248046,
    "arrivals": 619792,
    "finished_requests": 82943,
    "scheduler_time": 82.12414285391786
}
#Debug simulation 
Total elapsed time: 6.116659282241017. Arrivals time: 0.34006679337471724 Scheduler time: 5.683411212172359 Scheduler overhead time: 0.033476813696324825 Adapter cache time: 0.009819918777793646 Engine time: 0.034340690821409225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.1169846737757325,
    "estimated_duration": 3600.1058658460365,
    "input_throughput": 5681.081268756967,
    "output_throughput": 4990.8460110734995,
    "total_throughput": 10671.927279830466,
    "itl": 169.4096493650997,
    "ttft": 2021642.8092081356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48957808177918355,
    "arrivals": 619792,
    "finished_requests": 82779,
    "scheduler_time": 82.05873725717029
}
#Debug simulation 
Total elapsed time: 6.117108564823866. Arrivals time: 0.33577062003314495 Scheduler time: 5.686449768953025 Scheduler overhead time: 0.034053823444992304 Adapter cache time: 0.010263387579470873 Engine time: 0.03486686618998647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.122556321788579,
    "estimated_duration": 3600.137953891729,
    "input_throughput": 5692.817959335446,
    "output_throughput": 4999.614523255381,
    "total_throughput": 10692.432482590826,
    "itl": 171.22273347369833,
    "ttft": 2020455.337828214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.467711524202023,
    "arrivals": 619792,
    "finished_requests": 82943,
    "scheduler_time": 82.1232475399995
}
#Debug simulation 
Total elapsed time: 6.122647580690682. Arrivals time: 0.3381997123360634 Scheduler time: 5.690567733719945 Scheduler overhead time: 0.03352840058505535 Adapter cache time: 0.009945430792868137 Engine time: 0.03484773449599743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.130646923091263,
    "estimated_duration": 3600.1129164219437,
    "input_throughput": 5681.070142746297,
    "output_throughput": 4990.836236841564,
    "total_throughput": 10671.906379587861,
    "itl": 169.40980638454502,
    "ttft": 2021646.73315018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4959915248863404,
    "arrivals": 619792,
    "finished_requests": 82779,
    "scheduler_time": 82.05875551149643
}
#Debug simulation 
Total elapsed time: 6.130757336970419. Arrivals time: 0.2777724484913051 Scheduler time: 5.758696472737938 Scheduler overhead time: 0.03389484062790871 Adapter cache time: 0.01012475322932005 Engine time: 0.03468379005789757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.1137483161874115,
    "estimated_duration": 3600.0544441681973,
    "input_throughput": 5692.814738732458,
    "output_throughput": 4999.514390443784,
    "total_throughput": 10692.329129176242,
    "itl": 171.22145685153492,
    "ttft": 2020432.431423956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4485075442353269,
    "arrivals": 619792,
    "finished_requests": 82940,
    "scheduler_time": 82.12133483292284
}
#Debug simulation 
Total elapsed time: 6.113869011867791. Arrivals time: 0.2754921419546008 Scheduler time: 5.74492312502116 Scheduler overhead time: 0.033703486900776625 Adapter cache time: 0.009864931926131248 Engine time: 0.03437277115881443 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 8640, 34560, 34560, 33, 33, 34560, 33, 8640, 8640, 8640, 33, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 8640, 33, 34560, 34560, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 34560, 34560, 34560, 34560, 8640, 8640, 33, 8640, 34560, 34560, 8640, 33, 33, 33, 34560, 8640, 8640, 8640, 8640, 34560, 8640, 34560, 33, 33, 8640, 33, 34560, 34560, 33, 33, 8640, 8640, 8640, 33, 34560, 33, 34560, 8640, 33, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 8640, 34560, 34560, 8640, 34560, 33, 8640, 34560, 33, 34560, 8640, 33, 8640, 8640, 34560, 34560, 33, 33, 34560, 33, 33, 33, 8640, 33]
Prompts retrieved: 1858986 . Total input tokens: 413595084 . Total output tokens: 371736285
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.107185997068882,
    "estimated_duration": 3600.118458579294,
    "input_throughput": 5681.061397093894,
    "output_throughput": 4990.828553761117,
    "total_throughput": 10671.889950855011,
    "itl": 169.40869265473262,
    "ttft": 2021650.5455787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5019019528478382,
    "arrivals": 619792,
    "finished_requests": 82779,
    "scheduler_time": 82.05876767621713
}
#Debug simulation 
Total elapsed time: 6.1072807321324944. Arrivals time: 0.3387635797262192 Scheduler time: 5.674224962014705 Scheduler overhead time: 0.03370893374085426 Adapter cache time: 0.010197706054896116 Engine time: 0.03479392686858773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.034439653158188,
    "estimated_duration": 3600.1541466837225,
    "input_throughput": 5381.5267931920735,
    "output_throughput": 4768.538040465238,
    "total_throughput": 10150.064833657312,
    "itl": 180.85518679514055,
    "ttft": 2041797.007827395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.78966331925489,
    "arrivals": 572657,
    "finished_requests": 78622,
    "scheduler_time": 78.27586624891113
}
#Debug simulation 
Total elapsed time: 6.034545193891972. Arrivals time: 0.35575229581445456 Scheduler time: 5.571035542991012 Scheduler overhead time: 0.03282955940812826 Adapter cache time: 0.026384834200143814 Engine time: 0.03339876513928175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.972525869961828,
    "estimated_duration": 3600.091369912182,
    "input_throughput": 5381.33730768966,
    "output_throughput": 4768.244534976549,
    "total_throughput": 10149.581842666208,
    "itl": 180.86958037038224,
    "ttft": 2041855.0721678834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.109270986521059,
    "arrivals": 572657,
    "finished_requests": 78615,
    "scheduler_time": 78.2676068702078
}
#Debug simulation 
Total elapsed time: 5.972645825240761. Arrivals time: 0.26055431738495827 Scheduler time: 5.604113577865064 Scheduler overhead time: 0.03319994267076254 Adapter cache time: 0.026319032069295645 Engine time: 0.033288219943642616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.969479659106582,
    "estimated_duration": 3600.0180046782475,
    "input_throughput": 5370.8534165312,
    "output_throughput": 4759.767861641977,
    "total_throughput": 10130.621278173177,
    "itl": 179.03899019975842,
    "ttft": 2043345.3362042706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1597,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.223377905599714,
    "arrivals": 572657,
    "finished_requests": 78451,
    "scheduler_time": 78.19195908888427
}
#Debug simulation 
Total elapsed time: 5.969574635848403. Arrivals time: 0.31919665448367596 Scheduler time: 5.54152554133907 Scheduler overhead time: 0.03328554891049862 Adapter cache time: 0.026705505792051554 Engine time: 0.03358826134353876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.990818421822041,
    "estimated_duration": 3600.0313726488253,
    "input_throughput": 5381.630601109179,
    "output_throughput": 4768.512055318297,
    "total_throughput": 10150.142656427475,
    "itl": 180.85788777645627,
    "ttft": 2041802.9335318087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.859210651635551,
    "arrivals": 572657,
    "finished_requests": 78618,
    "scheduler_time": 78.27170593691437
}
#Debug simulation 
Total elapsed time: 5.990913508925587. Arrivals time: 0.34943642001599073 Scheduler time: 5.533671280834824 Scheduler overhead time: 0.03294566459953785 Adapter cache time: 0.026176162995398045 Engine time: 0.03357396041974425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.967345669865608,
    "estimated_duration": 3600.097751883883,
    "input_throughput": 5370.73444460839,
    "output_throughput": 4759.662426119778,
    "total_throughput": 10130.396870728167,
    "itl": 179.04282303410923,
    "ttft": 2043375.9581890756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1597,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.302477037254667,
    "arrivals": 572657,
    "finished_requests": 78451,
    "scheduler_time": 78.19198780381473
}
#Debug simulation 
Total elapsed time: 5.967435699887574. Arrivals time: 0.31925283931195736 Scheduler time: 5.539610812906176 Scheduler overhead time: 0.03317748848348856 Adapter cache time: 0.02647763304412365 Engine time: 0.033658144529908895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.001753688789904,
    "estimated_duration": 3600.0077993647087,
    "input_throughput": 5381.745562723218,
    "output_throughput": 4768.731890811329,
    "total_throughput": 10150.477453534548,
    "itl": 180.84868285946737,
    "ttft": 2041729.9742567684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.6824187618166775,
    "arrivals": 572657,
    "finished_requests": 78622,
    "scheduler_time": 78.2751017403501
}
#Debug simulation 
Total elapsed time: 6.001872522756457. Arrivals time: 0.2651028009131551 Scheduler time: 5.6285390285775065 Scheduler overhead time: 0.03313292283564806 Adapter cache time: 0.02634690422564745 Engine time: 0.033546100836247206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 4320, 4320, 4320, 1080, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 1080, 34560, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 4320, 1080, 1080, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 4320, 1080, 34560, 34560, 1080, 1080, 4320, 4320, 4320, 1080, 34560, 1080, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 34560, 1080, 1080, 34560, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 1717200 . Total input tokens: 382067877 . Total output tokens: 343382699
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.043960064183921,
    "estimated_duration": 3600.084923292977,
    "input_throughput": 5370.753582755551,
    "output_throughput": 4759.679386764712,
    "total_throughput": 10130.432969520263,
    "itl": 179.05045017904484,
    "ttft": 2043370.671617097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.348632420673893,
    "arrivals": 572657,
    "finished_requests": 78451,
    "scheduler_time": 78.191181872357
}
#Debug simulation 
Total elapsed time: 6.044082527048886. Arrivals time: 0.3528078873641789 Scheduler time: 5.582056424580514 Scheduler overhead time: 0.03320033522322774 Adapter cache time: 0.02700953744351864 Engine time: 0.03367557004094124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.979464835952967,
    "estimated_duration": 3600.1352290592754,
    "input_throughput": 5484.590645546248,
    "output_throughput": 4827.723653186649,
    "total_throughput": 10312.314298732897,
    "itl": 177.97375393960493,
    "ttft": 2028177.6822178122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9480291896733535,
    "arrivals": 564989,
    "finished_requests": 79779,
    "scheduler_time": 79.24140091376815
}
#Debug simulation 
Total elapsed time: 5.979561202228069. Arrivals time: 0.26699018152430654 Scheduler time: 5.606050617061555 Scheduler overhead time: 0.03293551364913583 Adapter cache time: 0.024859831668436527 Engine time: 0.033634952269494534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.92902022972703,
    "estimated_duration": 3600.103341691474,
    "input_throughput": 5484.406175608078,
    "output_throughput": 4827.455589603849,
    "total_throughput": 10311.861765211928,
    "itl": 177.9849953068446,
    "ttft": 2028241.900131921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2048984425375115,
    "arrivals": 564989,
    "finished_requests": 79774,
    "scheduler_time": 79.23536242509843
}
#Debug simulation 
Total elapsed time: 5.929112760815769. Arrivals time: 0.3203401300124824 Scheduler time: 5.5027026403695345 Scheduler overhead time: 0.03276152815669775 Adapter cache time: 0.024931370746344328 Engine time: 0.0333941369317472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.939303535968065,
    "estimated_duration": 3600.126100242214,
    "input_throughput": 5478.280607635684,
    "output_throughput": 4822.001928996889,
    "total_throughput": 10300.282536632572,
    "itl": 176.4696882253234,
    "ttft": 2029242.6768917332,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.210414078608097,
    "arrivals": 564989,
    "finished_requests": 79694,
    "scheduler_time": 79.23215405967463
}
#Debug simulation 
Total elapsed time: 5.939404848963022. Arrivals time: 0.32099476736038923 Scheduler time: 5.511104920413345 Scheduler overhead time: 0.03311482258141041 Adapter cache time: 0.02538724010810256 Engine time: 0.0336278323084116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.940434582065791,
    "estimated_duration": 3600.040755741738,
    "input_throughput": 5484.613464030591,
    "output_throughput": 4827.732289497119,
    "total_throughput": 10312.34575352771,
    "itl": 177.97580816086702,
    "ttft": 2028196.115682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9974619038355876,
    "arrivals": 564989,
    "finished_requests": 79776,
    "scheduler_time": 79.23866775151897
}
#Debug simulation 
Total elapsed time: 5.940526731777936. Arrivals time: 0.31953528290614486 Scheduler time: 5.514419280923903 Scheduler overhead time: 0.03277028584852815 Adapter cache time: 0.025345976930111647 Engine time: 0.03345113154500723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.8905321271158755,
    "estimated_duration": 3600.1903808724364,
    "input_throughput": 5478.182794105636,
    "output_throughput": 4821.915833182462,
    "total_throughput": 10300.098627288098,
    "itl": 176.47274158352548,
    "ttft": 2029267.2394025088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.274674263466152,
    "arrivals": 564989,
    "finished_requests": 79694,
    "scheduler_time": 79.23217450510377
}
#Debug simulation 
Total elapsed time: 5.890633917879313. Arrivals time: 0.31881971284747124 Scheduler time: 5.464338687714189 Scheduler overhead time: 0.03260280657559633 Adapter cache time: 0.025186879094690084 Engine time: 0.03454028721898794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.991515811998397,
    "estimated_duration": 3600.0456547571066,
    "input_throughput": 5484.727110032221,
    "output_throughput": 4827.84377387921,
    "total_throughput": 10312.57088391143,
    "itl": 177.96958150153696,
    "ttft": 2028142.6366617652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8571648804237064,
    "arrivals": 564989,
    "finished_requests": 79779,
    "scheduler_time": 79.24137685195892
}
#Debug simulation 
Total elapsed time: 5.991609429940581. Arrivals time: 0.3000051728449762 Scheduler time: 5.585908769164234 Scheduler overhead time: 0.03249884583055973 Adapter cache time: 0.02488865191116929 Engine time: 0.033339308109134436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 540, 540, 34560, 540, 4320, 4320, 4320, 540, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 4320, 540, 34560, 34560, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 540, 4320, 34560, 34560, 4320, 540, 540, 540, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 540, 540, 4320, 540, 34560, 34560, 540, 540, 4320, 4320, 4320, 540, 34560, 540, 34560, 4320, 540, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 4320, 34560, 34560, 4320, 34560, 540, 4320, 34560, 540, 34560, 4320, 540, 4320, 4320, 34560, 34560, 540, 540, 34560, 540, 540, 540, 4320, 540]
Prompts retrieved: 1694520 . Total input tokens: 376980168 . Total output tokens: 338859602
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.928571320138872,
    "estimated_duration": 3600.0408363012916,
    "input_throughput": 5478.140081394755,
    "output_throughput": 4821.756971466544,
    "total_throughput": 10299.897052861299,
    "itl": 176.47530082113974,
    "ttft": 2029204.3403307023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.319819872789115,
    "arrivals": 564989,
    "finished_requests": 79690,
    "scheduler_time": 79.22783620612786
}
#Debug simulation 
Total elapsed time: 5.928689763881266. Arrivals time: 0.31838357262313366 Scheduler time: 5.502805124502629 Scheduler overhead time: 0.03291691746562719 Adapter cache time: 0.02561752637848258 Engine time: 0.033772135619074106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.047622716985643,
    "estimated_duration": 3600.133475308338,
    "input_throughput": 5599.898486617455,
    "output_throughput": 4959.404733867771,
    "total_throughput": 10559.303220485226,
    "itl": 173.66530409593847,
    "ttft": 2015086.4358571747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7903853302006871,
    "arrivals": 561196,
    "finished_requests": 81616,
    "scheduler_time": 81.37372477915662
}
#Debug simulation 
Total elapsed time: 6.047716336790472. Arrivals time: 0.3133471803739667 Scheduler time: 5.63080368982628 Scheduler overhead time: 0.03304353216663003 Adapter cache time: 0.021306262351572514 Engine time: 0.03392582107335329 
