INFO 06-01 00:47:09 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:09 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 270, 33, 33, 33, 540, 33, 270, 540, 270, 33, 270, 270, 270, 270, 540, 540, 540, 270, 270, 270, 540, 540, 33, 540, 270, 540, 540, 540, 270, 540, 33, 270, 270, 270, 540, 33, 33, 540, 33, 540, 33, 33, 270, 33, 270, 540, 33, 33, 270, 33, 33, 33, 33, 33, 540, 270, 270, 540, 270, 33, 540, 540, 540, 270, 270, 33, 33, 33, 540, 33, 270, 270, 33, 540, 540, 540, 33, 33, 540, 270, 540, 540, 33, 540, 33, 270, 33, 33, 270, 270, 540, 540, 540, 33, 540, 270, 270, 33, 33, 33, 33, 540, 270, 540, 270, 540, 540, 540, 33, 270, 270, 540, 270, 540, 33, 33, 33, 270, 270, 270, 540, 540, 270, 540, 33, 270, 270, 33, 540, 540, 540, 540, 540, 540, 270, 270, 270, 270, 270, 540, 33, 540, 33, 540, 270, 33, 270, 33, 33, 270, 270, 270, 540, 270, 33, 33]
Prompts retrieved: 45219 . Total input tokens: 10025903 . Total output tokens: 9036130
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.622847217018716,
    "estimated_duration": 3599.6724240020503,
    "input_throughput": 1045.509301042404,
    "output_throughput": 940.6011439884485,
    "total_throughput": 1986.1104450308526,
    "itl": 23.10927659809762,
    "ttft": 6388.596459517793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9189258154458673,
    "arrivals": 15306,
    "finished_requests": 15279,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6229338750708848. Arrivals time: 0.05026107293087989 Scheduler time: 1.1123235083650798 Scheduler overhead time: 0.1415106295607984 Adapter cache time: 0.11763004818931222 Engine time: 0.13524830422829837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 270, 33, 33, 33, 540, 33, 270, 540, 270, 33, 270, 270, 270, 270, 540, 540, 540, 270, 270, 270, 540, 540, 33, 540, 270, 540, 540, 540, 270, 540, 33, 270, 270, 270, 540, 33, 33, 540, 33, 540, 33, 33, 270, 33, 270, 540, 33, 33, 270, 33, 33, 33, 33, 33, 540, 270, 270, 540, 270, 33, 540, 540, 540, 270, 270, 33, 33, 33, 540, 33, 270, 270, 33, 540, 540, 540, 33, 33, 540, 270, 540, 540, 33, 540, 33, 270, 33, 33, 270, 270, 540, 540, 540, 33, 540, 270, 270, 33, 33, 33, 33, 540, 270, 540, 270, 540, 540, 540, 33, 270, 270, 540, 270, 540, 33, 33, 33, 270, 270, 270, 540, 540, 270, 540, 33, 270, 270, 33, 540, 540, 540, 540, 540, 540, 270, 270, 270, 270, 270, 540, 33, 540, 33, 540, 270, 33, 270, 33, 33, 270, 270, 270, 540, 270, 33, 33]
Prompts retrieved: 45219 . Total input tokens: 10025903 . Total output tokens: 9036130
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.5833444179734215,
    "estimated_duration": 3599.665952767529,
    "input_throughput": 1045.5111805878869,
    "output_throughput": 940.6028349371849,
    "total_throughput": 1986.1140155250719,
    "itl": 23.110960872714287,
    "ttft": 6388.633766806289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 628,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.042415498530029,
    "arrivals": 15306,
    "finished_requests": 15279,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5834524310193956. Arrivals time: 0.048837484675459564 Scheduler time: 1.0778471122030169 Scheduler overhead time: 0.14061406825203449 Adapter cache time: 0.1175450230948627 Engine time: 0.13337587751448154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 270, 33, 33, 33, 540, 33, 270, 540, 270, 33, 270, 270, 270, 270, 540, 540, 540, 270, 270, 270, 540, 540, 33, 540, 270, 540, 540, 540, 270, 540, 33, 270, 270, 270, 540, 33, 33, 540, 33, 540, 33, 33, 270, 33, 270, 540, 33, 33, 270, 33, 33, 33, 33, 33, 540, 270, 270, 540, 270, 33, 540, 540, 540, 270, 270, 33, 33, 33, 540, 33, 270, 270, 33, 540, 540, 540, 33, 33, 540, 270, 540, 540, 33, 540, 33, 270, 33, 33, 270, 270, 540, 540, 540, 33, 540, 270, 270, 33, 33, 33, 33, 540, 270, 540, 270, 540, 540, 540, 33, 270, 270, 540, 270, 540, 33, 33, 33, 270, 270, 270, 540, 540, 270, 540, 33, 270, 270, 33, 540, 540, 540, 540, 540, 540, 270, 270, 270, 270, 270, 540, 33, 540, 33, 540, 270, 33, 270, 33, 33, 270, 270, 270, 540, 270, 33, 33]
Prompts retrieved: 45219 . Total input tokens: 10025903 . Total output tokens: 9036130
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.59112058498431,
    "estimated_duration": 3599.6731137154343,
    "input_throughput": 1045.5091007181704,
    "output_throughput": 940.6009637650844,
    "total_throughput": 1986.110064483255,
    "itl": 23.11102409581475,
    "ttft": 6388.682051867669,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 628,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0472405773215105,
    "arrivals": 15306,
    "finished_requests": 15279,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5912399069638923. Arrivals time: 0.04849500302225351 Scheduler time: 1.087545130518265 Scheduler overhead time: 0.14061960368417203 Adapter cache time: 0.11949793831445277 Engine time: 0.129771510604769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 270, 33, 33, 33, 540, 33, 270, 540, 270, 33, 270, 270, 270, 270, 540, 540, 540, 270, 270, 270, 540, 540, 33, 540, 270, 540, 540, 540, 270, 540, 33, 270, 270, 270, 540, 33, 33, 540, 33, 540, 33, 33, 270, 33, 270, 540, 33, 33, 270, 33, 33, 33, 33, 33, 540, 270, 270, 540, 270, 33, 540, 540, 540, 270, 270, 33, 33, 33, 540, 33, 270, 270, 33, 540, 540, 540, 33, 33, 540, 270, 540, 540, 33, 540, 33, 270, 33, 33, 270, 270, 540, 540, 540, 33, 540, 270, 270, 33, 33, 33, 33, 540, 270, 540, 270, 540, 540, 540, 33, 270, 270, 540, 270, 540, 33, 33, 33, 270, 270, 270, 540, 540, 270, 540, 33, 270, 270, 33, 540, 540, 540, 540, 540, 540, 270, 270, 270, 270, 270, 540, 33, 540, 33, 540, 270, 33, 270, 33, 33, 270, 270, 270, 540, 270, 33, 33]
Prompts retrieved: 45219 . Total input tokens: 10025903 . Total output tokens: 9036130
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 1.6152264829725027,
    "estimated_duration": 3599.6862917405656,
    "input_throughput": 1045.5052732331933,
    "output_throughput": 940.5975203363703,
    "total_throughput": 1986.1027935695636,
    "itl": 23.109773165213515,
    "ttft": 6388.683202122611,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9609751470945644,
    "arrivals": 15306,
    "finished_requests": 15279,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6153043720405549. Arrivals time: 0.049816076876595616 Scheduler time: 1.1102320383070037 Scheduler overhead time: 0.13891612365841866 Adapter cache time: 0.11872265499550849 Engine time: 0.1322295708814636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 270, 33, 33, 33, 540, 33, 270, 540, 270, 33, 270, 270, 270, 270, 540, 540, 540, 270, 270, 270, 540, 540, 33, 540, 270, 540, 540, 540, 270, 540, 33, 270, 270, 270, 540, 33, 33, 540, 33, 540, 33, 33, 270, 33, 270, 540, 33, 33, 270, 33, 33, 33, 33, 33, 540, 270, 270, 540, 270, 33, 540, 540, 540, 270, 270, 33, 33, 33, 540, 33, 270, 270, 33, 540, 540, 540, 33, 33, 540, 270, 540, 540, 33, 540, 33, 270, 33, 33, 270, 270, 540, 540, 540, 33, 540, 270, 270, 33, 33, 33, 33, 540, 270, 540, 270, 540, 540, 540, 33, 270, 270, 540, 270, 540, 33, 33, 33, 270, 270, 270, 540, 540, 270, 540, 33, 270, 270, 33, 540, 540, 540, 540, 540, 540, 270, 270, 270, 270, 270, 540, 33, 540, 33, 540, 270, 33, 270, 33, 33, 270, 270, 270, 540, 270, 33, 33]
Prompts retrieved: 45219 . Total input tokens: 10025903 . Total output tokens: 9036130
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 1.62165336497128,
    "estimated_duration": 3599.677030545379,
    "input_throughput": 1045.5079630935118,
    "output_throughput": 940.5999402915925,
    "total_throughput": 1986.1079033851042,
    "itl": 23.110715669744096,
    "ttft": 6388.508394758443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.067878784183417,
    "arrivals": 15306,
    "finished_requests": 15279,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.621726228040643. Arrivals time: 0.04988690128084272 Scheduler time: 1.116566805401817 Scheduler overhead time: 0.1382796304533258 Adapter cache time: 0.11879144993145019 Engine time: 0.1326603073393926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 270, 33, 33, 33, 540, 33, 270, 540, 270, 33, 270, 270, 270, 270, 540, 540, 540, 270, 270, 270, 540, 540, 33, 540, 270, 540, 540, 540, 270, 540, 33, 270, 270, 270, 540, 33, 33, 540, 33, 540, 33, 33, 270, 33, 270, 540, 33, 33, 270, 33, 33, 33, 33, 33, 540, 270, 270, 540, 270, 33, 540, 540, 540, 270, 270, 33, 33, 33, 540, 33, 270, 270, 33, 540, 540, 540, 33, 33, 540, 270, 540, 540, 33, 540, 33, 270, 33, 33, 270, 270, 540, 540, 540, 33, 540, 270, 270, 33, 33, 33, 33, 540, 270, 540, 270, 540, 540, 540, 33, 270, 270, 540, 270, 540, 33, 33, 33, 270, 270, 270, 540, 540, 270, 540, 33, 270, 270, 33, 540, 540, 540, 540, 540, 540, 270, 270, 270, 270, 270, 540, 33, 540, 33, 540, 270, 33, 270, 33, 33, 270, 270, 270, 540, 270, 33, 33]
Prompts retrieved: 45219 . Total input tokens: 10025903 . Total output tokens: 9036130
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.603019854053855,
    "estimated_duration": 3599.677995413425,
    "input_throughput": 1045.5076828525494,
    "output_throughput": 940.5996881704783,
    "total_throughput": 1986.1073710230278,
    "itl": 23.10916812822789,
    "ttft": 6388.54268138629,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 628,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8777515851985365,
    "arrivals": 15306,
    "finished_requests": 15279,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6031044611008838. Arrivals time: 0.049698596354573965 Scheduler time: 1.093289074022323 Scheduler overhead time: 0.14024754334241152 Adapter cache time: 0.11987362243235111 Engine time: 0.13468421599827707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.05-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 270, 33, 33, 33, 540, 33, 270, 540, 270, 33, 270, 270, 270, 270, 540, 540, 540, 270, 270, 270, 540, 540, 33, 540, 270, 540, 540, 540, 270, 540, 33, 270, 270, 270, 540, 33, 33, 540, 33, 540, 33, 33, 270, 33, 270, 540, 33, 33, 270, 33, 33, 33, 33, 33, 540, 270, 270, 540, 270, 33, 540, 540, 540, 270, 270, 33, 33, 33, 540, 33, 270, 270, 33, 540, 540, 540, 33, 33, 540, 270, 540, 540, 33, 540, 33, 270, 33, 33, 270, 270, 540, 540, 540, 33, 540, 270, 270, 33, 33, 33, 33, 540, 270, 540, 270, 540, 540, 540, 33, 270, 270, 540, 270, 540, 33, 33, 33, 270, 270, 270, 540, 540, 270, 540, 33, 270, 270, 33, 540, 540, 540, 540, 540, 540, 270, 270, 270, 270, 270, 540, 33, 540, 33, 540, 270, 33, 270, 33, 33, 270, 270, 270, 540, 270, 33, 33]
Prompts retrieved: 45219 . Total input tokens: 10025903 . Total output tokens: 9036130
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.6430434529902413,
    "estimated_duration": 3599.691687262202,
    "input_throughput": 1045.503706141672,
    "output_throughput": 940.5961104894409,
    "total_throughput": 1986.0998166311128,
    "itl": 23.110729516549767,
    "ttft": 6388.58271234558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 628,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0979193532466875,
    "arrivals": 15306,
    "finished_requests": 15279,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.6431447300128639. Arrivals time: 0.05012826342135668 Scheduler time: 1.1343688138294965 Scheduler overhead time: 0.1387668758397922 Adapter cache time: 0.12169672106392682 Engine time: 0.13293835136573762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [53 53 54]
Adapter prompts. [66, 540, 66, 66, 135, 66, 66, 66, 540, 66, 135, 540, 135, 66, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 66, 540, 135, 540, 540, 540, 135, 540, 66, 135, 135, 135, 540, 66, 66, 540, 66, 540, 66, 66, 135, 66, 135, 540, 66, 66, 135, 66, 66, 66, 66, 66, 540, 135, 135, 540, 135, 66, 540, 540, 540, 135, 135, 66, 66, 66, 540, 66, 135, 135, 66, 540, 540, 540, 66, 66, 540, 135, 540, 540, 66, 540, 66, 135, 66, 66, 135, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 66, 540, 135, 540, 135, 540, 540, 540, 66, 135, 135, 540, 135, 540, 66, 66, 66, 135, 135, 135, 540, 540, 135, 540, 66, 135, 135, 66, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 66, 540, 66, 540, 135, 66, 135, 66, 66, 135, 135, 135, 540, 135, 66, 66]
Prompts retrieved: 39813 . Total input tokens: 8823958 . Total output tokens: 7944526
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.4745476839598268,
    "estimated_duration": 3599.85471003176,
    "input_throughput": 920.4990942456021,
    "output_throughput": 810.9602289960874,
    "total_throughput": 1731.4593232416896,
    "itl": 22.044339500651713,
    "ttft": 6442.326975021115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1056,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.231875057593064,
    "arrivals": 13488,
    "finished_requests": 13464,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4746188119752333. Arrivals time: 0.04602462542243302 Scheduler time: 0.9666845641331747 Scheduler overhead time: 0.14584608527366072 Adapter cache time: 0.11027199251111597 Engine time: 0.13789467536844313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [53 53 54]
Adapter prompts. [66, 540, 66, 66, 135, 66, 66, 66, 540, 66, 135, 540, 135, 66, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 66, 540, 135, 540, 540, 540, 135, 540, 66, 135, 135, 135, 540, 66, 66, 540, 66, 540, 66, 66, 135, 66, 135, 540, 66, 66, 135, 66, 66, 66, 66, 66, 540, 135, 135, 540, 135, 66, 540, 540, 540, 135, 135, 66, 66, 66, 540, 66, 135, 135, 66, 540, 540, 540, 66, 66, 540, 135, 540, 540, 66, 540, 66, 135, 66, 66, 135, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 66, 540, 135, 540, 135, 540, 540, 540, 66, 135, 135, 540, 135, 540, 66, 66, 66, 135, 135, 135, 540, 540, 135, 540, 66, 135, 135, 66, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 66, 540, 66, 540, 135, 66, 135, 66, 66, 135, 135, 135, 540, 135, 66, 66]
Prompts retrieved: 39813 . Total input tokens: 8823958 . Total output tokens: 7944526
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.5037539639743045,
    "estimated_duration": 3599.8733778781,
    "input_throughput": 920.4943208178051,
    "output_throughput": 810.9560236034656,
    "total_throughput": 1731.4503444212708,
    "itl": 22.04646749768528,
    "ttft": 6442.281040639736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1056,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.434929332637695,
    "arrivals": 13488,
    "finished_requests": 13464,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5038256200496107. Arrivals time: 0.04649601795244962 Scheduler time: 0.9958289636997506 Scheduler overhead time: 0.14425984083209187 Adapter cache time: 0.11081180965993553 Engine time: 0.13821488432586193 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [53 53 54]
Adapter prompts. [66, 540, 66, 66, 135, 66, 66, 66, 540, 66, 135, 540, 135, 66, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 66, 540, 135, 540, 540, 540, 135, 540, 66, 135, 135, 135, 540, 66, 66, 540, 66, 540, 66, 66, 135, 66, 135, 540, 66, 66, 135, 66, 66, 66, 66, 66, 540, 135, 135, 540, 135, 66, 540, 540, 540, 135, 135, 66, 66, 66, 540, 66, 135, 135, 66, 540, 540, 540, 66, 66, 540, 135, 540, 540, 66, 540, 66, 135, 66, 66, 135, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 66, 540, 135, 540, 135, 540, 540, 540, 66, 135, 135, 540, 135, 540, 66, 66, 66, 135, 135, 135, 540, 540, 135, 540, 66, 135, 135, 66, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 66, 540, 66, 540, 135, 66, 135, 66, 66, 135, 135, 135, 540, 135, 66, 66]
Prompts retrieved: 39813 . Total input tokens: 8823958 . Total output tokens: 7944526
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.4895081999711692,
    "estimated_duration": 3599.8620728733085,
    "input_throughput": 920.497211537643,
    "output_throughput": 810.9585703292976,
    "total_throughput": 1731.4557818669407,
    "itl": 22.04601199563859,
    "ttft": 6442.275908764895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1057,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4463277194276056,
    "arrivals": 13488,
    "finished_requests": 13464,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4895798320649192. Arrivals time: 0.0465104024624452 Scheduler time: 0.9805774680571631 Scheduler overhead time: 0.14461300685070455 Adapter cache time: 0.11157503968570381 Engine time: 0.13867415161803365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [53 53 54]
Adapter prompts. [66, 540, 66, 66, 135, 66, 66, 66, 540, 66, 135, 540, 135, 66, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 66, 540, 135, 540, 540, 540, 135, 540, 66, 135, 135, 135, 540, 66, 66, 540, 66, 540, 66, 66, 135, 66, 135, 540, 66, 66, 135, 66, 66, 66, 66, 66, 540, 135, 135, 540, 135, 66, 540, 540, 540, 135, 135, 66, 66, 66, 540, 66, 135, 135, 66, 540, 540, 540, 66, 66, 540, 135, 540, 540, 66, 540, 66, 135, 66, 66, 135, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 66, 540, 135, 540, 135, 540, 540, 540, 66, 135, 135, 540, 135, 540, 66, 66, 66, 135, 135, 135, 540, 540, 135, 540, 66, 135, 135, 66, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 66, 540, 66, 540, 135, 66, 135, 66, 66, 135, 135, 135, 540, 135, 66, 66]
Prompts retrieved: 39813 . Total input tokens: 8823958 . Total output tokens: 7944526
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 1.4892161339521408,
    "estimated_duration": 3599.8711380193117,
    "input_throughput": 920.4948935542213,
    "output_throughput": 810.9565281845761,
    "total_throughput": 1731.4514217387975,
    "itl": 22.04467139478348,
    "ttft": 6442.286903572272,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1056,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.304587426055187,
    "arrivals": 13488,
    "finished_requests": 13464,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.489292667945847. Arrivals time: 0.04616196092683822 Scheduler time: 0.9810286102583632 Scheduler overhead time: 0.14636529644485563 Adapter cache time: 0.11108382570091635 Engine time: 0.13654457381926477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [53 53 54]
Adapter prompts. [66, 540, 66, 66, 135, 66, 66, 66, 540, 66, 135, 540, 135, 66, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 66, 540, 135, 540, 540, 540, 135, 540, 66, 135, 135, 135, 540, 66, 66, 540, 66, 540, 66, 66, 135, 66, 135, 540, 66, 66, 135, 66, 66, 66, 66, 66, 540, 135, 135, 540, 135, 66, 540, 540, 540, 135, 135, 66, 66, 66, 540, 66, 135, 135, 66, 540, 540, 540, 66, 66, 540, 135, 540, 540, 66, 540, 66, 135, 66, 66, 135, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 66, 540, 135, 540, 135, 540, 540, 540, 66, 135, 135, 540, 135, 540, 66, 66, 66, 135, 135, 135, 540, 540, 135, 540, 66, 135, 135, 66, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 66, 540, 66, 540, 135, 66, 135, 66, 66, 135, 135, 135, 540, 135, 66, 66]
Prompts retrieved: 39813 . Total input tokens: 8823958 . Total output tokens: 7944526
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 1.4836780830519274,
    "estimated_duration": 3599.854961147024,
    "input_throughput": 920.4990300343005,
    "output_throughput": 810.9601724258939,
    "total_throughput": 1731.4592024601943,
    "itl": 22.046605040657724,
    "ttft": 6442.32368330438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1056,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.483062410950659,
    "arrivals": 13488,
    "finished_requests": 13464,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.483753107022494. Arrivals time: 0.046404159627854824 Scheduler time: 0.9727011376526207 Scheduler overhead time: 0.14812320028431714 Adapter cache time: 0.11140663491096348 Engine time: 0.13684292254038155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [53 53 54]
Adapter prompts. [66, 540, 66, 66, 135, 66, 66, 66, 540, 66, 135, 540, 135, 66, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 66, 540, 135, 540, 540, 540, 135, 540, 66, 135, 135, 135, 540, 66, 66, 540, 66, 540, 66, 66, 135, 66, 135, 540, 66, 66, 135, 66, 66, 66, 66, 66, 540, 135, 135, 540, 135, 66, 540, 540, 540, 135, 135, 66, 66, 66, 540, 66, 135, 135, 66, 540, 540, 540, 66, 66, 540, 135, 540, 540, 66, 540, 66, 135, 66, 66, 135, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 66, 540, 135, 540, 135, 540, 540, 540, 66, 135, 135, 540, 135, 540, 66, 66, 66, 135, 135, 135, 540, 540, 135, 540, 66, 135, 135, 66, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 66, 540, 66, 540, 135, 66, 135, 66, 66, 135, 135, 135, 540, 135, 66, 66]
Prompts retrieved: 39813 . Total input tokens: 8823958 . Total output tokens: 7944526
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.4807024559704587,
    "estimated_duration": 3599.8737042383796,
    "input_throughput": 920.4942373668821,
    "output_throughput": 810.9559500831546,
    "total_throughput": 1731.4501874500365,
    "itl": 22.043942710170835,
    "ttft": 6442.156408172336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1056,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1574931114166223,
    "arrivals": 13488,
    "finished_requests": 13464,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4807803590083495. Arrivals time: 0.04695829038973898 Scheduler time: 0.9736229979898781 Scheduler overhead time: 0.14441423828247935 Adapter cache time: 0.11060508200898767 Engine time: 0.1375375505303964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_160_slots_128_rate_0.05-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [53 53 54]
Adapter prompts. [66, 540, 66, 66, 135, 66, 66, 66, 540, 66, 135, 540, 135, 66, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 66, 540, 135, 540, 540, 540, 135, 540, 66, 135, 135, 135, 540, 66, 66, 540, 66, 540, 66, 66, 135, 66, 135, 540, 66, 66, 135, 66, 66, 66, 66, 66, 540, 135, 135, 540, 135, 66, 540, 540, 540, 135, 135, 66, 66, 66, 540, 66, 135, 135, 66, 540, 540, 540, 66, 66, 540, 135, 540, 540, 66, 540, 66, 135, 66, 66, 135, 135, 540, 540, 540, 66, 540, 135, 135, 66, 66, 66, 66, 540, 135, 540, 135, 540, 540, 540, 66, 135, 135, 540, 135, 540, 66, 66, 66, 135, 135, 135, 540, 540, 135, 540, 66, 135, 135, 66, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 66, 540, 66, 540, 135, 66, 135, 66, 66, 135, 135, 135, 540, 135, 66, 66]
Prompts retrieved: 39813 . Total input tokens: 8823958 . Total output tokens: 7944526
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.4763598879799247,
    "estimated_duration": 3599.868179338404,
    "input_throughput": 920.4956500959978,
    "output_throughput": 810.9571946983142,
    "total_throughput": 1731.4528447943119,
    "itl": 22.046689755783227,
    "ttft": 6442.257603258456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1057,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5318402941897964,
    "arrivals": 13488,
    "finished_requests": 13464,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4764464899199083. Arrivals time: 0.046553239692002535 Scheduler time: 0.9699406283907592 Scheduler overhead time: 0.14457760588265955 Adapter cache time: 0.11102332454174757 Engine time: 0.13692714239005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 135, 33, 33, 33, 540, 33, 135, 540, 135, 33, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 33, 540, 135, 540, 540, 540, 135, 540, 33, 135, 135, 135, 540, 33, 33, 540, 33, 540, 33, 33, 135, 33, 135, 540, 33, 33, 135, 33, 33, 33, 33, 33, 540, 135, 135, 540, 135, 33, 540, 540, 540, 135, 135, 33, 33, 33, 540, 33, 135, 135, 33, 540, 540, 540, 33, 33, 540, 135, 540, 540, 33, 540, 33, 135, 33, 33, 135, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 33, 540, 135, 540, 135, 540, 540, 540, 33, 135, 135, 540, 135, 540, 33, 33, 33, 135, 135, 135, 540, 540, 135, 540, 33, 135, 135, 33, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 33, 540, 33, 540, 135, 33, 135, 33, 33, 135, 135, 135, 540, 135, 33, 33]
Prompts retrieved: 38064 . Total input tokens: 8436520 . Total output tokens: 7585704
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.4334693070268258,
    "estimated_duration": 3599.619095548544,
    "input_throughput": 880.695683585071,
    "output_throughput": 786.6668458064946,
    "total_throughput": 1667.3625293915657,
    "itl": 21.823001007774078,
    "ttft": 4229.3551369560655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 613,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8760789870308072,
    "arrivals": 12878,
    "finished_requests": 12863,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4335503459442407. Arrivals time: 0.044391693896614015 Scheduler time: 0.9324156268266961 Scheduler overhead time: 0.14699543185997754 Adapter cache time: 0.10393466870300472 Engine time: 0.13719828287139535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 135, 33, 33, 33, 540, 33, 135, 540, 135, 33, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 33, 540, 135, 540, 540, 540, 135, 540, 33, 135, 135, 135, 540, 33, 33, 540, 33, 540, 33, 33, 135, 33, 135, 540, 33, 33, 135, 33, 33, 33, 33, 33, 540, 135, 135, 540, 135, 33, 540, 540, 540, 135, 135, 33, 33, 33, 540, 33, 135, 135, 33, 540, 540, 540, 33, 33, 540, 135, 540, 540, 33, 540, 33, 135, 33, 33, 135, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 33, 540, 135, 540, 135, 540, 540, 540, 33, 135, 135, 540, 135, 540, 33, 33, 33, 135, 135, 135, 540, 540, 135, 540, 33, 135, 135, 33, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 33, 540, 33, 540, 135, 33, 135, 33, 33, 135, 135, 135, 540, 135, 33, 33]
Prompts retrieved: 38064 . Total input tokens: 8436520 . Total output tokens: 7585704
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.4445119760930538,
    "estimated_duration": 3599.609475419337,
    "input_throughput": 880.6980372865839,
    "output_throughput": 786.6689482114225,
    "total_throughput": 1667.3669854980064,
    "itl": 21.824086616362127,
    "ttft": 4229.289846082105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 613,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9938873862405342,
    "arrivals": 12878,
    "finished_requests": 12863,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.444580115028657. Arrivals time: 0.044741182937286794 Scheduler time: 0.9452422292670235 Scheduler overhead time: 0.14555006893351674 Adapter cache time: 0.10469841514714062 Engine time: 0.13608625414781272 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 135, 33, 33, 33, 540, 33, 135, 540, 135, 33, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 33, 540, 135, 540, 540, 540, 135, 540, 33, 135, 135, 135, 540, 33, 33, 540, 33, 540, 33, 33, 135, 33, 135, 540, 33, 33, 135, 33, 33, 33, 33, 33, 540, 135, 135, 540, 135, 33, 540, 540, 540, 135, 135, 33, 33, 33, 540, 33, 135, 135, 33, 540, 540, 540, 33, 33, 540, 135, 540, 540, 33, 540, 33, 135, 33, 33, 135, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 33, 540, 135, 540, 135, 540, 540, 540, 33, 135, 135, 540, 135, 540, 33, 33, 33, 135, 135, 135, 540, 540, 135, 540, 33, 135, 135, 33, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 33, 540, 33, 540, 135, 33, 135, 33, 33, 135, 135, 135, 540, 135, 33, 33]
Prompts retrieved: 38064 . Total input tokens: 8436520 . Total output tokens: 7585704
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.4673475560266525,
    "estimated_duration": 3599.6120474811996,
    "input_throughput": 880.6974079938145,
    "output_throughput": 786.6683861060695,
    "total_throughput": 1667.3657940998842,
    "itl": 21.824099363988864,
    "ttft": 4229.378820723588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 613,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9985525671392803,
    "arrivals": 12878,
    "finished_requests": 12863,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.46742183691822. Arrivals time: 0.04526019631884992 Scheduler time: 0.964018969098106 Scheduler overhead time: 0.1450367927318439 Adapter cache time: 0.10549317649565637 Engine time: 0.13922179769724607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 135, 33, 33, 33, 540, 33, 135, 540, 135, 33, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 33, 540, 135, 540, 540, 540, 135, 540, 33, 135, 135, 135, 540, 33, 33, 540, 33, 540, 33, 33, 135, 33, 135, 540, 33, 33, 135, 33, 33, 33, 33, 33, 540, 135, 135, 540, 135, 33, 540, 540, 540, 135, 135, 33, 33, 33, 540, 33, 135, 135, 33, 540, 540, 540, 33, 33, 540, 135, 540, 540, 33, 540, 33, 135, 33, 33, 135, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 33, 540, 135, 540, 135, 540, 540, 540, 33, 135, 135, 540, 135, 540, 33, 33, 33, 135, 135, 135, 540, 540, 135, 540, 33, 135, 135, 33, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 33, 540, 33, 540, 135, 33, 135, 33, 33, 135, 135, 135, 540, 135, 33, 33]
Prompts retrieved: 38064 . Total input tokens: 8436520 . Total output tokens: 7585704
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 1.4670716059627011,
    "estimated_duration": 3599.625183843092,
    "input_throughput": 880.6941940036687,
    "output_throughput": 786.6655152625563,
    "total_throughput": 1667.359709266225,
    "itl": 21.82311334582413,
    "ttft": 4229.329113384358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 613,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9191144429659357,
    "arrivals": 12878,
    "finished_requests": 12863,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.467146459966898. Arrivals time: 0.045304975588805974 Scheduler time: 0.9624323304742575 Scheduler overhead time: 0.1459997951751575 Adapter cache time: 0.10633795603644103 Engine time: 0.13869397877715528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 135, 33, 33, 33, 540, 33, 135, 540, 135, 33, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 33, 540, 135, 540, 540, 540, 135, 540, 33, 135, 135, 135, 540, 33, 33, 540, 33, 540, 33, 33, 135, 33, 135, 540, 33, 33, 135, 33, 33, 33, 33, 33, 540, 135, 135, 540, 135, 33, 540, 540, 540, 135, 135, 33, 33, 33, 540, 33, 135, 135, 33, 540, 540, 540, 33, 33, 540, 135, 540, 540, 33, 540, 33, 135, 33, 33, 135, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 33, 540, 135, 540, 135, 540, 540, 540, 33, 135, 135, 540, 135, 540, 33, 33, 33, 135, 135, 135, 540, 540, 135, 540, 33, 135, 135, 33, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 33, 540, 33, 540, 135, 33, 135, 33, 33, 135, 135, 135, 540, 135, 33, 33]
Prompts retrieved: 38064 . Total input tokens: 8436520 . Total output tokens: 7585704
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 1.4567657249281183,
    "estimated_duration": 3599.614779101391,
    "input_throughput": 880.6967396637376,
    "output_throughput": 786.6677891312878,
    "total_throughput": 1667.3645287950253,
    "itl": 21.82434298082275,
    "ttft": 4229.2339560029495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 613,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0215655100531924,
    "arrivals": 12878,
    "finished_requests": 12863,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4568293809425086. Arrivals time: 0.04476398404221982 Scheduler time: 0.95072298462037 Scheduler overhead time: 0.1465586827835068 Adapter cache time: 0.10560540820006281 Engine time: 0.14049249538220465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 135, 33, 33, 33, 540, 33, 135, 540, 135, 33, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 33, 540, 135, 540, 540, 540, 135, 540, 33, 135, 135, 135, 540, 33, 33, 540, 33, 540, 33, 33, 135, 33, 135, 540, 33, 33, 135, 33, 33, 33, 33, 33, 540, 135, 135, 540, 135, 33, 540, 540, 540, 135, 135, 33, 33, 33, 540, 33, 135, 135, 33, 540, 540, 540, 33, 33, 540, 135, 540, 540, 33, 540, 33, 135, 33, 33, 135, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 33, 540, 135, 540, 135, 540, 540, 540, 33, 135, 135, 540, 135, 540, 33, 33, 33, 135, 135, 135, 540, 540, 135, 540, 33, 135, 135, 33, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 33, 540, 33, 540, 135, 33, 135, 33, 33, 135, 135, 135, 540, 135, 33, 33]
Prompts retrieved: 38064 . Total input tokens: 8436520 . Total output tokens: 7585704
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.473564760060981,
    "estimated_duration": 3599.616370464469,
    "input_throughput": 880.696350314393,
    "output_throughput": 786.6674413514286,
    "total_throughput": 1667.3637916658215,
    "itl": 21.82255519747458,
    "ttft": 4229.408798793655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 613,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8329008307750054,
    "arrivals": 12878,
    "finished_requests": 12863,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4736365809803829. Arrivals time: 0.04480305628385395 Scheduler time: 0.9705888469470665 Scheduler overhead time: 0.1461683448869735 Adapter cache time: 0.10516774572897702 Engine time: 0.13811521942261606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.05-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 135, 33, 33, 33, 540, 33, 135, 540, 135, 33, 135, 135, 135, 135, 540, 540, 540, 135, 135, 135, 540, 540, 33, 540, 135, 540, 540, 540, 135, 540, 33, 135, 135, 135, 540, 33, 33, 540, 33, 540, 33, 33, 135, 33, 135, 540, 33, 33, 135, 33, 33, 33, 33, 33, 540, 135, 135, 540, 135, 33, 540, 540, 540, 135, 135, 33, 33, 33, 540, 33, 135, 135, 33, 540, 540, 540, 33, 33, 540, 135, 540, 540, 33, 540, 33, 135, 33, 33, 135, 135, 540, 540, 540, 33, 540, 135, 135, 33, 33, 33, 33, 540, 135, 540, 135, 540, 540, 540, 33, 135, 135, 540, 135, 540, 33, 33, 33, 135, 135, 135, 540, 540, 135, 540, 33, 135, 135, 33, 540, 540, 540, 540, 540, 540, 135, 135, 135, 135, 135, 540, 33, 540, 33, 540, 135, 33, 135, 33, 33, 135, 135, 135, 540, 135, 33, 33]
Prompts retrieved: 38064 . Total input tokens: 8436520 . Total output tokens: 7585704
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.4557909819995984,
    "estimated_duration": 3599.6251404225804,
    "input_throughput": 880.6942046270507,
    "output_throughput": 786.6655247517164,
    "total_throughput": 1667.359729378767,
    "itl": 21.824247206387387,
    "ttft": 4229.293004732685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 613,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0480995589867206,
    "arrivals": 12878,
    "finished_requests": 12863,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4558682109927759. Arrivals time: 0.04514279414433986 Scheduler time: 0.9525662256637588 Scheduler overhead time: 0.1454123135190457 Adapter cache time: 0.1054517850279808 Engine time: 0.13905885943677276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 66, 33, 33, 33, 540, 33, 66, 540, 66, 33, 66, 66, 66, 66, 540, 540, 540, 66, 66, 66, 540, 540, 33, 540, 66, 540, 540, 540, 66, 540, 33, 66, 66, 66, 540, 33, 33, 540, 33, 540, 33, 33, 66, 33, 66, 540, 33, 33, 66, 33, 33, 33, 33, 33, 540, 66, 66, 540, 66, 33, 540, 540, 540, 66, 66, 33, 33, 33, 540, 33, 66, 66, 33, 540, 540, 540, 33, 33, 540, 66, 540, 540, 33, 540, 33, 66, 33, 33, 66, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 33, 540, 66, 540, 66, 540, 540, 540, 33, 66, 66, 540, 66, 540, 33, 33, 33, 66, 66, 66, 540, 540, 66, 540, 33, 66, 66, 33, 540, 540, 540, 540, 540, 540, 66, 66, 66, 66, 66, 540, 33, 540, 33, 540, 66, 33, 66, 33, 33, 66, 66, 66, 540, 66, 33, 33]
Prompts retrieved: 34407 . Total input tokens: 7613207 . Total output tokens: 6870626
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.362746784929186,
    "estimated_duration": 3599.8758280549964,
    "input_throughput": 797.1733296007893,
    "output_throughput": 700.3574902088214,
    "total_throughput": 1497.5308198096106,
    "itl": 21.252967703069494,
    "ttft": 5910.398009673419,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6496028939797756,
    "arrivals": 11642,
    "finished_requests": 11623,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3628333018859848. Arrivals time: 0.04215235449373722 Scheduler time: 0.8632424781098962 Scheduler overhead time: 0.14830415311735123 Adapter cache time: 0.09788609913084656 Engine time: 0.14157436834648252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 66, 33, 33, 33, 540, 33, 66, 540, 66, 33, 66, 66, 66, 66, 540, 540, 540, 66, 66, 66, 540, 540, 33, 540, 66, 540, 540, 540, 66, 540, 33, 66, 66, 66, 540, 33, 33, 540, 33, 540, 33, 33, 66, 33, 66, 540, 33, 33, 66, 33, 33, 33, 33, 33, 540, 66, 66, 540, 66, 33, 540, 540, 540, 66, 66, 33, 33, 33, 540, 33, 66, 66, 33, 540, 540, 540, 33, 33, 540, 66, 540, 540, 33, 540, 33, 66, 33, 33, 66, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 33, 540, 66, 540, 66, 540, 540, 540, 33, 66, 66, 540, 66, 540, 33, 33, 33, 66, 66, 66, 540, 540, 66, 540, 33, 66, 66, 33, 540, 540, 540, 540, 540, 540, 66, 66, 66, 66, 66, 540, 33, 540, 33, 540, 66, 33, 66, 33, 33, 66, 66, 66, 540, 66, 33, 33]
Prompts retrieved: 34407 . Total input tokens: 7613207 . Total output tokens: 6870626
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.3977141509531066,
    "estimated_duration": 3599.864890817866,
    "input_throughput": 797.1757516010599,
    "output_throughput": 700.3596180597766,
    "total_throughput": 1497.5353696608365,
    "itl": 21.25359722898196,
    "ttft": 5910.385071680374,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.753828279769518,
    "arrivals": 11642,
    "finished_requests": 11623,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.397789441049099. Arrivals time: 0.042040555039420724 Scheduler time: 0.8937462759204209 Scheduler overhead time: 0.14970415900461376 Adapter cache time: 0.0976530424086377 Engine time: 0.14338618598412722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 66, 33, 33, 33, 540, 33, 66, 540, 66, 33, 66, 66, 66, 66, 540, 540, 540, 66, 66, 66, 540, 540, 33, 540, 66, 540, 540, 540, 66, 540, 33, 66, 66, 66, 540, 33, 33, 540, 33, 540, 33, 33, 66, 33, 66, 540, 33, 33, 66, 33, 33, 33, 33, 33, 540, 66, 66, 540, 66, 33, 540, 540, 540, 66, 66, 33, 33, 33, 540, 33, 66, 66, 33, 540, 540, 540, 33, 33, 540, 66, 540, 540, 33, 540, 33, 66, 33, 33, 66, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 33, 540, 66, 540, 66, 540, 540, 540, 33, 66, 66, 540, 66, 540, 33, 33, 33, 66, 66, 66, 540, 540, 66, 540, 33, 66, 66, 33, 540, 540, 540, 540, 540, 540, 66, 66, 66, 66, 66, 540, 33, 540, 33, 540, 66, 33, 66, 33, 33, 66, 66, 66, 540, 66, 33, 33]
Prompts retrieved: 34407 . Total input tokens: 7613207 . Total output tokens: 6870626
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.3551504659699276,
    "estimated_duration": 3599.8679901599717,
    "input_throughput": 797.1750652646779,
    "output_throughput": 700.3590150782063,
    "total_throughput": 1497.5340803428842,
    "itl": 21.253484820575668,
    "ttft": 5910.424922651232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7578187875822293,
    "arrivals": 11642,
    "finished_requests": 11623,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.355235385010019. Arrivals time: 0.04239924647845328 Scheduler time: 0.8568115532398224 Scheduler overhead time: 0.14797811151947826 Adapter cache time: 0.09739894873928279 Engine time: 0.1409333205083385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 66, 33, 33, 33, 540, 33, 66, 540, 66, 33, 66, 66, 66, 66, 540, 540, 540, 66, 66, 66, 540, 540, 33, 540, 66, 540, 540, 540, 66, 540, 33, 66, 66, 66, 540, 33, 33, 540, 33, 540, 33, 33, 66, 33, 66, 540, 33, 33, 66, 33, 33, 33, 33, 33, 540, 66, 66, 540, 66, 33, 540, 540, 540, 66, 66, 33, 33, 33, 540, 33, 66, 66, 33, 540, 540, 540, 33, 33, 540, 66, 540, 540, 33, 540, 33, 66, 33, 33, 66, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 33, 540, 66, 540, 66, 540, 540, 540, 33, 66, 66, 540, 66, 540, 33, 33, 33, 66, 66, 66, 540, 540, 66, 540, 33, 66, 66, 33, 540, 540, 540, 540, 540, 540, 66, 66, 66, 66, 66, 540, 33, 540, 33, 540, 66, 33, 66, 33, 33, 66, 66, 66, 540, 66, 33, 33]
Prompts retrieved: 34407 . Total input tokens: 7613207 . Total output tokens: 6870626
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 1.3423857999732718,
    "estimated_duration": 3599.87197101573,
    "input_throughput": 797.1741837225078,
    "output_throughput": 700.3582405983802,
    "total_throughput": 1497.5324243208881,
    "itl": 21.253182739574726,
    "ttft": 5910.289261770154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6843670756346465,
    "arrivals": 11642,
    "finished_requests": 11623,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3424641569145024. Arrivals time: 0.04163568210788071 Scheduler time: 0.8482565552694723 Scheduler overhead time: 0.14749465871136636 Adapter cache time: 0.09668741375207901 Engine time: 0.13887258886825293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 66, 33, 33, 33, 540, 33, 66, 540, 66, 33, 66, 66, 66, 66, 540, 540, 540, 66, 66, 66, 540, 540, 33, 540, 66, 540, 540, 540, 66, 540, 33, 66, 66, 66, 540, 33, 33, 540, 33, 540, 33, 33, 66, 33, 66, 540, 33, 33, 66, 33, 33, 33, 33, 33, 540, 66, 66, 540, 66, 33, 540, 540, 540, 66, 66, 33, 33, 33, 540, 33, 66, 66, 33, 540, 540, 540, 33, 33, 540, 66, 540, 540, 33, 540, 33, 66, 33, 33, 66, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 33, 540, 66, 540, 66, 540, 540, 540, 33, 66, 66, 540, 66, 540, 33, 33, 33, 66, 66, 66, 540, 540, 66, 540, 33, 66, 66, 33, 540, 540, 540, 540, 540, 540, 66, 66, 66, 66, 66, 540, 33, 540, 33, 540, 66, 33, 66, 33, 33, 66, 66, 66, 540, 66, 33, 33]
Prompts retrieved: 34407 . Total input tokens: 7613207 . Total output tokens: 6870626
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 1.3442751250695437,
    "estimated_duration": 3599.8735337994362,
    "input_throughput": 797.173837651788,
    "output_throughput": 700.3579365575753,
    "total_throughput": 1497.5317742093632,
    "itl": 21.253444164017207,
    "ttft": 5910.361330216461,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7791969312727496,
    "arrivals": 11642,
    "finished_requests": 11623,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3443505530012771. Arrivals time: 0.040862462017685175 Scheduler time: 0.8490248575108126 Scheduler overhead time: 0.14814810291863978 Adapter cache time: 0.09678889706265181 Engine time: 0.13991409167647362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 66, 33, 33, 33, 540, 33, 66, 540, 66, 33, 66, 66, 66, 66, 540, 540, 540, 66, 66, 66, 540, 540, 33, 540, 66, 540, 540, 540, 66, 540, 33, 66, 66, 66, 540, 33, 33, 540, 33, 540, 33, 33, 66, 33, 66, 540, 33, 33, 66, 33, 33, 33, 33, 33, 540, 66, 66, 540, 66, 33, 540, 540, 540, 66, 66, 33, 33, 33, 540, 33, 66, 66, 33, 540, 540, 540, 33, 33, 540, 66, 540, 540, 33, 540, 33, 66, 33, 33, 66, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 33, 540, 66, 540, 66, 540, 540, 540, 33, 66, 66, 540, 66, 540, 33, 33, 33, 66, 66, 66, 540, 540, 66, 540, 33, 66, 66, 33, 540, 540, 540, 540, 540, 540, 66, 66, 66, 66, 66, 540, 33, 540, 33, 540, 66, 33, 66, 33, 33, 66, 66, 66, 540, 66, 33, 33]
Prompts retrieved: 34407 . Total input tokens: 7613207 . Total output tokens: 6870626
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.3483809710014611,
    "estimated_duration": 3599.8749988921545,
    "input_throughput": 797.1735132145269,
    "output_throughput": 700.357651522869,
    "total_throughput": 1497.531164737396,
    "itl": 21.252535408881222,
    "ttft": 5910.3373377133685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6116371089522523,
    "arrivals": 11642,
    "finished_requests": 11623,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3484555160393938. Arrivals time: 0.04174932383466512 Scheduler time: 0.8487055571749806 Scheduler overhead time: 0.1483682810794562 Adapter cache time: 0.09705232211854309 Engine time: 0.14221682120114565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.05-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [53 53 54]
Adapter prompts. [33, 540, 33, 33, 66, 33, 33, 33, 540, 33, 66, 540, 66, 33, 66, 66, 66, 66, 540, 540, 540, 66, 66, 66, 540, 540, 33, 540, 66, 540, 540, 540, 66, 540, 33, 66, 66, 66, 540, 33, 33, 540, 33, 540, 33, 33, 66, 33, 66, 540, 33, 33, 66, 33, 33, 33, 33, 33, 540, 66, 66, 540, 66, 33, 540, 540, 540, 66, 66, 33, 33, 33, 540, 33, 66, 66, 33, 540, 540, 540, 33, 33, 540, 66, 540, 540, 33, 540, 33, 66, 33, 33, 66, 66, 540, 540, 540, 33, 540, 66, 66, 33, 33, 33, 33, 540, 66, 540, 66, 540, 540, 540, 33, 66, 66, 540, 66, 540, 33, 33, 33, 66, 66, 66, 540, 540, 66, 540, 33, 66, 66, 33, 540, 540, 540, 540, 540, 540, 66, 66, 66, 66, 66, 540, 33, 540, 33, 540, 66, 33, 66, 33, 33, 66, 66, 66, 540, 66, 33, 33]
Prompts retrieved: 34407 . Total input tokens: 7613207 . Total output tokens: 6870626
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.3556757810292765,
    "estimated_duration": 3599.878007977617,
    "input_throughput": 797.1728468688273,
    "output_throughput": 700.3570661041346,
    "total_throughput": 1497.529912972962,
    "itl": 21.25362256714967,
    "ttft": 5910.294883830157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 539,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8015811052545878,
    "arrivals": 11642,
    "finished_requests": 11623,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3557817379478365. Arrivals time: 0.04200630192644894 Scheduler time: 0.8566307594301179 Scheduler overhead time: 0.14779256260953844 Adapter cache time: 0.09768531250301749 Engine time: 0.141890850616619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.1564097930677235,
    "estimated_duration": 3599.988634269073,
    "input_throughput": 573.1981985601369,
    "output_throughput": 518.3649698879916,
    "total_throughput": 1091.5631684481284,
    "itl": 20.001730143564178,
    "ttft": 4663.444218133016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.47365358936376,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.156470429035835. Arrivals time: 0.03493503725621849 Scheduler time: 0.6670026606880128 Scheduler overhead time: 0.15225983003620058 Adapter cache time: 0.08520772389601916 Engine time: 0.1442932029021904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.1561961659463122,
    "estimated_duration": 3599.9883632778133,
    "input_throughput": 573.1982417079713,
    "output_throughput": 518.3650089082222,
    "total_throughput": 1091.5632506161935,
    "itl": 20.187126025389556,
    "ttft": 4663.726241162147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7108879887685204,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.15625788399484. Arrivals time: 0.034718553302809596 Scheduler time: 0.6682567837415263 Scheduler overhead time: 0.15125339990481734 Adapter cache time: 0.08519375731702894 Engine time: 0.14463980530854315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.1852003339445218,
    "estimated_duration": 3599.9711457663134,
    "input_throughput": 573.2009831319768,
    "output_throughput": 518.3674880823991,
    "total_throughput": 1091.568471214376,
    "itl": 20.18713293033375,
    "ttft": 4663.6423713043905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1139,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7184918471611534,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.185334465932101. Arrivals time: 0.03529835550580174 Scheduler time: 0.6950881447410211 Scheduler overhead time: 0.15230484725907445 Adapter cache time: 0.08507203822955489 Engine time: 0.14503821812104434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 1.1611604370409623,
    "estimated_duration": 3599.9858164102,
    "input_throughput": 573.1986472262462,
    "output_throughput": 518.3653756338484,
    "total_throughput": 1091.5640228600946,
    "itl": 20.002240542310304,
    "ttft": 4663.549152530502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5484348273370023,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1612609500298277. Arrivals time: 0.03485517774242908 Scheduler time: 0.671452944399789 Scheduler overhead time: 0.1527135573560372 Adapter cache time: 0.08507915888912976 Engine time: 0.14473666402045637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 1.1503304640064016,
    "estimated_duration": 3599.9768930389155,
    "input_throughput": 573.2000680310182,
    "output_throughput": 518.3666605217368,
    "total_throughput": 1091.566728552755,
    "itl": 20.00433857107175,
    "ttft": 4663.657259107488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7484738796949357,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1503880430245772. Arrivals time: 0.035060010850429535 Scheduler time: 0.6608172601554543 Scheduler overhead time: 0.15163310687057674 Adapter cache time: 0.08488588395994157 Engine time: 0.1453237576643005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.1446366179734468,
    "estimated_duration": 3599.9829985194924,
    "input_throughput": 573.1990958981266,
    "output_throughput": 518.3657813849245,
    "total_throughput": 1091.5648772830511,
    "itl": 20.000994402561627,
    "ttft": 4663.548842621609,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3937070847138857,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1447335679549724. Arrivals time: 0.0348538204561919 Scheduler time: 0.6585915521718562 Scheduler overhead time: 0.1517817456042394 Adapter cache time: 0.08431319333612919 Engine time: 0.14286729611922055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_160_slots_128_rate_0.025-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [53 53 54]
Adapter prompts. [66, 270, 66, 66, 135, 66, 66, 66, 270, 66, 135, 270, 135, 66, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 66, 270, 135, 270, 270, 270, 135, 270, 66, 135, 135, 135, 270, 66, 66, 270, 66, 270, 66, 66, 135, 66, 135, 270, 66, 66, 135, 66, 66, 66, 66, 66, 270, 135, 135, 270, 135, 66, 270, 270, 270, 135, 135, 66, 66, 66, 270, 66, 135, 135, 66, 270, 270, 270, 66, 66, 270, 135, 270, 270, 66, 270, 66, 135, 66, 66, 135, 135, 270, 270, 270, 66, 270, 135, 135, 66, 66, 66, 66, 270, 135, 270, 135, 270, 270, 270, 66, 135, 135, 270, 135, 270, 66, 66, 66, 135, 135, 135, 270, 270, 135, 270, 66, 135, 135, 66, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 66, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 25233 . Total input tokens: 5554084 . Total output tokens: 5037594
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.175250455038622,
    "estimated_duration": 3599.990048862611,
    "input_throughput": 573.1979733254955,
    "output_throughput": 518.3647661997239,
    "total_throughput": 1091.5627395252195,
    "itl": 20.00296908536967,
    "ttft": 4663.48982708091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7970148412511286,
    "arrivals": 8553,
    "finished_requests": 8542,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.175322417053394. Arrivals time: 0.03489660145714879 Scheduler time: 0.680725377984345 Scheduler overhead time: 0.15391135402023792 Adapter cache time: 0.0857301225187257 Engine time: 0.14711549843195826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.1215832720045,
    "estimated_duration": 3598.5236368634683,
    "input_throughput": 538.7503864473175,
    "output_throughput": 483.0047473343706,
    "total_throughput": 1021.7551337816882,
    "itl": 19.791830840120756,
    "ttft": 2750.3623965095862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9158653277019344,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.121643085964024. Arrivals time: 0.033707491820678115 Scheduler time: 0.6345276492647827 Scheduler overhead time: 0.15323916904162616 Adapter cache time: 0.080382484709844 Engine time: 0.14690337621141225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.1222572260303423,
    "estimated_duration": 3598.5246010823116,
    "input_throughput": 538.7502420900233,
    "output_throughput": 483.00461791403023,
    "total_throughput": 1021.7548600040535,
    "itl": 19.792931820583505,
    "ttft": 2750.3340736969767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0451457826932944,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1223176149651408. Arrivals time: 0.03351189068052918 Scheduler time: 0.6337755328277126 Scheduler overhead time: 0.15305566403549165 Adapter cache time: 0.08043918362818658 Engine time: 0.14797347376588732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.121285979053937,
    "estimated_duration": 3598.507295597206,
    "input_throughput": 538.7528329793906,
    "output_throughput": 483.0069407186085,
    "total_throughput": 1021.7597736979991,
    "itl": 19.79297223316093,
    "ttft": 2750.2470979181508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0489185233973073,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.121361401048489. Arrivals time: 0.03324525710195303 Scheduler time: 0.6351004254538566 Scheduler overhead time: 0.15284572693053633 Adapter cache time: 0.08038269123062491 Engine time: 0.14678835950326174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 1.121521588996984,
    "estimated_duration": 3598.511933955017,
    "input_throughput": 538.7521385455643,
    "output_throughput": 483.0063181393154,
    "total_throughput": 1021.7584566848798,
    "itl": 19.791909457809606,
    "ttft": 2750.3316412275576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 626,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9600280733918647,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1215876380447298. Arrivals time: 0.033296791603788733 Scheduler time: 0.6344254345167428 Scheduler overhead time: 0.15242532326374203 Adapter cache time: 0.08053511846810579 Engine time: 0.14804470643866807 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 1.117431101971306,
    "estimated_duration": 3598.5242649552188,
    "input_throughput": 538.7502924130278,
    "output_throughput": 483.00466303000724,
    "total_throughput": 1021.7549554430351,
    "itl": 19.79330237526781,
    "ttft": 2750.398965320765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 625,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0673077479936226,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1175032609608024. Arrivals time: 0.0337147272657603 Scheduler time: 0.6300762939499691 Scheduler overhead time: 0.15368514077272266 Adapter cache time: 0.0797914694994688 Engine time: 0.1470039520645514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.118227066937834,
    "estimated_duration": 3598.514650990186,
    "input_throughput": 538.7517317642532,
    "output_throughput": 483.00595344852474,
    "total_throughput": 1021.7576852127779,
    "itl": 19.79196639175159,
    "ttft": 2750.378494964014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 625,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8687814343138303,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1183036640286446. Arrivals time: 0.0335747703211382 Scheduler time: 0.6323180481558666 Scheduler overhead time: 0.1523812609957531 Adapter cache time: 0.0801422328222543 Engine time: 0.1467809493187815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 135, 33, 33, 33, 270, 33, 135, 270, 135, 33, 135, 135, 135, 135, 270, 270, 270, 135, 135, 135, 270, 270, 33, 270, 135, 270, 270, 270, 135, 270, 33, 135, 135, 135, 270, 33, 33, 270, 33, 270, 33, 33, 135, 33, 135, 270, 33, 33, 135, 33, 33, 33, 33, 33, 270, 135, 135, 270, 135, 33, 270, 270, 270, 135, 135, 33, 33, 33, 270, 33, 135, 135, 33, 270, 270, 270, 33, 33, 270, 135, 270, 270, 33, 270, 33, 135, 33, 33, 135, 135, 270, 270, 270, 33, 270, 135, 135, 33, 33, 33, 33, 270, 135, 270, 135, 270, 270, 270, 33, 135, 135, 270, 135, 270, 33, 33, 33, 135, 135, 135, 270, 270, 135, 270, 33, 135, 135, 33, 270, 270, 270, 270, 270, 270, 135, 135, 135, 135, 135, 270, 33, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 23484 . Total input tokens: 5172175 . Total output tokens: 4691996
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.128488703048788,
    "estimated_duration": 3598.5216720901785,
    "input_throughput": 538.7506806021026,
    "output_throughput": 483.0050110523395,
    "total_throughput": 1021.7556916544421,
    "itl": 19.793177553397264,
    "ttft": 2750.309204700011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1013578523322924,
    "arrivals": 7949,
    "finished_requests": 7943,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1285515900235623. Arrivals time: 0.03366067470051348 Scheduler time: 0.641519928118214 Scheduler overhead time: 0.15210954658687115 Adapter cache time: 0.08010709180962294 Engine time: 0.14830705453641713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.0235638150479645,
    "estimated_duration": 3597.5214629827924,
    "input_throughput": 445.2673921427726,
    "output_throughput": 404.239700850662,
    "total_throughput": 849.5070929934346,
    "itl": 19.288783248350946,
    "ttft": 7528.963272219553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7138731366023656,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0236683420371264. Arrivals time: 0.030322422739118338 Scheduler time: 0.5428270169068128 Scheduler overhead time: 0.1561210840009153 Adapter cache time: 0.0715227093314752 Engine time: 0.1484216059325263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.0258600730448961,
    "estimated_duration": 3597.5170427595895,
    "input_throughput": 445.2679392371254,
    "output_throughput": 404.240197534815,
    "total_throughput": 849.5081367719405,
    "itl": 19.289384626576215,
    "ttft": 7528.742060234924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8256084329681521,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0259321120101959. Arrivals time: 0.031020665890537202 Scheduler time: 0.5472930235555395 Scheduler overhead time: 0.15446986828465015 Adapter cache time: 0.07165390567388386 Engine time: 0.14726002886891365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.0278177401050925,
    "estimated_duration": 3597.518530091027,
    "input_throughput": 445.26775514884383,
    "output_throughput": 404.24003040873936,
    "total_throughput": 849.5077855575831,
    "itl": 19.28944771330708,
    "ttft": 7528.71896055862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.829152128286671,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0278702740324661. Arrivals time: 0.03110993013251573 Scheduler time: 0.5469741696724668 Scheduler overhead time: 0.15408773091621697 Adapter cache time: 0.07198392262216657 Engine time: 0.14951946609653533 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 1.0269084150204435,
    "estimated_duration": 3597.5051166734725,
    "input_throughput": 445.2694153444877,
    "output_throughput": 404.24153763114606,
    "total_throughput": 849.5109529756337,
    "itl": 19.289302106594064,
    "ttft": 7528.819484217864,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7561472288332793,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0270076849265024. Arrivals time: 0.030676759546622634 Scheduler time: 0.5489641779568046 Scheduler overhead time: 0.15395436249673367 Adapter cache time: 0.07170150789897889 Engine time: 0.14752126636449248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 1.0325302169658244,
    "estimated_duration": 3597.500238021493,
    "input_throughput": 445.27001918447957,
    "output_throughput": 404.24208583229887,
    "total_throughput": 849.5121050167785,
    "itl": 19.28930165808606,
    "ttft": 7528.754399505696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8505302719771903,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.032583525055088. Arrivals time: 0.03053273621480912 Scheduler time: 0.5533436021069065 Scheduler overhead time: 0.15421375329606235 Adapter cache time: 0.07236517360433936 Engine time: 0.14844422962050885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 1.026464554015547,
    "estimated_duration": 3597.5150546958093,
    "input_throughput": 445.26818530171414,
    "output_throughput": 404.240420926596,
    "total_throughput": 849.5086062283102,
    "itl": 19.28852586912859,
    "ttft": 7528.776070757233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6744281651451958,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.026544417021796. Arrivals time: 0.030763335176743567 Scheduler time: 0.5471900767879561 Scheduler overhead time: 0.15425417351070791 Adapter cache time: 0.07186352577991784 Engine time: 0.14852113486267626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [53 53 54]
Adapter prompts. [33, 270, 33, 33, 66, 33, 33, 33, 270, 33, 66, 270, 66, 33, 66, 66, 66, 66, 270, 270, 270, 66, 66, 66, 270, 270, 33, 270, 66, 270, 270, 270, 66, 270, 33, 66, 66, 66, 270, 33, 33, 270, 33, 270, 33, 33, 66, 33, 66, 270, 33, 33, 66, 33, 33, 33, 33, 33, 270, 66, 66, 270, 66, 33, 270, 270, 270, 66, 66, 33, 33, 33, 270, 33, 66, 66, 33, 270, 270, 270, 33, 33, 270, 66, 270, 270, 33, 270, 33, 66, 33, 33, 66, 66, 270, 270, 270, 33, 270, 66, 66, 33, 33, 33, 33, 270, 66, 270, 66, 270, 270, 270, 33, 66, 66, 270, 66, 270, 33, 33, 33, 66, 66, 66, 270, 270, 66, 270, 33, 66, 66, 33, 270, 270, 270, 270, 270, 270, 66, 66, 66, 66, 66, 270, 33, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 19827 . Total input tokens: 4360331 . Total output tokens: 3963665
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 1.0375852989964187,
    "estimated_duration": 3597.5033500846826,
    "input_throughput": 445.2696339983376,
    "output_throughput": 404.24173613786013,
    "total_throughput": 849.5113701361977,
    "itl": 19.289820330106423,
    "ttft": 7528.67877026059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8756810292601547,
    "arrivals": 6723,
    "finished_requests": 6709,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0376735419267789. Arrivals time: 0.031073999358341098 Scheduler time: 0.5533620454370975 Scheduler overhead time: 0.1561696920543909 Adapter cache time: 0.07305250351782888 Engine time: 0.14948055322747678 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 0.85925178299658,
    "estimated_duration": 3599.6455489312566,
    "input_throughput": 293.2397053130406,
    "output_throughput": 249.4323365439631,
    "total_throughput": 542.6720418570037,
    "itl": 18.507584099197775,
    "ttft": 4278.17116836765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7383570385538285,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8593813660554588. Arrivals time: 0.024910352774895728 Scheduler time: 0.39090584125369787 Scheduler overhead time: 0.15773758210707456 Adapter cache time: 0.056279763113707304 Engine time: 0.1530291736125946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 0.8561982288956642,
    "estimated_duration": 3599.6509929989534,
    "input_throughput": 293.2392618209326,
    "output_throughput": 249.43195930557846,
    "total_throughput": 542.671221126511,
    "itl": 18.50786808081622,
    "ttft": 4278.193683826751,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8552491697855353,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8562544519081712. Arrivals time: 0.024985170224681497 Scheduler time: 0.3898116697091609 Scheduler overhead time: 0.15674656152259558 Adapter cache time: 0.05612769536674023 Engine time: 0.15271924040280282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 0.8571802990045398,
    "estimated_duration": 3599.650901292276,
    "input_throughput": 293.23926929165657,
    "output_throughput": 249.4319656602436,
    "total_throughput": 542.6712349519001,
    "itl": 18.507887479168005,
    "ttft": 4278.195586984263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8582217440009232,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8572580000618473. Arrivals time: 0.02476412383839488 Scheduler time: 0.39279542909935117 Scheduler overhead time: 0.15706696338020265 Adapter cache time: 0.056086945813149214 Engine time: 0.15100516856182367 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 0.8612721189856529,
    "estimated_duration": 3599.6481442648696,
    "input_throughput": 293.2394938882476,
    "output_throughput": 249.4321567041284,
    "total_throughput": 542.671650592376,
    "itl": 18.50838387198431,
    "ttft": 4278.245497087292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7812934171478176,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8613667340250686. Arrivals time: 0.02541122294496745 Scheduler time: 0.3934310977347195 Scheduler overhead time: 0.15763897891156375 Adapter cache time: 0.056241237092763186 Engine time: 0.1526213678298518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 0.8569379090331495,
    "estimated_duration": 3599.6517848353906,
    "input_throughput": 293.23919731537865,
    "output_throughput": 249.43190443657284,
    "total_throughput": 542.6711017519515,
    "itl": 18.507974030545157,
    "ttft": 4278.182906768927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8809831793420055,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8569932699901983. Arrivals time: 0.025209553074091673 Scheduler time: 0.3892011650605127 Scheduler overhead time: 0.1567727611400187 Adapter cache time: 0.05611245019827038 Engine time: 0.15353990986477584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 0.8520922960015014,
    "estimated_duration": 3599.644051778461,
    "input_throughput": 293.23982727638986,
    "output_throughput": 249.43244028708733,
    "total_throughput": 542.6722675634771,
    "itl": 18.507358205439854,
    "ttft": 4278.251695547857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6983485675044123,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8521647599991411. Arrivals time: 0.02495879353955388 Scheduler time: 0.3877136465162039 Scheduler overhead time: 0.1567848869599402 Adapter cache time: 0.05590086383745074 Engine time: 0.15109629440121353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_160_slots_128_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [53 53 54]
Adapter prompts. [33, 135, 33, 33, 66, 33, 33, 33, 135, 33, 66, 135, 66, 33, 66, 66, 66, 66, 135, 135, 135, 66, 66, 66, 135, 135, 33, 135, 66, 135, 135, 135, 66, 135, 33, 66, 66, 66, 135, 33, 33, 135, 33, 135, 33, 33, 66, 33, 66, 135, 33, 33, 66, 33, 33, 33, 33, 33, 135, 66, 66, 135, 66, 33, 135, 135, 135, 66, 66, 33, 33, 33, 135, 33, 66, 66, 33, 135, 135, 135, 33, 33, 135, 66, 135, 135, 33, 135, 33, 66, 33, 33, 66, 66, 135, 135, 135, 33, 135, 66, 66, 33, 33, 33, 33, 135, 66, 135, 66, 135, 135, 135, 33, 66, 66, 135, 66, 135, 33, 33, 33, 66, 66, 66, 135, 135, 66, 135, 33, 66, 66, 33, 135, 135, 135, 135, 135, 135, 66, 66, 66, 66, 66, 135, 33, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 12537 . Total input tokens: 2760590 . Total output tokens: 2526223
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 0.8493062480119988,
    "estimated_duration": 3599.633104176862,
    "input_throughput": 293.24071910972646,
    "output_throughput": 249.43319888856226,
    "total_throughput": 542.6739179982887,
    "itl": 18.50819393413619,
    "ttft": 4278.107920756132,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.906511197984214,
    "arrivals": 4238,
    "finished_requests": 4233,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.8494070060551167. Arrivals time: 0.02549567085225135 Scheduler time: 0.3878103226888925 Scheduler overhead time: 0.15588965360075235 Adapter cache time: 0.05571959330700338 Engine time: 0.14896557480096817 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.437825253000483,
    "estimated_duration": 3600.0872667320805,
    "input_throughput": 4228.34055737151,
    "output_throughput": 3721.5747862028375,
    "total_throughput": 7949.915343574347,
    "itl": 229.78762434233968,
    "ttft": 2237291.7548634117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 1078565,
    "finished_requests": 61426,
    "scheduler_time": 37.631255113556556
}
#Debug simulation 
Total elapsed time: 4.437919821008109. Arrivals time: 0.22936974524054676 Scheduler time: 4.124415580998175 Scheduler overhead time: 0.024672867031767964 Adapter cache time: 0.022648655343800783 Engine time: 0.025318460073322058 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.582325007067993,
    "estimated_duration": 3600.1195523757374,
    "input_throughput": 4228.302637881751,
    "output_throughput": 3721.541411356352,
    "total_throughput": 7949.844049238103,
    "itl": 229.78853454607665,
    "ttft": 2237314.2327841404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574656,
    "arrivals": 1078565,
    "finished_requests": 61426,
    "scheduler_time": 37.63109104998514
}
#Debug simulation 
Total elapsed time: 4.582418862031773. Arrivals time: 0.3054051058134064 Scheduler time: 4.192817011615261 Scheduler overhead time: 0.02469201327767223 Adapter cache time: 0.02263320644851774 Engine time: 0.025315916049294174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.370667485985905,
    "estimated_duration": 3600.0397929149663,
    "input_throughput": 3920.2213897121082,
    "output_throughput": 3458.6773247631445,
    "total_throughput": 7378.898714475253,
    "itl": 146.63035335754023,
    "ttft": 2279895.216738344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 1078565,
    "finished_requests": 56882,
    "scheduler_time": 23.5866735482372
}
#Debug simulation 
Total elapsed time: 4.370828783023171. Arrivals time: 0.21869629400316626 Scheduler time: 3.9950669230893254 Scheduler overhead time: 0.036585878347977996 Adapter cache time: 0.06543510197661817 Engine time: 0.03776045818813145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.512356719002128,
    "estimated_duration": 3600.0980094229344,
    "input_throughput": 4228.327940005173,
    "output_throughput": 3721.563681025336,
    "total_throughput": 7949.891621030509,
    "itl": 229.78793040017916,
    "ttft": 2237299.225724759,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801272,
    "arrivals": 1078565,
    "finished_requests": 61426,
    "scheduler_time": 37.63120364905963
}
#Debug simulation 
Total elapsed time: 4.5124495920026675. Arrivals time: 0.23556332662701607 Scheduler time: 4.192584795993753 Scheduler overhead time: 0.024748823256231844 Adapter cache time: 0.02255974558647722 Engine time: 0.025492369895800948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.360291172051802,
    "estimated_duration": 3600.0464129161614,
    "input_throughput": 3920.21418095219,
    "output_throughput": 3458.6709647206903,
    "total_throughput": 7378.88514567288,
    "itl": 146.63055446479504,
    "ttft": 2279899.7419352205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978162,
    "arrivals": 1078565,
    "finished_requests": 56882,
    "scheduler_time": 23.586628598752228
}
#Debug simulation 
Total elapsed time: 4.360382155049592. Arrivals time: 0.21867609291803092 Scheduler time: 3.9840750369476154 Scheduler overhead time: 0.03675460955128074 Adapter cache time: 0.06548996025230736 Engine time: 0.03805343830026686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.489655906916596,
    "estimated_duration": 3600.0760546377946,
    "input_throughput": 4228.353726135803,
    "output_throughput": 3721.5863766933608,
    "total_throughput": 7949.940102829164,
    "itl": 229.78730307699738,
    "ttft": 2237283.947286457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 1078565,
    "finished_requests": 61426,
    "scheduler_time": 37.631313011115594
}
#Debug simulation 
Total elapsed time: 4.489746720995754. Arrivals time: 0.23346909729298204 Scheduler time: 4.17180020455271 Scheduler overhead time: 0.02484332036692649 Adapter cache time: 0.022617989336140454 Engine time: 0.025426580104976892 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [8640, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 17280, 8640, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 8640, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 17280, 34560, 8640, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 8640, 17280, 34560, 8640, 8640, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 17280, 34560, 17280, 8640, 34560, 34560, 34560, 17280, 17280, 8640, 8640, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 8640, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 8640, 8640, 8640, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 8640, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3240000 . Total input tokens: 721625136 . Total output tokens: 648199760
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.298739677993581,
    "estimated_duration": 3600.053160760864,
    "input_throughput": 3920.2068330061147,
    "output_throughput": 3458.6644818790473,
    "total_throughput": 7378.871314885162,
    "itl": 146.63075784508993,
    "ttft": 2279904.349445351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.536504152864218,
    "arrivals": 1078565,
    "finished_requests": 56882,
    "scheduler_time": 23.586585738988067
}
#Debug simulation 
Total elapsed time: 4.298842869000509. Arrivals time: 0.2184526842320338 Scheduler time: 3.923895009793341 Scheduler overhead time: 0.03658853645902127 Adapter cache time: 0.0647174984915182 Engine time: 0.037862786557525396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.760564441094175,
    "estimated_duration": 3600.224255249754,
    "input_throughput": 4373.691160221255,
    "output_throughput": 3844.839104068459,
    "total_throughput": 8218.530264289713,
    "itl": 222.44956512849976,
    "ttft": 2212012.3691355805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 1002554,
    "finished_requests": 63513,
    "scheduler_time": 38.87299030968902
}
#Debug simulation 
Total elapsed time: 4.7606560010463. Arrivals time: 0.4090677760541439 Scheduler time: 4.256835993845016 Scheduler overhead time: 0.02532019861973822 Adapter cache time: 0.03147044393699616 Engine time: 0.026131361490115523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.721746788942255,
    "estimated_duration": 3600.0083400245508,
    "input_throughput": 4373.953200311924,
    "output_throughput": 3844.9516480580164,
    "total_throughput": 8218.90484836994,
    "itl": 222.4499420842408,
    "ttft": 2211977.0295791645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 1002554,
    "finished_requests": 63512,
    "scheduler_time": 38.870222060560344
}
#Debug simulation 
Total elapsed time: 4.721837313030846. Arrivals time: 0.23784460872411728 Scheduler time: 4.388728988007642 Scheduler overhead time: 0.02544122328981757 Adapter cache time: 0.03153512510471046 Engine time: 0.026373427943326533 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.6071009238949046,
    "estimated_duration": 3600.124413140024,
    "input_throughput": 4091.9571963212115,
    "output_throughput": 3610.209122929687,
    "total_throughput": 7702.166319250899,
    "itl": 140.65585176649407,
    "ttft": 2250589.622435193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 1002554,
    "finished_requests": 59421,
    "scheduler_time": 24.698503761573168
}
#Debug simulation 
Total elapsed time: 4.607190214912407. Arrivals time: 0.2302087836433202 Scheduler time: 4.216181097668596 Scheduler overhead time: 0.038201052229851484 Adapter cache time: 0.06532726343721151 Engine time: 0.03928713209461421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.705170115921646,
    "estimated_duration": 3600.234996547461,
    "input_throughput": 4373.678111317815,
    "output_throughput": 3844.827632994629,
    "total_throughput": 8218.505744312444,
    "itl": 222.44984018821364,
    "ttft": 2212019.9265605416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 1002554,
    "finished_requests": 63513,
    "scheduler_time": 38.87293745204489
}
#Debug simulation 
Total elapsed time: 4.705301558016799. Arrivals time: 0.23759690555743873 Scheduler time: 4.372482373728417 Scheduler overhead time: 0.025510209379717708 Adapter cache time: 0.03151859808713198 Engine time: 0.02622386801522225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.558626065030694,
    "estimated_duration": 3600.1310301500503,
    "input_throughput": 4091.9496753389008,
    "output_throughput": 3610.2024873962123,
    "total_throughput": 7702.152162735113,
    "itl": 140.6560187372723,
    "ttft": 2250594.202602142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978162,
    "arrivals": 1002554,
    "finished_requests": 59421,
    "scheduler_time": 24.698455820919182
}
#Debug simulation 
Total elapsed time: 4.558715809020214. Arrivals time: 0.33292899664957076 Scheduler time: 4.066025355015881 Scheduler overhead time: 0.03799531445838511 Adapter cache time: 0.06470195227302611 Engine time: 0.039177533821202815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.696957419975661,
    "estimated_duration": 3600.181275112975,
    "input_throughput": 4373.743374771004,
    "output_throughput": 3844.8850050100946,
    "total_throughput": 8218.6283797811,
    "itl": 222.44963723375537,
    "ttft": 2211979.1415857836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 1002554,
    "finished_requests": 63513,
    "scheduler_time": 38.872653291205154
}
#Debug simulation 
Total elapsed time: 4.697074107942171. Arrivals time: 0.23848829476628453 Scheduler time: 4.363047623424791 Scheduler overhead time: 0.02539338869974017 Adapter cache time: 0.03164600988384336 Engine time: 0.026422507129609585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 17280, 4320, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 4320, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 17280, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 4320, 17280, 34560, 4320, 4320, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 17280, 34560, 17280, 4320, 34560, 34560, 34560, 17280, 17280, 4320, 4320, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 4320, 4320, 4320, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 4320, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3011040 . Total input tokens: 670651597 . Total output tokens: 602378110
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.674526755930856,
    "estimated_duration": 3600.1377768064795,
    "input_throughput": 4091.9420070272145,
    "output_throughput": 3610.1957218785205,
    "total_throughput": 7702.1377289057355,
    "itl": 140.6561899965627,
    "ttft": 2250598.87088571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642178,
    "arrivals": 1002554,
    "finished_requests": 59421,
    "scheduler_time": 24.6984117728824
}
#Debug simulation 
Total elapsed time: 4.674617356969975. Arrivals time: 0.2939797822618857 Scheduler time: 4.219953717896715 Scheduler overhead time: 0.0382689880207181 Adapter cache time: 0.06509441253729165 Engine time: 0.039357844390906394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.070283986045979,
    "estimated_duration": 3600.0937242729146,
    "input_throughput": 4788.70949491233,
    "output_throughput": 4237.492178924042,
    "total_throughput": 9026.201673836373,
    "itl": 202.81047735245107,
    "ttft": 2165968.067089148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 945185,
    "finished_requests": 69818,
    "scheduler_time": 42.884409222685626
}
#Debug simulation 
Total elapsed time: 5.07039073901251. Arrivals time: 0.26274230098351836 Scheduler time: 4.712204453186132 Scheduler overhead time: 0.027933943085372448 Adapter cache time: 0.02566363278310746 Engine time: 0.028746295836754143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.01936296094209,
    "estimated_duration": 3600.1260172510815,
    "input_throughput": 4788.666540390621,
    "output_throughput": 4237.454168798351,
    "total_throughput": 9026.120709188972,
    "itl": 202.8114349394384,
    "ttft": 2165991.247310367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 945185,
    "finished_requests": 69818,
    "scheduler_time": 42.884252493624516
}
#Debug simulation 
Total elapsed time: 5.019458921975456. Arrivals time: 0.2502912012860179 Scheduler time: 4.673864365904592 Scheduler overhead time: 0.027957574115134776 Adapter cache time: 0.025550418999046087 Engine time: 0.02874052047263831 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.580012858030386,
    "estimated_duration": 3600.040236514995,
    "input_throughput": 4152.054426611056,
    "output_throughput": 3680.8321933722937,
    "total_throughput": 7832.886619983351,
    "itl": 114.7666193185322,
    "ttft": 2222488.917436509,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 945185,
    "finished_requests": 60393,
    "scheduler_time": 18.876354232972027
}
#Debug simulation 
Total elapsed time: 4.58010337408632. Arrivals time: 0.2210718224523589 Scheduler time: 4.205687775043771 Scheduler overhead time: 0.04553364880848676 Adapter cache time: 0.03910976112820208 Engine time: 0.047058778698556125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.995936169987544,
    "estimated_duration": 3600.104466103295,
    "input_throughput": 4788.695206575528,
    "output_throughput": 4237.479535284766,
    "total_throughput": 9026.174741860294,
    "itl": 202.81084328540658,
    "ttft": 2165975.7563484227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 945185,
    "finished_requests": 69818,
    "scheduler_time": 42.88435689771542
}
#Debug simulation 
Total elapsed time: 4.996055872994475. Arrivals time: 0.24709230149164796 Scheduler time: 4.654597591958009 Scheduler overhead time: 0.027575497515499592 Adapter cache time: 0.02533820748794824 Engine time: 0.028463569818995893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.588055517990142,
    "estimated_duration": 3600.046855532792,
    "input_throughput": 4152.046792676487,
    "output_throughput": 3680.8254258232105,
    "total_throughput": 7832.872218499697,
    "itl": 114.76679468404116,
    "ttft": 2222493.50060198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978162,
    "arrivals": 945185,
    "finished_requests": 60393,
    "scheduler_time": 18.876308300089022
}
#Debug simulation 
Total elapsed time: 4.5881991209462285. Arrivals time: 0.22341021860484034 Scheduler time: 4.211590190883726 Scheduler overhead time: 0.04578242253046483 Adapter cache time: 0.03913375688716769 Engine time: 0.04665369854774326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.111057366011664,
    "estimated_duration": 3600.082507466513,
    "input_throughput": 4788.724415133522,
    "output_throughput": 4237.50538171295,
    "total_throughput": 9026.229796846472,
    "itl": 202.81012944414547,
    "ttft": 2165960.029621891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 945185,
    "finished_requests": 69818,
    "scheduler_time": 42.8844624081291
}
#Debug simulation 
Total elapsed time: 5.111178239923902. Arrivals time: 0.3102401976939291 Scheduler time: 4.705168993095867 Scheduler overhead time: 0.027977276826277375 Adapter cache time: 0.02602686861064285 Engine time: 0.028739900677464902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 17280, 1080, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 1080, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 17280, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 1080, 17280, 34560, 1080, 1080, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 17280, 34560, 17280, 1080, 34560, 34560, 34560, 17280, 17280, 1080, 1080, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 1080, 1080, 1080, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 1080, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 2839320 . Total input tokens: 632348578 . Total output tokens: 567998722
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.530575105920434,
    "estimated_duration": 3600.0536014516733,
    "input_throughput": 4152.0390124115365,
    "output_throughput": 3680.818528550979,
    "total_throughput": 7832.857540962515,
    "itl": 114.76700576393351,
    "ttft": 2222498.1520147785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642179,
    "arrivals": 945185,
    "finished_requests": 60393,
    "scheduler_time": 18.876263514503716
}
#Debug simulation 
Total elapsed time: 4.53066880395636. Arrivals time: 0.22378756443504244 Scheduler time: 4.154361594817601 Scheduler overhead time: 0.04538757971022278 Adapter cache time: 0.039109600824303925 Engine time: 0.046607622876763344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.487070212024264,
    "estimated_duration": 3600.023070979318,
    "input_throughput": 4991.557733298547,
    "output_throughput": 4381.938028999544,
    "total_throughput": 9373.495762298091,
    "itl": 195.2086084453642,
    "ttft": 2145788.1566172964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 935737,
    "finished_requests": 72539,
    "scheduler_time": 44.25506955889584
}
#Debug simulation 
Total elapsed time: 5.487134627066553. Arrivals time: 0.6038669721456245 Scheduler time: 4.791784694418311 Scheduler overhead time: 0.028887053718790412 Adapter cache time: 0.019608607050031424 Engine time: 0.029605004703626037 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.234335378976539,
    "estimated_duration": 3600.055369038375,
    "input_throughput": 4991.5129513132915,
    "output_throughput": 4381.898716244951,
    "total_throughput": 9373.411667558243,
    "itl": 195.20955631739142,
    "ttft": 2145811.312096229,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 935737,
    "finished_requests": 72539,
    "scheduler_time": 44.25491791072456
}
#Debug simulation 
Total elapsed time: 5.23442381597124. Arrivals time: 0.26460034726187587 Scheduler time: 4.877342924010009 Scheduler overhead time: 0.02886780456174165 Adapter cache time: 0.020564932376146317 Engine time: 0.029589943820610642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.933353403001092,
    "estimated_duration": 3600.0298634515466,
    "input_throughput": 4547.734219155416,
    "output_throughput": 4009.0861874580264,
    "total_throughput": 8556.820406613442,
    "itl": 127.3008680843149,
    "ttft": 2194076.398857758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 935737,
    "finished_requests": 66097,
    "scheduler_time": 27.552976786696345
}
#Debug simulation 
Total elapsed time: 4.933469995041378. Arrivals time: 0.23826304369140416 Scheduler time: 4.555783970165066 Scheduler overhead time: 0.04159406397957355 Adapter cache time: 0.03505769558250904 Engine time: 0.04294492467306554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.150671602925286,
    "estimated_duration": 3600.0338208817575,
    "input_throughput": 4991.542828227839,
    "output_throughput": 4381.924944287386,
    "total_throughput": 9373.467772515225,
    "itl": 195.20894010677642,
    "ttft": 2145795.7813070985,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 935737,
    "finished_requests": 72539,
    "scheduler_time": 44.25502530598447
}
#Debug simulation 
Total elapsed time: 5.150760860997252. Arrivals time: 0.25645455450285226 Scheduler time: 4.8035707282833755 Scheduler overhead time: 0.02855042228475213 Adapter cache time: 0.019682060810737312 Engine time: 0.02914460434112698 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.960741056012921,
    "estimated_duration": 3600.0364851327136,
    "input_throughput": 4547.725854338517,
    "output_throughput": 4009.0788133965093,
    "total_throughput": 8556.804667735027,
    "itl": 127.30104868223272,
    "ttft": 2194081.0625083903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978161,
    "arrivals": 935737,
    "finished_requests": 66097,
    "scheduler_time": 27.552933517183003
}
#Debug simulation 
Total elapsed time: 4.960848192917183. Arrivals time: 0.24858045985456556 Scheduler time: 4.571662321221083 Scheduler overhead time: 0.04208605096209794 Adapter cache time: 0.03493791166692972 Engine time: 0.04382643778808415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.184376655030064,
    "estimated_duration": 3600.011853517318,
    "input_throughput": 4991.573286749889,
    "output_throughput": 4381.951682905511,
    "total_throughput": 9373.5249696554,
    "itl": 195.20826894432193,
    "ttft": 2145780.1585828667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 935737,
    "finished_requests": 72539,
    "scheduler_time": 44.255122088740634
}
#Debug simulation 
Total elapsed time: 5.18452645407524. Arrivals time: 0.25840512791182846 Scheduler time: 4.833614279516041 Scheduler overhead time: 0.02910027839243412 Adapter cache time: 0.019986526225693524 Engine time: 0.02991597354412079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 17280, 540, 540, 540, 34560, 540, 17280, 34560, 17280, 540, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 540, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 540, 17280, 17280, 17280, 34560, 540, 540, 34560, 540, 34560, 540, 540, 17280, 540, 17280, 34560, 540, 540, 17280, 540, 540, 540, 540, 540, 34560, 17280, 17280, 34560, 17280, 540, 34560, 34560, 34560, 17280, 17280, 540, 540, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 540, 540, 34560, 17280, 34560, 34560, 540, 34560, 540, 17280, 540, 540, 17280, 17280, 34560, 34560, 34560, 540, 34560, 17280, 17280, 540, 540, 540, 540, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 540, 17280, 17280, 34560, 17280, 34560, 540, 540, 540, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 540, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 540, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 2810700 . Total input tokens: 625934726 . Total output tokens: 562216680
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.875053212977946,
    "estimated_duration": 3600.0432335100895,
    "input_throughput": 4547.717329504709,
    "output_throughput": 4009.0712982709933,
    "total_throughput": 8556.788627775702,
    "itl": 127.30123058025752,
    "ttft": 2194085.7459499496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642179,
    "arrivals": 935737,
    "finished_requests": 66097,
    "scheduler_time": 27.552891190092776
}
#Debug simulation 
Total elapsed time: 4.875143573968671. Arrivals time: 0.238502713968046 Scheduler time: 4.496885962900706 Scheduler overhead time: 0.04203094623517245 Adapter cache time: 0.035013390821404755 Engine time: 0.04301016661338508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.231395100010559,
    "estimated_duration": 3600.04795752041,
    "input_throughput": 5052.587969558146,
    "output_throughput": 4469.251018278632,
    "total_throughput": 9521.838987836778,
    "itl": 192.20750460872338,
    "ttft": 2140104.8432965395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 931056,
    "finished_requests": 73556,
    "scheduler_time": 45.189368041427905
}
#Debug simulation 
Total elapsed time: 5.2315163280582055. Arrivals time: 0.25831390311941504 Scheduler time: 4.886815722682513 Scheduler overhead time: 0.02898870629724115 Adapter cache time: 0.01402366568800062 Engine time: 0.02971823769621551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.343846683041193,
    "estimated_duration": 3600.0802585706356,
    "input_throughput": 5052.542636152763,
    "output_throughput": 4469.210918755498,
    "total_throughput": 9521.75355490826,
    "itl": 192.20842679688803,
    "ttft": 2140127.8491126555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 931056,
    "finished_requests": 73556,
    "scheduler_time": 45.189219384425634
}
#Debug simulation 
Total elapsed time: 5.343941122060642. Arrivals time: 0.3224258901318535 Scheduler time: 4.934355030185543 Scheduler overhead time: 0.02927270985674113 Adapter cache time: 0.01423905580304563 Engine time: 0.02998141013085842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.924209570046514,
    "estimated_duration": 3600.0827772843286,
    "input_throughput": 4565.370858610659,
    "output_throughput": 4057.1586553956713,
    "total_throughput": 8622.52951400633,
    "itl": 125.38661263147286,
    "ttft": 2191525.251893443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 931056,
    "finished_requests": 66602,
    "scheduler_time": 27.78965072711893
}
#Debug simulation 
Total elapsed time: 4.924303501029499. Arrivals time: 0.2367998770205304 Scheduler time: 4.551854045013897 Scheduler overhead time: 0.042237342800945044 Adapter cache time: 0.03021629573777318 Engine time: 0.04339553357567638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.234348351019435,
    "estimated_duration": 3600.0587064804263,
    "input_throughput": 5052.572883674695,
    "output_throughput": 4469.237674107212,
    "total_throughput": 9521.810557781908,
    "itl": 192.2078314694837,
    "ttft": 2140112.413440321,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801272,
    "arrivals": 931056,
    "finished_requests": 73556,
    "scheduler_time": 45.18932284609342
}
#Debug simulation 
Total elapsed time: 5.23444274906069. Arrivals time: 0.2594739916967228 Scheduler time: 4.888506146380678 Scheduler overhead time: 0.029142748680897057 Adapter cache time: 0.014255312155000865 Engine time: 0.029553294996730983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.924221540917642,
    "estimated_duration": 3600.089399211345,
    "input_throughput": 4565.362461165685,
    "output_throughput": 4057.151192745295,
    "total_throughput": 8622.513653910979,
    "itl": 125.38678864734729,
    "ttft": 2191529.875197177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978162,
    "arrivals": 931056,
    "finished_requests": 66602,
    "scheduler_time": 27.789607703455097
}
#Debug simulation 
Total elapsed time: 4.924311475944705. Arrivals time: 0.23845253884792328 Scheduler time: 4.550247536972165 Scheduler overhead time: 0.042232615407556295 Adapter cache time: 0.03010862902738154 Engine time: 0.04338587180245668 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.295756247011013,
    "estimated_duration": 3600.0367382964887,
    "input_throughput": 5052.603715540738,
    "output_throughput": 4469.264946338699,
    "total_throughput": 9521.868661879436,
    "itl": 192.20717751064225,
    "ttft": 2140096.953890568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 931056,
    "finished_requests": 73556,
    "scheduler_time": 45.189418809351224
}
#Debug simulation 
Total elapsed time: 5.2958506379509345. Arrivals time: 0.2661306490190327 Scheduler time: 4.942914688843302 Scheduler overhead time: 0.0291448364732787 Adapter cache time: 0.01421856100205332 Engine time: 0.0298229088075459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 17280, 270, 270, 270, 34560, 270, 17280, 34560, 17280, 270, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 270, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 270, 17280, 17280, 17280, 34560, 270, 270, 34560, 270, 34560, 270, 270, 17280, 270, 17280, 34560, 270, 270, 17280, 270, 270, 270, 270, 270, 34560, 17280, 17280, 34560, 17280, 270, 34560, 34560, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 270, 270, 34560, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 270, 17280, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 270, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 270, 17280, 17280, 34560, 17280, 34560, 270, 270, 270, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 270, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 270, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 2796390 . Total input tokens: 622717176 . Total output tokens: 559314112
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.9866056779865175,
    "estimated_duration": 3600.0961529564356,
    "input_throughput": 4565.353896590464,
    "output_throughput": 4057.14358156943,
    "total_throughput": 8622.497478159894,
    "itl": 125.38695645497344,
    "ttft": 2191534.4994675326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642178,
    "arrivals": 931056,
    "finished_requests": 66602,
    "scheduler_time": 27.789570744079118
}
#Debug simulation 
Total elapsed time: 4.98674518999178. Arrivals time: 0.24668786488473415 Scheduler time: 4.601723534753546 Scheduler overhead time: 0.04386135144159198 Adapter cache time: 0.03036453272216022 Engine time: 0.04375463933683932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.414516846067272,
    "estimated_duration": 3600.091612910327,
    "input_throughput": 5147.144015321357,
    "output_throughput": 4520.116638599922,
    "total_throughput": 9667.26065392128,
    "itl": 189.52169293897387,
    "ttft": 2134643.969195716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 928543,
    "finished_requests": 74708,
    "scheduler_time": 45.59716623078782
}
#Debug simulation 
Total elapsed time: 5.414607702055946. Arrivals time: 0.35962540947366506 Scheduler time: 4.970330238924362 Scheduler overhead time: 0.029645248199813068 Adapter cache time: 0.010760613484308124 Engine time: 0.030418515671044588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.366978247999214,
    "estimated_duration": 3600.1239102728114,
    "input_throughput": 5147.097839361816,
    "output_throughput": 4520.076087816342,
    "total_throughput": 9667.173927178157,
    "itl": 189.52261006166336,
    "ttft": 2134665.9974342324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574656,
    "arrivals": 928543,
    "finished_requests": 74708,
    "scheduler_time": 45.59701388604294
}
#Debug simulation 
Total elapsed time: 5.3670977839501575. Arrivals time: 0.35978527658153325 Scheduler time: 4.923008938785642 Scheduler overhead time: 0.029495138092897832 Adapter cache time: 0.010691014002077281 Engine time: 0.03027358604595065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.154065012000501,
    "estimated_duration": 3600.087410388107,
    "input_throughput": 4636.587976123754,
    "output_throughput": 4081.0222989602707,
    "total_throughput": 8717.610275084025,
    "itl": 124.23472985669,
    "ttft": 2190389.3506745375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.519667731374504,
    "arrivals": 928543,
    "finished_requests": 67207,
    "scheduler_time": 27.808631853747308
}
#Debug simulation 
Total elapsed time: 5.154155705939047. Arrivals time: 0.40264874196145684 Scheduler time: 4.619474676554091 Scheduler overhead time: 0.042888456606306136 Adapter cache time: 0.02525508007965982 Engine time: 0.04378608171828091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.456910751061514,
    "estimated_duration": 3600.1023560519075,
    "input_throughput": 5147.128655620042,
    "output_throughput": 4520.103150024263,
    "total_throughput": 9667.231805644305,
    "itl": 189.52201950012423,
    "ttft": 2134651.5663669845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 928543,
    "finished_requests": 74708,
    "scheduler_time": 45.59711521701499
}
#Debug simulation 
Total elapsed time: 5.457018326036632. Arrivals time: 0.3716895419638604 Scheduler time: 5.0009630606509745 Scheduler overhead time: 0.029516942566260695 Adapter cache time: 0.01068659289740026 Engine time: 0.03036068263463676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.079302793019451,
    "estimated_duration": 3600.093909388605,
    "input_throughput": 4636.579606012217,
    "output_throughput": 4081.0149317730193,
    "total_throughput": 8717.594537785237,
    "itl": 124.23488675638154,
    "ttft": 2190393.6314616282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5262069282680762,
    "arrivals": 928543,
    "finished_requests": 67207,
    "scheduler_time": 27.808591657352814
}
#Debug simulation 
Total elapsed time: 5.079393679974601. Arrivals time: 0.3412740414496511 Scheduler time: 4.606073373113759 Scheduler overhead time: 0.04254646017216146 Adapter cache time: 0.02534284070134163 Engine time: 0.043998788692988455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.436942421016283,
    "estimated_duration": 3600.0803932356844,
    "input_throughput": 5147.1600564301325,
    "output_throughput": 4520.130725573682,
    "total_throughput": 9667.290782003814,
    "itl": 189.52136597977622,
    "ttft": 2134636.3263423177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 928543,
    "finished_requests": 74708,
    "scheduler_time": 45.597216547987045
}
#Debug simulation 
Total elapsed time: 5.437032054993324. Arrivals time: 0.35977514926344156 Scheduler time: 4.992692952626385 Scheduler overhead time: 0.029525287332944572 Adapter cache time: 0.010765612707473338 Engine time: 0.030442788847722113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 17280, 135, 135, 135, 34560, 135, 17280, 34560, 17280, 135, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 135, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 135, 17280, 17280, 17280, 34560, 135, 135, 34560, 135, 34560, 135, 135, 17280, 135, 17280, 34560, 135, 135, 17280, 135, 135, 135, 135, 135, 34560, 17280, 17280, 34560, 17280, 135, 34560, 34560, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 135, 135, 34560, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 135, 17280, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 135, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 135, 17280, 17280, 34560, 17280, 34560, 135, 135, 135, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 135, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 135, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 2789235 . Total input tokens: 621133368 . Total output tokens: 557879875
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.152147494023666,
    "estimated_duration": 3600.1006607571503,
    "input_throughput": 4636.57091090598,
    "output_throughput": 4081.007278532613,
    "total_throughput": 8717.578189438593,
    "itl": 124.23506914042994,
    "ttft": 2190398.1681724093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5329976327344776,
    "arrivals": 928543,
    "finished_requests": 67207,
    "scheduler_time": 27.808552321431595
}
#Debug simulation 
Total elapsed time: 5.152261872077361. Arrivals time: 0.3996495137689635 Scheduler time: 4.621242530527525 Scheduler overhead time: 0.04265217867214233 Adapter cache time: 0.025079984334297478 Engine time: 0.04345049255061895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.4726492000045255,
    "estimated_duration": 3600.1836756681737,
    "input_throughput": 5159.340931835281,
    "output_throughput": 4544.311477931362,
    "total_throughput": 9703.652409766642,
    "itl": 188.49836977374903,
    "ttft": 2127577.2227773783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4468312106141815,
    "arrivals": 927414,
    "finished_requests": 75235,
    "scheduler_time": 45.906144996751884
}
#Debug simulation 
Total elapsed time: 5.472792194923386. Arrivals time: 0.26784427219536155 Scheduler time: 5.1219655104214326 Scheduler overhead time: 0.029762018006294966 Adapter cache time: 0.008699741214513779 Engine time: 0.030523129156790674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.475669439998455,
    "estimated_duration": 3600.003821669612,
    "input_throughput": 5159.297578574785,
    "output_throughput": 4544.330453630666,
    "total_throughput": 9703.62803220545,
    "itl": 188.50017099467163,
    "ttft": 2127486.9740433544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47781547021819276,
    "arrivals": 927414,
    "finished_requests": 75229,
    "scheduler_time": 45.903295950981466
}
#Debug simulation 
Total elapsed time: 5.475761595997028. Arrivals time: 0.3604029018897563 Scheduler time: 5.032259368454106 Scheduler overhead time: 0.02980835852213204 Adapter cache time: 0.008696503238752484 Engine time: 0.030580314807593822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.092372686020099,
    "estimated_duration": 3600.093476417622,
    "input_throughput": 4633.543298047667,
    "output_throughput": 4097.902484098895,
    "total_throughput": 8731.445782146562,
    "itl": 124.17300364245622,
    "ttft": 2182371.978289547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46861069733277105,
    "arrivals": 927414,
    "finished_requests": 67680,
    "scheduler_time": 28.12370708678606
}
#Debug simulation 
Total elapsed time: 5.092465340043418. Arrivals time: 0.3435210542520508 Scheduler time: 4.6189623104874045 Scheduler overhead time: 0.04279082000721246 Adapter cache time: 0.02302483725361526 Engine time: 0.04407213081140071 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.481993526918814,
    "estimated_duration": 3600.1933691280597,
    "input_throughput": 5159.327040396895,
    "output_throughput": 4544.299242449401,
    "total_throughput": 9703.626282846297,
    "itl": 188.49864785283282,
    "ttft": 2127584.007183566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.45656851365929496,
    "arrivals": 927414,
    "finished_requests": 75235,
    "scheduler_time": 45.90610115358969
}
#Debug simulation 
Total elapsed time: 5.482114470913075. Arrivals time: 0.36333624611143023 Scheduler time: 5.0354231231613085 Scheduler overhead time: 0.029947906616143882 Adapter cache time: 0.008661956642754376 Engine time: 0.0307714733062312 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.108873337972909,
    "estimated_duration": 3600.099850237983,
    "input_throughput": 4633.535094560585,
    "output_throughput": 4097.895228940601,
    "total_throughput": 8731.430323501185,
    "itl": 124.17315479798289,
    "ttft": 2182376.2459638515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47502414043992813,
    "arrivals": 927414,
    "finished_requests": 67680,
    "scheduler_time": 28.123667464040416
}
#Debug simulation 
Total elapsed time: 5.108966249041259. Arrivals time: 0.3404779675183818 Scheduler time: 4.638712620246224 Scheduler overhead time: 0.04280947463121265 Adapter cache time: 0.023050524527207017 Engine time: 0.043781745713204145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.535563344019465,
    "estimated_duration": 3600.1734396183483,
    "input_throughput": 5159.355600926015,
    "output_throughput": 4544.324398364082,
    "total_throughput": 9703.679999290098,
    "itl": 188.49810228103797,
    "ttft": 2127570.391818582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4365473430557182,
    "arrivals": 927414,
    "finished_requests": 75235,
    "scheduler_time": 45.906192814481116
}
#Debug simulation 
Total elapsed time: 5.5356694790534675. Arrivals time: 0.37444783828686923 Scheduler time: 5.077811127412133 Scheduler overhead time: 0.029859431902877986 Adapter cache time: 0.008787721511907876 Engine time: 0.0307130990549922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 17280, 66, 66, 66, 34560, 66, 17280, 34560, 17280, 66, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 66, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 66, 17280, 17280, 17280, 34560, 66, 66, 34560, 66, 34560, 66, 66, 17280, 66, 17280, 34560, 66, 66, 17280, 66, 66, 66, 66, 66, 34560, 17280, 17280, 34560, 17280, 66, 34560, 34560, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 66, 66, 34560, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 66, 17280, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 66, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 66, 17280, 17280, 34560, 17280, 34560, 66, 66, 66, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 66, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 66, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 2785578 . Total input tokens: 620302199 . Total output tokens: 557146111
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.093681170023046,
    "estimated_duration": 3600.1058490915807,
    "input_throughput": 4633.527373704633,
    "output_throughput": 4097.888400620943,
    "total_throughput": 8731.415774325576,
    "itl": 124.17330025235457,
    "ttft": 2182380.296368831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4810603221878406,
    "arrivals": 927414,
    "finished_requests": 67680,
    "scheduler_time": 28.123630135890174
}
#Debug simulation 
Total elapsed time: 5.093770998995751. Arrivals time: 0.33650769642554224 Scheduler time: 4.626662232214585 Scheduler overhead time: 0.04294457461219281 Adapter cache time: 0.02351897966582328 Engine time: 0.04399767820723355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.464460254996084,
    "estimated_duration": 3600.126315693631,
    "input_throughput": 5155.292446016336,
    "output_throughput": 4555.189891119245,
    "total_throughput": 9710.482337135581,
    "itl": 188.4338133177484,
    "ttft": 2131211.446525101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.416226333174854,
    "arrivals": 926784,
    "finished_requests": 75233,
    "scheduler_time": 46.038919046476266
}
#Debug simulation 
Total elapsed time: 5.464549012016505. Arrivals time: 0.3597688671434298 Scheduler time: 5.022863888298161 Scheduler overhead time: 0.02966384426690638 Adapter cache time: 0.007639145478606224 Engine time: 0.030711681116372347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.5257858879631385,
    "estimated_duration": 3600.154998945389,
    "input_throughput": 5155.251372631675,
    "output_throughput": 4555.153598887802,
    "total_throughput": 9710.404971519476,
    "itl": 188.43448557571517,
    "ttft": 2131230.6752349576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.44505480004008857,
    "arrivals": 926784,
    "finished_requests": 75233,
    "scheduler_time": 46.0387738313671
}
#Debug simulation 
Total elapsed time: 5.525952227064408. Arrivals time: 0.4227783613605425 Scheduler time: 5.020946649252437 Scheduler overhead time: 0.029888474149629474 Adapter cache time: 0.007699740584939718 Engine time: 0.03058568644337356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.080504238023423,
    "estimated_duration": 3600.1087067124786,
    "input_throughput": 4623.855098864785,
    "output_throughput": 4102.947217249039,
    "total_throughput": 8726.802316113824,
    "itl": 123.90977242855774,
    "ttft": 2186508.5241989186,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43953279022127517,
    "arrivals": 926784,
    "finished_requests": 67468,
    "scheduler_time": 28.120723719977118
}
#Debug simulation 
Total elapsed time: 5.080592204001732. Arrivals time: 0.3371883563231677 Scheduler time: 4.615329476306215 Scheduler overhead time: 0.04257451184093952 Adapter cache time: 0.02150231401901692 Engine time: 0.043789078132249415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.548069100012071,
    "estimated_duration": 3600.1363044819636,
    "input_throughput": 5155.2781423565075,
    "output_throughput": 4555.177252478986,
    "total_throughput": 9710.455394835493,
    "itl": 188.43407499058014,
    "ttft": 2131217.6180834905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4262594153918328,
    "arrivals": 926784,
    "finished_requests": 75233,
    "scheduler_time": 46.03887475258998
}
#Debug simulation 
Total elapsed time: 5.548184093087912. Arrivals time: 0.3730247204657644 Scheduler time: 5.092851273715496 Scheduler overhead time: 0.02974464255385101 Adapter cache time: 0.00771076581440866 Engine time: 0.030809696530923247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.075924061005935,
    "estimated_duration": 3600.1144503707606,
    "input_throughput": 4623.847721920524,
    "output_throughput": 4102.940671366376,
    "total_throughput": 8726.788393286899,
    "itl": 123.90989471518273,
    "ttft": 2186512.4077670304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4453174643963581,
    "arrivals": 926784,
    "finished_requests": 67468,
    "scheduler_time": 28.120682704084263
}
#Debug simulation 
Total elapsed time: 5.076023016008548. Arrivals time: 0.24361681228037924 Scheduler time: 4.704201137879863 Scheduler overhead time: 0.042689775000326335 Adapter cache time: 0.02171460387762636 Engine time: 0.04366645903792232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.445039383950643,
    "estimated_duration": 3600.116783362697,
    "input_throughput": 5155.306096116212,
    "output_throughput": 4555.201952277292,
    "total_throughput": 9710.508048393503,
    "itl": 188.4335727518289,
    "ttft": 2131205.0532708606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 136,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4066468401066964,
    "arrivals": 926784,
    "finished_requests": 75233,
    "scheduler_time": 46.03896620860681
}
#Debug simulation 
Total elapsed time: 5.445132303982973. Arrivals time: 0.36609590449370444 Scheduler time: 4.996244105161168 Scheduler overhead time: 0.030280716600827873 Adapter cache time: 0.007852989132516086 Engine time: 0.030830177594907582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 17280, 33, 33, 33, 34560, 33, 17280, 34560, 17280, 33, 17280, 17280, 17280, 17280, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 34560, 33, 34560, 17280, 34560, 34560, 34560, 17280, 34560, 33, 17280, 17280, 17280, 34560, 33, 33, 34560, 33, 34560, 33, 33, 17280, 33, 17280, 34560, 33, 33, 17280, 33, 33, 33, 33, 33, 34560, 17280, 17280, 34560, 17280, 33, 34560, 34560, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 33, 33, 34560, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 33, 17280, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 33, 34560, 17280, 34560, 17280, 34560, 34560, 34560, 33, 17280, 17280, 34560, 17280, 34560, 33, 33, 33, 17280, 17280, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 33, 34560, 34560, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 34560, 33, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 2783829 . Total input tokens: 619926946 . Total output tokens: 556795169
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.0927358500193805,
    "estimated_duration": 3600.120447462435,
    "input_throughput": 4623.840019500818,
    "output_throughput": 4102.933836675232,
    "total_throughput": 8726.773856176049,
    "itl": 123.91003559109932,
    "ttft": 2186516.104814815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 134,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.45135364614427065,
    "arrivals": 926784,
    "finished_requests": 67468,
    "scheduler_time": 28.12064361401255
}
#Debug simulation 
Total elapsed time: 5.092857306008227. Arrivals time: 0.34080739284399897 Scheduler time: 4.623515528510325 Scheduler overhead time: 0.04270118603017181 Adapter cache time: 0.02196511160582304 Engine time: 0.04368810052983463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.665256736101583,
    "estimated_duration": 3600.2214845096514,
    "input_throughput": 4305.801758780222,
    "output_throughput": 3815.911620746057,
    "total_throughput": 8121.71337952628,
    "itl": 225.04817194449572,
    "ttft": 2200279.2731865183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 850231,
    "finished_requests": 63119,
    "scheduler_time": 38.75337254249325
}
#Debug simulation 
Total elapsed time: 4.665346588008106. Arrivals time: 0.23397145315539092 Scheduler time: 4.333503035479225 Scheduler overhead time: 0.02518845140002668 Adapter cache time: 0.03481166681740433 Engine time: 0.025906410301104188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.710558737046085,
    "estimated_duration": 3600.0029148216245,
    "input_throughput": 4306.062902387426,
    "output_throughput": 3816.0096880590118,
    "total_throughput": 8122.072590446438,
    "itl": 225.04867053945827,
    "ttft": 2200243.7225345923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 850231,
    "finished_requests": 63118,
    "scheduler_time": 38.75044262396768
}
#Debug simulation 
Total elapsed time: 4.710667083971202. Arrivals time: 0.33172567607834935 Scheduler time: 4.2811513940105215 Scheduler overhead time: 0.02532406710088253 Adapter cache time: 0.03467973100487143 Engine time: 0.025948501541279256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.6689466630341485,
    "estimated_duration": 3600.1176699475336,
    "input_throughput": 4090.0143689510537,
    "output_throughput": 3637.017508983483,
    "total_throughput": 7727.031877934537,
    "itl": 140.85597262138245,
    "ttft": 2233340.6765024974,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 850231,
    "finished_requests": 59943,
    "scheduler_time": 25.13928680142027
}
#Debug simulation 
Total elapsed time: 4.669086149078794. Arrivals time: 0.3694136643316597 Scheduler time: 4.12307809421327 Scheduler overhead time: 0.03836030140519142 Adapter cache time: 0.08090833318419755 Engine time: 0.03931903839111328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.71241658704821,
    "estimated_duration": 3600.2322231439884,
    "input_throughput": 4305.788915600186,
    "output_throughput": 3815.9002387915,
    "total_throughput": 8121.689154391686,
    "itl": 225.048438452279,
    "ttft": 2200286.812783624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 850231,
    "finished_requests": 63119,
    "scheduler_time": 38.75331702147945
}
#Debug simulation 
Total elapsed time: 4.712513386970386. Arrivals time: 0.33001853025052696 Scheduler time: 4.2842998639680445 Scheduler overhead time: 0.025318248197436333 Adapter cache time: 0.034989971783943474 Engine time: 0.026040112366899848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.63520596397575,
    "estimated_duration": 3600.1242919974743,
    "input_throughput": 4090.0068457998477,
    "output_throughput": 3637.0108190723504,
    "total_throughput": 7727.017664872198,
    "itl": 140.85618575715813,
    "ttft": 2233345.268998174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978161,
    "arrivals": 850231,
    "finished_requests": 59943,
    "scheduler_time": 25.139243900681194
}
#Debug simulation 
Total elapsed time: 4.6353024820564315. Arrivals time: 0.30877218139357865 Scheduler time: 4.149665756733157 Scheduler overhead time: 0.0381876501487568 Adapter cache time: 0.08107548742555082 Engine time: 0.03950765333138406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.659761283081025,
    "estimated_duration": 3600.2102722924405,
    "input_throughput": 4305.8151684371405,
    "output_throughput": 3815.923504726912,
    "total_throughput": 8121.738673164053,
    "itl": 225.04783624660308,
    "ttft": 2200271.410045984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 850231,
    "finished_requests": 63119,
    "scheduler_time": 38.75343031712754
}
#Debug simulation 
Total elapsed time: 4.659855761099607. Arrivals time: 0.2290068456204608 Scheduler time: 4.33352104621008 Scheduler overhead time: 0.02507924591191113 Adapter cache time: 0.03476829803548753 Engine time: 0.025745489518158138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [4320, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 8640, 4320, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 4320, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 8640, 34560, 4320, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 4320, 8640, 34560, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 8640, 34560, 8640, 4320, 34560, 34560, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 4320, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 4320, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 4320, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 2553120 . Total input tokens: 568776188 . Total output tokens: 510562220
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.608717461931519,
    "estimated_duration": 3600.1310355807855,
    "input_throughput": 4089.999184605954,
    "output_throughput": 3637.0040064076948,
    "total_throughput": 7727.003191013649,
    "itl": 140.85637104459818,
    "ttft": 2233349.949668118,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642179,
    "arrivals": 850231,
    "finished_requests": 59943,
    "scheduler_time": 25.139196779525566
}
#Debug simulation 
Total elapsed time: 4.608809669967741. Arrivals time: 0.30807082762476057 Scheduler time: 4.125155483954586 Scheduler overhead time: 0.038108144188299775 Adapter cache time: 0.08036610076669604 Engine time: 0.03915246936958283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.059385191067122,
    "estimated_duration": 3600.0097963730304,
    "input_throughput": 4828.81852641456,
    "output_throughput": 4238.8573540478155,
    "total_throughput": 9067.675880462377,
    "itl": 201.7207697160306,
    "ttft": 2143391.2244407274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 793158,
    "finished_requests": 70226,
    "scheduler_time": 42.83788381450383
}
#Debug simulation 
Total elapsed time: 5.059491530060768. Arrivals time: 0.2976769933011383 Scheduler time: 4.65818689938169 Scheduler overhead time: 0.027995713287964463 Adapter cache time: 0.03401844424661249 Engine time: 0.028598962002433836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.06133402895648,
    "estimated_duration": 3600.042084434207,
    "input_throughput": 4828.775217701958,
    "output_throughput": 4238.819336579587,
    "total_throughput": 9067.594554281546,
    "itl": 201.721754221608,
    "ttft": 2143414.4591114568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 793158,
    "finished_requests": 70226,
    "scheduler_time": 42.83772216845257
}
#Debug simulation 
Total elapsed time: 5.061426949920133. Arrivals time: 0.25174904440063983 Scheduler time: 4.7052318811183795 Scheduler overhead time: 0.028189731179736555 Adapter cache time: 0.03404145105741918 Engine time: 0.028866299311630428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.9223492400487885,
    "estimated_duration": 3600.131438209866,
    "input_throughput": 4533.739470388893,
    "output_throughput": 3991.9542512998173,
    "total_throughput": 8525.69372168871,
    "itl": 127.63092269421273,
    "ttft": 2181301.546069025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 793158,
    "finished_requests": 65922,
    "scheduler_time": 27.471321861060673
}
#Debug simulation 
Total elapsed time: 4.922462426009588. Arrivals time: 0.22970878949854523 Scheduler time: 4.526681554620154 Scheduler overhead time: 0.041785609559156 Adapter cache time: 0.061850419151596725 Engine time: 0.042768989223986864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.075828535016626,
    "estimated_duration": 3600.020535908816,
    "input_throughput": 4828.804121144133,
    "output_throughput": 4238.844708742105,
    "total_throughput": 9067.648829886237,
    "itl": 201.7210978816088,
    "ttft": 2143398.9702227185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 793158,
    "finished_requests": 70226,
    "scheduler_time": 42.83782919493822
}
#Debug simulation 
Total elapsed time: 5.075981231988408. Arrivals time: 0.24297861370723695 Scheduler time: 4.729018913349137 Scheduler overhead time: 0.02803828683681786 Adapter cache time: 0.034065278130583465 Engine time: 0.028717099456116557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.8543268060311675,
    "estimated_duration": 3600.13805862081,
    "input_throughput": 4533.731133147954,
    "output_throughput": 3991.9469103653355,
    "total_throughput": 8525.67804351329,
    "itl": 127.63109784246052,
    "ttft": 2181306.2378825755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978163,
    "arrivals": 793158,
    "finished_requests": 65922,
    "scheduler_time": 27.47127732132488
}
#Debug simulation 
Total elapsed time: 4.854448085068725. Arrivals time: 0.22723815275821835 Scheduler time: 4.460610055015422 Scheduler overhead time: 0.04176093661226332 Adapter cache time: 0.06220389937516302 Engine time: 0.04306793853174895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.001521182013676,
    "estimated_duration": 3600.223723804544,
    "input_throughput": 4829.014898448533,
    "output_throughput": 4238.881850340397,
    "total_throughput": 9067.89674878893,
    "itl": 201.72049312194736,
    "ttft": 2143451.347902977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 793158,
    "finished_requests": 70232,
    "scheduler_time": 42.840601295685175
}
#Debug simulation 
Total elapsed time: 5.001619249000214. Arrivals time: 0.2442864878103137 Scheduler time: 4.653552466421388 Scheduler overhead time: 0.02797804679721594 Adapter cache time: 0.03405957156792283 Engine time: 0.028687675367109478 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 8640, 1080, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 1080, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 8640, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 1080, 8640, 34560, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 8640, 34560, 8640, 1080, 34560, 34560, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 1080, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 1080, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2381400 . Total input tokens: 530601632 . Total output tokens: 476239805
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.898761832038872,
    "estimated_duration": 3600.006329869091,
    "input_throughput": 4533.723972811321,
    "output_throughput": 3991.990203117945,
    "total_throughput": 8525.714175929266,
    "itl": 127.63104242050977,
    "ttft": 2181318.0827536057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642179,
    "arrivals": 793158,
    "finished_requests": 65920,
    "scheduler_time": 27.470188691954284
}
#Debug simulation 
Total elapsed time: 4.8988514160737395. Arrivals time: 0.2272826216649264 Scheduler time: 4.505200002808124 Scheduler overhead time: 0.0418663740856573 Adapter cache time: 0.06168414850253612 Engine time: 0.04312018665950745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.243114735931158,
    "estimated_duration": 3600.0141871372816,
    "input_throughput": 4999.386131395175,
    "output_throughput": 4374.311650288918,
    "total_throughput": 9373.697781684094,
    "itl": 195.05160819887737,
    "ttft": 2125493.2028136402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 783512,
    "finished_requests": 72540,
    "scheduler_time": 44.186687327721124
}
#Debug simulation 
Total elapsed time: 5.243229114916176. Arrivals time: 0.3297507285606116 Scheduler time: 4.814488650998101 Scheduler overhead time: 0.028850155766122043 Adapter cache time: 0.027385673369280994 Engine time: 0.02933959919027984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.267092140042223,
    "estimated_duration": 3600.064462885581,
    "input_throughput": 4999.316313790134,
    "output_throughput": 4374.250561996255,
    "total_throughput": 9373.566875786388,
    "itl": 195.05314221627316,
    "ttft": 2125511.9768988695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574656,
    "arrivals": 783512,
    "finished_requests": 72540,
    "scheduler_time": 44.186871462961435
}
#Debug simulation 
Total elapsed time: 5.2671825010329485. Arrivals time: 0.33280110103078187 Scheduler time: 4.835162347997539 Scheduler overhead time: 0.028692893218249083 Adapter cache time: 0.027411873335950077 Engine time: 0.02962382521945983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.0959236229537055,
    "estimated_duration": 3600.091939760294,
    "input_throughput": 4642.096724094424,
    "output_throughput": 4076.9592125964627,
    "total_throughput": 8719.055936690887,
    "itl": 124.2337702875601,
    "ttft": 2167636.1704234867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 783512,
    "finished_requests": 67433,
    "scheduler_time": 27.846145172874042
}
#Debug simulation 
Total elapsed time: 5.096028423984535. Arrivals time: 0.3359880590578541 Scheduler time: 4.596221702871844 Scheduler overhead time: 0.04481819458305836 Adapter cache time: 0.05484395509120077 Engine time: 0.04400894686114043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.233151284977794,
    "estimated_duration": 3600.0331875814363,
    "input_throughput": 4999.359745372589,
    "output_throughput": 4374.288563317245,
    "total_throughput": 9373.648308689833,
    "itl": 195.05240448362719,
    "ttft": 2125499.9240548615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801272,
    "arrivals": 783512,
    "finished_requests": 72540,
    "scheduler_time": 44.186698958713315
}
#Debug simulation 
Total elapsed time: 5.233243921073154. Arrivals time: 0.3320729860570282 Scheduler time: 4.802342163515277 Scheduler overhead time: 0.028716502361930907 Adapter cache time: 0.02726724394597113 Engine time: 0.029375688871368766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.130156178027391,
    "estimated_duration": 3600.098558491266,
    "input_throughput": 4642.088189664362,
    "output_throughput": 4076.9517171638313,
    "total_throughput": 8719.039906828193,
    "itl": 124.23393302734708,
    "ttft": 2167640.825641641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978162,
    "arrivals": 783512,
    "finished_requests": 67433,
    "scheduler_time": 27.84609895316661
}
#Debug simulation 
Total elapsed time: 5.1302460860461. Arrivals time: 0.3240487478906289 Scheduler time: 4.643346861586906 Scheduler overhead time: 0.043039627838879824 Adapter cache time: 0.055254079634323716 Engine time: 0.044309551594778895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.29085727292113,
    "estimated_duration": 3600.0029698801563,
    "input_throughput": 4999.401708993353,
    "output_throughput": 4374.325280216153,
    "total_throughput": 9373.726989209506,
    "itl": 195.05126935176352,
    "ttft": 2125485.1999880904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 783512,
    "finished_requests": 72540,
    "scheduler_time": 44.18674006244051
}
#Debug simulation 
Total elapsed time: 5.290949801914394. Arrivals time: 0.25025595107581466 Scheduler time: 4.941077727125958 Scheduler overhead time: 0.028807564871385694 Adapter cache time: 0.027623590896837413 Engine time: 0.029684437671676278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 8640, 540, 540, 540, 34560, 540, 8640, 34560, 8640, 540, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 540, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 540, 8640, 8640, 8640, 34560, 540, 540, 34560, 540, 34560, 540, 540, 8640, 540, 8640, 34560, 540, 540, 8640, 540, 540, 540, 540, 540, 34560, 8640, 8640, 34560, 8640, 540, 34560, 34560, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 540, 540, 34560, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 540, 8640, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 540, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 540, 8640, 8640, 34560, 8640, 34560, 540, 540, 540, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 540, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 540, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2352780 . Total input tokens: 524176632 . Total output tokens: 470532189
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.100821416941471,
    "estimated_duration": 3600.125211968646,
    "input_throughput": 4637.107327406309,
    "output_throughput": 4071.774768073724,
    "total_throughput": 8708.882095480034,
    "itl": 123.96653856974301,
    "ttft": 2168506.1313788327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.536504152864218,
    "arrivals": 783512,
    "finished_requests": 67351,
    "scheduler_time": 27.692066387624614
}
#Debug simulation 
Total elapsed time: 5.1009161829715595. Arrivals time: 0.32393814192619175 Scheduler time: 4.6134266153676435 Scheduler overhead time: 0.04299078299663961 Adapter cache time: 0.0548869464546442 Engine time: 0.04549270530696958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.378938156994991,
    "estimated_duration": 3600.030953697858,
    "input_throughput": 5064.962283524393,
    "output_throughput": 4474.890968215839,
    "total_throughput": 9539.853251740233,
    "itl": 191.7541686715182,
    "ttft": 2118980.06544847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 778725,
    "finished_requests": 73996,
    "scheduler_time": 45.29979251714913
}
#Debug simulation 
Total elapsed time: 5.3790342680877075. Arrivals time: 0.33785275579430163 Scheduler time: 4.946473013958894 Scheduler overhead time: 0.02938237541820854 Adapter cache time: 0.021565817412920296 Engine time: 0.029917531181126833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.35046318906825,
    "estimated_duration": 3600.033039917699,
    "input_throughput": 5065.052125304066,
    "output_throughput": 4475.358370702155,
    "total_throughput": 9540.410496006221,
    "itl": 191.73022773196902,
    "ttft": 2119044.5804473003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 778725,
    "finished_requests": 74001,
    "scheduler_time": 45.305218435655746
}
#Debug simulation 
Total elapsed time: 5.350553850992583. Arrivals time: 0.33924991090316325 Scheduler time: 4.916991814505309 Scheduler overhead time: 0.029271046514622867 Adapter cache time: 0.021609512739814818 Engine time: 0.02978981623891741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.484786699991673,
    "estimated_duration": 3600.1341866904622,
    "input_throughput": 4672.137239268466,
    "output_throughput": 4139.735417390264,
    "total_throughput": 8811.87265665873,
    "itl": 122.95341194017394,
    "ttft": 2165399.0863803183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 778725,
    "finished_requests": 68274,
    "scheduler_time": 28.503837358629678
}
#Debug simulation 
Total elapsed time: 5.484889241983183. Arrivals time: 0.5664708660915494 Scheduler time: 4.761680000927299 Scheduler overhead time: 0.04314647684805095 Adapter cache time: 0.048860441078431904 Engine time: 0.04428827797528356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.368966110050678,
    "estimated_duration": 3600.0114863523927,
    "input_throughput": 5065.082450188355,
    "output_throughput": 4475.385165041362,
    "total_throughput": 9540.467615229716,
    "itl": 191.72961026274456,
    "ttft": 2119029.197565244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 778725,
    "finished_requests": 74001,
    "scheduler_time": 45.305320422226494
}
#Debug simulation 
Total elapsed time: 5.369059828110039. Arrivals time: 0.3350183083675802 Scheduler time: 4.93885775806848 Scheduler overhead time: 0.029169541667215526 Adapter cache time: 0.022471091127954423 Engine time: 0.029844892327673733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.168797263992019,
    "estimated_duration": 3600.140808166755,
    "input_throughput": 4672.128646147359,
    "output_throughput": 4139.72780347698,
    "total_throughput": 8811.856449624338,
    "itl": 122.95358608436608,
    "ttft": 2165403.716466925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978162,
    "arrivals": 778725,
    "finished_requests": 68274,
    "scheduler_time": 28.503793884241748
}
#Debug simulation 
Total elapsed time: 5.168890696950257. Arrivals time: 0.32190919818822294 Scheduler time: 4.6902429005131125 Scheduler overhead time: 0.043228897266089916 Adapter cache time: 0.04863328323699534 Engine time: 0.044361387263052166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.381095518008806,
    "estimated_duration": 3600.01973283494,
    "input_throughput": 5064.978070451045,
    "output_throughput": 4474.904915955533,
    "total_throughput": 9539.88298640658,
    "itl": 191.7538248196861,
    "ttft": 2118972.082612369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 778725,
    "finished_requests": 73996,
    "scheduler_time": 45.29984164607574
}
#Debug simulation 
Total elapsed time: 5.381190971005708. Arrivals time: 0.34460696193855256 Scheduler time: 4.941375183756463 Scheduler overhead time: 0.02951645024586469 Adapter cache time: 0.021683655329979956 Engine time: 0.030259227263741195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 8640, 270, 270, 270, 34560, 270, 8640, 34560, 8640, 270, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 270, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 270, 8640, 8640, 8640, 34560, 270, 270, 34560, 270, 34560, 270, 270, 8640, 270, 8640, 34560, 270, 270, 8640, 270, 270, 270, 270, 270, 34560, 8640, 8640, 34560, 8640, 270, 34560, 34560, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 270, 270, 34560, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 270, 8640, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 270, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 270, 8640, 8640, 34560, 8640, 34560, 270, 270, 270, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 270, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 270, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2338470 . Total input tokens: 520978248 . Total output tokens: 467723721
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.181867375038564,
    "estimated_duration": 3600.0049225675384,
    "input_throughput": 4672.0958336924205,
    "output_throughput": 4139.847672588953,
    "total_throughput": 8811.943506281374,
    "itl": 122.95348305507243,
    "ttft": 2165442.38060712,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642179,
    "arrivals": 778725,
    "finished_requests": 68273,
    "scheduler_time": 28.50256085347196
}
#Debug simulation 
Total elapsed time: 5.1819867820013314. Arrivals time: 0.32486106641590595 Scheduler time: 4.700309666222893 Scheduler overhead time: 0.04327984375413507 Adapter cache time: 0.04846015363000333 Engine time: 0.044686718843877316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.329910686938092,
    "estimated_duration": 3600.1289258537686,
    "input_throughput": 5106.375737818983,
    "output_throughput": 4536.467536680483,
    "total_throughput": 9642.843274499466,
    "itl": 189.9639116928132,
    "ttft": 2111372.7899765447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 776317,
    "finished_requests": 74639,
    "scheduler_time": 45.968901481441414
}
#Debug simulation 
Total elapsed time: 5.330004328978248. Arrivals time: 0.25367072399239987 Scheduler time: 4.986057313508354 Scheduler overhead time: 0.029243685072287917 Adapter cache time: 0.01736325945239514 Engine time: 0.02992279187310487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.350084545090795,
    "estimated_duration": 3600.178708702288,
    "input_throughput": 5106.650388093362,
    "output_throughput": 4536.582298129078,
    "total_throughput": 9643.23268622244,
    "itl": 189.96561659502385,
    "ttft": 2111375.4092305326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 776317,
    "finished_requests": 74642,
    "scheduler_time": 45.96921484843138
}
#Debug simulation 
Total elapsed time: 5.350194697035477. Arrivals time: 0.2675864576594904 Scheduler time: 4.992344626924023 Scheduler overhead time: 0.029218324343673885 Adapter cache time: 0.017362129874527454 Engine time: 0.029912789002992213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.089733954053372,
    "estimated_duration": 3600.1218257247947,
    "input_throughput": 4669.053385885322,
    "output_throughput": 4165.063774468572,
    "total_throughput": 8834.117160353893,
    "itl": 122.02634689194186,
    "ttft": 2161115.8334738407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.519667731374504,
    "arrivals": 776317,
    "finished_requests": 68330,
    "scheduler_time": 28.549829370882254
}
#Debug simulation 
Total elapsed time: 5.089826821000315. Arrivals time: 0.23522974946536124 Scheduler time: 4.703772730077617 Scheduler overhead time: 0.04354541399516165 Adapter cache time: 0.04199641081504524 Engine time: 0.0448008052771911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.379561887006275,
    "estimated_duration": 3600.1569553882723,
    "input_throughput": 5106.681244128485,
    "output_throughput": 4536.609709628218,
    "total_throughput": 9643.290953756703,
    "itl": 189.9650093686794,
    "ttft": 2111362.6944000535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 776317,
    "finished_requests": 74642,
    "scheduler_time": 45.969310630232314
}
#Debug simulation 
Total elapsed time: 5.379653562093154. Arrivals time: 0.26448156288824975 Scheduler time: 5.0237050268333405 Scheduler overhead time: 0.029452219023369253 Adapter cache time: 0.017853540368378162 Engine time: 0.030345043051056564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.092408439959399,
    "estimated_duration": 3600.1283230862964,
    "input_throughput": 4669.0449593724325,
    "output_throughput": 4165.0562575351205,
    "total_throughput": 8834.101216907553,
    "itl": 122.02649025851454,
    "ttft": 2161120.209496017,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5262069282680759,
    "arrivals": 776317,
    "finished_requests": 68330,
    "scheduler_time": 28.549787535491042
}
#Debug simulation 
Total elapsed time: 5.092502407031134. Arrivals time: 0.2862830776721239 Scheduler time: 4.656154118129052 Scheduler overhead time: 0.04332260671071708 Adapter cache time: 0.04180776153225452 Engine time: 0.04437864804640412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.3955437541008,
    "estimated_duration": 3600.1177064659505,
    "input_throughput": 5106.391651301379,
    "output_throughput": 4536.48167410397,
    "total_throughput": 9642.87332540535,
    "itl": 189.96359634784258,
    "ttft": 2111365.1470170976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 776317,
    "finished_requests": 74639,
    "scheduler_time": 45.96895208546506
}
#Debug simulation 
Total elapsed time: 5.395641767070629. Arrivals time: 0.3061357404803857 Scheduler time: 4.998174357460812 Scheduler overhead time: 0.029586888733319938 Adapter cache time: 0.017628342262469232 Engine time: 0.030313847004435956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 8640, 135, 135, 135, 34560, 135, 8640, 34560, 8640, 135, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 135, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 135, 8640, 8640, 8640, 34560, 135, 135, 34560, 135, 34560, 135, 135, 8640, 135, 8640, 34560, 135, 135, 8640, 135, 135, 135, 135, 135, 34560, 8640, 8640, 34560, 8640, 135, 34560, 34560, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 135, 135, 34560, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 135, 8640, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 135, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 135, 8640, 8640, 34560, 8640, 34560, 135, 135, 135, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 135, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 135, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2331315 . Total input tokens: 519399039 . Total output tokens: 466309476
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.4149431471014395,
    "estimated_duration": 3600.0051598851387,
    "input_throughput": 4669.140807713816,
    "output_throughput": 4165.110974584939,
    "total_throughput": 8834.251782298754,
    "itl": 122.02646935277522,
    "ttft": 2161098.072381896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5329976327344776,
    "arrivals": 776317,
    "finished_requests": 68329,
    "scheduler_time": 28.54874633164575
}
#Debug simulation 
Total elapsed time: 5.415041748085059. Arrivals time: 0.2409766527125612 Scheduler time: 5.023403016617522 Scheduler overhead time: 0.04374730575364083 Adapter cache time: 0.04143401444889605 Engine time: 0.04480689647607505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.461784215061925,
    "estimated_duration": 3600.0525282887656,
    "input_throughput": 5188.892065660889,
    "output_throughput": 4561.673995297533,
    "total_throughput": 9750.566060958423,
    "itl": 187.8422017777842,
    "ttft": 2107405.7699726126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48049657579744176,
    "arrivals": 775108,
    "finished_requests": 75545,
    "scheduler_time": 46.132806593131065
}
#Debug simulation 
Total elapsed time: 5.46188048296608. Arrivals time: 0.34232461894862354 Scheduler time: 5.0281670656986535 Scheduler overhead time: 0.03000049840193242 Adapter cache time: 0.016487217508256435 Engine time: 0.030922841047868133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.498657157993875,
    "estimated_duration": 3600.0859922425266,
    "input_throughput": 5188.843833245183,
    "output_throughput": 4561.631593074925,
    "total_throughput": 9750.475426320108,
    "itl": 187.84323685996503,
    "ttft": 2107426.9406062504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5123404047358783,
    "arrivals": 775108,
    "finished_requests": 75545,
    "scheduler_time": 46.13268862633027
}
#Debug simulation 
Total elapsed time: 5.498747683945112. Arrivals time: 0.26614199101459235 Scheduler time: 5.141996581107378 Scheduler overhead time: 0.030153468367643654 Adapter cache time: 0.015780023531988263 Engine time: 0.030692521948367357 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.166270854067989,
    "estimated_duration": 3600.0183453388127,
    "input_throughput": 4744.665543750846,
    "output_throughput": 4188.983931019585,
    "total_throughput": 8933.649474770431,
    "itl": 121.98711512343966,
    "ttft": 2156719.738562969,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5132434461824613,
    "arrivals": 775108,
    "finished_requests": 69108,
    "scheduler_time": 28.91306842597971
}
#Debug simulation 
Total elapsed time: 5.1663585810456425. Arrivals time: 0.24062031926587224 Scheduler time: 4.777420600992627 Scheduler overhead time: 0.04350818297825754 Adapter cache time: 0.039659192552790046 Engine time: 0.044681259081698954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.513235570047982,
    "estimated_duration": 3600.063078777541,
    "input_throughput": 5188.8768588863695,
    "output_throughput": 4561.660626673365,
    "total_throughput": 9750.537485559735,
    "itl": 187.84250522794676,
    "ttft": 2107412.2879424426,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4910934481769803,
    "arrivals": 775108,
    "finished_requests": 75545,
    "scheduler_time": 46.132760209523966
}
#Debug simulation 
Total elapsed time: 5.51332829403691. Arrivals time: 0.34413565357681364 Scheduler time: 5.078393049072474 Scheduler overhead time: 0.02999500371515751 Adapter cache time: 0.016000785632058978 Engine time: 0.030794534482993186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.158261109958403,
    "estimated_duration": 3600.024843110063,
    "input_throughput": 4744.656979990121,
    "output_throughput": 4188.97637022194,
    "total_throughput": 8933.63335021206,
    "itl": 121.98728799093067,
    "ttft": 2156723.972500126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5197826430760333,
    "arrivals": 775108,
    "finished_requests": 69108,
    "scheduler_time": 28.913027000337674
}
#Debug simulation 
Total elapsed time: 5.158350329962559. Arrivals time: 0.2383810612373054 Scheduler time: 4.772042662720196 Scheduler overhead time: 0.04349013511091471 Adapter cache time: 0.03965826577041298 Engine time: 0.04435840738005936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.493362635024823,
    "estimated_duration": 3600.041515460206,
    "input_throughput": 5188.907938916374,
    "output_throughput": 4561.687949840401,
    "total_throughput": 9750.595888756776,
    "itl": 187.8418648946268,
    "ttft": 2107398.488819667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46943789629964217,
    "arrivals": 775108,
    "finished_requests": 75545,
    "scheduler_time": 46.13285244406424
}
#Debug simulation 
Total elapsed time: 5.493453999049962. Arrivals time: 0.3414387626107782 Scheduler time: 5.061123896157369 Scheduler overhead time: 0.029963545151986182 Adapter cache time: 0.016129124094732106 Engine time: 0.03081843559630215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 8640, 66, 66, 66, 34560, 66, 8640, 34560, 8640, 66, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 66, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 66, 8640, 8640, 8640, 34560, 66, 66, 34560, 66, 34560, 66, 66, 8640, 66, 8640, 34560, 66, 66, 8640, 66, 66, 66, 66, 66, 34560, 8640, 8640, 34560, 8640, 66, 34560, 34560, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 66, 66, 34560, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 66, 8640, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 66, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 66, 8640, 8640, 34560, 8640, 34560, 66, 66, 66, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 66, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 66, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2327658 . Total input tokens: 518585562 . Total output tokens: 465577310
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.194808084052056,
    "estimated_duration": 3600.0314674955725,
    "input_throughput": 4744.6482493895055,
    "output_throughput": 4188.9686621242145,
    "total_throughput": 8933.61691151372,
    "itl": 121.98745290160201,
    "ttft": 2156728.065350201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 157,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5264475937560203,
    "arrivals": 775108,
    "finished_requests": 69108,
    "scheduler_time": 28.912986435168918
}
#Debug simulation 
Total elapsed time: 5.194901344017126. Arrivals time: 0.239803925040178 Scheduler time: 4.806057198438793 Scheduler overhead time: 0.04362695710733533 Adapter cache time: 0.04016142163891345 Engine time: 0.044654008001089096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.467373913968913,
    "estimated_duration": 3600.0547081488075,
    "input_throughput": 5180.356831185445,
    "output_throughput": 4574.0460451134195,
    "total_throughput": 9754.402876298866,
    "itl": 187.71456517272847,
    "ttft": 2106164.167403746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.452952186102047,
    "arrivals": 774530,
    "finished_requests": 75534,
    "scheduler_time": 46.28522136461851
}
#Debug simulation 
Total elapsed time: 5.467491519986652. Arrivals time: 0.34024977253284305 Scheduler time: 5.038472878630273 Scheduler overhead time: 0.029795624199323356 Adapter cache time: 0.014461366576142609 Engine time: 0.03054000693373382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.44804981991183,
    "estimated_duration": 3600.0862243695165,
    "input_throughput": 5180.311480807964,
    "output_throughput": 4574.006002560073,
    "total_throughput": 9754.317483368037,
    "itl": 187.71549326576604,
    "ttft": 2106184.980034943,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4846127614448778,
    "arrivals": 774530,
    "finished_requests": 75534,
    "scheduler_time": 46.285077009982615
}
#Debug simulation 
Total elapsed time: 5.448138812906109. Arrivals time: 0.259034922812134 Scheduler time: 5.100156605010852 Scheduler overhead time: 0.029938227380625904 Adapter cache time: 0.014461475308053195 Engine time: 0.030563757638446987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.575847661937587,
    "estimated_duration": 3600.04089919096,
    "input_throughput": 4739.243380216663,
    "output_throughput": 4191.38654880032,
    "total_throughput": 8930.629929016985,
    "itl": 121.68122858700364,
    "ttft": 2157327.183895466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4757094775140301,
    "arrivals": 774530,
    "finished_requests": 69063,
    "scheduler_time": 28.892261702966774
}
#Debug simulation 
Total elapsed time: 5.575935569009744. Arrivals time: 0.34860405942890793 Scheduler time: 5.079890793887898 Scheduler overhead time: 0.04393162683118135 Adapter cache time: 0.03766952583100647 Engine time: 0.045112360967323184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.42817789898254,
    "estimated_duration": 3600.0658934760727,
    "input_throughput": 5180.340735928241,
    "output_throughput": 4574.031833650782,
    "total_throughput": 9754.372569579024,
    "itl": 187.71492603186454,
    "ttft": 2106171.3607587744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46418299552286063,
    "arrivals": 774530,
    "finished_requests": 75534,
    "scheduler_time": 46.2851758824596
}
#Debug simulation 
Total elapsed time: 5.428269149037078. Arrivals time: 0.3374827529769391 Scheduler time: 5.001747804461047 Scheduler overhead time: 0.029620389104820788 Adapter cache time: 0.01488939393311739 Engine time: 0.030621464364230633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.1624773340299726,
    "estimated_duration": 3600.0471436517414,
    "input_throughput": 4739.235159763363,
    "output_throughput": 4191.379278631936,
    "total_throughput": 8930.6144383953,
    "itl": 121.68138257051781,
    "ttft": 2157331.2433516276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48199716683477234,
    "arrivals": 774530,
    "finished_requests": 69063,
    "scheduler_time": 28.89221847442835
}
#Debug simulation 
Total elapsed time: 5.162563510937616. Arrivals time: 0.23769614740740508 Scheduler time: 4.779006048454903 Scheduler overhead time: 0.04358760896138847 Adapter cache time: 0.03711404069326818 Engine time: 0.044590592151507735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.443750367034227,
    "estimated_duration": 3600.0443306914112,
    "input_throughput": 5180.371764038315,
    "output_throughput": 4574.059230219936,
    "total_throughput": 9754.43099425825,
    "itl": 187.71426191215136,
    "ttft": 2106157.461092242,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.44252744364552254,
    "arrivals": 774530,
    "finished_requests": 75534,
    "scheduler_time": 46.2852686496738
}
#Debug simulation 
Total elapsed time: 5.443865091074258. Arrivals time: 0.3381154740927741 Scheduler time: 5.017145240446553 Scheduler overhead time: 0.02981784159783274 Adapter cache time: 0.014364998671226203 Engine time: 0.030606956919655204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 8640, 33, 33, 33, 34560, 33, 8640, 34560, 8640, 33, 8640, 8640, 8640, 8640, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 33, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 33, 8640, 8640, 8640, 34560, 33, 33, 34560, 33, 34560, 33, 33, 8640, 33, 8640, 34560, 33, 33, 8640, 33, 33, 33, 33, 33, 34560, 8640, 8640, 34560, 8640, 33, 34560, 34560, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 33, 33, 34560, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 33, 8640, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 33, 34560, 8640, 34560, 8640, 34560, 34560, 34560, 33, 8640, 8640, 34560, 8640, 34560, 33, 33, 33, 8640, 8640, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 33, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 34560, 33, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2325909 . Total input tokens: 518205538 . Total output tokens: 465220012
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.160420696018264,
    "estimated_duration": 3600.053644004412,
    "input_throughput": 4739.226602474229,
    "output_throughput": 4191.3717105659625,
    "total_throughput": 8930.598313040193,
    "itl": 121.68155821625155,
    "ttft": 2157335.3768671057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48853636372834414,
    "arrivals": 774530,
    "finished_requests": 69063,
    "scheduler_time": 28.892179630206144
}
#Debug simulation 
Total elapsed time: 5.16050890495535. Arrivals time: 0.24142259743530303 Scheduler time: 4.773096153279766 Scheduler overhead time: 0.04352324514184147 Adapter cache time: 0.03732121642678976 Engine time: 0.04467502050101757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.2481279879575595,
    "estimated_duration": 3600.166251903201,
    "input_throughput": 4877.207820810412,
    "output_throughput": 4306.460289661329,
    "total_throughput": 9183.66811047174,
    "itl": 199.41853271796214,
    "ttft": 2133980.293899319,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 716829,
    "finished_requests": 71079,
    "scheduler_time": 43.63173662256464
}
#Debug simulation 
Total elapsed time: 5.248245027964003. Arrivals time: 0.32419545645825565 Scheduler time: 4.808570934692398 Scheduler overhead time: 0.028566120425239205 Adapter cache time: 0.044226569472812116 Engine time: 0.029344734619371593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.20333697006572,
    "estimated_duration": 3600.1985317693943,
    "input_throughput": 4877.164091106491,
    "output_throughput": 4306.421677356843,
    "total_throughput": 9183.585768463334,
    "itl": 199.41956390179425,
    "ttft": 2134003.576739423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574656,
    "arrivals": 716829,
    "finished_requests": 71079,
    "scheduler_time": 43.63156678152979
}
#Debug simulation 
Total elapsed time: 5.203484648023732. Arrivals time: 0.31967690004967153 Scheduler time: 4.768902969779447 Scheduler overhead time: 0.02846919698640704 Adapter cache time: 0.043983273790217936 Engine time: 0.029231528867967427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.161572709912434,
    "estimated_duration": 3600.06720080208,
    "input_throughput": 4675.169395796317,
    "output_throughput": 4143.065717405755,
    "total_throughput": 8818.235113202072,
    "itl": 123.09804096037426,
    "ttft": 2161411.126701157,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 716829,
    "finished_requests": 68154,
    "scheduler_time": 28.564347124356946
}
#Debug simulation 
Total elapsed time: 5.1616643539164215. Arrivals time: 0.31189998146146536 Scheduler time: 4.666543686296791 Scheduler overhead time: 0.043492509168572724 Adapter cache time: 0.07484850089531392 Engine time: 0.04454586748033762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.24287292198278,
    "estimated_duration": 3600.1769906604627,
    "input_throughput": 4877.19327287262,
    "output_throughput": 4306.447444172946,
    "total_throughput": 9183.640717045566,
    "itl": 199.41892514682544,
    "ttft": 2133988.024311831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 716829,
    "finished_requests": 71079,
    "scheduler_time": 43.631681224475585
}
#Debug simulation 
Total elapsed time: 5.242994584958069. Arrivals time: 0.32012115290854126 Scheduler time: 4.808796022320166 Scheduler overhead time: 0.02827864291612059 Adapter cache time: 0.043506137444637716 Engine time: 0.029052068362943828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.231645543011837,
    "estimated_duration": 3600.0738187545294,
    "input_throughput": 4675.1608015145575,
    "output_throughput": 4143.058101280839,
    "total_throughput": 8818.218902795397,
    "itl": 123.09822367772134,
    "ttft": 2161415.866302812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978163,
    "arrivals": 716829,
    "finished_requests": 68154,
    "scheduler_time": 28.564300126126074
}
#Debug simulation 
Total elapsed time: 5.231738019967452. Arrivals time: 0.2395693565485999 Scheduler time: 4.806185879046097 Scheduler overhead time: 0.04380243760533631 Adapter cache time: 0.07562667015008628 Engine time: 0.0460926943924278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.555537317064591,
    "estimated_duration": 3600.1550387435673,
    "input_throughput": 4877.223011520055,
    "output_throughput": 4306.473702702202,
    "total_throughput": 9183.696714222257,
    "itl": 199.41817692490903,
    "ttft": 2133972.212908052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 716829,
    "finished_requests": 71079,
    "scheduler_time": 43.63179345477581
}
#Debug simulation 
Total elapsed time: 5.55563450907357. Arrivals time: 0.5713822791585699 Scheduler time: 4.8692442397587 Scheduler overhead time: 0.02849767671432346 Adapter cache time: 0.04399932688102126 Engine time: 0.029226608225144446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [53 53 54]
Adapter prompts. [1080, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 4320, 1080, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 1080, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 4320, 34560, 1080, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 1080, 4320, 34560, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 4320, 34560, 4320, 1080, 34560, 34560, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 1080, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 1080, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 1080, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2152440 . Total input tokens: 479428458 . Total output tokens: 430504039
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.192490278976038,
    "estimated_duration": 3600.0668665677513,
    "input_throughput": 4675.38121480701,
    "output_throughput": 4143.450261583989,
    "total_throughput": 8818.831476390998,
    "itl": 123.09802484350375,
    "ttft": 2161393.597383184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642179,
    "arrivals": 716829,
    "finished_requests": 68157,
    "scheduler_time": 28.563337567119575
}
#Debug simulation 
Total elapsed time: 5.192583257914521. Arrivals time: 0.31387177237775177 Scheduler time: 4.693464731913991 Scheduler overhead time: 0.043591701542027295 Adapter cache time: 0.07610953645780683 Engine time: 0.04511518927756697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.363081377930939,
    "estimated_duration": 3600.083823668492,
    "input_throughput": 5038.171856097924,
    "output_throughput": 4471.038116995296,
    "total_throughput": 9509.20997309322,
    "itl": 192.67776451769654,
    "ttft": 2102469.1956660794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 707393,
    "finished_requests": 73569,
    "scheduler_time": 45.38345804536246
}
#Debug simulation 
Total elapsed time: 5.363177485996857. Arrivals time: 0.25701029459014535 Scheduler time: 4.995707166846842 Scheduler overhead time: 0.02929946011863649 Adapter cache time: 0.037437561666592956 Engine time: 0.030098441638983786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.375982861965895,
    "estimated_duration": 3600.1296446332403,
    "input_throughput": 5038.107732325227,
    "output_throughput": 4470.9812114668375,
    "total_throughput": 9509.088943792065,
    "itl": 192.67696767201397,
    "ttft": 2102482.958120919,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 707393,
    "finished_requests": 73569,
    "scheduler_time": 45.38389126697315
}
#Debug simulation 
Total elapsed time: 5.376078170957044. Arrivals time: 0.3260219309013337 Scheduler time: 4.939705253345892 Scheduler overhead time: 0.029070766060613096 Adapter cache time: 0.03763986425474286 Engine time: 0.030031442060135305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.2721168210264295,
    "estimated_duration": 3600.089957253595,
    "input_throughput": 4781.212470904796,
    "output_throughput": 4263.989562002679,
    "total_throughput": 9045.202032907475,
    "itl": 119.7399952341324,
    "ttft": 2135200.6902089105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 707393,
    "finished_requests": 69877,
    "scheduler_time": 29.407332403266484
}
#Debug simulation 
Total elapsed time: 5.27220970694907. Arrivals time: 0.31746963248588145 Scheduler time: 4.779160878970288 Scheduler overhead time: 0.044470028253272176 Adapter cache time: 0.06451824493706226 Engine time: 0.04558983165770769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.451238358975388,
    "estimated_duration": 3600.0945676705433,
    "input_throughput": 5038.156820346019,
    "output_throughput": 4471.024773778389,
    "total_throughput": 9509.18159412441,
    "itl": 192.6781550721692,
    "ttft": 2102476.896847419,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 707393,
    "finished_requests": 73569,
    "scheduler_time": 45.383407892062905
}
#Debug simulation 
Total elapsed time: 5.451341319014318. Arrivals time: 0.266463978565298 Scheduler time: 5.074259760673158 Scheduler overhead time: 0.029337571933865547 Adapter cache time: 0.037524019833654165 Engine time: 0.030045996070839465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.278724301955663,
    "estimated_duration": 3600.0965756567684,
    "input_throughput": 4781.203681142875,
    "output_throughput": 4263.981723101289,
    "total_throughput": 9045.185404244165,
    "itl": 119.74017738341976,
    "ttft": 2135205.4397943723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978161,
    "arrivals": 707393,
    "finished_requests": 69877,
    "scheduler_time": 29.40728585575971
}
#Debug simulation 
Total elapsed time: 5.278815655037761. Arrivals time: 0.3203227089252323 Scheduler time: 4.782171534840018 Scheduler overhead time: 0.04432406323030591 Adapter cache time: 0.06519887340255082 Engine time: 0.045857537421397865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.374007187085226,
    "estimated_duration": 3600.071352318567,
    "input_throughput": 5038.189309308723,
    "output_throughput": 4471.053605544113,
    "total_throughput": 9509.242914852835,
    "itl": 192.67777687479165,
    "ttft": 2102465.7105766744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 707393,
    "finished_requests": 73569,
    "scheduler_time": 45.38332670328932
}
#Debug simulation 
Total elapsed time: 5.374100555083714. Arrivals time: 0.33112933032680303 Scheduler time: 4.9318080636439845 Scheduler overhead time: 0.029267242876812816 Adapter cache time: 0.03795089875347912 Engine time: 0.030245172092691064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 4320, 540, 540, 540, 34560, 540, 4320, 34560, 4320, 540, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 540, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 540, 4320, 4320, 4320, 34560, 540, 540, 34560, 540, 34560, 540, 540, 4320, 540, 4320, 34560, 540, 540, 4320, 540, 540, 540, 540, 540, 34560, 4320, 4320, 34560, 4320, 540, 34560, 34560, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 540, 540, 34560, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 540, 4320, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 540, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 540, 4320, 4320, 34560, 4320, 34560, 540, 540, 540, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 540, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 540, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2123820 . Total input tokens: 473063017 . Total output tokens: 424780982
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.355806914973073,
    "estimated_duration": 3600.1033226000227,
    "input_throughput": 4781.194720702845,
    "output_throughput": 4263.973731985439,
    "total_throughput": 9045.168452688285,
    "itl": 119.74038406369901,
    "ttft": 2135210.2250556755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642178,
    "arrivals": 707393,
    "finished_requests": 69877,
    "scheduler_time": 29.407242094547357
}
#Debug simulation 
Total elapsed time: 5.355898717069067. Arrivals time: 0.3160598701797426 Scheduler time: 4.863477481296286 Scheduler overhead time: 0.04472535813692957 Adapter cache time: 0.06458663649391383 Engine time: 0.04593061120249331 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.442879865062423,
    "estimated_duration": 3600.0830057703865,
    "input_throughput": 5189.863947595785,
    "output_throughput": 4589.218074560625,
    "total_throughput": 9779.08202215641,
    "itl": 187.37573061252633,
    "ttft": 2094998.6547590215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 702770,
    "finished_requests": 75493,
    "scheduler_time": 46.493596045513335
}
#Debug simulation 
Total elapsed time: 5.442970725009218. Arrivals time: 0.253042685915716 Scheduler time: 5.081569990608841 Scheduler overhead time: 0.02988052589353174 Adapter cache time: 0.03385879285633564 Engine time: 0.030605360865592957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.476679652929306,
    "estimated_duration": 3600.1714861421133,
    "input_throughput": 5189.817227295949,
    "output_throughput": 4589.155284295761,
    "total_throughput": 9778.97251159171,
    "itl": 187.37571198533166,
    "ttft": 2095032.0708143385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574656,
    "arrivals": 702770,
    "finished_requests": 75494,
    "scheduler_time": 46.49438925501579
}
#Debug simulation 
Total elapsed time: 5.476785801001824. Arrivals time: 0.26732095680199564 Scheduler time: 5.100856146076694 Scheduler overhead time: 0.0298861819319427 Adapter cache time: 0.03385255124885589 Engine time: 0.030827573616988957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.276426490978338,
    "estimated_duration": 3600.017803388399,
    "input_throughput": 4893.03857981492,
    "output_throughput": 4341.8004725666215,
    "total_throughput": 9234.839052381541,
    "itl": 117.52574570989347,
    "ttft": 2131699.746617771,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 702770,
    "finished_requests": 71149,
    "scheduler_time": 29.912354260782514
}
#Debug simulation 
Total elapsed time: 5.276517966995016. Arrivals time: 0.24487282836344093 Scheduler time: 4.859229566180147 Scheduler overhead time: 0.04546143300831318 Adapter cache time: 0.059009439195506275 Engine time: 0.04668667516671121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.462440601084381,
    "estimated_duration": 3600.1219647047433,
    "input_throughput": 5189.807785173835,
    "output_throughput": 4589.168412063779,
    "total_throughput": 9778.976197237615,
    "itl": 187.37568734599165,
    "ttft": 2095007.6892567487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801272,
    "arrivals": 702770,
    "finished_requests": 75493,
    "scheduler_time": 46.49394732911696
}
#Debug simulation 
Total elapsed time: 5.462534486083314. Arrivals time: 0.26054055127315223 Scheduler time: 5.092382727074437 Scheduler overhead time: 0.030262871296145022 Adapter cache time: 0.03409901016857475 Engine time: 0.03108092152979225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.299325330997817,
    "estimated_duration": 3600.0244212998737,
    "input_throughput": 4893.029584960337,
    "output_throughput": 4341.792491050996,
    "total_throughput": 9234.822076011335,
    "itl": 117.52588452859753,
    "ttft": 2131704.4484494696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978163,
    "arrivals": 702770,
    "finished_requests": 71149,
    "scheduler_time": 29.912307221576725
}
#Debug simulation 
Total elapsed time: 5.29943962697871. Arrivals time: 0.2494797232793644 Scheduler time: 4.876621610950679 Scheduler overhead time: 0.04549173719715327 Adapter cache time: 0.060008984291926026 Engine time: 0.04646591155324131 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.490002288017422,
    "estimated_duration": 3600.071769287198,
    "input_throughput": 5189.806536467829,
    "output_throughput": 4589.173510635643,
    "total_throughput": 9778.98004710347,
    "itl": 187.3772398364437,
    "ttft": 2095016.096643947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 702770,
    "finished_requests": 75492,
    "scheduler_time": 46.493127737615744
}
#Debug simulation 
Total elapsed time: 5.490093498956412. Arrivals time: 0.3066773632308468 Scheduler time: 5.07417888334021 Scheduler overhead time: 0.030073811765760183 Adapter cache time: 0.03407846379559487 Engine time: 0.03100411978084594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 4320, 270, 270, 270, 34560, 270, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 270, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 270, 4320, 4320, 4320, 34560, 270, 270, 34560, 270, 34560, 270, 270, 4320, 270, 4320, 34560, 270, 270, 4320, 270, 270, 270, 270, 270, 34560, 4320, 4320, 34560, 4320, 270, 34560, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 270, 270, 34560, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 270, 4320, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 270, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 270, 4320, 4320, 34560, 4320, 34560, 270, 270, 270, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 270, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 270, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2109510 . Total input tokens: 469859313 . Total output tokens: 421960306
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.304617002024315,
    "estimated_duration": 3600.0311704557735,
    "input_throughput": 4893.020411756571,
    "output_throughput": 4341.784351278584,
    "total_throughput": 9234.804763035154,
    "itl": 117.52607193949919,
    "ttft": 2131709.178368261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642179,
    "arrivals": 702770,
    "finished_requests": 71149,
    "scheduler_time": 29.912265673009937
}
#Debug simulation 
Total elapsed time: 5.3047100260155275. Arrivals time: 0.24661558959633112 Scheduler time: 4.884043294470757 Scheduler overhead time: 0.04539135156664997 Adapter cache time: 0.06067316920962185 Engine time: 0.046613961923867464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.523176679038443,
    "estimated_duration": 3600.095295849117,
    "input_throughput": 5281.34964147262,
    "output_throughput": 4650.692446753974,
    "total_throughput": 9932.042088226593,
    "itl": 184.67846526330672,
    "ttft": 2087826.8754081754,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48661755128530726,
    "arrivals": 700393,
    "finished_requests": 76746,
    "scheduler_time": 47.07772728674161
}
#Debug simulation 
Total elapsed time: 5.523268430028111. Arrivals time: 0.3290807417361066 Scheduler time: 5.087977262912318 Scheduler overhead time: 0.030684312107041478 Adapter cache time: 0.029902947484515607 Engine time: 0.03136184543836862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.610938908066601,
    "estimated_duration": 3600.165984595304,
    "input_throughput": 5281.245942924851,
    "output_throughput": 4650.601131070372,
    "total_throughput": 9931.847073995223,
    "itl": 184.67949776712393,
    "ttft": 2087847.3069605592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5187291006441229,
    "arrivals": 700393,
    "finished_requests": 76746,
    "scheduler_time": 47.07812161734243
}
#Debug simulation 
Total elapsed time: 5.61103243206162. Arrivals time: 0.3350217059487477 Scheduler time: 5.1690801362274215 Scheduler overhead time: 0.030667696380987763 Adapter cache time: 0.030556155717931688 Engine time: 0.03142843395471573 
