INFO 06-01 00:47:06 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:07 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.098096500150859,
    "estimated_duration": 3600.116061487224,
    "input_throughput": 5599.8402983907645,
    "output_throughput": 4959.407056622572,
    "total_throughput": 10559.247355013336,
    "itl": 173.670146752466,
    "ttft": 2015126.6808123274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9039071651408497,
    "arrivals": 561196,
    "finished_requests": 81614,
    "scheduler_time": 81.37115802665213
}
#Debug simulation 
Total elapsed time: 6.098252168856561. Arrivals time: 0.33604810712859035 Scheduler time: 5.6582140158861876 Scheduler overhead time: 0.03295848844572902 Adapter cache time: 0.02202411275357008 Engine time: 0.033580745570361614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.062522334977984,
    "estimated_duration": 3600.1697408423915,
    "input_throughput": 5590.481129728796,
    "output_throughput": 4952.083174786199,
    "total_throughput": 10542.564304514995,
    "itl": 171.54623461535243,
    "ttft": 2016056.795254899,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9146913207508742,
    "arrivals": 561196,
    "finished_requests": 81484,
    "scheduler_time": 81.36256680409909
}
#Debug simulation 
Total elapsed time: 6.062637748196721. Arrivals time: 0.3156861327588558 Scheduler time: 5.641834695357829 Scheduler overhead time: 0.033375360537320375 Adapter cache time: 0.022184257861226797 Engine time: 0.03397773811593652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.083887271117419,
    "estimated_duration": 3600.172583522057,
    "input_throughput": 5599.945150484057,
    "output_throughput": 4959.501964356485,
    "total_throughput": 10559.447114840541,
    "itl": 173.6668768140396,
    "ttft": 2015095.286960068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.811694506239131,
    "arrivals": 561196,
    "finished_requests": 81617,
    "scheduler_time": 81.37405823628897
}
#Debug simulation 
Total elapsed time: 6.08402804331854. Arrivals time: 0.27449485613033175 Scheduler time: 5.705180349294096 Scheduler overhead time: 0.03305008029565215 Adapter cache time: 0.02200575126335025 Engine time: 0.03385899635031819 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.054425687994808,
    "estimated_duration": 3600.0111083283377,
    "input_throughput": 5590.18913953988,
    "output_throughput": 4951.671943111732,
    "total_throughput": 10541.861082651612,
    "itl": 171.54615203084234,
    "ttft": 2016049.3886910349,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9443692143447717,
    "arrivals": 561196,
    "finished_requests": 81477,
    "scheduler_time": 81.3582111811067
}
#Debug simulation 
Total elapsed time: 6.054550037253648. Arrivals time: 0.3205872527323663 Scheduler time: 5.628084733150899 Scheduler overhead time: 0.03348878026008606 Adapter cache time: 0.022665602155029774 Engine time: 0.034134835470467806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.042483917903155,
    "estimated_duration": 3600.0888663518417,
    "input_throughput": 5599.967875356801,
    "output_throughput": 4959.466186203597,
    "total_throughput": 10559.4340615604,
    "itl": 173.6636443093917,
    "ttft": 2015068.0613996696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 586,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7521694728126496,
    "arrivals": 561196,
    "finished_requests": 81616,
    "scheduler_time": 81.37360427796644
}
#Debug simulation 
Total elapsed time: 6.042577576823533. Arrivals time: 0.3240083074197173 Scheduler time: 5.614411488175392 Scheduler overhead time: 0.033090478740632534 Adapter cache time: 0.022086855955421925 Engine time: 0.033614595886319876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 270, 270, 34560, 270, 4320, 4320, 4320, 270, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 4320, 270, 34560, 34560, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 270, 4320, 34560, 34560, 4320, 270, 270, 270, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 270, 270, 4320, 270, 34560, 34560, 270, 270, 4320, 4320, 4320, 270, 34560, 270, 34560, 4320, 270, 34560, 34560, 4320, 4320, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 4320, 34560, 34560, 4320, 34560, 270, 4320, 34560, 270, 34560, 4320, 270, 4320, 4320, 34560, 34560, 270, 270, 34560, 270, 270, 270, 4320, 270]
Prompts retrieved: 1683180 . Total input tokens: 374418592 . Total output tokens: 336607404
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.037859648000449,
    "estimated_duration": 3600.0290145732547,
    "input_throughput": 5590.161334404016,
    "output_throughput": 4951.6473139073,
    "total_throughput": 10541.808648311317,
    "itl": 171.54424641024772,
    "ttft": 2016053.8948864585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 587,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9666530625149563,
    "arrivals": 561196,
    "finished_requests": 81477,
    "scheduler_time": 81.35823788623422
}
#Debug simulation 
Total elapsed time: 6.037958360277116. Arrivals time: 0.26693886844441295 Scheduler time: 5.666003916878253 Scheduler overhead time: 0.0332298269495368 Adapter cache time: 0.022366355638951063 Engine time: 0.03392513655126095 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.152704591397196,
    "estimated_duration": 3600.0303455127314,
    "input_throughput": 5700.078063391266,
    "output_throughput": 5038.034477294619,
    "total_throughput": 10738.112540685885,
    "itl": 170.71990364047795,
    "ttft": 2006632.7761489933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8661180315329682,
    "arrivals": 559351,
    "finished_requests": 82941,
    "scheduler_time": 82.67392233867675
}
#Debug simulation 
Total elapsed time: 6.152828756254166. Arrivals time: 0.324236701708287 Scheduler time: 5.726429094560444 Scheduler overhead time: 0.03346144687384367 Adapter cache time: 0.018997634761035442 Engine time: 0.0341467522084713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.150817897636443,
    "estimated_duration": 3600.130328843248,
    "input_throughput": 5700.096142517595,
    "output_throughput": 5037.896226892318,
    "total_throughput": 10737.992369409912,
    "itl": 170.72090467918076,
    "ttft": 2006684.9907289206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9201399860950229,
    "arrivals": 559351,
    "finished_requests": 82943,
    "scheduler_time": 82.67474421909479
}
#Debug simulation 
Total elapsed time: 6.150914217811078. Arrivals time: 0.33261179085820913 Scheduler time: 5.716521409805864 Scheduler overhead time: 0.033455734606832266 Adapter cache time: 0.018652592319995165 Engine time: 0.03417845768854022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.203064991161227,
    "estimated_duration": 3600.151810671544,
    "input_throughput": 5689.890059435852,
    "output_throughput": 5030.497310229204,
    "total_throughput": 10720.387369665055,
    "itl": 168.85452075924763,
    "ttft": 2008043.785059233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9257383970543789,
    "arrivals": 559351,
    "finished_requests": 82807,
    "scheduler_time": 82.6614065442226
}
#Debug simulation 
Total elapsed time: 6.20317927794531. Arrivals time: 0.2820087163709104 Scheduler time: 5.817569198086858 Scheduler overhead time: 0.03390098363161087 Adapter cache time: 0.01929342746734619 Engine time: 0.03459746530279517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.160339191090316,
    "estimated_duration": 3600.0483511884663,
    "input_throughput": 5700.0495543971465,
    "output_throughput": 5038.009279517731,
    "total_throughput": 10738.058833914878,
    "itl": 170.7203491169526,
    "ttft": 2006643.7019508728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8800976448878669,
    "arrivals": 559351,
    "finished_requests": 82941,
    "scheduler_time": 82.67402421407459
}
#Debug simulation 
Total elapsed time: 6.160434213001281. Arrivals time: 0.2737614274956286 Scheduler time: 5.78414482017979 Scheduler overhead time: 0.033636379055678844 Adapter cache time: 0.01898501208052039 Engine time: 0.034222040325403214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.140671554021537,
    "estimated_duration": 3600.163960763287,
    "input_throughput": 5689.870856786477,
    "output_throughput": 5030.480332945808,
    "total_throughput": 10720.351189732286,
    "itl": 168.8547973619236,
    "ttft": 2008049.699693034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9380622681230353,
    "arrivals": 559351,
    "finished_requests": 82807,
    "scheduler_time": 82.66139867865871
}
#Debug simulation 
Total elapsed time: 6.140765639953315. Arrivals time: 0.2742414684034884 Scheduler time: 5.76339300815016 Scheduler overhead time: 0.033883593045175076 Adapter cache time: 0.019033958204090595 Engine time: 0.03445833222940564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.158663092646748,
    "estimated_duration": 3600.0105231248476,
    "input_throughput": 5700.109449176839,
    "output_throughput": 5038.06221773397,
    "total_throughput": 10738.17166691081,
    "itl": 170.71931555124016,
    "ttft": 2006624.225646855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8461842334573169,
    "arrivals": 559351,
    "finished_requests": 82941,
    "scheduler_time": 82.67391601761643
}
#Debug simulation 
Total elapsed time: 6.158786373678595. Arrivals time: 0.2744871899485588 Scheduler time: 5.782129371073097 Scheduler overhead time: 0.033491626381874084 Adapter cache time: 0.018873751163482666 Engine time: 0.03422196675091982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 135, 135, 34560, 135, 4320, 4320, 4320, 135, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 4320, 135, 34560, 34560, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 135, 4320, 34560, 34560, 4320, 135, 135, 135, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 135, 135, 4320, 135, 34560, 34560, 135, 135, 4320, 4320, 4320, 135, 34560, 135, 34560, 4320, 135, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 4320, 34560, 34560, 4320, 34560, 135, 4320, 34560, 135, 34560, 4320, 135, 4320, 4320, 34560, 34560, 135, 135, 34560, 135, 135, 135, 4320, 135]
Prompts retrieved: 1677510 . Total input tokens: 373145980 . Total output tokens: 335471213
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.139360356610268,
    "estimated_duration": 3600.185698072845,
    "input_throughput": 5689.836502312977,
    "output_throughput": 5030.449959760258,
    "total_throughput": 10720.286462073236,
    "itl": 168.84601344176014,
    "ttft": 2008059.1207373296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9420755472034262,
    "arrivals": 559351,
    "finished_requests": 82807,
    "scheduler_time": 82.66202861286664
}
#Debug simulation 
Total elapsed time: 6.139457360841334. Arrivals time: 0.32889986503869295 Scheduler time: 5.707066560164094 Scheduler overhead time: 0.033766038715839386 Adapter cache time: 0.01941691432148218 Engine time: 0.03456922248005867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.1928119491785765,
    "estimated_duration": 3600.1349028082686,
    "input_throughput": 5731.321896828063,
    "output_throughput": 5060.2856536818545,
    "total_throughput": 10791.607550509918,
    "itl": 169.93919363150198,
    "ttft": 2001980.625043902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6641258404334067,
    "arrivals": 558425,
    "finished_requests": 83458,
    "scheduler_time": 83.02394429330347
}
#Debug simulation 
Total elapsed time: 6.192904300056398. Arrivals time: 0.27518831845372915 Scheduler time: 5.814719704911113 Scheduler overhead time: 0.03362300060689449 Adapter cache time: 0.01917876536026597 Engine time: 0.03453375771641731 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.188916052225977,
    "estimated_duration": 3600.0328246210593,
    "input_throughput": 5731.283853549812,
    "output_throughput": 5060.281360606079,
    "total_throughput": 10791.565214155891,
    "itl": 169.94028515914857,
    "ttft": 2001965.373290917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.709313021122946,
    "arrivals": 558425,
    "finished_requests": 83455,
    "scheduler_time": 83.0205063019737
}
#Debug simulation 
Total elapsed time: 6.189035104122013. Arrivals time: 0.27723253332078457 Scheduler time: 5.808441782835871 Scheduler overhead time: 0.03362817643210292 Adapter cache time: 0.019334509503096342 Engine time: 0.03458959423005581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.183675779029727,
    "estimated_duration": 3600.1059450112703,
    "input_throughput": 5721.608840024155,
    "output_throughput": 5053.523223451427,
    "total_throughput": 10775.132063475583,
    "itl": 168.32136650685285,
    "ttft": 2002690.6171193593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7103562193736466,
    "arrivals": 558425,
    "finished_requests": 83332,
    "scheduler_time": 83.00460658735135
}
#Debug simulation 
Total elapsed time: 6.1837705336511135. Arrivals time: 0.2804123037494719 Scheduler time: 5.799537799321115 Scheduler overhead time: 0.03401242336258292 Adapter cache time: 0.019247359596192837 Engine time: 0.03472517617046833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.209467670880258,
    "estimated_duration": 3600.003050347997,
    "input_throughput": 5731.2618104602825,
    "output_throughput": 5060.19849017602,
    "total_throughput": 10791.460300636301,
    "itl": 169.9397530955345,
    "ttft": 2001942.412527111,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6758082050108364,
    "arrivals": 558425,
    "finished_requests": 83454,
    "scheduler_time": 83.02023905888959
}
#Debug simulation 
Total elapsed time: 6.209575065877289. Arrivals time: 0.34337363578379154 Scheduler time: 5.762829080224037 Scheduler overhead time: 0.03363429103046656 Adapter cache time: 0.01927902502939105 Engine time: 0.0345889450982213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.181641883216798,
    "estimated_duration": 3600.116639588329,
    "input_throughput": 5721.591843300781,
    "output_throughput": 5053.508211356281,
    "total_throughput": 10775.100054657061,
    "itl": 168.3217307643099,
    "ttft": 2002695.820044198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206680298596647,
    "arrivals": 558425,
    "finished_requests": 83332,
    "scheduler_time": 83.00461179503229
}
#Debug simulation 
Total elapsed time: 6.181733461096883. Arrivals time: 0.2756572109647095 Scheduler time: 5.801914744544774 Scheduler overhead time: 0.03431596513837576 Adapter cache time: 0.01936847949400544 Engine time: 0.03463391400873661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.169563772156835,
    "estimated_duration": 3600.049840866938,
    "input_throughput": 5731.457316444036,
    "output_throughput": 5060.405218060243,
    "total_throughput": 10791.862534504278,
    "itl": 169.93744078236296,
    "ttft": 2001929.991913126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.648840913993773,
    "arrivals": 558425,
    "finished_requests": 83458,
    "scheduler_time": 83.022250922301
}
#Debug simulation 
Total elapsed time: 6.1696584722958505. Arrivals time: 0.27497251285240054 Scheduler time: 5.791181315667927 Scheduler overhead time: 0.03378478158265352 Adapter cache time: 0.019304835703223944 Engine time: 0.03455517068505287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 66, 66, 34560, 66, 4320, 4320, 4320, 66, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 4320, 66, 34560, 34560, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 66, 4320, 34560, 34560, 4320, 66, 66, 66, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 66, 66, 4320, 66, 34560, 34560, 66, 66, 4320, 4320, 4320, 66, 34560, 66, 34560, 4320, 66, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 4320, 34560, 34560, 4320, 34560, 66, 4320, 34560, 66, 34560, 4320, 66, 4320, 4320, 34560, 34560, 66, 66, 34560, 66, 66, 66, 4320, 66]
Prompts retrieved: 1674612 . Total input tokens: 372519196 . Total output tokens: 334871618
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.192449190188199,
    "estimated_duration": 3600.10837102316,
    "input_throughput": 5721.6308169483955,
    "output_throughput": 5053.637314487097,
    "total_throughput": 10775.268131435492,
    "itl": 168.3285419944702,
    "ttft": 2002714.4320535888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 217,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7289677797630453,
    "arrivals": 558425,
    "finished_requests": 83334,
    "scheduler_time": 83.00467805461162
}
#Debug simulation 
Total elapsed time: 6.192580486182123. Arrivals time: 0.27869496028870344 Scheduler time: 5.809289827477187 Scheduler overhead time: 0.03406575741246343 Adapter cache time: 0.0192006160505116 Engine time: 0.035433399491012096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.213464817032218,
    "estimated_duration": 3600.0492435272295,
    "input_throughput": 5758.8045600419,
    "output_throughput": 5087.472076369521,
    "total_throughput": 10846.276636411421,
    "itl": 169.112108242194,
    "ttft": 1996658.8896970788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 557951,
    "finished_requests": 83985,
    "scheduler_time": 83.46858644509496
}
#Debug simulation 
Total elapsed time: 6.213557027280331. Arrivals time: 0.33544780034571886 Scheduler time: 5.7758310893550515 Scheduler overhead time: 0.03376580076292157 Adapter cache time: 0.017816477920860052 Engine time: 0.03483196906745434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.2145813489332795,
    "estimated_duration": 3600.139900891296,
    "input_throughput": 5758.659543999202,
    "output_throughput": 5087.343965568025,
    "total_throughput": 10846.003509567227,
    "itl": 169.1141402863499,
    "ttft": 1996695.84065614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5204933649837041,
    "arrivals": 557951,
    "finished_requests": 83985,
    "scheduler_time": 83.46966744335718
}
#Debug simulation 
Total elapsed time: 6.214672765694559. Arrivals time: 0.33266200171783566 Scheduler time: 5.779971339739859 Scheduler overhead time: 0.033748860470950603 Adapter cache time: 0.01791939092800021 Engine time: 0.03458840260282159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.508696279022843,
    "estimated_duration": 3600.116167635051,
    "input_throughput": 5750.305000185363,
    "output_throughput": 5079.856079203578,
    "total_throughput": 10830.16107938894,
    "itl": 167.49146311194096,
    "ttft": 1997939.5570840545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5186559888906794,
    "arrivals": 557951,
    "finished_requests": 83858,
    "scheduler_time": 83.44250020858357
}
#Debug simulation 
Total elapsed time: 6.508763193152845. Arrivals time: 0.6304330267012119 Scheduler time: 5.77570583159104 Scheduler overhead time: 0.03383140405640006 Adapter cache time: 0.018251748755574226 Engine time: 0.034534802194684744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.556269790045917,
    "estimated_duration": 3600.0754124312916,
    "input_throughput": 5758.762699362113,
    "output_throughput": 5087.435095597334,
    "total_throughput": 10846.197794959447,
    "itl": 169.11274732790423,
    "ttft": 1996670.1323390862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4988378131063658,
    "arrivals": 557951,
    "finished_requests": 83985,
    "scheduler_time": 83.46880427180879
}
#Debug simulation 
Total elapsed time: 6.556334726978093. Arrivals time: 0.3508043373003602 Scheduler time: 6.103071300312877 Scheduler overhead time: 0.03402130305767059 Adapter cache time: 0.01792024215683341 Engine time: 0.03476963425055146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.484523909166455,
    "estimated_duration": 3600.123093147665,
    "input_throughput": 5750.293938394201,
    "output_throughput": 5079.846307146778,
    "total_throughput": 10830.140245540979,
    "itl": 167.49158973740376,
    "ttft": 1997943.5692973011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5253209395706661,
    "arrivals": 557951,
    "finished_requests": 83858,
    "scheduler_time": 83.44251999672738
}
#Debug simulation 
Total elapsed time: 6.484634329099208. Arrivals time: 0.6332140131853521 Scheduler time: 5.7483962825499475 Scheduler overhead time: 0.03381351940333843 Adapter cache time: 0.018141611013561487 Engine time: 0.035127488896250725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.53864434780553,
    "estimated_duration": 3600.1239118315793,
    "input_throughput": 5758.780949695794,
    "output_throughput": 5087.433224119767,
    "total_throughput": 10846.21417381556,
    "itl": 169.11224046892858,
    "ttft": 1996677.1693064596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 557951,
    "finished_requests": 83986,
    "scheduler_time": 83.46972235693424
}
#Debug simulation 
Total elapsed time: 6.53871368477121. Arrivals time: 0.5721378638409078 Scheduler time: 5.863791976124048 Scheduler overhead time: 0.03420395264402032 Adapter cache time: 0.01791070308536291 Engine time: 0.03485147934406996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 4320, 34560, 34560, 33, 33, 34560, 33, 4320, 4320, 4320, 33, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 4320, 33, 34560, 34560, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 34560, 34560, 34560, 34560, 4320, 4320, 33, 4320, 34560, 34560, 4320, 33, 33, 33, 34560, 4320, 4320, 4320, 4320, 34560, 4320, 34560, 33, 33, 4320, 33, 34560, 34560, 33, 33, 4320, 4320, 4320, 33, 34560, 33, 34560, 4320, 33, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 4320, 34560, 34560, 4320, 34560, 33, 4320, 34560, 33, 34560, 4320, 33, 4320, 4320, 34560, 34560, 33, 33, 34560, 33, 33, 33, 4320, 33]
Prompts retrieved: 1673226 . Total input tokens: 372210452 . Total output tokens: 334603713
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.546753515955061,
    "estimated_duration": 3600.1286572540835,
    "input_throughput": 5750.28505114309,
    "output_throughput": 5079.838456092514,
    "total_throughput": 10830.123507235605,
    "itl": 167.49166747111846,
    "ttft": 1997946.9453331747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5316086288914087,
    "arrivals": 557951,
    "finished_requests": 83858,
    "scheduler_time": 83.44252486272649
}
#Debug simulation 
Total elapsed time: 6.546856546774507. Arrivals time: 0.6439029271714389 Scheduler time: 5.797775936778635 Scheduler overhead time: 0.0340608605183661 Adapter cache time: 0.01827156264334917 Engine time: 0.034792706836014986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.434008254203945,
    "estimated_duration": 3600.172033640525,
    "input_throughput": 5922.823354204497,
    "output_throughput": 5262.482409998002,
    "total_throughput": 11185.3057642025,
    "itl": 164.19022359532664,
    "ttft": 1962213.3891136092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1408,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.309166743457431,
    "arrivals": 518627,
    "finished_requests": 86485,
    "scheduler_time": 86.25544716981771
}
#Debug simulation 
Total elapsed time: 6.434105612337589. Arrivals time: 0.3296183003112674 Scheduler time: 5.983981119468808 Scheduler overhead time: 0.035038388799875975 Adapter cache time: 0.03323601093143225 Engine time: 0.0359910991974175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.41938058193773,
    "estimated_duration": 3600.073746864305,
    "input_throughput": 5922.325346410085,
    "output_throughput": 5262.019428490899,
    "total_throughput": 11184.344774900985,
    "itl": 164.20089306433206,
    "ttft": 1962130.0316469779,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1407,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.59230615021656,
    "arrivals": 518627,
    "finished_requests": 86473,
    "scheduler_time": 86.247763750442
}
#Debug simulation 
Total elapsed time: 6.419470185879618. Arrivals time: 0.2805630727671087 Scheduler time: 6.01972000207752 Scheduler overhead time: 0.03479681350290775 Adapter cache time: 0.032660637982189655 Engine time: 0.03559993952512741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.414322181139141,
    "estimated_duration": 3600.0772438542213,
    "input_throughput": 5918.155794121272,
    "output_throughput": 5257.843573304863,
    "total_throughput": 11175.999367426135,
    "itl": 162.22055743215753,
    "ttft": 1962848.7057334178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.607016940247212,
    "arrivals": 518627,
    "finished_requests": 86397,
    "scheduler_time": 86.30028064088725
}
#Debug simulation 
Total elapsed time: 6.414447332266718. Arrivals time: 0.2783927251584828 Scheduler time: 6.014922187663615 Scheduler overhead time: 0.035306673496961594 Adapter cache time: 0.03322995360940695 Engine time: 0.036104668863117695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.394447674043477,
    "estimated_duration": 3600.0561943404255,
    "input_throughput": 5922.498663637201,
    "output_throughput": 5262.11008310962,
    "total_throughput": 11184.608746746822,
    "itl": 164.1931229346463,
    "ttft": 1962156.7968732698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1408,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.37914927705649,
    "arrivals": 518627,
    "finished_requests": 86476,
    "scheduler_time": 86.25117885293466
}
#Debug simulation 
Total elapsed time: 6.394539950881153. Arrivals time: 0.33304055221378803 Scheduler time: 5.941661237273365 Scheduler overhead time: 0.0349543085321784 Adapter cache time: 0.03284831065684557 Engine time: 0.035826627630740404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.7148959641344845,
    "estimated_duration": 3600.104214993564,
    "input_throughput": 5918.111456681287,
    "output_throughput": 5257.804182769703,
    "total_throughput": 11175.91563945099,
    "itl": 162.2220464216172,
    "ttft": 1962853.1142957949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1407,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.667242161855075,
    "arrivals": 518627,
    "finished_requests": 86397,
    "scheduler_time": 86.3002660290196
}
#Debug simulation 
Total elapsed time: 6.714962626807392. Arrivals time: 0.6246757255867124 Scheduler time: 5.969133349135518 Scheduler overhead time: 0.035351308062672615 Adapter cache time: 0.03299345448613167 Engine time: 0.03624969255179167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.431055154185742,
    "estimated_duration": 3600.0285145185994,
    "input_throughput": 5922.8183649126595,
    "output_throughput": 5262.464706486736,
    "total_throughput": 11185.283071399395,
    "itl": 164.18636527776013,
    "ttft": 1962184.1893400329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.212980865517053,
    "arrivals": 518627,
    "finished_requests": 86481,
    "scheduler_time": 86.25425727626597
}
#Debug simulation 
Total elapsed time: 6.431172375101596. Arrivals time: 0.3294478370808065 Scheduler time: 5.9812260190956295 Scheduler overhead time: 0.03492205636575818 Adapter cache time: 0.03314364701509476 Engine time: 0.03607862815260887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 540, 540, 34560, 540, 1080, 1080, 1080, 540, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 1080, 540, 34560, 34560, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 540, 1080, 34560, 34560, 1080, 540, 540, 540, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 540, 540, 1080, 540, 34560, 34560, 540, 540, 1080, 1080, 1080, 540, 34560, 540, 34560, 1080, 540, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 540, 34560, 540, 540, 34560, 1080, 34560, 34560, 1080, 34560, 540, 1080, 34560, 540, 34560, 1080, 540, 1080, 1080, 34560, 34560, 540, 540, 34560, 540, 540, 540, 1080, 540]
Prompts retrieved: 1555200 . Total input tokens: 346024938 . Total output tokens: 310983998
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.42915776418522,
    "estimated_duration": 3600.0207373063627,
    "input_throughput": 5918.191186848957,
    "output_throughput": 5257.925545774201,
    "total_throughput": 11176.116732623159,
    "itl": 162.226198256173,
    "ttft": 1962844.6876031158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.725854268409302,
    "arrivals": 518627,
    "finished_requests": 86396,
    "scheduler_time": 86.29601196634283
}
#Debug simulation 
Total elapsed time: 6.429254182148725. Arrivals time: 0.33079346641898155 Scheduler time: 5.976834336295724 Scheduler overhead time: 0.035508510656654835 Adapter cache time: 0.03323634760454297 Engine time: 0.036352491937577724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.66704170498997,
    "estimated_duration": 3600.0594458193955,
    "input_throughput": 6156.842500403521,
    "output_throughput": 5451.455259382003,
    "total_throughput": 11608.297759785522,
    "itl": 157.81980843579586,
    "ttft": 1946077.0283943922,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 811,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4820555603295134,
    "arrivals": 514902,
    "finished_requests": 90126,
    "scheduler_time": 89.3343740844987
}
#Debug simulation 
Total elapsed time: 6.667151441331953. Arrivals time: 0.30428218096494675 Scheduler time: 6.24393524043262 Scheduler overhead time: 0.036149544175714254 Adapter cache time: 0.0284133143723011 Engine time: 0.03731632186099887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.627200262155384,
    "estimated_duration": 3600.0614530750736,
    "input_throughput": 6156.673514855635,
    "output_throughput": 5451.351943794374,
    "total_throughput": 11608.025458650009,
    "itl": 157.8254464292824,
    "ttft": 1946077.2000199775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 812,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6416161910048723,
    "arrivals": 514902,
    "finished_requests": 90122,
    "scheduler_time": 89.33101794571763
}
#Debug simulation 
Total elapsed time: 6.6272955210879445. Arrivals time: 0.293772941455245 Scheduler time: 6.2144810617901385 Scheduler overhead time: 0.03605950763449073 Adapter cache time: 0.02881911536678672 Engine time: 0.03715945454314351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.63650164520368,
    "estimated_duration": 3600.1259809682347,
    "input_throughput": 6152.312479365824,
    "output_throughput": 5447.640472494078,
    "total_throughput": 11599.952951859903,
    "itl": 156.2255595305429,
    "ttft": 1946954.839982254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 812,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6473804973438275,
    "arrivals": 514902,
    "finished_requests": 90055,
    "scheduler_time": 89.3621886691857
}
#Debug simulation 
Total elapsed time: 6.636594402138144. Arrivals time: 0.2957458607852459 Scheduler time: 6.220802486874163 Scheduler overhead time: 0.03652719920501113 Adapter cache time: 0.02870091423392296 Engine time: 0.03770986991003156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.627468815073371,
    "estimated_duration": 3600.1172742196923,
    "input_throughput": 6156.743881295965,
    "output_throughput": 5451.368248622885,
    "total_throughput": 11608.11212991885,
    "itl": 157.81969476520388,
    "ttft": 1946120.7654216327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 811,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5156389498593934,
    "arrivals": 514902,
    "finished_requests": 90127,
    "scheduler_time": 89.33471100154144
}
#Debug simulation 
Total elapsed time: 6.62759112007916. Arrivals time: 0.29065226949751377 Scheduler time: 6.217746674548835 Scheduler overhead time: 0.036375742405653 Adapter cache time: 0.028456509578973055 Engine time: 0.03730836696922779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.67461505997926,
    "estimated_duration": 3600.1629723465376,
    "input_throughput": 6152.24926486134,
    "output_throughput": 5447.58449843648,
    "total_throughput": 11599.83376329782,
    "itl": 156.21103761642095,
    "ttft": 1946973.7322640116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 812,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6849808794818886,
    "arrivals": 514902,
    "finished_requests": 90055,
    "scheduler_time": 89.36316709579222
}
#Debug simulation 
Total elapsed time: 6.674728209152818. Arrivals time: 0.30291921785101295 Scheduler time: 6.251308719161898 Scheduler overhead time: 0.036623675376176834 Adapter cache time: 0.02892717532813549 Engine time: 0.037769227754324675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.618529774714261,
    "estimated_duration": 3600.0062198366295,
    "input_throughput": 6156.772418300387,
    "output_throughput": 5451.503359038811,
    "total_throughput": 11608.275777339199,
    "itl": 157.8177087673092,
    "ttft": 1946070.2713375483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 812,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4279208394605174,
    "arrivals": 514902,
    "finished_requests": 90125,
    "scheduler_time": 89.33408684142667
}
#Debug simulation 
Total elapsed time: 6.618621693924069. Arrivals time: 0.28874043840914965 Scheduler time: 6.2113426718860865 Scheduler overhead time: 0.03610869310796261 Adapter cache time: 0.028439108282327652 Engine time: 0.037064420990645885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 270, 270, 34560, 270, 1080, 1080, 1080, 270, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 1080, 270, 34560, 34560, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 270, 1080, 34560, 34560, 1080, 270, 270, 270, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 270, 270, 1080, 270, 34560, 34560, 270, 270, 1080, 1080, 1080, 270, 34560, 270, 34560, 1080, 270, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 1080, 34560, 34560, 1080, 34560, 270, 1080, 34560, 270, 34560, 1080, 270, 1080, 1080, 34560, 34560, 270, 270, 34560, 270, 270, 270, 1080, 270]
Prompts retrieved: 1543860 . Total input tokens: 343508723 . Total output tokens: 308670763
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.609138714149594,
    "estimated_duration": 3600.011277605937,
    "input_throughput": 6152.052949880869,
    "output_throughput": 5447.443212745024,
    "total_throughput": 11599.496162625892,
    "itl": 156.21997224393266,
    "ttft": 1946967.6665607654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 812,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.713023973852414,
    "arrivals": 514902,
    "finished_requests": 90050,
    "scheduler_time": 89.35807708924482
}
#Debug simulation 
Total elapsed time: 6.60925879701972. Arrivals time: 0.28789108246564865 Scheduler time: 6.201528721023351 Scheduler overhead time: 0.03639031620696187 Adapter cache time: 0.02905695838853717 Engine time: 0.037319773342460394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.021341344807297,
    "estimated_duration": 3600.1644263118387,
    "input_throughput": 6253.9113034527245,
    "output_throughput": 5519.813165966411,
    "total_throughput": 11773.724469419136,
    "itl": 156.00755055097142,
    "ttft": 1930388.5152461056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5669697248935883,
    "arrivals": 512974,
    "finished_requests": 91004,
    "scheduler_time": 90.41460748973003
}
#Debug simulation 
Total elapsed time: 7.021433248650283. Arrivals time: 0.6415040143765509 Scheduler time: 6.261625063139945 Scheduler overhead time: 0.03664154978469014 Adapter cache time: 0.026667323894798756 Engine time: 0.037790133617818356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.699761786032468,
    "estimated_duration": 3600.12490351839,
    "input_throughput": 6253.691358873986,
    "output_throughput": 5519.655715439122,
    "total_throughput": 11773.34707431311,
    "itl": 156.01117289844916,
    "ttft": 1930367.2153170346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6702367545780779,
    "arrivals": 512974,
    "finished_requests": 91001,
    "scheduler_time": 90.41112962137973
}
#Debug simulation 
Total elapsed time: 6.699847590178251. Arrivals time: 0.3397098183631897 Scheduler time: 6.241674496792257 Scheduler overhead time: 0.036685064900666475 Adapter cache time: 0.026756871957331896 Engine time: 0.03800338041037321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.69010204076767,
    "estimated_duration": 3600.150418317378,
    "input_throughput": 6248.326427014561,
    "output_throughput": 5514.558197065254,
    "total_throughput": 11762.884624079816,
    "itl": 154.32219820950957,
    "ttft": 1931035.7756253134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6800445788912577,
    "arrivals": 512974,
    "finished_requests": 90910,
    "scheduler_time": 90.43600424421943
}
#Debug simulation 
Total elapsed time: 6.6902338759973645. Arrivals time: 0.2933884202502668 Scheduler time: 6.278131999541074 Scheduler overhead time: 0.03681601444259286 Adapter cache time: 0.02656214078888297 Engine time: 0.037989001255482435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.669797653332353,
    "estimated_duration": 3600.020015118642,
    "input_throughput": 6253.764675043909,
    "output_throughput": 5519.646812115054,
    "total_throughput": 11773.411487158963,
    "itl": 156.00723689471124,
    "ttft": 1930331.4439523323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.590152072163755,
    "arrivals": 512974,
    "finished_requests": 90999,
    "scheduler_time": 90.4102975432508
}
#Debug simulation 
Total elapsed time: 6.669889992102981. Arrivals time: 0.34214811539277434 Scheduler time: 6.210027010645717 Scheduler overhead time: 0.0365539169870317 Adapter cache time: 0.026325165294110775 Engine time: 0.03770217392593622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.695938004180789,
    "estimated_duration": 3600.162009591768,
    "input_throughput": 6248.306309568207,
    "output_throughput": 5514.540442098386,
    "total_throughput": 11762.846751666593,
    "itl": 154.32228432498948,
    "ttft": 1931037.5692090325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7048180748149782,
    "arrivals": 512974,
    "finished_requests": 90910,
    "scheduler_time": 90.4358856999396
}
#Debug simulation 
Total elapsed time: 6.696062934119254. Arrivals time: 0.29209738271310925 Scheduler time: 6.283992316108197 Scheduler overhead time: 0.037207527086138725 Adapter cache time: 0.026838257908821106 Engine time: 0.038332171738147736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.7182763600721955,
    "estimated_duration": 3600.115519498461,
    "input_throughput": 6253.892653738115,
    "output_throughput": 5519.859263507307,
    "total_throughput": 11773.751917245421,
    "itl": 156.00580733602777,
    "ttft": 1930380.4846303822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 513,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5338958012847985,
    "arrivals": 512974,
    "finished_requests": 91003,
    "scheduler_time": 90.41368097278455
}
#Debug simulation 
Total elapsed time: 6.718377023935318. Arrivals time: 0.29370264429599047 Scheduler time: 6.306923125870526 Scheduler overhead time: 0.0364451608620584 Adapter cache time: 0.026408386882394552 Engine time: 0.03779267845675349 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 135, 135, 34560, 135, 1080, 1080, 1080, 135, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 1080, 135, 34560, 34560, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 135, 1080, 34560, 34560, 1080, 135, 135, 135, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 135, 135, 1080, 135, 34560, 34560, 135, 135, 1080, 1080, 1080, 135, 34560, 135, 34560, 1080, 135, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 1080, 34560, 34560, 1080, 34560, 135, 1080, 34560, 135, 34560, 1080, 135, 1080, 1080, 34560, 34560, 135, 135, 34560, 135, 135, 135, 1080, 135]
Prompts retrieved: 1538190 . Total input tokens: 342270947 . Total output tokens: 307575244
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.688122425228357,
    "estimated_duration": 3600.0096308860143,
    "input_throughput": 6248.403839537458,
    "output_throughput": 5514.413580919824,
    "total_throughput": 11762.817420457282,
    "itl": 154.32174472385546,
    "ttft": 1930969.8449162247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 514,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.723178127631543,
    "arrivals": 512974,
    "finished_requests": 90906,
    "scheduler_time": 90.43170613301993
}
#Debug simulation 
Total elapsed time: 6.68821967439726. Arrivals time: 0.3373236763291061 Scheduler time: 6.2315651956014335 Scheduler overhead time: 0.037135206162929535 Adapter cache time: 0.026715510990470648 Engine time: 0.038133532740175724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.754505062010139,
    "estimated_duration": 3600.1469386267913,
    "input_throughput": 6254.154173103906,
    "output_throughput": 5581.498572851186,
    "total_throughput": 11835.652745955093,
    "itl": 155.0426668323079,
    "ttft": 1926752.7609159623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8324526663497079,
    "arrivals": 512003,
    "finished_requests": 91780,
    "scheduler_time": 91.41248814388354
}
#Debug simulation 
Total elapsed time: 6.754622646141797. Arrivals time: 0.29098662408068776 Scheduler time: 6.346792095340788 Scheduler overhead time: 0.03696061996743083 Adapter cache time: 0.02442846493795514 Engine time: 0.03811667766422033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.797277752775699,
    "estimated_duration": 3600.0942214727547,
    "input_throughput": 6254.184089323396,
    "output_throughput": 5581.453363123337,
    "total_throughput": 11835.637452446734,
    "itl": 155.043930599455,
    "ttft": 1926765.919494026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8879367404221604,
    "arrivals": 512003,
    "finished_requests": 91777,
    "scheduler_time": 91.41023917542034
}
#Debug simulation 
Total elapsed time: 6.797389804851264. Arrivals time: 0.3054650640115142 Scheduler time: 6.375105103477836 Scheduler overhead time: 0.03677029861137271 Adapter cache time: 0.024634878151118755 Engine time: 0.03804967412725091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.75838960101828,
    "estimated_duration": 3600.110957427869,
    "input_throughput": 6247.4410555582535,
    "output_throughput": 5576.095080786746,
    "total_throughput": 11823.536136345,
    "itl": 153.3576588734633,
    "ttft": 1927265.6697542483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8892078794725284,
    "arrivals": 512003,
    "finished_requests": 91671,
    "scheduler_time": 91.43276001823978
}
#Debug simulation 
Total elapsed time: 6.758476917166263. Arrivals time: 0.34470218140631914 Scheduler time: 6.296261880546808 Scheduler overhead time: 0.03713203128427267 Adapter cache time: 0.02481669932603836 Engine time: 0.03826885810121894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.775452129077166,
    "estimated_duration": 3600.0254468568237,
    "input_throughput": 6254.3035687868205,
    "output_throughput": 5581.559990789461,
    "total_throughput": 11835.863559576283,
    "itl": 155.04321294107544,
    "ttft": 1926751.2958421966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8492500682361446,
    "arrivals": 512003,
    "finished_requests": 91777,
    "scheduler_time": 91.40866851272196
}
#Debug simulation 
Total elapsed time: 6.775549605023116. Arrivals time: 0.2963119917549193 Scheduler time: 6.362699635792524 Scheduler overhead time: 0.03683594660833478 Adapter cache time: 0.02460067020729184 Engine time: 0.037892020773142576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.7559765870682895,
    "estimated_duration": 3600.1643459187794,
    "input_throughput": 6247.348687150021,
    "output_throughput": 5576.127385061575,
    "total_throughput": 11823.476072211595,
    "itl": 153.35149180005325,
    "ttft": 1927276.0707003216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8991170006245424,
    "arrivals": 512003,
    "finished_requests": 91672,
    "scheduler_time": 91.43474759183584
}
#Debug simulation 
Total elapsed time: 6.756104353349656. Arrivals time: 0.29068179009482265 Scheduler time: 6.3478987058624625 Scheduler overhead time: 0.03715138975530863 Adapter cache time: 0.024828243535012007 Engine time: 0.03806182136759162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.765206133946776,
    "estimated_duration": 3600.134558176769,
    "input_throughput": 6254.1756804231245,
    "output_throughput": 5581.517766984908,
    "total_throughput": 11835.693447408034,
    "itl": 155.04244892283108,
    "ttft": 1926747.6834818884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8132936802133929,
    "arrivals": 512003,
    "finished_requests": 91780,
    "scheduler_time": 91.4125180227949
}
#Debug simulation 
Total elapsed time: 6.765300204046071. Arrivals time: 0.29372401908040047 Scheduler time: 6.355649157427251 Scheduler overhead time: 0.03663358464837074 Adapter cache time: 0.024474128149449825 Engine time: 0.0375965996645391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 66, 66, 34560, 66, 1080, 1080, 1080, 66, 34560, 1080, 66, 34560, 1080, 66, 66, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 1080, 66, 34560, 34560, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 66, 1080, 34560, 34560, 1080, 66, 66, 66, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 66, 66, 1080, 66, 34560, 34560, 66, 66, 1080, 1080, 1080, 66, 34560, 66, 34560, 1080, 66, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 1080, 34560, 34560, 1080, 34560, 66, 1080, 34560, 66, 34560, 1080, 66, 1080, 1080, 34560, 34560, 66, 66, 34560, 66, 66, 66, 1080, 66]
Prompts retrieved: 1535292 . Total input tokens: 341617963 . Total output tokens: 306985827
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.762842528987676,
    "estimated_duration": 3600.0028963574123,
    "input_throughput": 6247.254140477546,
    "output_throughput": 5575.948291683565,
    "total_throughput": 11823.202432161112,
    "itl": 153.35179467193305,
    "ttft": 1927260.7336003596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.91018333382905,
    "arrivals": 512003,
    "finished_requests": 91666,
    "scheduler_time": 91.43038455460132
}
#Debug simulation 
Total elapsed time: 6.762937155086547. Arrivals time: 0.2937278258614242 Scheduler time: 6.351973738986999 Scheduler overhead time: 0.03713166154921055 Adapter cache time: 0.024672243278473616 Engine time: 0.03803383884951472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.735881883185357,
    "estimated_duration": 3600.0457445431357,
    "input_throughput": 6310.594534648916,
    "output_throughput": 5583.881268861786,
    "total_throughput": 11894.475803510702,
    "itl": 154.33767078155577,
    "ttft": 1927815.1214047028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.550887793907895,
    "arrivals": 511517,
    "finished_requests": 91960,
    "scheduler_time": 91.47823563514976
}
#Debug simulation 
Total elapsed time: 6.736003334168345. Arrivals time: 0.29237915482372046 Scheduler time: 6.328142422251403 Scheduler overhead time: 0.036741596180945635 Adapter cache time: 0.023704583290964365 Engine time: 0.03773484472185373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.809976369142532,
    "estimated_duration": 3600.1217289111523,
    "input_throughput": 6310.461342892184,
    "output_throughput": 5583.763415155372,
    "total_throughput": 11894.224758047556,
    "itl": 154.33953104689505,
    "ttft": 1927848.718209158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5901006585243168,
    "arrivals": 511517,
    "finished_requests": 91960,
    "scheduler_time": 91.4787314311402
}
#Debug simulation 
Total elapsed time: 6.810091036837548. Arrivals time: 0.30531463865190744 Scheduler time: 6.388667100574821 Scheduler overhead time: 0.03701555263251066 Adapter cache time: 0.023738550022244453 Engine time: 0.037967872340232134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.014270062092692,
    "estimated_duration": 3600.077268922336,
    "input_throughput": 6303.165267003476,
    "output_throughput": 5578.081663233661,
    "total_throughput": 11881.246930237137,
    "itl": 152.5372989773083,
    "ttft": 1929006.6582415223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587283058241012,
    "arrivals": 511517,
    "finished_requests": 91862,
    "scheduler_time": 91.49041826132994
}
#Debug simulation 
Total elapsed time: 7.0143375201150775. Arrivals time: 0.5817419798113406 Scheduler time: 6.315181975252926 Scheduler overhead time: 0.03747716126963496 Adapter cache time: 0.023839532863348722 Engine time: 0.03865110455080867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.781176360789686,
    "estimated_duration": 3600.063771735062,
    "input_throughput": 6310.562934570124,
    "output_throughput": 5583.853307773953,
    "total_throughput": 11894.416242344078,
    "itl": 154.33811775578556,
    "ttft": 1927824.5894627168,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5627247721888131,
    "arrivals": 511517,
    "finished_requests": 91960,
    "scheduler_time": 91.47826313715977
}
#Debug simulation 
Total elapsed time: 6.781272510997951. Arrivals time: 0.29373432882130146 Scheduler time: 6.370705762412399 Scheduler overhead time: 0.037102156318724155 Adapter cache time: 0.023603277280926704 Engine time: 0.03889427473768592 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.7693856079131365,
    "estimated_duration": 3600.0672880407355,
    "input_throughput": 6303.182741995248,
    "output_throughput": 5578.0971279925625,
    "total_throughput": 11881.279869987811,
    "itl": 152.53581512302435,
    "ttft": 1928996.8359166512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.595582808144392,
    "arrivals": 511517,
    "finished_requests": 91862,
    "scheduler_time": 91.49067926636157
}
#Debug simulation 
Total elapsed time: 6.769497669767588. Arrivals time: 0.3070635059848428 Scheduler time: 6.345443957019597 Scheduler overhead time: 0.037082004360854626 Adapter cache time: 0.023915011435747147 Engine time: 0.03848152467980981 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.736155834980309,
    "estimated_duration": 3600.0202125462565,
    "input_throughput": 6310.639290530953,
    "output_throughput": 5583.920870761419,
    "total_throughput": 11894.560161292371,
    "itl": 154.3369759917756,
    "ttft": 1927806.8679826532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5382090530823923,
    "arrivals": 511517,
    "finished_requests": 91960,
    "scheduler_time": 91.47812564353589
}
#Debug simulation 
Total elapsed time: 6.7362528457306325. Arrivals time: 0.2953435485251248 Scheduler time: 6.32510962896049 Scheduler overhead time: 0.03700244380161166 Adapter cache time: 0.023622377309948206 Engine time: 0.037951549515128136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 1080, 34560, 34560, 33, 33, 34560, 33, 1080, 1080, 1080, 33, 34560, 1080, 33, 34560, 1080, 33, 33, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 1080, 33, 34560, 34560, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 34560, 34560, 34560, 34560, 1080, 1080, 33, 1080, 34560, 34560, 1080, 33, 33, 33, 34560, 1080, 1080, 1080, 1080, 34560, 1080, 34560, 33, 33, 1080, 33, 34560, 34560, 33, 33, 1080, 1080, 1080, 33, 34560, 33, 34560, 1080, 33, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 1080, 34560, 34560, 1080, 34560, 33, 1080, 34560, 33, 34560, 1080, 33, 1080, 1080, 34560, 34560, 33, 33, 34560, 33, 33, 33, 1080, 33]
Prompts retrieved: 1533906 . Total input tokens: 341312110 . Total output tokens: 306713314
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.740158700849861,
    "estimated_duration": 3600.058380572585,
    "input_throughput": 6303.1983376866465,
    "output_throughput": 5578.110929636106,
    "total_throughput": 11881.309267322753,
    "itl": 152.5411607358521,
    "ttft": 1928987.838968921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6031280353292832,
    "arrivals": 511517,
    "finished_requests": 91862,
    "scheduler_time": 91.49045168708001
}
#Debug simulation 
Total elapsed time: 6.740252412855625. Arrivals time: 0.2935849176719785 Scheduler time: 6.329439998138696 Scheduler overhead time: 0.03723740857094526 Adapter cache time: 0.023991398978978395 Engine time: 0.03857766790315509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.893016473390162,
    "estimated_duration": 3600.0619791787785,
    "input_throughput": 6475.667400959012,
    "output_throughput": 5703.81457840459,
    "total_throughput": 12179.481979363603,
    "itl": 150.68191110735063,
    "ttft": 1900075.8586353082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 761,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3290311731328703,
    "arrivals": 507129,
    "finished_requests": 94474,
    "scheduler_time": 93.44671402070257
}
#Debug simulation 
Total elapsed time: 6.893141456414014. Arrivals time: 0.29642916563898325 Scheduler time: 6.4773954269476235 Scheduler overhead time: 0.03764934977516532 Adapter cache time: 0.0252695525996387 Engine time: 0.03863524692133069 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.89179315790534,
    "estimated_duration": 3600.133552857438,
    "input_throughput": 6475.386998212048,
    "output_throughput": 5703.631462144571,
    "total_throughput": 12179.01846035662,
    "itl": 150.6883356727474,
    "ttft": 1900123.2224843318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 761,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4756399804563296,
    "arrivals": 507129,
    "finished_requests": 94473,
    "scheduler_time": 93.44519008297905
}
#Debug simulation 
Total elapsed time: 6.891887994017452. Arrivals time: 0.347345779184252 Scheduler time: 6.424951434135437 Scheduler overhead time: 0.037887330166995525 Adapter cache time: 0.02507972391322255 Engine time: 0.03886849666014314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.877628391608596,
    "estimated_duration": 3600.0581067671037,
    "input_throughput": 6470.361118953718,
    "output_throughput": 5700.465767878679,
    "total_throughput": 12170.826886832396,
    "itl": 149.36734540797733,
    "ttft": 1900340.5110777207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 759,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.474944831039752,
    "arrivals": 507129,
    "finished_requests": 94398,
    "scheduler_time": 93.4634066293312
}
#Debug simulation 
Total elapsed time: 6.8777242307551205. Arrivals time: 0.3463864531368017 Scheduler time: 6.410433031618595 Scheduler overhead time: 0.03832700289785862 Adapter cache time: 0.02550871344283223 Engine time: 0.03920742170885205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.879176102112979,
    "estimated_duration": 3600.0055789014677,
    "input_throughput": 6475.617186991603,
    "output_throughput": 5703.834216353033,
    "total_throughput": 12179.451403344636,
    "itl": 150.68433405424685,
    "ttft": 1900058.0655197068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 760,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3606948129087435,
    "arrivals": 507129,
    "finished_requests": 94473,
    "scheduler_time": 93.44366401215623
}
#Debug simulation 
Total elapsed time: 6.879293425939977. Arrivals time: 0.2984012421220541 Scheduler time: 6.461323829367757 Scheduler overhead time: 0.03772304626181722 Adapter cache time: 0.0252113351598382 Engine time: 0.03891233494505286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.919285958632827,
    "estimated_duration": 3600.090753837543,
    "input_throughput": 6470.302443117562,
    "output_throughput": 5700.414073763117,
    "total_throughput": 12170.716516880679,
    "itl": 149.36855569822058,
    "ttft": 1900353.281856832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 759,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.509275614731018,
    "arrivals": 507129,
    "finished_requests": 94398,
    "scheduler_time": 93.4633673675388
}
#Debug simulation 
Total elapsed time: 6.9193854769691825. Arrivals time: 0.37059583608061075 Scheduler time: 6.427093235310167 Scheduler overhead time: 0.03829736355692148 Adapter cache time: 0.0254270494915545 Engine time: 0.03990102233365178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.256429695058614,
    "estimated_duration": 3600.01057679131,
    "input_throughput": 6475.759863122043,
    "output_throughput": 5703.896019745041,
    "total_throughput": 12179.655882867082,
    "itl": 150.68003642126723,
    "ttft": 1900056.3354779237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 761,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.275428274420512,
    "arrivals": 507129,
    "finished_requests": 94474,
    "scheduler_time": 93.44665368549131
}
#Debug simulation 
Total elapsed time: 7.256525779142976. Arrivals time: 0.31200751196593046 Scheduler time: 6.824631387833506 Scheduler overhead time: 0.03787210304290056 Adapter cache time: 0.025095847435295582 Engine time: 0.03919935645535588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 270, 270, 34560, 270, 540, 540, 540, 270, 34560, 540, 270, 34560, 540, 270, 270, 270, 270, 540, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 540, 270, 540, 270, 34560, 34560, 270, 540, 540, 270, 540, 270, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 270, 540, 34560, 34560, 540, 270, 270, 270, 34560, 540, 540, 540, 540, 34560, 540, 34560, 270, 270, 540, 270, 34560, 34560, 270, 270, 540, 540, 540, 270, 34560, 270, 34560, 540, 270, 34560, 34560, 540, 540, 270, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 270, 34560, 270, 270, 34560, 540, 34560, 34560, 540, 34560, 270, 540, 34560, 270, 34560, 540, 270, 540, 540, 34560, 34560, 270, 270, 34560, 270, 270, 270, 540, 270]
Prompts retrieved: 1520640 . Total input tokens: 338323536 . Total output tokens: 304044908
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.94045522576198,
    "estimated_duration": 3600.110423885789,
    "input_throughput": 6470.267091101586,
    "output_throughput": 5700.38292821294,
    "total_throughput": 12170.650019314526,
    "itl": 149.3753089256205,
    "ttft": 1900361.0527661138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 759,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5364384325966367,
    "arrivals": 507129,
    "finished_requests": 94398,
    "scheduler_time": 93.4629060925887
}
#Debug simulation 
Total elapsed time: 6.940548512618989. Arrivals time: 0.3256713799200952 Scheduler time: 6.4942681272514164 Scheduler overhead time: 0.03815662628039718 Adapter cache time: 0.025597136933356524 Engine time: 0.039093599654734135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.044883894734085,
    "estimated_duration": 3600.1387966429143,
    "input_throughput": 6571.786349476873,
    "output_throughput": 5821.953870096563,
    "total_throughput": 12393.740219573436,
    "itl": 148.1908613112614,
    "ttft": 1903706.3829695475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4108848499530124,
    "arrivals": 505238,
    "finished_requests": 95802,
    "scheduler_time": 95.25536825921033
}
#Debug simulation 
Total elapsed time: 7.044980125036091. Arrivals time: 0.3591305776499212 Scheduler time: 6.567991049028933 Scheduler overhead time: 0.03825768595561385 Adapter cache time: 0.02228402905166149 Engine time: 0.03921520709991455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.007588054984808,
    "estimated_duration": 3600.1413346395475,
    "input_throughput": 6571.514504823939,
    "output_throughput": 5821.641444556905,
    "total_throughput": 12393.155949380844,
    "itl": 148.1943837960589,
    "ttft": 1903726.9942372688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4985402095713687,
    "arrivals": 505238,
    "finished_requests": 95798,
    "scheduler_time": 95.25364406359384
}
#Debug simulation 
Total elapsed time: 7.007710983045399. Arrivals time: 0.2987584280781448 Scheduler time: 6.591736369766295 Scheduler overhead time: 0.03812570217996836 Adapter cache time: 0.022069218568503857 Engine time: 0.0391262867487967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.086921350099146,
    "estimated_duration": 3600.0786375822986,
    "input_throughput": 6560.7936319585615,
    "output_throughput": 5813.539677026445,
    "total_throughput": 12374.333308985008,
    "itl": 146.44927264688175,
    "ttft": 1904850.0468840848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.502212952673444,
    "arrivals": 505238,
    "finished_requests": 95641,
    "scheduler_time": 95.24669722151079
}
#Debug simulation 
Total elapsed time: 7.087008118163794. Arrivals time: 0.37748160446062684 Scheduler time: 6.59110989747569 Scheduler overhead time: 0.03867520950734615 Adapter cache time: 0.021984351333230734 Engine time: 0.03966213809326291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.068787969183177,
    "estimated_duration": 3600.0766606222383,
    "input_throughput": 6571.632559599683,
    "output_throughput": 5821.74602814638,
    "total_throughput": 12393.378587746063,
    "itl": 148.19257777563757,
    "ttft": 1903702.5539205135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4298961960733796,
    "arrivals": 505238,
    "finished_requests": 95798,
    "scheduler_time": 95.2536201508192
}
#Debug simulation 
Total elapsed time: 7.068897699005902. Arrivals time: 0.35783743346109986 Scheduler time: 6.592560804914683 Scheduler overhead time: 0.0383888971991837 Adapter cache time: 0.022409613244235516 Engine time: 0.03961009765043855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.050545171368867,
    "estimated_duration": 3600.1003103352064,
    "input_throughput": 6560.754135709289,
    "output_throughput": 5813.504679276916,
    "total_throughput": 12374.258814986206,
    "itl": 146.44995130251013,
    "ttft": 1904859.8425932738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5232138350047213,
    "arrivals": 505238,
    "finished_requests": 95641,
    "scheduler_time": 95.24672734733556
}
#Debug simulation 
Total elapsed time: 7.050672851037234. Arrivals time: 0.303455563262105 Scheduler time: 6.6277876337990165 Scheduler overhead time: 0.0392670682631433 Adapter cache time: 0.021961146965622902 Engine time: 0.039911337196826935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.063889576122165,
    "estimated_duration": 3600.1125504963743,
    "input_throughput": 6571.834260219978,
    "output_throughput": 5821.996314284705,
    "total_throughput": 12393.830574504684,
    "itl": 148.19024743889648,
    "ttft": 1903692.8846472744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.378413185949891,
    "arrivals": 505238,
    "finished_requests": 95802,
    "scheduler_time": 95.25531984979642
}
#Debug simulation 
Total elapsed time: 7.063983323983848. Arrivals time: 0.3311923719011247 Scheduler time: 6.61519145173952 Scheduler overhead time: 0.03827914968132973 Adapter cache time: 0.022021394688636065 Engine time: 0.039313563611358404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 135, 135, 34560, 135, 540, 540, 540, 135, 34560, 540, 135, 34560, 540, 135, 135, 135, 135, 540, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 540, 135, 540, 135, 34560, 34560, 135, 540, 540, 135, 540, 135, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 135, 540, 34560, 34560, 540, 135, 135, 135, 34560, 540, 540, 540, 540, 34560, 540, 34560, 135, 135, 540, 135, 34560, 34560, 135, 135, 540, 540, 540, 135, 34560, 135, 34560, 540, 135, 34560, 34560, 540, 540, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 540, 34560, 34560, 540, 34560, 135, 540, 34560, 135, 34560, 540, 135, 540, 540, 34560, 34560, 135, 135, 34560, 135, 135, 135, 540, 135]
Prompts retrieved: 1514970 . Total input tokens: 337092769 . Total output tokens: 302899808
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.0283360737375915,
    "estimated_duration": 3600.1092519236126,
    "input_throughput": 6560.737840769605,
    "output_throughput": 5813.490240280096,
    "total_throughput": 12374.228081049701,
    "itl": 146.4551928894256,
    "ttft": 1904857.3072936225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.539184565879405,
    "arrivals": 505238,
    "finished_requests": 95641,
    "scheduler_time": 95.24651939229148
}
#Debug simulation 
Total elapsed time: 7.028444377705455. Arrivals time: 0.30908365128561854 Scheduler time: 6.600430959369987 Scheduler overhead time: 0.038561447989195585 Adapter cache time: 0.022415305487811565 Engine time: 0.03979679197072983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.395694472827017,
    "estimated_duration": 3600.0339755458012,
    "input_throughput": 6727.008735057393,
    "output_throughput": 5878.888961538563,
    "total_throughput": 12605.897696595957,
    "itl": 145.48948869343295,
    "ttft": 1886453.7831748652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8967229089722957,
    "arrivals": 504263,
    "finished_requests": 97376,
    "scheduler_time": 96.33091288889246
}
#Debug simulation 
Total elapsed time: 7.395792849827558. Arrivals time: 0.3066217480227351 Scheduler time: 6.970718570519239 Scheduler overhead time: 0.03917394205927849 Adapter cache time: 0.020927689969539642 Engine time: 0.040093410294502974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.104671439155936,
    "estimated_duration": 3600.1550847330554,
    "input_throughput": 6726.822992347756,
    "output_throughput": 5878.7909136890175,
    "total_throughput": 12605.613906036773,
    "itl": 145.4921031432456,
    "ttft": 1886531.6160123432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9561694188206508,
    "arrivals": 504263,
    "finished_requests": 97377,
    "scheduler_time": 96.33292201726114
}
#Debug simulation 
Total elapsed time: 7.104763719253242. Arrivals time: 0.356732205953449 Scheduler time: 6.629873923491687 Scheduler overhead time: 0.038807760924100876 Adapter cache time: 0.02085406333208084 Engine time: 0.04021730134263635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.106584615074098,
    "estimated_duration": 3600.004315850024,
    "input_throughput": 6721.847497087311,
    "output_throughput": 5874.048235691328,
    "total_throughput": 12595.89573277864,
    "itl": 144.45522186365056,
    "ttft": 1887398.7461982628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9578515316173484,
    "arrivals": 504263,
    "finished_requests": 97290,
    "scheduler_time": 96.32383003099525
}
#Debug simulation 
Total elapsed time: 7.106718115042895. Arrivals time: 0.3064073254354298 Scheduler time: 6.680820698849857 Scheduler overhead time: 0.03937856946140528 Adapter cache time: 0.02101774187758565 Engine time: 0.040644458029419184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.098059881012887,
    "estimated_duration": 3600.089590431582,
    "input_throughput": 6726.904814915117,
    "output_throughput": 5878.798143315877,
    "total_throughput": 12605.702958230993,
    "itl": 145.49038558551098,
    "ttft": 1886481.3953889313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9108153384737696,
    "arrivals": 504263,
    "finished_requests": 97376,
    "scheduler_time": 96.33238061444831
}
#Debug simulation 
Total elapsed time: 7.098156144376844. Arrivals time: 0.3071314422413707 Scheduler time: 6.672601496335119 Scheduler overhead time: 0.03910280345007777 Adapter cache time: 0.02086665341630578 Engine time: 0.040203943848609924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.1376389297656715,
    "estimated_duration": 3600.0186501531566,
    "input_throughput": 6721.820732503847,
    "output_throughput": 5874.0248468158225,
    "total_throughput": 12595.84557931967,
    "itl": 144.45564700072757,
    "ttft": 1887405.5012392777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9718102019093982,
    "arrivals": 504263,
    "finished_requests": 97290,
    "scheduler_time": 96.32384729486526
}
#Debug simulation 
Total elapsed time: 7.137729395646602. Arrivals time: 0.3121136613190174 Scheduler time: 6.705777696799487 Scheduler overhead time: 0.03938283305615187 Adapter cache time: 0.021095418836921453 Engine time: 0.0406996039673686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.125651199836284,
    "estimated_duration": 3600.0083344108543,
    "input_throughput": 6727.056648318348,
    "output_throughput": 5878.930834048623,
    "total_throughput": 12605.98748236697,
    "itl": 145.48867521270878,
    "ttft": 1886444.6995542294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8760847364063387,
    "arrivals": 504263,
    "finished_requests": 97376,
    "scheduler_time": 96.33079571826323
}
#Debug simulation 
Total elapsed time: 7.125770120881498. Arrivals time: 0.30353631591424346 Scheduler time: 6.703520652838051 Scheduler overhead time: 0.03924847673624754 Adapter cache time: 0.02082171430811286 Engine time: 0.04022870725020766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 66, 66, 34560, 66, 540, 540, 540, 66, 34560, 540, 66, 34560, 540, 66, 66, 66, 66, 540, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 540, 66, 540, 66, 34560, 34560, 66, 540, 540, 66, 540, 66, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 66, 540, 34560, 34560, 540, 66, 66, 66, 34560, 540, 540, 540, 540, 34560, 540, 34560, 66, 66, 540, 66, 34560, 34560, 66, 66, 540, 540, 540, 66, 34560, 66, 34560, 540, 66, 34560, 34560, 540, 540, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 540, 34560, 34560, 540, 34560, 66, 540, 34560, 66, 34560, 540, 66, 540, 540, 34560, 34560, 66, 66, 34560, 66, 66, 66, 540, 66]
Prompts retrieved: 1512072 . Total input tokens: 336431480 . Total output tokens: 302343199
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.103458745870739,
    "estimated_duration": 3600.0677010121217,
    "input_throughput": 6721.9077555671,
    "output_throughput": 5874.184253272407,
    "total_throughput": 12596.092008839507,
    "itl": 144.45478292084297,
    "ttft": 1887426.0802385618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9824992737546616,
    "arrivals": 504263,
    "finished_requests": 97295,
    "scheduler_time": 96.3261652242218
}
#Debug simulation 
Total elapsed time: 7.103553063236177. Arrivals time: 0.30643568746745586 Scheduler time: 6.677910940721631 Scheduler overhead time: 0.0394465490244329 Adapter cache time: 0.020939867943525314 Engine time: 0.04042178438976407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.126354002859443,
    "estimated_duration": 3600.009940208363,
    "input_throughput": 6643.169434864341,
    "output_throughput": 5898.513990983861,
    "total_throughput": 12541.683425848201,
    "itl": 146.34992266803783,
    "ttft": 1888355.639914268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5661902326275587,
    "arrivals": 503772,
    "finished_requests": 96876,
    "scheduler_time": 96.57722861664566
}
#Debug simulation 
Total elapsed time: 7.126462115906179. Arrivals time: 0.318012997508049 Scheduler time: 6.691069383639842 Scheduler overhead time: 0.038736003916710615 Adapter cache time: 0.019487142097204924 Engine time: 0.04086722573265433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.049607908818871,
    "estimated_duration": 3600.082311877373,
    "input_throughput": 6643.036166450468,
    "output_throughput": 5898.462912901118,
    "total_throughput": 12541.499079351586,
    "itl": 146.35059670645137,
    "ttft": 1888410.60949055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6013735521328646,
    "arrivals": 503772,
    "finished_requests": 96877,
    "scheduler_time": 96.57849456480704
}
#Debug simulation 
Total elapsed time: 7.049731212668121. Arrivals time: 0.3337015425786376 Scheduler time: 6.599473744165152 Scheduler overhead time: 0.039096645545214415 Adapter cache time: 0.019416710827499628 Engine time: 0.039942399598658085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.080921861808747,
    "estimated_duration": 3600.1016548821126,
    "input_throughput": 6632.009951058404,
    "output_throughput": 5889.646191308539,
    "total_throughput": 12521.656142366943,
    "itl": 144.57102591698802,
    "ttft": 1889366.9065533339,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.60284619137645,
    "arrivals": 503772,
    "finished_requests": 96724,
    "scheduler_time": 96.57383407113747
}
#Debug simulation 
Total elapsed time: 7.081017739139497. Arrivals time: 0.3145289276726544 Scheduler time: 6.648661580402404 Scheduler overhead time: 0.039319287054240704 Adapter cache time: 0.01945671997964382 Engine time: 0.04050838993862271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.075015200302005,
    "estimated_duration": 3600.060607255959,
    "input_throughput": 6643.076217049822,
    "output_throughput": 5898.498474498107,
    "total_throughput": 12541.574691547929,
    "itl": 146.3502868634288,
    "ttft": 1888399.0242835633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.575223451752682,
    "arrivals": 503772,
    "finished_requests": 96877,
    "scheduler_time": 96.57843667352564
}
#Debug simulation 
Total elapsed time: 7.075140478089452. Arrivals time: 0.30747611820697784 Scheduler time: 6.650769285392016 Scheduler overhead time: 0.03886200441047549 Adapter cache time: 0.019766198936849833 Engine time: 0.039976597763597965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.155617340002209,
    "estimated_duration": 3600.0928054309497,
    "input_throughput": 6632.026253318192,
    "output_throughput": 5889.660668750969,
    "total_throughput": 12521.68692206916,
    "itl": 144.56896133092442,
    "ttft": 1889358.5858322033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6108944337070004,
    "arrivals": 503772,
    "finished_requests": 96724,
    "scheduler_time": 96.5739954369997
}
#Debug simulation 
Total elapsed time: 7.155727566685528. Arrivals time: 0.34108548844233155 Scheduler time: 6.697162481490523 Scheduler overhead time: 0.03922481508925557 Adapter cache time: 0.019690614193677902 Engine time: 0.04012526338919997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.059837544336915,
    "estimated_duration": 3600.148853755446,
    "input_throughput": 6643.058654381013,
    "output_throughput": 5898.388611862235,
    "total_throughput": 12541.447266243247,
    "itl": 146.34961659937588,
    "ttft": 1888377.5186454945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5531593045569032,
    "arrivals": 503772,
    "finished_requests": 96878,
    "scheduler_time": 96.58145455831352
}
#Debug simulation 
Total elapsed time: 7.059934610966593. Arrivals time: 0.30302964290603995 Scheduler time: 6.640793345402926 Scheduler overhead time: 0.03885854221880436 Adapter cache time: 0.019296713639050722 Engine time: 0.03977831220254302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 540, 34560, 34560, 33, 33, 34560, 33, 540, 540, 540, 33, 34560, 540, 33, 34560, 540, 33, 33, 33, 33, 540, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 540, 33, 540, 33, 34560, 34560, 33, 540, 540, 33, 540, 33, 540, 540, 34560, 34560, 34560, 34560, 540, 540, 33, 540, 34560, 34560, 540, 33, 33, 33, 34560, 540, 540, 540, 540, 34560, 540, 34560, 33, 33, 540, 33, 34560, 34560, 33, 33, 540, 540, 540, 33, 34560, 33, 34560, 540, 33, 34560, 34560, 540, 540, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 540, 34560, 34560, 540, 34560, 33, 540, 34560, 33, 34560, 540, 33, 540, 540, 34560, 34560, 33, 33, 34560, 33, 33, 33, 540, 33]
Prompts retrieved: 1510686 . Total input tokens: 336135648 . Total output tokens: 302063913
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.062016473617405,
    "estimated_duration": 3600.1003796297514,
    "input_throughput": 6632.012300294664,
    "output_throughput": 5889.648277579592,
    "total_throughput": 12521.660577874256,
    "itl": 144.56898895469803,
    "ttft": 1889363.1872550624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6176851381734024,
    "arrivals": 503772,
    "finished_requests": 96724,
    "scheduler_time": 96.57400291083417
}
#Debug simulation 
Total elapsed time: 7.062137492932379. Arrivals time: 0.3043463216163218 Scheduler time: 6.640463097020984 Scheduler overhead time: 0.03913993574678898 Adapter cache time: 0.01960051618516445 Engine time: 0.04010769445449114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.296768860891461,
    "estimated_duration": 3600.077751925228,
    "input_throughput": 6792.8144015562675,
    "output_throughput": 6026.719836369421,
    "total_throughput": 12819.534237925687,
    "itl": 143.29455595364917,
    "ttft": 1876748.7230229848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3925219234894153,
    "arrivals": 501405,
    "finished_requests": 99229,
    "scheduler_time": 98.62970007945415
}
#Debug simulation 
Total elapsed time: 7.296862646006048. Arrivals time: 0.366817650385201 Scheduler time: 6.8118984079919755 Scheduler overhead time: 0.03986319154500961 Adapter cache time: 0.018801415339112282 Engine time: 0.040787494741380215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.267845741007477,
    "estimated_duration": 3600.0733146208354,
    "input_throughput": 6792.5208358084465,
    "output_throughput": 6026.6289333291215,
    "total_throughput": 12819.149769137568,
    "itl": 143.29976293679445,
    "ttft": 1876745.220523071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4838686703494812,
    "arrivals": 501405,
    "finished_requests": 99224,
    "scheduler_time": 98.62741062621262
}
#Debug simulation 
Total elapsed time: 7.267970839049667. Arrivals time: 0.35952520463615656 Scheduler time: 6.790623067412525 Scheduler overhead time: 0.039769251830875874 Adapter cache time: 0.018504039384424686 Engine time: 0.04090980440378189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.272037771996111,
    "estimated_duration": 3600.049420451899,
    "input_throughput": 6784.555751163567,
    "output_throughput": 6018.524044950537,
    "total_throughput": 12803.079796114105,
    "itl": 141.85796436729115,
    "ttft": 1878049.4418513887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4771820154972461,
    "arrivals": 501405,
    "finished_requests": 99089,
    "scheduler_time": 98.60422114380196
}
#Debug simulation 
Total elapsed time: 7.272131793666631. Arrivals time: 0.3792106984183192 Scheduler time: 6.774371207691729 Scheduler overhead time: 0.04013613658025861 Adapter cache time: 0.01865644846111536 Engine time: 0.04085062397643924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.291441474109888,
    "estimated_duration": 3600.1284110265815,
    "input_throughput": 6792.7188166676315,
    "output_throughput": 6026.63503155799,
    "total_throughput": 12819.35384822562,
    "itl": 143.29564365416232,
    "ttft": 1876766.8169419605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.420127800672777,
    "arrivals": 501405,
    "finished_requests": 99229,
    "scheduler_time": 98.63015525816549
}
#Debug simulation 
Total elapsed time: 7.291546056047082. Arrivals time: 0.37013652129098773 Scheduler time: 6.802972526289523 Scheduler overhead time: 0.03963111387565732 Adapter cache time: 0.018889485392719507 Engine time: 0.04125946154817939 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.252395673189312,
    "estimated_duration": 3600.068908356277,
    "input_throughput": 6784.51902498496,
    "output_throughput": 6018.49146545718,
    "total_throughput": 12803.01049044214,
    "itl": 141.8586758653082,
    "ttft": 1878058.5020414835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.49679960617796,
    "arrivals": 501405,
    "finished_requests": 99089,
    "scheduler_time": 98.60423232875236
}
#Debug simulation 
Total elapsed time: 7.252487248275429. Arrivals time: 0.35559446923434734 Scheduler time: 6.7784436219371855 Scheduler overhead time: 0.039871618151664734 Adapter cache time: 0.018766689114272594 Engine time: 0.04110740590840578 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.271849509328604,
    "estimated_duration": 3600.0322030723614,
    "input_throughput": 6792.8422915578185,
    "output_throughput": 6026.795532963151,
    "total_throughput": 12819.637824520969,
    "itl": 143.2943373246718,
    "ttft": 1876708.2782429508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3604728841804785,
    "arrivals": 501405,
    "finished_requests": 99228,
    "scheduler_time": 98.62928427908982
}
#Debug simulation 
Total elapsed time: 7.271974493283778. Arrivals time: 0.35740679036825895 Scheduler time: 6.797631326597184 Scheduler overhead time: 0.039392922073602676 Adapter cache time: 0.018409641925245523 Engine time: 0.04060237295925617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 135, 135, 34560, 135, 270, 270, 270, 135, 34560, 270, 135, 34560, 270, 135, 135, 135, 135, 270, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 270, 135, 270, 135, 34560, 34560, 135, 270, 270, 135, 270, 135, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 135, 270, 34560, 34560, 270, 135, 135, 135, 34560, 270, 270, 270, 270, 34560, 270, 34560, 135, 135, 270, 135, 34560, 34560, 135, 135, 270, 270, 270, 135, 34560, 135, 34560, 270, 135, 34560, 34560, 270, 270, 135, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 135, 34560, 135, 135, 34560, 270, 34560, 34560, 270, 34560, 135, 270, 34560, 135, 34560, 270, 135, 270, 270, 34560, 34560, 135, 135, 34560, 135, 135, 135, 270, 135]
Prompts retrieved: 1503360 . Total input tokens: 334512403 . Total output tokens: 300581398
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.31526063522324,
    "estimated_duration": 3600.104439720897,
    "input_throughput": 6784.452064922194,
    "output_throughput": 6018.432065731894,
    "total_throughput": 12802.884130654089,
    "itl": 141.8536005829747,
    "ttft": 1878078.924342315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5150339052081103,
    "arrivals": 501405,
    "finished_requests": 99089,
    "scheduler_time": 98.6046838870224
}
#Debug simulation 
Total elapsed time: 7.315369633026421. Arrivals time: 0.3963182596489787 Scheduler time: 6.800897767301649 Scheduler overhead time: 0.03994907019659877 Adapter cache time: 0.018594329711049795 Engine time: 0.04089642548933625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.350999549031258,
    "estimated_duration": 3600.072420553019,
    "input_throughput": 6810.977984779928,
    "output_throughput": 6099.693404675108,
    "total_throughput": 12910.671389455036,
    "itl": 142.46428770837295,
    "ttft": 1869341.033100806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9579326638509507,
    "arrivals": 500396,
    "finished_requests": 99736,
    "scheduler_time": 99.7405421367896
}
#Debug simulation 
Total elapsed time: 7.351096957921982. Arrivals time: 0.31379609182477 Scheduler time: 6.921192835550755 Scheduler overhead time: 0.039699005894362926 Adapter cache time: 0.016784837003797293 Engine time: 0.04091941425576806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.372011845000088,
    "estimated_duration": 3600.039755519011,
    "input_throughput": 6810.932841063876,
    "output_throughput": 6099.6740289703275,
    "total_throughput": 12910.606870034204,
    "itl": 142.4680173440105,
    "ttft": 1869348.8434313447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0249595217243828,
    "arrivals": 500396,
    "finished_requests": 99734,
    "scheduler_time": 99.73799104938546
}
#Debug simulation 
Total elapsed time: 7.372128622140735. Arrivals time: 0.3376136110164225 Scheduler time: 6.918767422437668 Scheduler overhead time: 0.039921003859490156 Adapter cache time: 0.016452030278742313 Engine time: 0.04070089478045702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.383871150203049,
    "estimated_duration": 3600.015352533065,
    "input_throughput": 6796.7796256155725,
    "output_throughput": 6087.154596321608,
    "total_throughput": 12883.93422193718,
    "itl": 140.61497491333688,
    "ttft": 1870432.555903297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0227605871297474,
    "arrivals": 500396,
    "finished_requests": 99546,
    "scheduler_time": 99.6939611656018
}
#Debug simulation 
Total elapsed time: 7.383984905201942. Arrivals time: 0.36927875131368637 Scheduler time: 6.896621930412948 Scheduler overhead time: 0.040424675680696964 Adapter cache time: 0.01681565400213003 Engine time: 0.04175795707851648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.319764817133546,
    "estimated_duration": 3600.0931770316784,
    "input_throughput": 6810.938715818755,
    "output_throughput": 6099.658236653127,
    "total_throughput": 12910.596952471882,
    "itl": 142.46474900551434,
    "ttft": 1869349.607921496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9779710601037401,
    "arrivals": 500396,
    "finished_requests": 99736,
    "scheduler_time": 99.74056750303751
}
#Debug simulation 
Total elapsed time: 7.3198560657911. Arrivals time: 0.34019500808790326 Scheduler time: 6.863408758305013 Scheduler overhead time: 0.039820984937250614 Adapter cache time: 0.016564898192882538 Engine time: 0.041163502261042595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.328036160673946,
    "estimated_duration": 3600.029393412179,
    "input_throughput": 6796.753116731711,
    "output_throughput": 6087.130855126052,
    "total_throughput": 12883.883971857764,
    "itl": 140.6154425441443,
    "ttft": 1870439.2875558152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0369707649946265,
    "arrivals": 500396,
    "finished_requests": 99546,
    "scheduler_time": 99.69396867084112
}
#Debug simulation 
Total elapsed time: 7.328131027985364. Arrivals time: 0.31260670674964786 Scheduler time: 6.897891078609973 Scheduler overhead time: 0.040410248562693596 Adapter cache time: 0.016600849572569132 Engine time: 0.041671032551676035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.366945629008114,
    "estimated_duration": 3600.0004296957745,
    "input_throughput": 6811.114187025837,
    "output_throughput": 6099.815383037529,
    "total_throughput": 12910.929570063367,
    "itl": 142.4621612275868,
    "ttft": 1869319.1456345082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9358857423043823,
    "arrivals": 500396,
    "finished_requests": 99736,
    "scheduler_time": 99.73972288865639
}
#Debug simulation 
Total elapsed time: 7.367067091166973. Arrivals time: 0.38278208300471306 Scheduler time: 6.867727242875844 Scheduler overhead time: 0.03998080035671592 Adapter cache time: 0.01670001447200775 Engine time: 0.04114198684692383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 66, 66, 34560, 66, 270, 270, 270, 66, 34560, 270, 66, 34560, 270, 66, 66, 66, 66, 270, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 270, 66, 270, 66, 34560, 34560, 66, 270, 270, 66, 270, 66, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 66, 270, 34560, 34560, 270, 66, 66, 66, 34560, 270, 270, 270, 270, 34560, 270, 34560, 66, 66, 270, 66, 34560, 34560, 66, 66, 270, 270, 270, 66, 34560, 66, 34560, 270, 66, 34560, 34560, 270, 270, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 270, 34560, 34560, 270, 34560, 66, 270, 34560, 66, 34560, 270, 66, 270, 270, 34560, 34560, 66, 66, 34560, 66, 66, 66, 270, 66]
Prompts retrieved: 1500462 . Total input tokens: 333877908 . Total output tokens: 299996975
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.371523743029684,
    "estimated_duration": 3600.0420198791317,
    "input_throughput": 6796.729278404787,
    "output_throughput": 6087.1095056650865,
    "total_throughput": 12883.838784069872,
    "itl": 140.61578143325204,
    "ttft": 1870445.4666684452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0500491587817715,
    "arrivals": 500396,
    "finished_requests": 99546,
    "scheduler_time": 99.69398836266124
}
#Debug simulation 
Total elapsed time: 7.3716345760039985. Arrivals time: 0.32281224988400936 Scheduler time: 6.930892054922879 Scheduler overhead time: 0.0403442601673305 Adapter cache time: 0.01674347883090377 Engine time: 0.041770392563194036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333573381 . Total output tokens: 299722984
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.366740245837718,
    "estimated_duration": 3600.032885128659,
    "input_throughput": 6906.378856345206,
    "output_throughput": 6135.742007037411,
    "total_throughput": 13042.120863382617,
    "itl": 140.9391582699292,
    "ttft": 1865223.4971115557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5784321836032897,
    "arrivals": 499947,
    "finished_requests": 100706,
    "scheduler_time": 100.42907928934693
}
#Debug simulation 
Total elapsed time: 7.366830768994987. Arrivals time: 0.320114538539201 Scheduler time: 6.93050042539835 Scheduler overhead time: 0.040217550471425056 Adapter cache time: 0.015489632729440928 Engine time: 0.04155625123530626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333573381 . Total output tokens: 299722984
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.332535920199007,
    "estimated_duration": 3600.08319644622,
    "input_throughput": 6906.282339403547,
    "output_throughput": 6135.656259778878,
    "total_throughput": 13041.938599182426,
    "itl": 140.94036947173902,
    "ttft": 1865244.327663911,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6137423486309138,
    "arrivals": 499947,
    "finished_requests": 100706,
    "scheduler_time": 100.42937874718767
}
#Debug simulation 
Total elapsed time: 7.332624062895775. Arrivals time: 0.31570686865597963 Scheduler time: 6.901003272738308 Scheduler overhead time: 0.04024409269914031 Adapter cache time: 0.015255011152476072 Engine time: 0.04160260176286101 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333573381 . Total output tokens: 299722984
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.299688087310642,
    "estimated_duration": 3600.085641858866,
    "input_throughput": 6891.440501173671,
    "output_throughput": 6122.64517924602,
    "total_throughput": 13014.085680419692,
    "itl": 139.09824284867594,
    "ttft": 1867285.6646400942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6153575142659271,
    "arrivals": 499947,
    "finished_requests": 100490,
    "scheduler_time": 100.37274378409509
}
#Debug simulation 
Total elapsed time: 7.299806542228907. Arrivals time: 0.3102088402956724 Scheduler time: 6.872342554386705 Scheduler overhead time: 0.040877881459891796 Adapter cache time: 0.015405009966343641 Engine time: 0.04180538980290294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333573381 . Total output tokens: 299722984
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.641741351690143,
    "estimated_duration": 3600.0422755735713,
    "input_throughput": 6906.360841565037,
    "output_throughput": 6135.726002406659,
    "total_throughput": 13042.086843971696,
    "itl": 140.9391196654107,
    "ttft": 1865228.6728405862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5875922482507309,
    "arrivals": 499947,
    "finished_requests": 100706,
    "scheduler_time": 100.42915050455808
}
#Debug simulation 
Total elapsed time: 7.641854481771588. Arrivals time: 0.5968241901136935 Scheduler time: 6.928739510476589 Scheduler overhead time: 0.04034837428480387 Adapter cache time: 0.01532928692176938 Engine time: 0.04159111576154828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333573381 . Total output tokens: 299722984
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.364402764011174,
    "estimated_duration": 3600.0817433903367,
    "input_throughput": 6891.447963799753,
    "output_throughput": 6122.651809356459,
    "total_throughput": 13014.099773156211,
    "itl": 139.0978396687826,
    "ttft": 1867276.6495420104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6234057565964779,
    "arrivals": 499947,
    "finished_requests": 100490,
    "scheduler_time": 100.37273299342995
}
#Debug simulation 
Total elapsed time: 7.364495609886944. Arrivals time: 0.3201158265583217 Scheduler time: 6.928127504419535 Scheduler overhead time: 0.04060517018660903 Adapter cache time: 0.015264046378433704 Engine time: 0.04156222240999341 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333573381 . Total output tokens: 299722984
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.3396103461273015,
    "estimated_duration": 3600.1422380517693,
    "input_throughput": 6906.32212727659,
    "output_throughput": 6135.74812865001,
    "total_throughput": 13042.0702559266,
    "itl": 140.93831071450444,
    "ttft": 1865333.2368206868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5651195057365119,
    "arrivals": 499947,
    "finished_requests": 100707,
    "scheduler_time": 100.43202602809319
}
#Debug simulation 
Total elapsed time: 7.3397084777243435. Arrivals time: 0.31244416953995824 Scheduler time: 6.9116609687916934 Scheduler overhead time: 0.04026845283806324 Adapter cache time: 0.01522315526381135 Engine time: 0.04120670445263386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 270, 34560, 34560, 33, 33, 34560, 33, 270, 270, 270, 33, 34560, 270, 33, 34560, 270, 33, 33, 33, 33, 270, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 270, 33, 270, 33, 34560, 34560, 33, 270, 270, 33, 270, 33, 270, 270, 34560, 34560, 34560, 34560, 270, 270, 33, 270, 34560, 34560, 270, 33, 33, 33, 34560, 270, 270, 270, 270, 34560, 270, 34560, 33, 33, 270, 33, 34560, 34560, 33, 33, 270, 270, 270, 33, 34560, 33, 34560, 270, 33, 34560, 34560, 270, 270, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 270, 34560, 34560, 270, 34560, 33, 270, 34560, 33, 34560, 270, 33, 270, 270, 34560, 34560, 33, 33, 34560, 33, 33, 33, 270, 33]
Prompts retrieved: 1499076 . Total input tokens: 333573381 . Total output tokens: 299722984
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.63142891228199,
    "estimated_duration": 3600.08836079059,
    "input_throughput": 6891.435296480251,
    "output_throughput": 6122.640555177791,
    "total_throughput": 13014.075851658043,
    "itl": 139.09794768798935,
    "ttft": 1867280.8109939557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6303222148492944,
    "arrivals": 499947,
    "finished_requests": 100490,
    "scheduler_time": 100.37274099415797
}
#Debug simulation 
Total elapsed time: 7.631527124904096. Arrivals time: 0.600444174837321 Scheduler time: 6.912585760932416 Scheduler overhead time: 0.04095622664317489 Adapter cache time: 0.01545723807066679 Engine time: 0.041935769841074944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332592553 . Total output tokens: 298874864
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.465307896025479,
    "estimated_duration": 3600.0871925781844,
    "input_throughput": 7092.765156533207,
    "output_throughput": 6215.189744883955,
    "total_throughput": 13307.954901417163,
    "itl": 137.91851218427126,
    "ttft": 1847397.084152491,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7804243747028512,
    "arrivals": 498492,
    "finished_requests": 102578,
    "scheduler_time": 101.77205250974197
}
#Debug simulation 
Total elapsed time: 7.465436962898821. Arrivals time: 0.3168690805323422 Scheduler time: 7.0323728690855205 Scheduler overhead time: 0.04110493417829275 Adapter cache time: 0.013571384828537703 Engine time: 0.042283409275114536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332592553 . Total output tokens: 298874864
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.503692428115755,
    "estimated_duration": 3600.0383075970867,
    "input_throughput": 7092.8614693112895,
    "output_throughput": 6215.274141050672,
    "total_throughput": 13308.135610361962,
    "itl": 137.92005288236984,
    "ttft": 1847401.8243501477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8298810527427156,
    "arrivals": 498492,
    "finished_requests": 102578,
    "scheduler_time": 101.76958463950398
}
#Debug simulation 
Total elapsed time: 7.503794758114964. Arrivals time: 0.35003356635570526 Scheduler time: 7.036991911008954 Scheduler overhead time: 0.041315614711493254 Adapter cache time: 0.01376706501469016 Engine time: 0.04245699988678098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332592553 . Total output tokens: 298874864
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.690042510163039,
    "estimated_duration": 3600.0603476553683,
    "input_throughput": 7079.057165416069,
    "output_throughput": 6203.566285922145,
    "total_throughput": 13282.623451338215,
    "itl": 136.41913756866856,
    "ttft": 1848908.2483814713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8317431430332403,
    "arrivals": 498492,
    "finished_requests": 102386,
    "scheduler_time": 101.71099169235272
}
#Debug simulation 
Total elapsed time: 7.690110830124468. Arrivals time: 0.5979741639457643 Scheduler time: 6.975424922537059 Scheduler overhead time: 0.04139613313600421 Adapter cache time: 0.013520985841751099 Engine time: 0.042605106718838215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332592553 . Total output tokens: 298874864
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.43081541499123,
    "estimated_duration": 3600.1032079252755,
    "input_throughput": 7092.733603800061,
    "output_throughput": 6215.162096115225,
    "total_throughput": 13307.895699915285,
    "itl": 137.91870062880818,
    "ttft": 1847417.1421193506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7931074740830827,
    "arrivals": 498492,
    "finished_requests": 102578,
    "scheduler_time": 101.77222640719195
}
#Debug simulation 
Total elapsed time: 7.4309366550296545. Arrivals time: 0.31592179741710424 Scheduler time: 7.000264787115157 Scheduler overhead time: 0.04068559315055609 Adapter cache time: 0.013221769127994776 Engine time: 0.04200920136645436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332592553 . Total output tokens: 298874864
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.45295384619385,
    "estimated_duration": 3600.1007248615206,
    "input_throughput": 7078.977769706788,
    "output_throughput": 6203.496709348057,
    "total_throughput": 13282.474479054847,
    "itl": 136.4140027667737,
    "ttft": 1848933.4224850268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8430609838105776,
    "arrivals": 498492,
    "finished_requests": 102386,
    "scheduler_time": 101.71157053273869
}
#Debug simulation 
Total elapsed time: 7.453046761918813. Arrivals time: 0.34487518994137645 Scheduler time: 6.991590436082333 Scheduler overhead time: 0.04139647865667939 Adapter cache time: 0.013658230658620596 Engine time: 0.04239307204261422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332592553 . Total output tokens: 298874864
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.6497956099919975,
    "estimated_duration": 3600.0572037066327,
    "input_throughput": 7092.824240045271,
    "output_throughput": 6215.241518096541,
    "total_throughput": 13308.065758141813,
    "itl": 137.91808093317658,
    "ttft": 1847384.585913253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7624628252000558,
    "arrivals": 498492,
    "finished_requests": 102578,
    "scheduler_time": 101.77138055913686
}
#Debug simulation 
Total elapsed time: 7.649861005600542. Arrivals time: 0.3104917146265507 Scheduler time: 7.22467315196991 Scheduler overhead time: 0.04069629171863198 Adapter cache time: 0.013284639455378056 Engine time: 0.04184710932895541 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_128_slots_96_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 66, 66, 34560, 66, 135, 135, 135, 66, 34560, 135, 66, 34560, 135, 66, 66, 66, 66, 135, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 135, 66, 135, 66, 34560, 34560, 66, 135, 135, 66, 135, 66, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 66, 135, 34560, 34560, 135, 66, 66, 66, 34560, 135, 135, 135, 135, 34560, 135, 34560, 66, 66, 135, 66, 34560, 34560, 66, 66, 135, 135, 135, 66, 34560, 66, 34560, 135, 66, 34560, 34560, 135, 135, 66, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 66, 34560, 66, 66, 34560, 135, 34560, 34560, 135, 34560, 66, 135, 34560, 66, 34560, 135, 66, 135, 135, 34560, 34560, 66, 66, 34560, 66, 66, 66, 135, 66]
Prompts retrieved: 1494657 . Total input tokens: 332592553 . Total output tokens: 298874864
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.470680733677,
    "estimated_duration": 3600.0901324166434,
    "input_throughput": 7078.998597985819,
    "output_throughput": 6203.51496172356,
    "total_throughput": 13282.51355970938,
    "itl": 136.41906038150432,
    "ttft": 1848923.7662998105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8524925177916921,
    "arrivals": 498492,
    "finished_requests": 102386,
    "scheduler_time": 101.71114737479657
}
#Debug simulation 
Total elapsed time: 7.470789109822363. Arrivals time: 0.343302967492491 Scheduler time: 7.011327568907291 Scheduler overhead time: 0.04128237860277295 Adapter cache time: 0.013455929234623909 Engine time: 0.04232742078602314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332290927 . Total output tokens: 298602223
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.448504817672074,
    "estimated_duration": 3600.007354348487,
    "input_throughput": 7038.199510730022,
    "output_throughput": 6246.652794427354,
    "total_throughput": 13284.852305157376,
    "itl": 138.44625012442253,
    "ttft": 1847178.6630158424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5692507203714915,
    "arrivals": 497997,
    "finished_requests": 102761,
    "scheduler_time": 102.12848754756503
}
#Debug simulation 
Total elapsed time: 7.448599372990429. Arrivals time: 0.35170364985242486 Scheduler time: 6.9825131110847 Scheduler overhead time: 0.04079214157536626 Adapter cache time: 0.01241279300302267 Engine time: 0.041909761261194944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332290927 . Total output tokens: 298602223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.4092626459896564,
    "estimated_duration": 3600.0895360067943,
    "input_throughput": 7038.065233262071,
    "output_throughput": 6246.521030958481,
    "total_throughput": 13284.586264220552,
    "itl": 138.4475728053114,
    "ttft": 1847214.7921256444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6031378164724458,
    "arrivals": 497997,
    "finished_requests": 102762,
    "scheduler_time": 102.12980846997216
}
#Debug simulation 
Total elapsed time: 7.409368345048279. Arrivals time: 0.2950695618055761 Scheduler time: 7.001448392868042 Scheduler overhead time: 0.04046704526990652 Adapter cache time: 0.01210379647091031 Engine time: 0.04153757030144334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332290927 . Total output tokens: 298602223
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.378743678797036,
    "estimated_duration": 3600.0812366180007,
    "input_throughput": 7025.042863687901,
    "output_throughput": 6236.189275853893,
    "total_throughput": 13261.232139541795,
    "itl": 137.0459565950827,
    "ttft": 1848097.6760349278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6048779677413427,
    "arrivals": 497997,
    "finished_requests": 102571,
    "scheduler_time": 102.0688144433981
}
#Debug simulation 
Total elapsed time: 7.378860193770379. Arrivals time: 0.2936824392527342 Scheduler time: 6.970481224823743 Scheduler overhead time: 0.0409128344617784 Adapter cache time: 0.01214960590004921 Engine time: 0.04242904111742973 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332290927 . Total output tokens: 298602223
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.403679785318673,
    "estimated_duration": 3600.052416515851,
    "input_throughput": 7038.111412978212,
    "output_throughput": 6246.57460453423,
    "total_throughput": 13284.686017512442,
    "itl": 138.44655285062427,
    "ttft": 1847211.0754918354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.577804906729144,
    "arrivals": 497997,
    "finished_requests": 102761,
    "scheduler_time": 102.12943137043877
}
#Debug simulation 
Total elapsed time: 7.40377639606595. Arrivals time: 0.33932319143787026 Scheduler time: 6.951234207488596 Scheduler overhead time: 0.04047622950747609 Adapter cache time: 0.012018982321023941 Engine time: 0.041767301969230175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332290927 . Total output tokens: 298602223
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.410236578900367,
    "estimated_duration": 3600.088381445368,
    "input_throughput": 7025.028921608377,
    "output_throughput": 6236.176899353351,
    "total_throughput": 13261.205820961728,
    "itl": 137.0460168838222,
    "ttft": 1848101.20981055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6126747024990634,
    "arrivals": 497997,
    "finished_requests": 102571,
    "scheduler_time": 102.06880503921134
}
#Debug simulation 
Total elapsed time: 7.410358691122383. Arrivals time: 0.338442703243345 Scheduler time: 6.95830262824893 Scheduler overhead time: 0.04068835638463497 Adapter cache time: 0.012080077081918716 Engine time: 0.0418970943428576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332290927 . Total output tokens: 298602223
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.420052595902234,
    "estimated_duration": 3600.106941705038,
    "input_throughput": 7038.175368201602,
    "output_throughput": 6246.654992240096,
    "total_throughput": 13284.830360441698,
    "itl": 138.44563482143673,
    "ttft": 1847246.6430748662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5561493548518054,
    "arrivals": 497997,
    "finished_requests": 102763,
    "scheduler_time": 102.13161724146075
}
#Debug simulation 
Total elapsed time: 7.420150456018746. Arrivals time: 0.3386291335336864 Scheduler time: 6.968290574382991 Scheduler overhead time: 0.04048827476799488 Adapter cache time: 0.011903638020157814 Engine time: 0.04207576345652342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 135, 34560, 34560, 33, 33, 34560, 33, 135, 135, 135, 33, 34560, 135, 33, 34560, 135, 33, 33, 33, 33, 135, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 135, 33, 135, 33, 34560, 34560, 33, 135, 135, 33, 135, 33, 135, 135, 34560, 34560, 34560, 34560, 135, 135, 33, 135, 34560, 34560, 135, 33, 33, 33, 34560, 135, 135, 135, 135, 34560, 135, 34560, 33, 33, 135, 33, 34560, 34560, 33, 33, 135, 135, 135, 33, 34560, 33, 34560, 135, 33, 34560, 34560, 135, 135, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 135, 34560, 34560, 135, 34560, 33, 135, 34560, 33, 34560, 135, 33, 135, 135, 34560, 34560, 33, 33, 34560, 33, 33, 33, 135, 33]
Prompts retrieved: 1493271 . Total input tokens: 332290927 . Total output tokens: 298602223
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.4078027359209955,
    "estimated_duration": 3600.0948735165953,
    "input_throughput": 7025.016253334418,
    "output_throughput": 6236.165653620658,
    "total_throughput": 13261.181906955077,
    "itl": 137.04611920445709,
    "ttft": 1848104.9980979457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6193396531790507,
    "arrivals": 497997,
    "finished_requests": 102571,
    "scheduler_time": 102.06883435660717
}
#Debug simulation 
Total elapsed time: 7.407925225328654. Arrivals time: 0.2908455664291978 Scheduler time: 7.003063326235861 Scheduler overhead time: 0.04081187676638365 Adapter cache time: 0.012222455348819494 Engine time: 0.04210215341299772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331632734 . Total output tokens: 298008294
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.706825147382915,
    "estimated_duration": 3600.031484058166,
    "input_throughput": 7122.141046138066,
    "output_throughput": 6313.440896460997,
    "total_throughput": 13435.581942599063,
    "itl": 136.89086847132808,
    "ttft": 1843974.4810554106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5570087693957605,
    "arrivals": 497033,
    "finished_requests": 103938,
    "scheduler_time": 103.28333371973419
}
#Debug simulation 
Total elapsed time: 7.706891919951886. Arrivals time: 0.3056106832809746 Scheduler time: 7.288002393208444 Scheduler overhead time: 0.04113926040008664 Adapter cache time: 0.010811229702085257 Engine time: 0.04221624694764614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331632734 . Total output tokens: 298008294
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.442267163190991,
    "estimated_duration": 3600.1071729231653,
    "input_throughput": 7122.1601936868865,
    "output_throughput": 6313.390659852194,
    "total_throughput": 13435.55085353908,
    "itl": 136.89121105858987,
    "ttft": 1844007.476347984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5952635684772406,
    "arrivals": 497033,
    "finished_requests": 103940,
    "scheduler_time": 103.28446078409874
}
#Debug simulation 
Total elapsed time: 7.442372445017099. Arrivals time: 0.3000730462372303 Scheduler time: 7.029194755014032 Scheduler overhead time: 0.04110439773648977 Adapter cache time: 0.010754870716482401 Engine time: 0.04232944501563907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331632734 . Total output tokens: 298008294
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.440609234850854,
    "estimated_duration": 3600.047075252256,
    "input_throughput": 7107.199285222448,
    "output_throughput": 6302.206200570372,
    "total_throughput": 13409.40548579282,
    "itl": 135.3969145473159,
    "ttft": 1845636.807673563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5960763672925555,
    "arrivals": 497033,
    "finished_requests": 103749,
    "scheduler_time": 103.19793394669091
}
#Debug simulation 
Total elapsed time: 7.440703582018614. Arrivals time: 0.2936579240486026 Scheduler time: 7.033260230906308 Scheduler overhead time: 0.04133541975170374 Adapter cache time: 0.010843283031135798 Engine time: 0.0425221212208271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331632734 . Total output tokens: 298008294
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.694364151917398,
    "estimated_duration": 3600.0709082393573,
    "input_throughput": 7122.218882221873,
    "output_throughput": 6313.453701142719,
    "total_throughput": 13435.672583364592,
    "itl": 136.89130451504204,
    "ttft": 1843988.2123332624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5691134680970579,
    "arrivals": 497033,
    "finished_requests": 103939,
    "scheduler_time": 103.2840967237417
}
#Debug simulation 
Total elapsed time: 7.694429151713848. Arrivals time: 0.298597346059978 Scheduler time: 7.282975848764181 Scheduler overhead time: 0.04069368541240692 Adapter cache time: 0.010584432631731033 Engine time: 0.042622250970453024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331632734 . Total output tokens: 298008294
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.4154840391129255,
    "estimated_duration": 3600.055108248732,
    "input_throughput": 7107.183426546652,
    "output_throughput": 6302.192138118916,
    "total_throughput": 13409.375564665568,
    "itl": 135.3971404483336,
    "ttft": 1845640.8001094605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6041246096231059,
    "arrivals": 497033,
    "finished_requests": 103749,
    "scheduler_time": 103.19794075047791
}
#Debug simulation 
Total elapsed time: 7.415594665333629. Arrivals time: 0.2963294582441449 Scheduler time: 7.005017101764679 Scheduler overhead time: 0.04161915462464094 Adapter cache time: 0.010890527162700891 Engine time: 0.042475913651287556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331632734 . Total output tokens: 298008294
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.450434272177517,
    "estimated_duration": 3600.015636680604,
    "input_throughput": 7122.172398018057,
    "output_throughput": 6313.468688418504,
    "total_throughput": 13435.64108643656,
    "itl": 136.8906453048022,
    "ttft": 1843969.1146096915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5441891536721967,
    "arrivals": 497033,
    "finished_requests": 103938,
    "scheduler_time": 103.28335308107977
}
#Debug simulation 
Total elapsed time: 7.4505199319683015. Arrivals time: 0.3004196621477604 Scheduler time: 7.036966381128877 Scheduler overhead time: 0.04114891402423382 Adapter cache time: 0.01078674616292119 Engine time: 0.042183506302535534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_128_slots_96_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [42 43 43]
Adapter prompts. [34560, 66, 34560, 34560, 33, 33, 34560, 33, 66, 66, 66, 33, 34560, 66, 33, 34560, 66, 33, 33, 33, 33, 66, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 66, 33, 66, 33, 34560, 34560, 33, 66, 66, 33, 66, 33, 66, 66, 34560, 34560, 34560, 34560, 66, 66, 33, 66, 34560, 34560, 66, 33, 33, 33, 34560, 66, 66, 66, 66, 34560, 66, 34560, 33, 33, 66, 33, 34560, 34560, 33, 33, 66, 66, 66, 33, 34560, 33, 34560, 66, 33, 34560, 34560, 66, 66, 33, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 34560, 33, 34560, 33, 33, 34560, 66, 34560, 34560, 66, 34560, 33, 66, 34560, 33, 34560, 66, 33, 66, 66, 34560, 34560, 33, 33, 34560, 33, 33, 33, 66, 33]
Prompts retrieved: 1490304 . Total input tokens: 331632734 . Total output tokens: 298008294
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.666841121856123,
    "estimated_duration": 3600.0631150211198,
    "input_throughput": 7107.167619712661,
    "output_throughput": 6302.178121637431,
    "total_throughput": 13409.345741350093,
    "itl": 135.39712555916057,
    "ttft": 1845645.5405729571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6117955905944119,
    "arrivals": 497033,
    "finished_requests": 103749,
    "scheduler_time": 103.19791184659705
}
#Debug simulation 
Total elapsed time: 7.666951913852245. Arrivals time: 0.29930937150493264 Scheduler time: 7.254212973173708 Scheduler overhead time: 0.04114323342218995 Adapter cache time: 0.010775689501315355 Engine time: 0.04242227738723159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288293443 . Total output tokens: 258982756
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 13.001217382028699,
    "estimated_duration": 3600.0615488675417,
    "input_throughput": 5407.977818080446,
    "output_throughput": 4774.832809568853,
    "total_throughput": 10182.8106276493,
    "itl": 180.0664580969076,
    "ttft": 1983620.4666821507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9120253476919594,
    "arrivals": 432423,
    "finished_requests": 78555,
    "scheduler_time": 78.27406848149292
}
#Debug simulation 
Total elapsed time: 13.001308239996433. Arrivals time: 0.28673125244677067 Scheduler time: 12.620399305131286 Scheduler overhead time: 0.03459555236622691 Adapter cache time: 0.00923864683136344 Engine time: 0.03511047223582864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288293443 . Total output tokens: 258982756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 13.047056739218533,
    "estimated_duration": 3600.1647274724974,
    "input_throughput": 5407.822828614924,
    "output_throughput": 4774.69596566712,
    "total_throughput": 10182.518794282045,
    "itl": 180.06722919387346,
    "ttft": 1983641.242989045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9707110749767214,
    "arrivals": 432423,
    "finished_requests": 78555,
    "scheduler_time": 78.27526706016953
}
#Debug simulation 
Total elapsed time: 13.047200478147715. Arrivals time: 0.32678191224113107 Scheduler time: 12.624558349139988 Scheduler overhead time: 0.035187366884201765 Adapter cache time: 0.0094444053247571 Engine time: 0.035852162167429924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288293443 . Total output tokens: 258982756
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 12.965004415251315,
    "estimated_duration": 3600.04677366889,
    "input_throughput": 5392.012721050648,
    "output_throughput": 4759.763713441131,
    "total_throughput": 10151.77643449178,
    "itl": 178.43417807591896,
    "ttft": 1984926.6476298997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8314058955386321,
    "arrivals": 432423,
    "finished_requests": 78284,
    "scheduler_time": 78.12283666614599
}
#Debug simulation 
Total elapsed time: 12.965070470236242. Arrivals time: 0.28246771870180964 Scheduler time: 12.58870260650292 Scheduler overhead time: 0.034681299701333046 Adapter cache time: 0.008855367079377174 Engine time: 0.034974670968949795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288293443 . Total output tokens: 258982756
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 12.990236764773726,
    "estimated_duration": 3600.121600704385,
    "input_throughput": 5407.887610293709,
    "output_throughput": 4774.753162958922,
    "total_throughput": 10182.640773252631,
    "itl": 180.06602436407246,
    "ttft": 1983620.8429672173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9298515431326846,
    "arrivals": 432423,
    "finished_requests": 78555,
    "scheduler_time": 78.27516513583228
}
#Debug simulation 
Total elapsed time: 12.990351276937872. Arrivals time: 0.28223162377253175 Scheduler time: 12.613045234698802 Scheduler overhead time: 0.035299067851155996 Adapter cache time: 0.009445884265005589 Engine time: 0.034914135467261076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288293443 . Total output tokens: 258982756
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 12.935098365880549,
    "estimated_duration": 3600.092080813774,
    "input_throughput": 5395.341442380388,
    "output_throughput": 4760.209076687357,
    "total_throughput": 10155.550519067745,
    "itl": 178.3125922336675,
    "ttft": 1985174.4879895188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8128055665642059,
    "arrivals": 432423,
    "finished_requests": 78326,
    "scheduler_time": 78.12001502746925
}
#Debug simulation 
Total elapsed time: 12.935182239860296. Arrivals time: 0.31893341755494475 Scheduler time: 12.520747625268996 Scheduler overhead time: 0.0353652355261147 Adapter cache time: 0.00876924442127347 Engine time: 0.03589566005393863 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288293443 . Total output tokens: 258982756
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 13.238119626417756,
    "estimated_duration": 3600.0173559143395,
    "input_throughput": 5407.838095007017,
    "output_throughput": 4774.829757906593,
    "total_throughput": 10182.66785291361,
    "itl": 180.06518445137337,
    "ttft": 1983604.9257291716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8910349878808496,
    "arrivals": 432423,
    "finished_requests": 78554,
    "scheduler_time": 78.27332955313287
}
#Debug simulation 
Total elapsed time: 13.238206265028566. Arrivals time: 0.32474471675232053 Scheduler time: 12.818260554689914 Scheduler overhead time: 0.03499694634228945 Adapter cache time: 0.009415196254849434 Engine time: 0.035516800824552774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 8640, 8640, 8640, 4320, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 4320, 17280, 17280, 4320, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 4320, 8640, 17280, 17280, 8640, 4320, 4320, 4320, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 8640, 4320, 17280, 17280, 4320, 4320, 8640, 8640, 8640, 4320, 17280, 4320, 17280, 8640, 4320, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 8640, 17280, 17280, 4320, 4320, 17280, 4320, 4320, 4320, 8640, 4320]
Prompts retrieved: 1296000 . Total input tokens: 288293443 . Total output tokens: 258982756
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 12.9420374808833,
    "estimated_duration": 3600.0594945778503,
    "input_throughput": 5394.566125712685,
    "output_throughput": 4759.802449322952,
    "total_throughput": 10154.368575035636,
    "itl": 178.3147692967195,
    "ttft": 1985416.8049218939,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8188163203373584,
    "arrivals": 432423,
    "finished_requests": 78319,
    "scheduler_time": 78.11993560936342
}
#Debug simulation 
Total elapsed time: 12.942134738899767. Arrivals time: 0.2883137911558151 Scheduler time: 12.559402594808489 Scheduler overhead time: 0.034876421093940735 Adapter cache time: 0.0085495519451797 Engine time: 0.03553975932300091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258104757 . Total output tokens: 231795576
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.65218209894374,
    "estimated_duration": 3600.167116799653,
    "input_throughput": 5391.0036313128385,
    "output_throughput": 4772.000144057773,
    "total_throughput": 10163.003775370611,
    "itl": 180.18222439244502,
    "ttft": 1951705.56064553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2731629014760337,
    "arrivals": 386952,
    "finished_requests": 78606,
    "scheduler_time": 78.12945266662456
}
#Debug simulation 
Total elapsed time: 6.652291817124933. Arrivals time: 0.24601126881316304 Scheduler time: 6.3150126580148935 Scheduler overhead time: 0.03268350753933191 Adapter cache time: 0.010892762802541256 Engine time: 0.03275763150304556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258104757 . Total output tokens: 231795576
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.643480363767594,
    "estimated_duration": 3600.0028287645373,
    "input_throughput": 5391.321597005738,
    "output_throughput": 4771.78041715466,
    "total_throughput": 10163.102014160399,
    "itl": 180.17821809754201,
    "ttft": 1951795.560226075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3574504212057272,
    "arrivals": 386952,
    "finished_requests": 78605,
    "scheduler_time": 78.12398100531755
}
#Debug simulation 
Total elapsed time: 6.643562778830528. Arrivals time: 0.2472558650188148 Scheduler time: 6.304748514667153 Scheduler overhead time: 0.03284447453916073 Adapter cache time: 0.01092878868803382 Engine time: 0.03294756542891264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258104757 . Total output tokens: 231795576
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.651970454026014,
    "estimated_duration": 3600.054162080512,
    "input_throughput": 5376.584386945432,
    "output_throughput": 4757.775919154891,
    "total_throughput": 10134.360306100323,
    "itl": 177.86512101890597,
    "ttft": 1954267.0214060582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3466728266887444,
    "arrivals": 386952,
    "finished_requests": 78372,
    "scheduler_time": 78.02484144011018
}
#Debug simulation 
Total elapsed time: 6.65206635883078. Arrivals time: 0.24844289431348443 Scheduler time: 6.310897571966052 Scheduler overhead time: 0.03313910821452737 Adapter cache time: 0.011004015803337097 Engine time: 0.03342644032090902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258104757 . Total output tokens: 231795576
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.686179830227047,
    "estimated_duration": 3600.070256344376,
    "input_throughput": 5392.186990181633,
    "output_throughput": 4772.0965916495725,
    "total_throughput": 10164.283581831205,
    "itl": 180.1766654428552,
    "ttft": 1951666.812326166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 408,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.266111791303844,
    "arrivals": 386952,
    "finished_requests": 78612,
    "scheduler_time": 78.12710101529338
}
#Debug simulation 
Total elapsed time: 6.686260859016329. Arrivals time: 0.24961191276088357 Scheduler time: 6.345110779162496 Scheduler overhead time: 0.03277724655345082 Adapter cache time: 0.01086730882525444 Engine time: 0.03298756666481495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258104757 . Total output tokens: 231795576
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.610227775294334,
    "estimated_duration": 3600.0751829914993,
    "input_throughput": 5376.552992961676,
    "output_throughput": 4757.748138406154,
    "total_throughput": 10134.30113136783,
    "itl": 177.86589055756946,
    "ttft": 1954276.869773099,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.366667678728704,
    "arrivals": 386952,
    "finished_requests": 78372,
    "scheduler_time": 78.02487310189457
}
#Debug simulation 
Total elapsed time: 6.610325331334025. Arrivals time: 0.24859235482290387 Scheduler time: 6.268830144777894 Scheduler overhead time: 0.033427467569708824 Adapter cache time: 0.010929893236607313 Engine time: 0.03343871608376503 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258104757 . Total output tokens: 231795576
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.674488541670144,
    "estimated_duration": 3600.0774882211404,
    "input_throughput": 5391.137847310636,
    "output_throughput": 4772.118949164322,
    "total_throughput": 10163.256796474958,
    "itl": 180.1803518368169,
    "ttft": 1951682.519900281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2438609226792978,
    "arrivals": 386952,
    "finished_requests": 78606,
    "scheduler_time": 78.12786469053006
}
#Debug simulation 
Total elapsed time: 6.674595709890127. Arrivals time: 0.2538932990282774 Scheduler time: 6.328940144740045 Scheduler overhead time: 0.0328929852694273 Adapter cache time: 0.010987667832523584 Engine time: 0.03294621966779232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 8640, 8640, 8640, 1080, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 1080, 17280, 17280, 1080, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 1080, 8640, 17280, 17280, 8640, 1080, 1080, 1080, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 8640, 1080, 17280, 17280, 1080, 1080, 8640, 8640, 8640, 1080, 17280, 1080, 17280, 8640, 1080, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 8640, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 8640, 1080]
Prompts retrieved: 1159920 . Total input tokens: 258104757 . Total output tokens: 231795576
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.617831954732537,
    "estimated_duration": 3600.070252114059,
    "input_throughput": 5376.531746466252,
    "output_throughput": 4757.684378503996,
    "total_throughput": 10134.216124970248,
    "itl": 177.8544542496969,
    "ttft": 1954290.9667506288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3812551179528252,
    "arrivals": 386952,
    "finished_requests": 78372,
    "scheduler_time": 78.02408641346663
}
#Debug simulation 
Total elapsed time: 6.617919894866645. Arrivals time: 0.24935857206583023 Scheduler time: 6.276226273737848 Scheduler overhead time: 0.032968685030937195 Adapter cache time: 0.010997786186635494 Engine time: 0.03321519214659929 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 252984560 . Total output tokens: 227280087
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.028236040845513,
    "estimated_duration": 3600.012777725222,
    "input_throughput": 5393.010302665621,
    "output_throughput": 4769.164739145396,
    "total_throughput": 10162.175041811017,
    "itl": 180.44537241008638,
    "ttft": 1951881.93015378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7322360630659628,
    "arrivals": 379429,
    "finished_requests": 78393,
    "scheduler_time": 78.10748162121885
}
#Debug simulation 
Total elapsed time: 6.028322479687631. Arrivals time: 0.23204875690862536 Scheduler time: 5.7026345962658525 Scheduler overhead time: 0.032890202943235636 Adapter cache time: 0.0130898617208004 Engine time: 0.03275852557271719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 252984560 . Total output tokens: 227280087
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.04011649871245,
    "estimated_duration": 3600.169087505185,
    "input_throughput": 5392.845593664644,
    "output_throughput": 4769.114056222858,
    "total_throughput": 10161.959649887503,
    "itl": 180.44749637385004,
    "ttft": 1951978.6859706994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8519804072380117,
    "arrivals": 379429,
    "finished_requests": 78398,
    "scheduler_time": 78.10843439529518
}
#Debug simulation 
Total elapsed time: 6.0401950450614095. Arrivals time: 0.27973339101299644 Scheduler time: 5.666655194479972 Scheduler overhead time: 0.03273156052455306 Adapter cache time: 0.013160871341824532 Engine time: 0.03296212665736675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 252984560 . Total output tokens: 227280087
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.011725841090083,
    "estimated_duration": 3600.140248102128,
    "input_throughput": 5378.3332497136425,
    "output_throughput": 4755.613342848392,
    "total_throughput": 10133.946592562035,
    "itl": 178.17534369944224,
    "ttft": 1954640.2146092416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9403884615190448,
    "arrivals": 379429,
    "finished_requests": 78176,
    "scheduler_time": 78.00879565691345
}
#Debug simulation 
Total elapsed time: 6.011801150627434. Arrivals time: 0.2573629505932331 Scheduler time: 5.659049534238875 Scheduler overhead time: 0.033146520145237446 Adapter cache time: 0.013768237084150314 Engine time: 0.03333664778620005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 252984560 . Total output tokens: 227280087
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.015101422090083,
    "estimated_duration": 3600.0450977780088,
    "input_throughput": 5393.026051807867,
    "output_throughput": 4769.15553380068,
    "total_throughput": 10162.181585608547,
    "itl": 180.444210062946,
    "ttft": 1951949.9839417816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 567,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7554220290202511,
    "arrivals": 379429,
    "finished_requests": 78397,
    "scheduler_time": 78.10773121669355
}
#Debug simulation 
Total elapsed time: 6.015200588852167. Arrivals time: 0.23061586963012815 Scheduler time: 5.6913615157827735 Scheduler overhead time: 0.0325558721087873 Adapter cache time: 0.013155228458344936 Engine time: 0.032693976536393166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 252984560 . Total output tokens: 227280087
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.995570316910744,
    "estimated_duration": 3600.1781950905306,
    "input_throughput": 5378.276560422616,
    "output_throughput": 4755.563217217218,
    "total_throughput": 10133.839777639834,
    "itl": 178.17515828933935,
    "ttft": 1954656.379889333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9708208778314311,
    "arrivals": 379429,
    "finished_requests": 78176,
    "scheduler_time": 78.0091386405269
}
#Debug simulation 
Total elapsed time: 5.99565273988992. Arrivals time: 0.2293604458682239 Scheduler time: 5.670815949328244 Scheduler overhead time: 0.03303433256223798 Adapter cache time: 0.013717134017497301 Engine time: 0.03334982879459858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 252984560 . Total output tokens: 227280087
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.052467680070549,
    "estimated_duration": 3600.1576416793946,
    "input_throughput": 5393.313552498356,
    "output_throughput": 4769.788356264767,
    "total_throughput": 10163.101908763123,
    "itl": 180.4446263001248,
    "ttft": 1951898.9942809406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6923684669146082,
    "arrivals": 379429,
    "finished_requests": 78401,
    "scheduler_time": 78.11142696713073
}
#Debug simulation 
Total elapsed time: 6.0525609641335905. Arrivals time: 0.263947238214314 Scheduler time: 5.694410423282534 Scheduler overhead time: 0.032963919918984175 Adapter cache time: 0.013219004031270742 Engine time: 0.033038996160030365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 540, 540, 17280, 540, 8640, 8640, 8640, 540, 17280, 8640, 540, 17280, 8640, 540, 540, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 8640, 540, 17280, 17280, 540, 8640, 8640, 540, 8640, 540, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 540, 8640, 17280, 17280, 8640, 540, 540, 540, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 540, 540, 8640, 540, 17280, 17280, 540, 540, 8640, 8640, 8640, 540, 17280, 540, 17280, 8640, 540, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 8640, 17280, 17280, 8640, 17280, 540, 8640, 17280, 540, 17280, 8640, 540, 8640, 8640, 17280, 17280, 540, 540, 17280, 540, 540, 540, 8640, 540]
Prompts retrieved: 1137240 . Total input tokens: 252984560 . Total output tokens: 227280087
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.022242355160415,
    "estimated_duration": 3600.197271368997,
    "input_throughput": 5378.248062678297,
    "output_throughput": 4755.538019029074,
    "total_throughput": 10133.786081707372,
    "itl": 178.1740431018805,
    "ttft": 1954665.9476569637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 594,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.989809699580069,
    "arrivals": 379429,
    "finished_requests": 78176,
    "scheduler_time": 78.00913015866006
}
#Debug simulation 
Total elapsed time: 6.022323381155729. Arrivals time: 0.2302648276090622 Scheduler time: 5.696900002658367 Scheduler overhead time: 0.03315482567995787 Adapter cache time: 0.013574236072599888 Engine time: 0.03340560430660844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250461681 . Total output tokens: 225031407
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.982259646058083,
    "estimated_duration": 3600.178328198839,
    "input_throughput": 5444.2714258007345,
    "output_throughput": 4811.879418391746,
    "total_throughput": 10256.150844192482,
    "itl": 178.81193865976428,
    "ttft": 1933112.9780346071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 670,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.05052678843498,
    "arrivals": 375670,
    "finished_requests": 79529,
    "scheduler_time": 78.76633430270176
}
#Debug simulation 
Total elapsed time: 5.982323465868831. Arrivals time: 0.44760448252782226 Scheduler time: 5.440639280248433 Scheduler overhead time: 0.03189196204766631 Adapter cache time: 0.014799926429986954 Engine time: 0.03267669538035989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250461681 . Total output tokens: 225031407
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.738574041053653,
    "estimated_duration": 3600.016574339496,
    "input_throughput": 5444.134935294256,
    "output_throughput": 4811.629236228753,
    "total_throughput": 10255.76417152301,
    "itl": 178.81603727770835,
    "ttft": 1933116.0979659054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 670,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.188835972156382,
    "arrivals": 375670,
    "finished_requests": 79523,
    "scheduler_time": 78.76042472313748
}
#Debug simulation 
Total elapsed time: 5.738656387198716. Arrivals time: 0.22751732263714075 Scheduler time: 5.4171199589036405 Scheduler overhead time: 0.03197505185380578 Adapter cache time: 0.01481614075601101 Engine time: 0.03259533317759633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250461681 . Total output tokens: 225031407
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.75331989582628,
    "estimated_duration": 3600.135859312072,
    "input_throughput": 5433.787991472648,
    "output_throughput": 4803.370671490262,
    "total_throughput": 10237.15866296291,
    "itl": 176.9697892397969,
    "ttft": 1934730.055548007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2020730427280117,
    "arrivals": 375670,
    "finished_requests": 79377,
    "scheduler_time": 78.71831472103058
}
#Debug simulation 
Total elapsed time: 5.753397634718567. Arrivals time: 0.22893735161051154 Scheduler time: 5.429231681860983 Scheduler overhead time: 0.032446556724607944 Adapter cache time: 0.01489437185227871 Engine time: 0.033036005683243275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250461681 . Total output tokens: 225031407
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.777830720413476,
    "estimated_duration": 3600.0636578536346,
    "input_throughput": 5444.444560651399,
    "output_throughput": 4811.961855787914,
    "total_throughput": 10256.406416439313,
    "itl": 178.81166215063848,
    "ttft": 1933078.6390786944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 670,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0711605204455332,
    "arrivals": 375670,
    "finished_requests": 79528,
    "scheduler_time": 78.76393642095965
}
#Debug simulation 
Total elapsed time: 5.777911253273487. Arrivals time: 0.23744227970018983 Scheduler time: 5.4464467745274305 Scheduler overhead time: 0.03190005011856556 Adapter cache time: 0.014924260322004557 Engine time: 0.03255876200273633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250461681 . Total output tokens: 225031407
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.751999530941248,
    "estimated_duration": 3600.1869014031154,
    "input_throughput": 5434.044269305966,
    "output_throughput": 4803.527003906408,
    "total_throughput": 10237.571273212374,
    "itl": 176.98073144324394,
    "ttft": 1934692.550780806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2382901332154908,
    "arrivals": 375670,
    "finished_requests": 79381,
    "scheduler_time": 78.71918475099608
}
#Debug simulation 
Total elapsed time: 5.752092889044434. Arrivals time: 0.22663254011422396 Scheduler time: 5.429841763805598 Scheduler overhead time: 0.03232022654265165 Adapter cache time: 0.015193783678114414 Engine time: 0.0331802130676806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250461681 . Total output tokens: 225031407
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.770470997784287,
    "estimated_duration": 3600.1097373839402,
    "input_throughput": 5444.375152364886,
    "output_throughput": 4811.971096355636,
    "total_throughput": 10256.346248720522,
    "itl": 178.8096429264347,
    "ttft": 1933092.6800829556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 670,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0033336975844236,
    "arrivals": 375670,
    "finished_requests": 79529,
    "scheduler_time": 78.76589691955945
}
#Debug simulation 
Total elapsed time: 5.770549387671053. Arrivals time: 0.22857842175289989 Scheduler time: 5.447761847637594 Scheduler overhead time: 0.03209595661610365 Adapter cache time: 0.01480074506253004 Engine time: 0.03262539906427264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 270, 270, 17280, 270, 8640, 8640, 8640, 270, 17280, 8640, 270, 17280, 8640, 270, 270, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 8640, 270, 17280, 17280, 270, 8640, 8640, 270, 8640, 270, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 270, 8640, 17280, 17280, 8640, 270, 270, 270, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 270, 270, 8640, 270, 17280, 17280, 270, 270, 8640, 8640, 8640, 270, 17280, 270, 17280, 8640, 270, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 8640, 17280, 17280, 8640, 17280, 270, 8640, 17280, 270, 17280, 8640, 270, 8640, 8640, 17280, 17280, 270, 270, 17280, 270, 270, 270, 8640, 270]
Prompts retrieved: 1125900 . Total input tokens: 250461681 . Total output tokens: 225031407
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.768579197116196,
    "estimated_duration": 3600.0150675074547,
    "input_throughput": 5433.800312825911,
    "output_throughput": 4803.506839756912,
    "total_throughput": 10237.307152582824,
    "itl": 176.9809681539745,
    "ttft": 1934686.4026294667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2594167693331872,
    "arrivals": 375670,
    "finished_requests": 79375,
    "scheduler_time": 78.7147819835597
}
#Debug simulation 
Total elapsed time: 5.768669087905437. Arrivals time: 0.2320110062137246 Scheduler time: 5.441905953921378 Scheduler overhead time: 0.0321737308986485 Adapter cache time: 0.015116855502128601 Engine time: 0.03275410970672965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249197750 . Total output tokens: 223903415
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.888004761189222,
    "estimated_duration": 3600.1773234687885,
    "input_throughput": 5555.6232937795185,
    "output_throughput": 4898.564269331022,
    "total_throughput": 10454.18756311054,
    "itl": 175.13816134313123,
    "ttft": 1922460.0524461246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2731629014760337,
    "arrivals": 373765,
    "finished_requests": 80931,
    "scheduler_time": 80.21767969036597
}
#Debug simulation 
Total elapsed time: 5.888109245803207. Arrivals time: 0.2663832954131067 Scheduler time: 5.529021194670349 Scheduler overhead time: 0.03239405760541558 Adapter cache time: 0.01195916486904025 Engine time: 0.03338133590295911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249197750 . Total output tokens: 223903415
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.879456694703549,
    "estimated_duration": 3600.1295869958385,
    "input_throughput": 5555.585296775352,
    "output_throughput": 4898.593668322969,
    "total_throughput": 10454.178965098321,
    "itl": 175.14108205725083,
    "ttft": 1922458.9970631376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3586762071610488,
    "arrivals": 373765,
    "finished_requests": 80930,
    "scheduler_time": 80.21543112373138
}
#Debug simulation 
Total elapsed time: 5.879534403793514. Arrivals time: 0.26195754995569587 Scheduler time: 5.525468216277659 Scheduler overhead time: 0.03222579788416624 Adapter cache time: 0.011983741540461779 Engine time: 0.03302026400342584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249197750 . Total output tokens: 223903415
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.890489785000682,
    "estimated_duration": 3600.1168294106783,
    "input_throughput": 5542.547907609527,
    "output_throughput": 4887.540275430543,
    "total_throughput": 10430.08818304007,
    "itl": 173.24463285037527,
    "ttft": 1923483.051063356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3409230364859182,
    "arrivals": 373765,
    "finished_requests": 80753,
    "scheduler_time": 80.15781793229799
}
#Debug simulation 
Total elapsed time: 5.890583920292556. Arrivals time: 0.2783272750675678 Scheduler time: 5.519216877873987 Scheduler overhead time: 0.03258803626522422 Adapter cache time: 0.012037018779665232 Engine time: 0.03332282556220889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249197750 . Total output tokens: 223903415
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.074403565842658,
    "estimated_duration": 3600.051155061757,
    "input_throughput": 5555.706332638736,
    "output_throughput": 4898.700390744161,
    "total_throughput": 10454.406723382897,
    "itl": 175.13899558103077,
    "ttft": 1922406.6793431859,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2878593340050388,
    "arrivals": 373765,
    "finished_requests": 80930,
    "scheduler_time": 80.21480154889738
}
#Debug simulation 
Total elapsed time: 6.074490634724498. Arrivals time: 0.2705020741559565 Scheduler time: 5.7118167988955975 Scheduler overhead time: 0.032134936191141605 Adapter cache time: 0.011975213419646025 Engine time: 0.03317472478374839 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249197750 . Total output tokens: 223903415
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.870461956132203,
    "estimated_duration": 3600.140928101383,
    "input_throughput": 5542.510806798639,
    "output_throughput": 4887.507559121999,
    "total_throughput": 10430.01836592064,
    "itl": 173.24559619779487,
    "ttft": 1923496.3537201649,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3612951498851216,
    "arrivals": 373765,
    "finished_requests": 80753,
    "scheduler_time": 80.15784960795266
}
#Debug simulation 
Total elapsed time: 5.8705398184247315. Arrivals time: 0.25988599425181746 Scheduler time: 5.517640443518758 Scheduler overhead time: 0.03266199724748731 Adapter cache time: 0.012059708591550589 Engine time: 0.03328936127945781 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249197750 . Total output tokens: 223903415
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.888833418022841,
    "estimated_duration": 3600.0594840937897,
    "input_throughput": 5555.693479057784,
    "output_throughput": 4898.689057200188,
    "total_throughput": 10454.382536257972,
    "itl": 175.13567648904564,
    "ttft": 1922426.0363437624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 416,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2438609226792978,
    "arrivals": 373765,
    "finished_requests": 80930,
    "scheduler_time": 80.2158585577374
}
#Debug simulation 
Total elapsed time: 5.888916466850787. Arrivals time: 0.26886818232014775 Scheduler time: 5.52801606990397 Scheduler overhead time: 0.0321656484156847 Adapter cache time: 0.012039666064083576 Engine time: 0.03291152324527502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 135, 135, 17280, 135, 8640, 8640, 8640, 135, 17280, 8640, 135, 17280, 8640, 135, 135, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 8640, 135, 17280, 17280, 135, 8640, 8640, 135, 8640, 135, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 135, 8640, 17280, 17280, 8640, 135, 135, 135, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 135, 135, 8640, 135, 17280, 17280, 135, 135, 8640, 8640, 8640, 135, 17280, 135, 17280, 8640, 135, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 8640, 17280, 17280, 8640, 17280, 135, 8640, 17280, 135, 17280, 8640, 135, 8640, 8640, 17280, 17280, 135, 135, 17280, 135, 135, 135, 8640, 135]
Prompts retrieved: 1120230 . Total input tokens: 249197750 . Total output tokens: 223903415
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.8734762612730265,
    "estimated_duration": 3600.1466063243533,
    "input_throughput": 5542.615393758282,
    "output_throughput": 4887.554014908555,
    "total_throughput": 10430.169408666838,
    "itl": 173.25459999926338,
    "ttft": 1923490.4312693742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 410,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.375631081536414,
    "arrivals": 373765,
    "finished_requests": 80754,
    "scheduler_time": 80.15785260836114
}
#Debug simulation 
Total elapsed time: 5.873554747086018. Arrivals time: 0.2612025188282132 Scheduler time: 5.519561551976949 Scheduler overhead time: 0.03247737977653742 Adapter cache time: 0.012046220246702433 Engine time: 0.03321739239618182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248565188 . Total output tokens: 223332285
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.881182042881846,
    "estimated_duration": 3600.1483651341823,
    "input_throughput": 5579.096460168075,
    "output_throughput": 4948.20662740549,
    "total_throughput": 10527.303087573566,
    "itl": 174.22713425611846,
    "ttft": 1919161.1014492419,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.746759009519591,
    "arrivals": 372804,
    "finished_requests": 81110,
    "scheduler_time": 80.9550977698813
}
#Debug simulation 
Total elapsed time: 5.881259064655751. Arrivals time: 0.233475083950907 Scheduler time: 5.557615333702415 Scheduler overhead time: 0.03250763984397054 Adapter cache time: 0.009119952097535133 Engine time: 0.03342928923666477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248565188 . Total output tokens: 223332285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.903988271951675,
    "estimated_duration": 3600.0960254748425,
    "input_throughput": 5579.0561856890545,
    "output_throughput": 4948.123570579043,
    "total_throughput": 10527.179756268099,
    "itl": 174.2286466824788,
    "ttft": 1919192.0065053527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7961733088619108,
    "arrivals": 372804,
    "finished_requests": 81108,
    "scheduler_time": 80.95319161379804
}
#Debug simulation 
Total elapsed time: 5.904067819006741. Arrivals time: 0.22929170541465282 Scheduler time: 5.585012532770634 Scheduler overhead time: 0.03248210437595844 Adapter cache time: 0.00915449159219861 Engine time: 0.03320376807823777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248565188 . Total output tokens: 223332285
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.893531974870712,
    "estimated_duration": 3600.057187385652,
    "input_throughput": 5564.104389837903,
    "output_throughput": 4933.573294955928,
    "total_throughput": 10497.67768479383,
    "itl": 171.94614478295208,
    "ttft": 1921489.3551684734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8009707070514598,
    "arrivals": 372804,
    "finished_requests": 80884,
    "scheduler_time": 80.8667433061294
}
#Debug simulation 
Total elapsed time: 5.8936098627746105. Arrivals time: 0.22876146854832768 Scheduler time: 5.574003518093377 Scheduler overhead time: 0.032696617767214775 Adapter cache time: 0.009400153066962957 Engine time: 0.033580050338059664 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248565188 . Total output tokens: 223332285
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.130763346794993,
    "estimated_duration": 3600.1612895151757,
    "input_throughput": 5579.076431518675,
    "output_throughput": 4948.1888636158865,
    "total_throughput": 10527.26529513456,
    "itl": 174.22678121764793,
    "ttft": 1919168.9356902495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7565395629731952,
    "arrivals": 372804,
    "finished_requests": 81110,
    "scheduler_time": 80.95524556234491
}
#Debug simulation 
Total elapsed time: 6.13085213676095. Arrivals time: 0.4535637744702399 Scheduler time: 5.587506870739162 Scheduler overhead time: 0.03246296010911465 Adapter cache time: 0.00920647382736206 Engine time: 0.03321128524839878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248565188 . Total output tokens: 223332285
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.101540657225996,
    "estimated_duration": 3600.074829661145,
    "input_throughput": 5564.0771227762,
    "output_throughput": 4933.5491178309085,
    "total_throughput": 10497.626240607107,
    "itl": 171.94431458274826,
    "ttft": 1921498.5487268318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.813294578120116,
    "arrivals": 372804,
    "finished_requests": 80884,
    "scheduler_time": 80.8669674335982
}
#Debug simulation 
Total elapsed time: 6.101625992916524. Arrivals time: 0.23770346585661173 Scheduler time: 5.773364463355392 Scheduler overhead time: 0.03263587737455964 Adapter cache time: 0.0093107963912189 Engine time: 0.033439775463193655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248565188 . Total output tokens: 223332285
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.106137551832944,
    "estimated_duration": 3600.129059820624,
    "input_throughput": 5579.126377486245,
    "output_throughput": 4948.233161643264,
    "total_throughput": 10527.35953912951,
    "itl": 174.22749415196142,
    "ttft": 1919155.0020203977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7295722719561318,
    "arrivals": 372804,
    "finished_requests": 81110,
    "scheduler_time": 80.95480623187032
}
#Debug simulation 
Total elapsed time: 6.106199427973479. Arrivals time: 0.4516782877035439 Scheduler time: 5.564903461840004 Scheduler overhead time: 0.03239179449155927 Adapter cache time: 0.009198067244142294 Engine time: 0.0331650055013597 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 66, 66, 17280, 66, 8640, 8640, 8640, 66, 17280, 8640, 66, 17280, 8640, 66, 66, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 8640, 66, 17280, 17280, 66, 8640, 8640, 66, 8640, 66, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 66, 8640, 17280, 17280, 8640, 66, 66, 66, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 66, 66, 8640, 66, 17280, 17280, 66, 66, 8640, 8640, 8640, 66, 17280, 66, 17280, 8640, 66, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 8640, 17280, 17280, 8640, 17280, 66, 8640, 17280, 66, 17280, 8640, 66, 8640, 8640, 17280, 17280, 66, 66, 17280, 66, 66, 66, 8640, 66]
Prompts retrieved: 1117332 . Total input tokens: 248565188 . Total output tokens: 223332285
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.093662052880973,
    "estimated_duration": 3600.079196348945,
    "input_throughput": 5564.070373872532,
    "output_throughput": 4933.543133721235,
    "total_throughput": 10497.613507593767,
    "itl": 171.9436113592774,
    "ttft": 1921500.077997067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8215943280234966,
    "arrivals": 372804,
    "finished_requests": 80884,
    "scheduler_time": 80.86688731657058
}
#Debug simulation 
Total elapsed time: 6.093739446718246. Arrivals time: 0.24127015890553594 Scheduler time: 5.762312259059399 Scheduler overhead time: 0.03250303165987134 Adapter cache time: 0.009341906756162643 Engine time: 0.0332736074924469 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248259933 . Total output tokens: 223055001
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.966768569778651,
    "estimated_duration": 3600.067576275072,
    "input_throughput": 5621.564754331618,
    "output_throughput": 4985.993351428271,
    "total_throughput": 10607.55810575989,
    "itl": 173.03443554652316,
    "ttft": 1911531.2250099923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49273852677317276,
    "arrivals": 372307,
    "finished_requests": 81777,
    "scheduler_time": 81.56878851639232
}
#Debug simulation 
Total elapsed time: 5.9668504246510565. Arrivals time: 0.2839298704639077 Scheduler time: 5.594580438453704 Scheduler overhead time: 0.032637417782098055 Adapter cache time: 0.00749325891956687 Engine time: 0.033254798501729965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248259933 . Total output tokens: 223055001
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.204620847012848,
    "estimated_duration": 3600.0117763102726,
    "input_throughput": 5621.493555429915,
    "output_throughput": 4986.060634056371,
    "total_throughput": 10607.554189486287,
    "itl": 173.03729059748593,
    "ttft": 1911540.1451785616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5247092012339275,
    "arrivals": 372307,
    "finished_requests": 81775,
    "scheduler_time": 81.56715812572102
}
#Debug simulation 
Total elapsed time: 6.204684271942824. Arrivals time: 0.4547438290901482 Scheduler time: 5.661473819054663 Scheduler overhead time: 0.032620593439787626 Adapter cache time: 0.0075760409235954285 Engine time: 0.033287986647337675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248259933 . Total output tokens: 223055001
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.963221398182213,
    "estimated_duration": 3600.180152558705,
    "input_throughput": 5605.645869042639,
    "output_throughput": 4972.21145649547,
    "total_throughput": 10577.857325538109,
    "itl": 170.86702769089837,
    "ttft": 1913520.40836656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5257547690719384,
    "arrivals": 372307,
    "finished_requests": 81546,
    "scheduler_time": 81.48016493126245
}
#Debug simulation 
Total elapsed time: 5.963317817077041. Arrivals time: 0.2765835467725992 Scheduler time: 5.597294316627085 Scheduler overhead time: 0.032982117496430874 Adapter cache time: 0.007624749559909105 Engine time: 0.03366529056802392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248259933 . Total output tokens: 223055001
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.149506818968803,
    "estimated_duration": 3600.1437434859336,
    "input_throughput": 5621.795528753748,
    "output_throughput": 4986.291736956625,
    "total_throughput": 10608.087265710372,
    "itl": 173.03648426807322,
    "ttft": 1911534.281183338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5010106727643874,
    "arrivals": 372307,
    "finished_requests": 81781,
    "scheduler_time": 81.57038311905147
}
#Debug simulation 
Total elapsed time: 6.149571532849222. Arrivals time: 0.27375197131186724 Scheduler time: 5.7869957238435745 Scheduler overhead time: 0.032586855348199606 Adapter cache time: 0.007548209745436907 Engine time: 0.033428048714995384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248259933 . Total output tokens: 223055001
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.983699086122215,
    "estimated_duration": 3600.1835928511346,
    "input_throughput": 5605.640512354417,
    "output_throughput": 4972.2067051096055,
    "total_throughput": 10577.847217464023,
    "itl": 170.86633661856916,
    "ttft": 1913521.7064808765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5330484886839995,
    "arrivals": 372307,
    "finished_requests": 81546,
    "scheduler_time": 81.48009291394979
}
#Debug simulation 
Total elapsed time: 5.983782559167594. Arrivals time: 0.2816348192282021 Scheduler time: 5.612263350747526 Scheduler overhead time: 0.03311171801760793 Adapter cache time: 0.007650682236999273 Engine time: 0.03389611607417464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248259933 . Total output tokens: 223055001
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.193758429959416,
    "estimated_duration": 3600.0435119264707,
    "input_throughput": 5621.602331459085,
    "output_throughput": 4986.02668010381,
    "total_throughput": 10607.629011562894,
    "itl": 173.03455438020953,
    "ttft": 1911526.9693046487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4813980974792509,
    "arrivals": 372307,
    "finished_requests": 81777,
    "scheduler_time": 81.56837190227199
}
#Debug simulation 
Total elapsed time: 6.193842696025968. Arrivals time: 0.4914876692928374 Scheduler time: 5.613522320985794 Scheduler overhead time: 0.03268629126250744 Adapter cache time: 0.00757225276902318 Engine time: 0.0332567086443305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 8640, 17280, 17280, 33, 33, 17280, 33, 8640, 8640, 8640, 33, 17280, 8640, 33, 17280, 8640, 33, 33, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 8640, 33, 17280, 17280, 33, 8640, 8640, 33, 8640, 33, 8640, 8640, 17280, 17280, 17280, 17280, 8640, 8640, 33, 8640, 17280, 17280, 8640, 33, 33, 33, 17280, 8640, 8640, 8640, 8640, 17280, 8640, 17280, 33, 33, 8640, 33, 17280, 17280, 33, 33, 8640, 8640, 8640, 33, 17280, 33, 17280, 8640, 33, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 8640, 17280, 17280, 8640, 17280, 33, 8640, 17280, 33, 17280, 8640, 33, 8640, 8640, 17280, 17280, 33, 33, 17280, 33, 33, 33, 8640, 33]
Prompts retrieved: 1115946 . Total input tokens: 248259933 . Total output tokens: 223055001
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.973958629183471,
    "estimated_duration": 3600.1541047380906,
    "input_throughput": 5605.686426989264,
    "output_throughput": 4972.247431419961,
    "total_throughput": 10577.933858409226,
    "itl": 170.8727423183228,
    "ttft": 1913509.7217239013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 161,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5390846704319123,
    "arrivals": 372307,
    "finished_requests": 81546,
    "scheduler_time": 81.47961862756362
}
#Debug simulation 
Total elapsed time: 5.9740380281582475. Arrivals time: 0.2789544779807329 Scheduler time: 5.605398171115667 Scheduler overhead time: 0.03314008889719844 Adapter cache time: 0.007698891684412956 Engine time: 0.03372440207749605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216811905 . Total output tokens: 194671193
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.590043005067855,
    "estimated_duration": 3600.0304179981076,
    "input_throughput": 5388.825022980735,
    "output_throughput": 4768.089990068807,
    "total_throughput": 10156.915013049542,
    "itl": 180.07018105022425,
    "ttft": 1897144.6655776838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8944419134944044,
    "arrivals": 324936,
    "finished_requests": 78496,
    "scheduler_time": 77.97214968229648
}
#Debug simulation 
Total elapsed time: 6.590120621025562. Arrivals time: 0.23220563912764192 Scheduler time: 6.262887589633465 Scheduler overhead time: 0.03281325288116932 Adapter cache time: 0.014306249562650919 Engine time: 0.032891891431063414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216811905 . Total output tokens: 194671193
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.593095426913351,
    "estimated_duration": 3600.1023937350446,
    "input_throughput": 5388.572289987964,
    "output_throughput": 4767.889110562692,
    "total_throughput": 10156.461400550656,
    "itl": 180.07638019010818,
    "ttft": 1897187.7764705317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0229896446736584,
    "arrivals": 324936,
    "finished_requests": 78494,
    "scheduler_time": 77.97146586656316
}
#Debug simulation 
Total elapsed time: 6.593192460015416. Arrivals time: 0.25885300850495696 Scheduler time: 6.239110047463328 Scheduler overhead time: 0.03287087380886078 Adapter cache time: 0.01441154070198536 Engine time: 0.03296598745509982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216811905 . Total output tokens: 194671193
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.49338063923642,
    "estimated_duration": 3600.0952938697787,
    "input_throughput": 5377.35488084562,
    "output_throughput": 4756.9015823444615,
    "total_throughput": 10134.25646319008,
    "itl": 177.87812781716522,
    "ttft": 1899080.0247832485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.14932977121324,
    "arrivals": 324936,
    "finished_requests": 78320,
    "scheduler_time": 77.8760590424136
}
#Debug simulation 
Total elapsed time: 6.493475347291678. Arrivals time: 0.25488696061074734 Scheduler time: 6.142188030760735 Scheduler overhead time: 0.03311974601820111 Adapter cache time: 0.014990317169576883 Engine time: 0.033205298241227865 
