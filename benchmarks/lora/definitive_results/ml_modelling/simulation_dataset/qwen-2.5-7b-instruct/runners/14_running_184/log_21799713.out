INFO 06-01 00:46:59 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:00 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 114.82312709419057,
    "estimated_duration": 3600.0577191816988,
    "input_throughput": 8443.270183709481,
    "output_throughput": 7455.933236009671,
    "total_throughput": 15899.203419719153,
    "itl": 110.10750661289782,
    "ttft": 1562163.263233198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3641980415279973,
    "arrivals": 344340,
    "finished_requests": 122848,
    "scheduler_time": 221.9049677107899
}
#Debug simulation 
Total elapsed time: 114.82335044210777. Arrivals time: 0.538854846265167 Scheduler time: 114.07650821423158 Scheduler overhead time: 0.0813480387441814 Adapter cache time: 0.015149338636547327 Engine time: 0.08105206536129117 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 118.73206719430164,
    "estimated_duration": 3600.042808837183,
    "input_throughput": 8416.277974701803,
    "output_throughput": 7434.186875306796,
    "total_throughput": 15850.464850008599,
    "itl": 109.780457341106,
    "ttft": 1567714.2737326373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3944837111583911,
    "arrivals": 344340,
    "finished_requests": 122617,
    "scheduler_time": 222.30568626642537
}
#Debug simulation 
Total elapsed time: 118.7322817123495. Arrivals time: 0.5420895610004663 Scheduler time: 117.9800497177057 Scheduler overhead time: 0.08425283152610064 Adapter cache time: 0.015639628749340773 Engine time: 0.08015913842245936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 118.89655622001737,
    "estimated_duration": 3600.0449549559225,
    "input_throughput": 8416.272957450046,
    "output_throughput": 7434.182443515537,
    "total_throughput": 15850.455400965582,
    "itl": 109.78047599898655,
    "ttft": 1567715.664500626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39524558026343676,
    "arrivals": 344340,
    "finished_requests": 122617,
    "scheduler_time": 222.30577573108408
}
#Debug simulation 
Total elapsed time: 118.89673745306209. Arrivals time: 0.5543517195619643 Scheduler time: 118.12943822983652 Scheduler overhead time: 0.08567872457206249 Adapter cache time: 0.01585438149049878 Engine time: 0.08057052828371525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 119.52353622904047,
    "estimated_duration": 3600.0784596410354,
    "input_throughput": 8403.641848132556,
    "output_throughput": 7417.84805508454,
    "total_throughput": 15821.489903217098,
    "itl": 109.42552147404047,
    "ttft": 1558985.8767398016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 112,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3504122551297771,
    "arrivals": 344340,
    "finished_requests": 122361,
    "scheduler_time": 222.9490760339183
}
#Debug simulation 
Total elapsed time: 119.5237129391171. Arrivals time: 0.5378262489102781 Scheduler time: 118.77593127125874 Scheduler overhead time: 0.0837496598251164 Adapter cache time: 0.015413601882755756 Engine time: 0.08052812470123172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 118.75150392996147,
    "estimated_duration": 3600.049727421903,
    "input_throughput": 8416.261800277392,
    "output_throughput": 7434.172588267556,
    "total_throughput": 15850.434388544949,
    "itl": 109.7805344605568,
    "ttft": 1567717.8007907378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39977271657437125,
    "arrivals": 344340,
    "finished_requests": 122617,
    "scheduler_time": 222.3058209835961
}
#Debug simulation 
Total elapsed time: 118.75167965097353. Arrivals time: 0.5407507098279893 Scheduler time: 117.99704542197287 Scheduler overhead time: 0.0854054857045412 Adapter cache time: 0.015619311016052961 Engine time: 0.08266090042889118 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 115.04106932226568,
    "estimated_duration": 3600.048137034207,
    "input_throughput": 8443.29265692571,
    "output_throughput": 7455.953081258745,
    "total_throughput": 15899.245738184456,
    "itl": 110.1071742289107,
    "ttft": 1562159.045564669,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3558159850933593,
    "arrivals": 344340,
    "finished_requests": 122848,
    "scheduler_time": 221.90456792835892
}
#Debug simulation 
Total elapsed time: 115.04124473594129. Arrivals time: 0.5357700712047517 Scheduler time: 114.29778474522755 Scheduler overhead time: 0.08378575975075364 Adapter cache time: 0.015573812182992697 Engine time: 0.07907842146232724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 119.13470081612468,
    "estimated_duration": 3600.0543833794773,
    "input_throughput": 8416.25091550908,
    "output_throughput": 7434.162973637197,
    "total_throughput": 15850.413889146277,
    "itl": 109.78057227920638,
    "ttft": 1567719.4744374938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40530588317662447,
    "arrivals": 344340,
    "finished_requests": 122617,
    "scheduler_time": 222.30584412179545
}
#Debug simulation 
Total elapsed time: 119.13487783400342. Arrivals time: 0.5452462173998356 Scheduler time: 118.37659766478464 Scheduler overhead time: 0.08509748475626111 Adapter cache time: 0.015645055565983057 Engine time: 0.08141564158722758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 116.86334714991972,
    "estimated_duration": 3600.01683444188,
    "input_throughput": 8496.328880290353,
    "output_throughput": 7509.949048388674,
    "total_throughput": 16006.277928679026,
    "itl": 111.22241656882436,
    "ttft": 1517447.473610399,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31829072536900604,
    "arrivals": 321710,
    "finished_requests": 123280,
    "scheduler_time": 218.67419481850348
}
#Debug simulation 
Total elapsed time: 116.86352441785857. Arrivals time: 0.5407848046161234 Scheduler time: 116.11288201576099 Scheduler overhead time: 0.08397019328549504 Adapter cache time: 0.015143977012485266 Engine time: 0.08100926410406828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 117.27834321605042,
    "estimated_duration": 3600.0391184748937,
    "input_throughput": 8496.276288508145,
    "output_throughput": 7509.902562240323,
    "total_throughput": 16006.178850748467,
    "itl": 111.22265221193837,
    "ttft": 1517457.8821146148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33874971232376994,
    "arrivals": 321710,
    "finished_requests": 123280,
    "scheduler_time": 218.67492025682873
}
#Debug simulation 
Total elapsed time: 117.27852315502241. Arrivals time: 0.5472956774756312 Scheduler time: 116.52068745857105 Scheduler overhead time: 0.08371113892644644 Adapter cache time: 0.015551128424704075 Engine time: 0.08101273514330387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 117.31879613455385,
    "estimated_duration": 3600.0398283533946,
    "input_throughput": 8496.274613158936,
    "output_throughput": 7509.901081390492,
    "total_throughput": 16006.175694549429,
    "itl": 111.2226697065446,
    "ttft": 1517458.1918902935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33945878989994577,
    "arrivals": 321710,
    "finished_requests": 123280,
    "scheduler_time": 218.6749210577532
}
#Debug simulation 
Total elapsed time: 117.31910970667377. Arrivals time: 0.5460661854594946 Scheduler time: 116.56423810916021 Scheduler overhead time: 0.08331161038950086 Adapter cache time: 0.015356487594544888 Engine time: 0.07971097528934479 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 119.33042062679306,
    "estimated_duration": 3600.112734125168,
    "input_throughput": 8508.957152822024,
    "output_throughput": 7524.836859472115,
    "total_throughput": 16033.794012294138,
    "itl": 111.54500921718038,
    "ttft": 1508642.0635651962,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 108,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33722626799484734,
    "arrivals": 321710,
    "finished_requests": 123502,
    "scheduler_time": 218.22151149193482
}
#Debug simulation 
Total elapsed time: 119.33060237998143. Arrivals time: 0.5505384849384427 Scheduler time: 118.56758543802425 Scheduler overhead time: 0.08438621321693063 Adapter cache time: 0.015434696339070797 Engine time: 0.0820815828628838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 117.4411516287364,
    "estimated_duration": 3600.0439435557046,
    "input_throughput": 8496.264901086122,
    "output_throughput": 7509.892496839092,
    "total_throughput": 16006.157397925213,
    "itl": 111.22274249566964,
    "ttft": 1517459.812901176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3438601724244655,
    "arrivals": 321710,
    "finished_requests": 123280,
    "scheduler_time": 218.6749349932815
}
#Debug simulation 
Total elapsed time: 117.44133718498051. Arrivals time: 0.5465274336747825 Scheduler time: 116.6854148088023 Scheduler overhead time: 0.08350165328010917 Adapter cache time: 0.01580896880477667 Engine time: 0.08001760672777891 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 117.9060122887604,
    "estimated_duration": 3600.0096154769326,
    "input_throughput": 8496.345917661616,
    "output_throughput": 7509.964107809265,
    "total_throughput": 16006.310025470882,
    "itl": 111.22231747847037,
    "ttft": 1517444.0519875488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3109652306698266,
    "arrivals": 321710,
    "finished_requests": 123280,
    "scheduler_time": 218.67410127108155
}
#Debug simulation 
Total elapsed time: 117.90619070688263. Arrivals time: 0.5432795505039394 Scheduler time: 117.15332610765472 Scheduler overhead time: 0.0830923905596137 Adapter cache time: 0.0156630277633667 Engine time: 0.08063134271651506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 117.02364666992798,
    "estimated_duration": 3600.046875807118,
    "input_throughput": 8496.25798084713,
    "output_throughput": 7509.886380004048,
    "total_throughput": 16006.144360851178,
    "itl": 111.22274812441987,
    "ttft": 1517461.6691175352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.34801004737615554,
    "arrivals": 321710,
    "finished_requests": 123280,
    "scheduler_time": 218.67501868793818
}
#Debug simulation 
Total elapsed time: 117.02381856692955. Arrivals time: 0.5453200577758253 Scheduler time: 116.26852162973955 Scheduler overhead time: 0.08354948554188013 Adapter cache time: 0.015570454765111208 Engine time: 0.08065081154927611 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 111.56915493821725,
    "estimated_duration": 3600.036631754725,
    "input_throughput": 8394.367360998267,
    "output_throughput": 7496.347609898061,
    "total_throughput": 15890.714970896328,
    "itl": 111.84538382932763,
    "ttft": 1520836.525087593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2968673111614768,
    "arrivals": 317887,
    "finished_requests": 122498,
    "scheduler_time": 218.67942386995145
}
#Debug simulation 
Total elapsed time: 111.56933545228094. Arrivals time: 0.5720970914699137 Scheduler time: 110.78895826684311 Scheduler overhead time: 0.08312346274033189 Adapter cache time: 0.015149961691349745 Engine time: 0.08001285046339035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 107.98592088697478,
    "estimated_duration": 3600.0809998314703,
    "input_throughput": 8429.80282983096,
    "output_throughput": 7515.861449024798,
    "total_throughput": 15945.664278855758,
    "itl": 111.92245544992312,
    "ttft": 1520633.2121434736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3353510667104274,
    "arrivals": 317887,
    "finished_requests": 122949,
    "scheduler_time": 217.8739260306017
}
#Debug simulation 
Total elapsed time: 107.9861021512188. Arrivals time: 0.5704188090749085 Scheduler time: 107.20997245144099 Scheduler overhead time: 0.08131234487518668 Adapter cache time: 0.014975972473621368 Engine time: 0.07968994090333581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.1944958739914,
    "estimated_duration": 3600.0817298318298,
    "input_throughput": 8429.80112049224,
    "output_throughput": 7515.859925008965,
    "total_throughput": 15945.661045501205,
    "itl": 111.92247434009742,
    "ttft": 1520633.555207311,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33607802355662036,
    "arrivals": 317887,
    "finished_requests": 122949,
    "scheduler_time": 217.87392907411473
}
#Debug simulation 
Total elapsed time: 106.1946722958237. Arrivals time: 0.5731627442874014 Scheduler time: 105.41680008452386 Scheduler overhead time: 0.0810511545278132 Adapter cache time: 0.015174676664173603 Engine time: 0.07802812149748206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 110.37354864226654,
    "estimated_duration": 3600.042712388202,
    "input_throughput": 8394.3531825356,
    "output_throughput": 7496.334948231,
    "total_throughput": 15890.6881307666,
    "itl": 111.84547413351653,
    "ttft": 1520839.391873256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30270133347716194,
    "arrivals": 317887,
    "finished_requests": 122498,
    "scheduler_time": 218.67947040394264
}
#Debug simulation 
Total elapsed time: 110.37372859707102. Arrivals time: 0.5784483845345676 Scheduler time: 109.58919514948502 Scheduler overhead time: 0.08204786339774728 Adapter cache time: 0.015007621143013239 Engine time: 0.0791419786401093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 106.09439360303804,
    "estimated_duration": 3600.0872288177356,
    "input_throughput": 8429.788244316023,
    "output_throughput": 7515.848444840522,
    "total_throughput": 15945.636689156545,
    "itl": 111.92255889639499,
    "ttft": 1520636.377462707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3403536522947253,
    "arrivals": 317887,
    "finished_requests": 122949,
    "scheduler_time": 217.87415204548157
}
#Debug simulation 
Total elapsed time: 106.09457850502804. Arrivals time: 0.5843481975607574 Scheduler time: 105.30487989075482 Scheduler overhead time: 0.08130561839789152 Adapter cache time: 0.014815612230449915 Engine time: 0.07903391076251864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 110.91498572193086,
    "estimated_duration": 3600.0283624069625,
    "input_throughput": 8394.38664305273,
    "output_throughput": 7496.364829180548,
    "total_throughput": 15890.75147223328,
    "itl": 111.84506028292607,
    "ttft": 1520832.692094686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29003487860551136,
    "arrivals": 317887,
    "finished_requests": 122498,
    "scheduler_time": 218.6789865238836
}
#Debug simulation 
Total elapsed time: 110.91516261873767. Arrivals time: 0.570040687918663 Scheduler time: 110.13846434326842 Scheduler overhead time: 0.08152417233213782 Adapter cache time: 0.015308408997952938 Engine time: 0.07991193747147918 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.63450065301731,
    "estimated_duration": 3600.0904799060118,
    "input_throughput": 8429.78063173354,
    "output_throughput": 7515.841657598117,
    "total_throughput": 15945.622289331655,
    "itl": 111.92260243384038,
    "ttft": 1520637.5929173264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3445035272464153,
    "arrivals": 317887,
    "finished_requests": 122949,
    "scheduler_time": 217.8741536060296
}
#Debug simulation 
Total elapsed time: 106.63468137104064. Arrivals time: 0.5636153789237142 Scheduler time: 105.86465923115611 Scheduler overhead time: 0.08177328063175082 Adapter cache time: 0.014921504072844982 Engine time: 0.0797440130263567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 105.25303048081696,
    "estimated_duration": 3600.0959784176725,
    "input_throughput": 8452.685479061844,
    "output_throughput": 7496.895405511534,
    "total_throughput": 15949.580884573377,
    "itl": 111.63639792753429,
    "ttft": 1514279.63512276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33665365183260254,
    "arrivals": 315978,
    "finished_requests": 123061,
    "scheduler_time": 218.55560267693778
}
#Debug simulation 
Total elapsed time: 105.2532047056593. Arrivals time: 0.530599501915276 Scheduler time: 104.5216153212823 Scheduler overhead time: 0.07969542220234871 Adapter cache time: 0.014792868867516518 Engine time: 0.0769252241589129 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.76121672382578,
    "estimated_duration": 3600.1209356573386,
    "input_throughput": 8452.62688222549,
    "output_throughput": 7496.8434345309115,
    "total_throughput": 15949.4703167564,
    "itl": 111.63670269977325,
    "ttft": 1514291.0328956686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3595501813222654,
    "arrivals": 315978,
    "finished_requests": 123061,
    "scheduler_time": 218.55606276981854
}
#Debug simulation 
Total elapsed time: 104.76150810485706. Arrivals time: 0.5230427901260555 Scheduler time: 104.03814771445468 Scheduler overhead time: 0.07975030457600951 Adapter cache time: 0.014809916727244854 Engine time: 0.07666092878207564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.02010060008615,
    "estimated_duration": 3600.121349132922,
    "input_throughput": 8452.625911437426,
    "output_throughput": 7496.842573515014,
    "total_throughput": 15949.468484952438,
    "itl": 111.6367013552975,
    "ttft": 1514291.1693866844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3600806354545063,
    "arrivals": 315978,
    "finished_requests": 123061,
    "scheduler_time": 218.55604582984753
}
#Debug simulation 
Total elapsed time: 105.02026934083551. Arrivals time: 0.5244887648150325 Scheduler time: 104.2950508641079 Scheduler overhead time: 0.07940609939396381 Adapter cache time: 0.01474398048594594 Engine time: 0.07704855874180794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 105.03975671296939,
    "estimated_duration": 3600.1041935321105,
    "input_throughput": 8452.66619079273,
    "output_throughput": 7496.878298269528,
    "total_throughput": 15949.544489062258,
    "itl": 111.63635144846218,
    "ttft": 1514283.1515497516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3452493451768534,
    "arrivals": 315978,
    "finished_requests": 123061,
    "scheduler_time": 218.55582232950104
}
#Debug simulation 
Total elapsed time: 105.03992591006681. Arrivals time: 0.535114677157253 Scheduler time: 104.30246940953657 Scheduler overhead time: 0.07969465479254723 Adapter cache time: 0.014986769296228886 Engine time: 0.07791372248902917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 105.19777021696791,
    "estimated_duration": 3600.1257812709446,
    "input_throughput": 8452.61550535526,
    "output_throughput": 7496.833344103867,
    "total_throughput": 15949.448849459124,
    "itl": 111.63680261941023,
    "ttft": 1514293.1527495834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3644820179790261,
    "arrivals": 315978,
    "finished_requests": 123061,
    "scheduler_time": 218.55607658534763
}
#Debug simulation 
Total elapsed time: 105.19794620387256. Arrivals time: 0.5426625614054501 Scheduler time: 104.45213693147525 Scheduler overhead time: 0.08078675111755729 Adapter cache time: 0.014759553596377373 Engine time: 0.07761207688599825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 108.76198285166174,
    "estimated_duration": 3600.0224766168812,
    "input_throughput": 8452.089173786055,
    "output_throughput": 7493.787656946068,
    "total_throughput": 15945.876830732122,
    "itl": 111.28319942468808,
    "ttft": 1514949.2075203364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 113,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33787568332394624,
    "arrivals": 315978,
    "finished_requests": 123040,
    "scheduler_time": 218.74885110021123
}
#Debug simulation 
Total elapsed time: 108.76215865975246. Arrivals time: 0.5513013293966651 Scheduler time: 108.00347166741267 Scheduler overhead time: 0.0838849195279181 Adapter cache time: 0.014916539192199707 Engine time: 0.07763284863904119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.02614481700584,
    "estimated_duration": 3600.001528808902,
    "input_throughput": 8452.435854955784,
    "output_throughput": 7496.692649719317,
    "total_throughput": 15949.1285046751,
    "itl": 111.63631139719338,
    "ttft": 1514292.3393085175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36951216943562,
    "arrivals": 315978,
    "finished_requests": 123057,
    "scheduler_time": 218.5482321255668
}
#Debug simulation 
Total elapsed time: 105.02632342139259. Arrivals time: 0.5385818765498698 Scheduler time: 104.28577624959871 Scheduler overhead time: 0.08001859625801444 Adapter cache time: 0.014815100003033876 Engine time: 0.07776997983455658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 110.57219186099246,
    "estimated_duration": 3600.0778407745734,
    "input_throughput": 8510.181266916228,
    "output_throughput": 7533.313222517682,
    "total_throughput": 16043.494489433911,
    "itl": 111.78330821518126,
    "ttft": 1509348.5502153914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3397141395765353,
    "arrivals": 314998,
    "finished_requests": 123685,
    "scheduler_time": 217.13951980971746
}
#Debug simulation 
Total elapsed time: 110.57237034104764. Arrivals time: 0.5286811026744545 Scheduler time: 109.84209257084876 Scheduler overhead time: 0.07997803296893835 Adapter cache time: 0.014967309311032295 Engine time: 0.07717393385246396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 110.61828326294199,
    "estimated_duration": 3600.1034414670744,
    "input_throughput": 8510.120750173506,
    "output_throughput": 7533.259652380474,
    "total_throughput": 16043.38040255398,
    "itl": 111.78375820223118,
    "ttft": 1509360.15456782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36254023161716753,
    "arrivals": 314998,
    "finished_requests": 123685,
    "scheduler_time": 217.1398934842408
}
#Debug simulation 
Total elapsed time: 110.61845796415582. Arrivals time: 0.531628199853003 Scheduler time: 109.88616863219067 Scheduler overhead time: 0.0789160798303783 Adapter cache time: 0.015085562132298946 Engine time: 0.07742047309875488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 110.26625650376081,
    "estimated_duration": 3600.1036396515256,
    "input_throughput": 8510.120281694324,
    "output_throughput": 7533.2592376771545,
    "total_throughput": 16043.379519371478,
    "itl": 111.7836725647416,
    "ttft": 1509360.237850904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36312415430322353,
    "arrivals": 314998,
    "finished_requests": 123685,
    "scheduler_time": 217.13990790032386
}
#Debug simulation 
Total elapsed time: 110.26643370697275. Arrivals time: 0.5197174390777946 Scheduler time: 109.54484792845324 Scheduler overhead time: 0.07960735773667693 Adapter cache time: 0.014545292127877474 Engine time: 0.07775502372533083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 110.58762274030596,
    "estimated_duration": 3600.0848908362004,
    "input_throughput": 8510.164601391885,
    "output_throughput": 7533.298469998204,
    "total_throughput": 16043.463071390088,
    "itl": 111.7834853379745,
    "ttft": 1509351.3833262282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3466050141979942,
    "arrivals": 314998,
    "finished_requests": 123685,
    "scheduler_time": 217.13957895812982
}
#Debug simulation 
Total elapsed time: 110.58779955795035. Arrivals time: 0.5377555224113166 Scheduler time: 109.84964866470546 Scheduler overhead time: 0.0790257565677166 Adapter cache time: 0.01467757998034358 Engine time: 0.077272386290133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 110.38439563196152,
    "estimated_duration": 3600.108284499859,
    "input_throughput": 8510.109301964025,
    "output_throughput": 7533.249518289888,
    "total_throughput": 16043.358820253914,
    "itl": 111.78376575319267,
    "ttft": 1509362.0894026482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3680285519734027,
    "arrivals": 314998,
    "finished_requests": 123685,
    "scheduler_time": 217.13994846673066
}
#Debug simulation 
Total elapsed time: 110.38456529797986. Arrivals time: 0.5378070878796279 Scheduler time: 109.6428520870395 Scheduler overhead time: 0.07992204464972019 Adapter cache time: 0.015095651615411043 Engine time: 0.07916320860385895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 109.6064811157994,
    "estimated_duration": 3600.0678169607595,
    "input_throughput": 8510.204962156675,
    "output_throughput": 7533.334197825088,
    "total_throughput": 16043.539159981763,
    "itl": 111.78284980565996,
    "ttft": 1509343.9834219709,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3318955827341419,
    "arrivals": 314998,
    "finished_requests": 123685,
    "scheduler_time": 217.13907509514425
}
#Debug simulation 
Total elapsed time: 109.60665183607489. Arrivals time: 0.5393976243212819 Scheduler time: 108.86643040319905 Scheduler overhead time: 0.07931426726281643 Adapter cache time: 0.014899561647325754 Engine time: 0.07734961062669754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 110.73491277778521,
    "estimated_duration": 3600.11359724571,
    "input_throughput": 8510.096743458116,
    "output_throughput": 7533.2384013517585,
    "total_throughput": 16043.335144809875,
    "itl": 111.78386946112518,
    "ttft": 1509364.6146761132,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3725556882843372,
    "arrivals": 314998,
    "finished_requests": 123685,
    "scheduler_time": 217.14003380621006
}
#Debug simulation 
Total elapsed time: 110.73508719680831. Arrivals time: 0.543357796035707 Scheduler time: 109.9894854715094 Scheduler overhead time: 0.07968245632946491 Adapter cache time: 0.014916375279426575 Engine time: 0.07828462868928909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 105.13411393109709,
    "estimated_duration": 3600.0266380279477,
    "input_throughput": 8537.064608176215,
    "output_throughput": 7537.8103354437835,
    "total_throughput": 16074.87494362,
    "itl": 111.64705487140185,
    "ttft": 1507476.7547103243,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39174243122339203,
    "arrivals": 314533,
    "finished_requests": 123802,
    "scheduler_time": 216.65037005525568
}
#Debug simulation 
Total elapsed time: 105.1344116278924. Arrivals time: 0.5541693405248225 Scheduler time: 104.38056839350611 Scheduler overhead time: 0.07870187377557158 Adapter cache time: 0.014763275627046824 Engine time: 0.07702310709282756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.43790707364678,
    "estimated_duration": 3600.0541038314836,
    "input_throughput": 8536.999476560817,
    "output_throughput": 7537.752827414239,
    "total_throughput": 16074.752303975058,
    "itl": 111.64753800772685,
    "ttft": 1507487.63051444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41827423045178874,
    "arrivals": 314533,
    "finished_requests": 123802,
    "scheduler_time": 216.6511171168687
}
#Debug simulation 
Total elapsed time: 104.43808413064107. Arrivals time: 0.5284798112697899 Scheduler time: 103.71150888688862 Scheduler overhead time: 0.07834791671484709 Adapter cache time: 0.014576418325304985 Engine time: 0.07629363983869553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.67671389412135,
    "estimated_duration": 3600.0548769462566,
    "input_throughput": 8536.997643233095,
    "output_throughput": 7537.751208675562,
    "total_throughput": 16074.748851908656,
    "itl": 111.64751152040834,
    "ttft": 1507488.054051539,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4189109446667145,
    "arrivals": 314533,
    "finished_requests": 123802,
    "scheduler_time": 216.65115347884017
}
#Debug simulation 
Total elapsed time: 104.67689798306674. Arrivals time: 0.5221906383521855 Scheduler time: 103.95478027965873 Scheduler overhead time: 0.07881646929308772 Adapter cache time: 0.014550138264894485 Engine time: 0.07742001907899976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 104.69161287602037,
    "estimated_duration": 3600.036002563475,
    "input_throughput": 8537.042401274739,
    "output_throughput": 7537.79072783635,
    "total_throughput": 16074.833129111088,
    "itl": 111.64714028420666,
    "ttft": 1507480.393712619,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4015218223957347,
    "arrivals": 314533,
    "finished_requests": 123802,
    "scheduler_time": 216.65045539250443
}
#Debug simulation 
Total elapsed time: 104.69178318791091. Arrivals time: 0.523014273494482 Scheduler time: 103.97167127160355 Scheduler overhead time: 0.07802669936791062 Adapter cache time: 0.014571219682693481 Engine time: 0.07569976570084691 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 104.65905137592927,
    "estimated_duration": 3600.061602868914,
    "input_throughput": 8536.981693732167,
    "output_throughput": 7537.7371260466425,
    "total_throughput": 16074.71881977881,
    "itl": 111.64760606678698,
    "ttft": 1507491.1674105441,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4240668499097232,
    "arrivals": 314533,
    "finished_requests": 123802,
    "scheduler_time": 216.6513229561334
}
#Debug simulation 
Total elapsed time: 104.65922116814181. Arrivals time: 0.536216355394572 Scheduler time: 103.92040612269193 Scheduler overhead time: 0.07931528333574533 Adapter cache time: 0.014929544180631638 Engine time: 0.07895223796367645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 104.85334675200284,
    "estimated_duration": 3600.043012426387,
    "input_throughput": 8537.025778279763,
    "output_throughput": 7537.77605054514,
    "total_throughput": 16074.801828824902,
    "itl": 111.64605424330621,
    "ttft": 1507490.4480534208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38272643774747894,
    "arrivals": 314533,
    "finished_requests": 123802,
    "scheduler_time": 216.6535605651352
}
#Debug simulation 
Total elapsed time: 104.85352000501007. Arrivals time: 0.5334899956360459 Scheduler time: 104.1216327524744 Scheduler overhead time: 0.07873923797160387 Adapter cache time: 0.014351403806358576 Engine time: 0.07595646008849144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.84630327997729,
    "estimated_duration": 3600.0685670657713,
    "input_throughput": 8536.965179263074,
    "output_throughput": 7537.722544578478,
    "total_throughput": 16074.687723841553,
    "itl": 111.64768538838445,
    "ttft": 1507494.4362332432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42985152408480615,
    "arrivals": 314533,
    "finished_requests": 123802,
    "scheduler_time": 216.6515168607869
}
#Debug simulation 
Total elapsed time: 104.84648345923051. Arrivals time: 0.5432830858044326 Scheduler time: 104.10517198592424 Scheduler overhead time: 0.07809170847758651 Adapter cache time: 0.014381482265889645 Engine time: 0.07658353261649609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 119.68723483290523,
    "estimated_duration": 3600.0081840340918,
    "input_throughput": 8472.564627845068,
    "output_throughput": 7495.81101500753,
    "total_throughput": 15968.375642852598,
    "itl": 111.53287740637953,
    "ttft": 1509827.9820548287,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2785043846978803,
    "arrivals": 314275,
    "finished_requests": 122849,
    "scheduler_time": 219.26950184083637
}
#Debug simulation 
Total elapsed time: 119.68741476116702. Arrivals time: 0.5575766763649881 Scheduler time: 118.91801973152906 Scheduler overhead time: 0.0843685120344162 Adapter cache time: 0.015402459539473057 Engine time: 0.08149185823276639 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 114.06518821790814,
    "estimated_duration": 3600.066035050352,
    "input_throughput": 8521.984513978756,
    "output_throughput": 7532.279334876349,
    "total_throughput": 16054.263848855104,
    "itl": 112.0308677219666,
    "ttft": 1500445.4921653436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.403862457361538,
    "arrivals": 314275,
    "finished_requests": 123458,
    "scheduler_time": 216.9724658500295
}
#Debug simulation 
Total elapsed time: 114.06536629097536. Arrivals time: 0.5689296931959689 Scheduler time: 113.29247483564541 Scheduler overhead time: 0.08088353695347905 Adapter cache time: 0.014744479674845934 Engine time: 0.0788285112939775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 114.06548782717437,
    "estimated_duration": 3600.066895588392,
    "input_throughput": 8521.982476935538,
    "output_throughput": 7532.277534406223,
    "total_throughput": 16054.260011341761,
    "itl": 112.03088882850712,
    "ttft": 1500445.8573484616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4047133843041966,
    "arrivals": 314275,
    "finished_requests": 123458,
    "scheduler_time": 216.97247546112328
}
#Debug simulation 
Total elapsed time: 114.06567460112274. Arrivals time: 0.5739000188186765 Scheduler time: 113.286466345191 Scheduler overhead time: 0.08127122279256582 Adapter cache time: 0.015521662309765816 Engine time: 0.07879631221294403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 114.20760007062927,
    "estimated_duration": 3600.049631534033,
    "input_throughput": 8522.023344141213,
    "output_throughput": 7532.313655477351,
    "total_throughput": 16054.336999618563,
    "itl": 112.03049249941826,
    "ttft": 1500438.4677314055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38833583526080495,
    "arrivals": 314275,
    "finished_requests": 123458,
    "scheduler_time": 216.97217931943544
}
#Debug simulation 
Total elapsed time: 114.20777721470222. Arrivals time: 0.5399034763686359 Scheduler time: 113.46169632440433 Scheduler overhead time: 0.08102560555562377 Adapter cache time: 0.015151530038565397 Engine time: 0.0804016375914216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 114.17041468806565,
    "estimated_duration": 3600.072445665174,
    "input_throughput": 8521.969338961846,
    "output_throughput": 7532.265922218055,
    "total_throughput": 16054.2352611799,
    "itl": 112.03097193533576,
    "ttft": 1500447.8353714694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.40949202818796077,
    "arrivals": 314275,
    "finished_requests": 123458,
    "scheduler_time": 216.97254662396045
}
#Debug simulation 
Total elapsed time: 114.17059621587396. Arrivals time: 0.561976398807019 Scheduler time: 113.40025160321966 Scheduler overhead time: 0.08275056350976229 Adapter cache time: 0.01546806050464511 Engine time: 0.07962175225839019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 114.80559761403129,
    "estimated_duration": 3600.0254086794607,
    "input_throughput": 8486.066494515711,
    "output_throughput": 7507.704233097126,
    "total_throughput": 15993.770727612837,
    "itl": 111.56768916166558,
    "ttft": 1511224.422906638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2960149791953157,
    "arrivals": 314275,
    "finished_requests": 123050,
    "scheduler_time": 218.52074438046864
}
#Debug simulation 
Total elapsed time: 114.80576613824815. Arrivals time: 0.5329447914846241 Scheduler time: 114.0660808305256 Scheduler overhead time: 0.08186394395306706 Adapter cache time: 0.015472616534680128 Engine time: 0.07925578532740474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 114.17028021533042,
    "estimated_duration": 3600.0758388878585,
    "input_throughput": 8521.961306647814,
    "output_throughput": 7532.2588227410615,
    "total_throughput": 16054.220129388876,
    "itl": 112.03139800991916,
    "ttft": 1500449.7188793977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4148994410037992,
    "arrivals": 314275,
    "finished_requests": 123458,
    "scheduler_time": 216.97333841399964
}
#Debug simulation 
Total elapsed time: 114.17057025013492. Arrivals time: 0.5694687934592366 Scheduler time: 113.39411042956635 Scheduler overhead time: 0.082360097207129 Adapter cache time: 0.015511826146394014 Engine time: 0.07910191500559449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 113.64704746799543,
    "estimated_duration": 3600.043494835618,
    "input_throughput": 8453.52092097143,
    "output_throughput": 7526.914338360595,
    "total_throughput": 15980.435259332025,
    "itl": 111.80490770111967,
    "ttft": 1449485.5246803241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26320194597821656,
    "arrivals": 291201,
    "finished_requests": 123126,
    "scheduler_time": 216.39125860303096
}
#Debug simulation 
Total elapsed time: 113.64722821721807. Arrivals time: 0.5323994043283165 Scheduler time: 112.91005127737299 Scheduler overhead time: 0.08112759748473763 Adapter cache time: 0.014718442689627409 Engine time: 0.07925566239282489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 125.55915267113596,
    "estimated_duration": 3600.0150410337137,
    "input_throughput": 8455.033563209758,
    "output_throughput": 7535.70268201164,
    "total_throughput": 15990.736245221397,
    "itl": 111.91907246703542,
    "ttft": 1440430.5524451067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 82,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2668396760593168,
    "arrivals": 291201,
    "finished_requests": 123272,
    "scheduler_time": 215.9786852683601
}
#Debug simulation 
Total elapsed time: 125.55934440018609. Arrivals time: 0.5469933138228953 Scheduler time: 124.79845394613221 Scheduler overhead time: 0.0851811901666224 Adapter cache time: 0.015493963845074177 Engine time: 0.08281512465327978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 125.74906595982611,
    "estimated_duration": 3600.0156529908213,
    "input_throughput": 8455.03212596104,
    "output_throughput": 7535.701401037538,
    "total_throughput": 15990.733526998576,
    "itl": 111.91908397416114,
    "ttft": 1440430.834749414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 82,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26744266280904405,
    "arrivals": 291201,
    "finished_requests": 123272,
    "scheduler_time": 215.9786942387143
}
#Debug simulation 
Total elapsed time: 125.74924619682133. Arrivals time: 0.5474460604600608 Scheduler time: 124.98928793240339 Scheduler overhead time: 0.08433783845975995 Adapter cache time: 0.015166736673563719 Engine time: 0.08289395039901137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 125.75752427708358,
    "estimated_duration": 3600.006750458419,
    "input_throughput": 8455.053034587794,
    "output_throughput": 7535.720036232011,
    "total_throughput": 15990.773070819805,
    "itl": 111.91900038514356,
    "ttft": 1440426.9741870028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 82,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25744198373518895,
    "arrivals": 291201,
    "finished_requests": 123272,
    "scheduler_time": 215.97859192242424
}
#Debug simulation 
Total elapsed time: 125.75770940212533. Arrivals time: 0.5511437803506851 Scheduler time: 124.99237704044208 Scheduler overhead time: 0.08512267796322703 Adapter cache time: 0.015545039903372526 Engine time: 0.08279872313141823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 125.32900454988703,
    "estimated_duration": 3600.018435402257,
    "input_throughput": 8455.02559116726,
    "output_throughput": 7535.6955767835425,
    "total_throughput": 15990.721167950802,
    "itl": 111.91903517036062,
    "ttft": 1440432.2110944851,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 82,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27033499989658577,
    "arrivals": 291201,
    "finished_requests": 123272,
    "scheduler_time": 215.978709735906
}
#Debug simulation 
Total elapsed time: 125.32919186493382. Arrivals time: 0.5528294760733843 Scheduler time: 124.56178178917617 Scheduler overhead time: 0.08548338990658522 Adapter cache time: 0.015826471149921417 Engine time: 0.08258054638281465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 112.91632506530732,
    "estimated_duration": 3600.041666569233,
    "input_throughput": 8453.525214057336,
    "output_throughput": 7526.91816087315,
    "total_throughput": 15980.443374930486,
    "itl": 111.80494436619506,
    "ttft": 1449485.763124697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 86,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2571443253615874,
    "arrivals": 291201,
    "finished_requests": 123126,
    "scheduler_time": 216.3912855202273
}
#Debug simulation 
Total elapsed time: 112.91651073424146. Arrivals time: 0.5417814846150577 Scheduler time: 112.16924322443083 Scheduler overhead time: 0.08131767576560378 Adapter cache time: 0.014844646211713552 Engine time: 0.07883494114503264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 125.96071551088244,
    "estimated_duration": 3600.021968843392,
    "input_throughput": 8455.017292513672,
    "output_throughput": 7535.688180457364,
    "total_throughput": 15990.705472971038,
    "itl": 111.91895896554021,
    "ttft": 1440434.2362501142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 82,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27410761348903145,
    "arrivals": 291201,
    "finished_requests": 123272,
    "scheduler_time": 215.9789707563582
}
#Debug simulation 
Total elapsed time: 125.96090281289071. Arrivals time: 0.5551788578741252 Scheduler time: 125.19346274249256 Scheduler overhead time: 0.08378987247124314 Adapter cache time: 0.015657468233257532 Engine time: 0.08215472847223282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 115.02326494408771,
    "estimated_duration": 3600.1189014505503,
    "input_throughput": 8497.761556617914,
    "output_throughput": 7532.0426192241575,
    "total_throughput": 16029.804175842071,
    "itl": 111.73930820834907,
    "ttft": 1435277.0218707176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25708097049035106,
    "arrivals": 287382,
    "finished_requests": 123599,
    "scheduler_time": 215.8571742517507
}
#Debug simulation 
Total elapsed time: 115.02345212129876. Arrivals time: 0.544232037384063 Scheduler time: 114.27398000145331 Scheduler overhead time: 0.08136096876114607 Adapter cache time: 0.014775939285755157 Engine time: 0.07937513245269656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 123.28968382207677,
    "estimated_duration": 3600.1097522233995,
    "input_throughput": 8488.987865196495,
    "output_throughput": 7527.867722161121,
    "total_throughput": 16016.855587357617,
    "itl": 111.13004256214835,
    "ttft": 1445989.38882106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31319492869079113,
    "arrivals": 287382,
    "finished_requests": 123535,
    "scheduler_time": 216.9642795498883
}
#Debug simulation 
Total elapsed time: 123.28985769115388. Arrivals time: 0.5698553863912821 Scheduler time: 122.51033427147195 Scheduler overhead time: 0.08329330151900649 Adapter cache time: 0.015468663070350885 Engine time: 0.08092595776543021 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 123.32815721957013,
    "estimated_duration": 3600.110440487388,
    "input_throughput": 8488.986242283881,
    "output_throughput": 7527.866282994087,
    "total_throughput": 16016.852525277967,
    "itl": 111.13004903797712,
    "ttft": 1445989.730792403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.31376164913177523,
    "arrivals": 287382,
    "finished_requests": 123535,
    "scheduler_time": 216.96430105485
}
#Debug simulation 
Total elapsed time: 123.32833045860752. Arrivals time: 0.5460136528126895 Scheduler time: 122.57239149697125 Scheduler overhead time: 0.08391880011186004 Adapter cache time: 0.014665692579001188 Engine time: 0.08011601958423853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 123.43409078195691,
    "estimated_duration": 3600.0968527647187,
    "input_throughput": 8489.0182819749,
    "output_throughput": 7527.89469516285,
    "total_throughput": 16016.912977137752,
    "itl": 111.12984536211951,
    "ttft": 1445984.1592114267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3005284738191405,
    "arrivals": 287382,
    "finished_requests": 123535,
    "scheduler_time": 216.96412365035607
}
#Debug simulation 
Total elapsed time: 123.43426015693694. Arrivals time: 0.5570665369741619 Scheduler time: 122.66474326374009 Scheduler overhead time: 0.08584887813776731 Adapter cache time: 0.015445591881871223 Engine time: 0.08038992341607809 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 114.29216037597507,
    "estimated_duration": 3600.0796473868368,
    "input_throughput": 8494.354568571041,
    "output_throughput": 7531.485593570409,
    "total_throughput": 16025.84016214145,
    "itl": 111.76018695390023,
    "ttft": 1435072.3616975355,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2779367952235042,
    "arrivals": 287382,
    "finished_requests": 123564,
    "scheduler_time": 215.84982246073992
}
#Debug simulation 
Total elapsed time: 114.29234160576016. Arrivals time: 0.5414317008107901 Scheduler time: 113.54381837137043 Scheduler overhead time: 0.08214460266754031 Adapter cache time: 0.015516775194555521 Engine time: 0.07922638300806284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 114.6743968911469,
    "estimated_duration": 3600.1112213810525,
    "input_throughput": 8497.779684779884,
    "output_throughput": 7532.058687230733,
    "total_throughput": 16029.838372010616,
    "itl": 111.73923074821171,
    "ttft": 1435272.8785212208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.251164224771783,
    "arrivals": 287382,
    "finished_requests": 123599,
    "scheduler_time": 215.8569115066622
}
#Debug simulation 
Total elapsed time: 114.67468407098204. Arrivals time: 0.5359152061864734 Scheduler time: 113.93164251977578 Scheduler overhead time: 0.08227082761004567 Adapter cache time: 0.015209267381578684 Engine time: 0.07923617819324136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 114.68763225898147,
    "estimated_duration": 3600.083553943147,
    "input_throughput": 8494.345351097629,
    "output_throughput": 7531.477420934378,
    "total_throughput": 16025.822772032006,
    "itl": 111.76056008685582,
    "ttft": 1435073.6014512333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 84,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2815836550295349,
    "arrivals": 287382,
    "finished_requests": 123564,
    "scheduler_time": 215.8501821958316
}
#Debug simulation 
Total elapsed time: 114.68781919265166. Arrivals time: 0.5259731677360833 Scheduler time: 113.95458990940824 Scheduler overhead time: 0.08283910993486643 Adapter cache time: 0.015050199814140797 Engine time: 0.07947398209944367 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 128.65089654410258,
    "estimated_duration": 3600.001737377407,
    "input_throughput": 8453.965920075168,
    "output_throughput": 7558.544129987833,
    "total_throughput": 16012.510050063001,
    "itl": 111.8108775655178,
    "ttft": 1407817.2446114977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28156487244181305,
    "arrivals": 285504,
    "finished_requests": 123224,
    "scheduler_time": 216.10651759068708
}
#Debug simulation 
Total elapsed time: 128.65106303384528. Arrivals time: 0.5333577557466924 Scheduler time: 127.9063722807914 Scheduler overhead time: 0.08374455943703651 Adapter cache time: 0.015573023352771997 Engine time: 0.08175659133121371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 111.0070400070399,
    "estimated_duration": 3600.078159694896,
    "input_throughput": 8401.51231676685,
    "output_throughput": 7488.408530076121,
    "total_throughput": 15889.92084684297,
    "itl": 110.84359107132592,
    "ttft": 1436321.6291299483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29742748657939955,
    "arrivals": 285504,
    "finished_requests": 122552,
    "scheduler_time": 218.22157702072417
}
#Debug simulation 
Total elapsed time: 111.00722508411855. Arrivals time: 0.5083732348866761 Scheduler time: 110.29710941528901 Scheduler overhead time: 0.07994548277929425 Adapter cache time: 0.014681507367640734 Engine time: 0.07676056865602732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 111.41643817583099,
    "estimated_duration": 3600.0778809391427,
    "input_throughput": 8401.512967299968,
    "output_throughput": 7488.409109907177,
    "total_throughput": 15889.922077207146,
    "itl": 110.84393185528697,
    "ttft": 1436320.921608959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29786955989897274,
    "arrivals": 285504,
    "finished_requests": 122552,
    "scheduler_time": 218.2219656672379
}
#Debug simulation 
Total elapsed time: 111.41662412602454. Arrivals time: 0.5129481945186853 Scheduler time: 110.70027403673157 Scheduler overhead time: 0.07961055310443044 Adapter cache time: 0.015169165562838316 Engine time: 0.07864088332280517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 110.83583742100745,
    "estimated_duration": 3600.0659535023665,
    "input_throughput": 8401.540802488556,
    "output_throughput": 7488.433919876596,
    "total_throughput": 15889.974722365152,
    "itl": 110.8431610657211,
    "ttft": 1436317.0039864013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28598681766306994,
    "arrivals": 285504,
    "finished_requests": 122552,
    "scheduler_time": 218.2208114971045
}
#Debug simulation 
Total elapsed time: 110.83601918071508. Arrivals time: 0.5120898238383234 Scheduler time: 110.11997006088495 Scheduler overhead time: 0.0816217246465385 Adapter cache time: 0.014711041003465652 Engine time: 0.07786397822201252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 110.76452305819839,
    "estimated_duration": 3600.081550499772,
    "input_throughput": 8401.504403643623,
    "output_throughput": 7488.40147697696,
    "total_throughput": 15889.905880620583,
    "itl": 110.84391226048916,
    "ttft": 1436322.6567393334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30139066591858865,
    "arrivals": 285504,
    "finished_requests": 122552,
    "scheduler_time": 218.22201408326225
}
#Debug simulation 
Total elapsed time: 110.7647055038251. Arrivals time: 0.5241105887107551 Scheduler time: 110.034986872226 Scheduler overhead time: 0.08165497612208128 Adapter cache time: 0.015007440000772476 Engine time: 0.07846026681363583 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 120.260644051712,
    "estimated_duration": 3600.053034965536,
    "input_throughput": 8436.376826957148,
    "output_throughput": 7524.958865018315,
    "total_throughput": 15961.335691975464,
    "itl": 111.30606468756324,
    "ttft": 1433998.5334217998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3109652306698266,
    "arrivals": 285504,
    "finished_requests": 123087,
    "scheduler_time": 216.3288395156894
}
#Debug simulation 
Total elapsed time: 120.26080723293126. Arrivals time: 0.5336915808729827 Scheduler time: 119.52164387330413 Scheduler overhead time: 0.08069833181798458 Adapter cache time: 0.014868903439491987 Engine time: 0.07985277194529772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 111.3573059826158,
    "estimated_duration": 3600.0869092880052,
    "input_throughput": 8401.491897866938,
    "output_throughput": 7488.390330368912,
    "total_throughput": 15889.882228235849,
    "itl": 110.84400409125276,
    "ttft": 1436325.3915611058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3056662946566936,
    "arrivals": 285504,
    "finished_requests": 122552,
    "scheduler_time": 218.22219689552998
}
#Debug simulation 
Total elapsed time: 111.35748992860317. Arrivals time: 0.5305732460692525 Scheduler time: 110.62230813363567 Scheduler overhead time: 0.08117901999503374 Adapter cache time: 0.014431627001613379 Engine time: 0.07840397907420993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 134.0617820499465,
    "estimated_duration": 3600.0699632239425,
    "input_throughput": 8467.224334913246,
    "output_throughput": 7506.031903835831,
    "total_throughput": 15973.256238749076,
    "itl": 110.61788226624446,
    "ttft": 1405238.8858266457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2662624337221493,
    "arrivals": 284495,
    "finished_requests": 123011,
    "scheduler_time": 217.79845627596518
}
#Debug simulation 
Total elapsed time: 134.06194661185145. Arrivals time: 0.5547849480062723 Scheduler time: 133.29317192547023 Scheduler overhead time: 0.08496617525815964 Adapter cache time: 0.015899760648608208 Engine time: 0.08203994762152433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 112.1824859152548,
    "estimated_duration": 3600.024869992967,
    "input_throughput": 8418.630174646323,
    "output_throughput": 7449.988810786297,
    "total_throughput": 15868.61898543262,
    "itl": 110.3470024604912,
    "ttft": 1433987.1335880202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2906301953527145,
    "arrivals": 284495,
    "finished_requests": 122419,
    "scheduler_time": 218.81036237936402
}
#Debug simulation 
Total elapsed time: 112.18266499321908. Arrivals time: 0.5350174596533179 Scheduler time: 111.44211950013414 Scheduler overhead time: 0.08103076601400971 Adapter cache time: 0.014710478018969297 Engine time: 0.07953997794538736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 112.44680670881644,
    "estimated_duration": 3600.0265330929683,
    "input_throughput": 8418.626285501696,
    "output_throughput": 7449.985369123775,
    "total_throughput": 15868.611654625469,
    "itl": 110.34701278680002,
    "ttft": 1433988.259101117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29110802721232193,
    "arrivals": 284495,
    "finished_requests": 122419,
    "scheduler_time": 218.8104480397708
}
#Debug simulation 
Total elapsed time: 112.44700126815587. Arrivals time: 0.5280235637910664 Scheduler time: 111.71278376085684 Scheduler overhead time: 0.08330507762730122 Adapter cache time: 0.01468979800119996 Engine time: 0.07826187694445252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 131.14997938321903,
    "estimated_duration": 3600.113520576726,
    "input_throughput": 8412.584999583081,
    "output_throughput": 7444.586079526324,
    "total_throughput": 15857.171079109405,
    "itl": 110.03430650128173,
    "ttft": 1423689.9886397296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28516962702618925,
    "arrivals": 284495,
    "finished_requests": 122227,
    "scheduler_time": 219.359806663926
}
#Debug simulation 
Total elapsed time: 131.15015395032242. Arrivals time: 0.5459127849899232 Scheduler time: 130.39095782162622 Scheduler overhead time: 0.08508191397413611 Adapter cache time: 0.015929128043353558 Engine time: 0.0806985767558217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 113.3099833149463,
    "estimated_duration": 3600.0297944894646,
    "input_throughput": 8418.618658765296,
    "output_throughput": 7449.978619914027,
    "total_throughput": 15868.597278679323,
    "itl": 110.34703793310312,
    "ttft": 1433989.9356675043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2946291332319379,
    "arrivals": 284495,
    "finished_requests": 122419,
    "scheduler_time": 218.81048844599266
}
#Debug simulation 
Total elapsed time: 113.31029000272974. Arrivals time: 0.5197505755349994 Scheduler time: 112.58658477012068 Scheduler overhead time: 0.08084971224889159 Adapter cache time: 0.014628486707806587 Engine time: 0.07811219710856676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 129.37214111769572,
    "estimated_duration": 3600.055946863694,
    "input_throughput": 8467.39146555662,
    "output_throughput": 7486.7469833297255,
    "total_throughput": 15954.138448886346,
    "itl": 110.65527932269235,
    "ttft": 1417130.0783796133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 87,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.26013437565648956,
    "arrivals": 284495,
    "finished_requests": 123087,
    "scheduler_time": 217.33779121244467
}
#Debug simulation 
Total elapsed time: 129.37232868978754. Arrivals time: 0.533017375972122 Scheduler time: 128.62662535952404 Scheduler overhead time: 0.08478087838739157 Adapter cache time: 0.01532335625961423 Engine time: 0.08203349262475967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 112.02964558498934,
    "estimated_duration": 3600.0348062210574,
    "input_throughput": 8418.606938918303,
    "output_throughput": 7449.968248543964,
    "total_throughput": 15868.575187462267,
    "itl": 110.34701848319722,
    "ttft": 1433992.5156729685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 89,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29865325439721313,
    "arrivals": 284495,
    "finished_requests": 122419,
    "scheduler_time": 218.81071426651084
}
#Debug simulation 
Total elapsed time: 112.02983368886635. Arrivals time: 0.5207691043615341 Scheduler time: 111.30367320496589 Scheduler overhead time: 0.08213405637070537 Adapter cache time: 0.01484586950391531 Engine time: 0.07800452830269933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 120.90694834804162,
    "estimated_duration": 3600.030662761443,
    "input_throughput": 8547.11025611033,
    "output_throughput": 7560.391715975666,
    "total_throughput": 16107.501972085996,
    "itl": 112.1591160839037,
    "ttft": 1415626.867008768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.28156487244181305,
    "arrivals": 284045,
    "finished_requests": 124159,
    "scheduler_time": 214.5319721855524
}
#Debug simulation 
Total elapsed time: 120.90713179018348. Arrivals time: 0.5328584774397314 Scheduler time: 120.16566934250295 Scheduler overhead time: 0.08231218252331018 Adapter cache time: 0.015631153248250484 Engine time: 0.0801586452871561 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 116.37094722595066,
    "estimated_duration": 3600.056282053719,
    "input_throughput": 8539.010668595234,
    "output_throughput": 7557.17101858188,
    "total_throughput": 16096.181687177115,
    "itl": 112.1964334045395,
    "ttft": 1420173.5680513787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2864143591024913,
    "arrivals": 284045,
    "finished_requests": 124063,
    "scheduler_time": 214.64400782471049
}
#Debug simulation 
Total elapsed time: 116.37113105598837. Arrivals time: 0.546438732650131 Scheduler time: 115.6161249880679 Scheduler overhead time: 0.0831159888766706 Adapter cache time: 0.01545991888269782 Engine time: 0.07962240651249886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 117.17119712475687,
    "estimated_duration": 3600.057792533096,
    "input_throughput": 8539.007085875106,
    "output_throughput": 7557.167847813068,
    "total_throughput": 16096.174933688175,
    "itl": 112.19645655551899,
    "ttft": 1420174.3826610148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2870527658797802,
    "arrivals": 284045,
    "finished_requests": 124063,
    "scheduler_time": 214.64407958865928
}
#Debug simulation 
Total elapsed time: 117.17137727793306. Arrivals time: 0.5502414195798337 Scheduler time: 116.41291609779 Scheduler overhead time: 0.08316146908327937 Adapter cache time: 0.01522554224357009 Engine time: 0.07965681422501802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 120.2794087510556,
    "estimated_duration": 3600.086130492933,
    "input_throughput": 8540.169286391565,
    "output_throughput": 7561.212430290613,
    "total_throughput": 16101.381716682177,
    "itl": 112.21576783793907,
    "ttft": 1415998.0385586189,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2877510820026511,
    "arrivals": 284045,
    "finished_requests": 124073,
    "scheduler_time": 214.50252787226358
}
#Debug simulation 
Total elapsed time: 120.27960540028289. Arrivals time: 0.5476404069922864 Scheduler time: 119.52454310609028 Scheduler overhead time: 0.08286956278607249 Adapter cache time: 0.01575509551912546 Engine time: 0.07910099066793919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 116.49883172987029,
    "estimated_duration": 3600.0612588979247,
    "input_throughput": 8538.998863983392,
    "output_throughput": 7557.160571297767,
    "total_throughput": 16096.159435281159,
    "itl": 112.19650056660143,
    "ttft": 1420175.8968159014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2906996256858109,
    "arrivals": 284045,
    "finished_requests": 124063,
    "scheduler_time": 214.6440991708446
}
#Debug simulation 
Total elapsed time: 116.49901486001909. Arrivals time: 0.5457549323327839 Scheduler time: 115.74315848015249 Scheduler overhead time: 0.08394393278285861 Adapter cache time: 0.015316241420805454 Engine time: 0.08099482860416174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 120.6796045890078,
    "estimated_duration": 3600.024193892668,
    "input_throughput": 8547.125614377852,
    "output_throughput": 7560.4053012126715,
    "total_throughput": 16107.530915590523,
    "itl": 112.15902233965396,
    "ttft": 1415623.939501922,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.27508462713100046,
    "arrivals": 284045,
    "finished_requests": 124159,
    "scheduler_time": 214.53188352349105
}
#Debug simulation 
Total elapsed time: 120.67978900903836. Arrivals time: 0.5207276050932705 Scheduler time: 119.95267366943881 Scheduler overhead time: 0.08195732161402702 Adapter cache time: 0.015551932621747255 Engine time: 0.07933467719703913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 116.84774562204257,
    "estimated_duration": 3600.0612587331602,
    "input_throughput": 8538.998864374198,
    "output_throughput": 7557.160571643637,
    "total_throughput": 16096.159436017835,
    "itl": 112.19645997695176,
    "ttft": 1420175.2454035778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 88,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2942207317054268,
    "arrivals": 284045,
    "finished_requests": 124063,
    "scheduler_time": 214.64398002844868
}
#Debug simulation 
Total elapsed time: 116.8479304201901. Arrivals time: 0.5286653130315244 Scheduler time: 116.11275745835155 Scheduler overhead time: 0.08202101336792111 Adapter cache time: 0.015245660208165646 Engine time: 0.0793128446675837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 113.01586011517793,
    "estimated_duration": 3600.065961017162,
    "input_throughput": 8499.947037457663,
    "output_throughput": 7539.070754229054,
    "total_throughput": 16039.017791686718,
    "itl": 111.94239745013074,
    "ttft": 1420743.5173073367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 80,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.24483901951462006,
    "arrivals": 283764,
    "finished_requests": 123583,
    "scheduler_time": 215.029606189082
}
#Debug simulation 
Total elapsed time: 113.01602975139394. Arrivals time: 0.5146910091862082 Scheduler time: 112.30216245166957 Scheduler overhead time: 0.07881773868575692 Adapter cache time: 0.014529712498188019 Engine time: 0.07642833655700088 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.9093726738356,
    "estimated_duration": 3600.1062880780464,
    "input_throughput": 8492.85619739935,
    "output_throughput": 7542.185098789328,
    "total_throughput": 16035.041296188676,
    "itl": 112.00305507743008,
    "ttft": 1430379.1258506924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25188942458480595,
    "arrivals": 283764,
    "finished_requests": 123406,
    "scheduler_time": 215.28013027338847
}
#Debug simulation 
Total elapsed time: 102.90954994596541. Arrivals time: 0.5302065387368202 Scheduler time: 102.17738067917526 Scheduler overhead time: 0.07884111581370234 Adapter cache time: 0.014627059455960989 Engine time: 0.07857669703662395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.98818080825731,
    "estimated_duration": 3600.107257352902,
    "input_throughput": 8492.853910825263,
    "output_throughput": 7542.183068169169,
    "total_throughput": 16035.03697899443,
    "itl": 112.00308122429024,
    "ttft": 1430379.7481682643,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2522250685654579,
    "arrivals": 283764,
    "finished_requests": 123406,
    "scheduler_time": 215.2801636727758
}
#Debug simulation 
Total elapsed time: 102.98836037702858. Arrivals time: 0.5277300206944346 Scheduler time: 102.26074617123231 Scheduler overhead time: 0.07940056268125772 Adapter cache time: 0.014743354637175798 Engine time: 0.07633511628955603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 113.83632385125384,
    "estimated_duration": 3600.0498996989986,
    "input_throughput": 8506.087930214619,
    "output_throughput": 7545.135138896574,
    "total_throughput": 16051.223069111193,
    "itl": 112.17844000761141,
    "ttft": 1424018.2586763517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 79,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2472460468951614,
    "arrivals": 283764,
    "finished_requests": 123678,
    "scheduler_time": 214.83227866127157
}
#Debug simulation 
Total elapsed time: 113.83663284219801. Arrivals time: 0.5142706232145429 Scheduler time: 113.12035000603646 Scheduler overhead time: 0.08001423394307494 Adapter cache time: 0.01470256457105279 Engine time: 0.07792702643200755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 102.9316315301694,
    "estimated_duration": 3600.108090550736,
    "input_throughput": 8492.851945265533,
    "output_throughput": 7542.181322629746,
    "total_throughput": 16035.033267895278,
    "itl": 112.00296115055443,
    "ttft": 1430379.5363230847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.25549466701224427,
    "arrivals": 283764,
    "finished_requests": 123406,
    "scheduler_time": 215.2801339146673
}
#Debug simulation 
Total elapsed time: 102.93181023700163. Arrivals time: 0.5087892627343535 Scheduler time: 102.22250728821382 Scheduler overhead time: 0.07964355545118451 Adapter cache time: 0.014557598624378443 Engine time: 0.07681714650243521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 112.49619266623631,
    "estimated_duration": 3600.062120740539,
    "input_throughput": 8499.95610456451,
    "output_throughput": 7539.078796345053,
    "total_throughput": 16039.034900909563,
    "itl": 111.94237058034182,
    "ttft": 1420742.6894111612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 80,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2392040235921743,
    "arrivals": 283764,
    "finished_requests": 123583,
    "scheduler_time": 215.0293477349339
}
#Debug simulation 
Total elapsed time: 112.4963680030778. Arrivals time: 0.52306706039235 Scheduler time: 111.77433729358017 Scheduler overhead time: 0.0785935390740633 Adapter cache time: 0.014477284625172615 Engine time: 0.07591931521892548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 103.53801469411701,
    "estimated_duration": 3600.037850329868,
    "input_throughput": 8496.764554071893,
    "output_throughput": 7540.132667636467,
    "total_throughput": 16036.897221708361,
    "itl": 111.93041659298864,
    "ttft": 1430442.3799289924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 77,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2588900192454454,
    "arrivals": 283764,
    "finished_requests": 123456,
    "scheduler_time": 215.3210683509897
}
#Debug simulation 
Total elapsed time: 103.53819190291688. Arrivals time: 0.5318841021507978 Scheduler time: 102.80498984875157 Scheduler overhead time: 0.07985719805583358 Adapter cache time: 0.014568626414984465 Engine time: 0.07731469348073006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 101.18820692133158,
    "estimated_duration": 3600.031948314891,
    "input_throughput": 8468.621511614792,
    "output_throughput": 7515.390526649146,
    "total_throughput": 15984.01203826394,
    "itl": 111.2700148157001,
    "ttft": 1398290.5084547217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 111,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3397141395765353,
    "arrivals": 264567,
    "finished_requests": 123428,
    "scheduler_time": 215.40880690520052
}
#Debug simulation 
Total elapsed time: 101.1883808132261. Arrivals time: 0.5016237739473581 Scheduler time: 100.49301451491192 Scheduler overhead time: 0.07646458735689521 Adapter cache time: 0.014105155598372221 Engine time: 0.07454567635431886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.44686959311366,
    "estimated_duration": 3600.0362435366233,
    "input_throughput": 8458.985949009157,
    "output_throughput": 7509.782727476301,
    "total_throughput": 15968.768676485457,
    "itl": 111.25409673808598,
    "ttft": 1394276.5877053544,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 102,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3327696117339656,
    "arrivals": 264567,
    "finished_requests": 123290,
    "scheduler_time": 215.49448004728137
}
#Debug simulation 
Total elapsed time: 104.4470483562909. Arrivals time: 0.5407895869575441 Scheduler time: 103.70891793817282 Scheduler overhead time: 0.07759301410987973 Adapter cache time: 0.014679713640362024 Engine time: 0.07603477779775858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.14793472224846,
    "estimated_duration": 3600.037190925352,
    "input_throughput": 8458.983722935502,
    "output_throughput": 7509.780751195742,
    "total_throughput": 15968.764474131243,
    "itl": 111.25409302344711,
    "ttft": 1394277.1567498243,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 102,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3333717522025113,
    "arrivals": 264567,
    "finished_requests": 123290,
    "scheduler_time": 215.49452517979415
}
#Debug simulation 
Total elapsed time: 104.1481174621731. Arrivals time: 0.5171267525292933 Scheduler time: 103.43237261055037 Scheduler overhead time: 0.07867755275219679 Adapter cache time: 0.014588660094887018 Engine time: 0.0760649461299181 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 106.8451017527841,
    "estimated_duration": 3600.0650340724073,
    "input_throughput": 8461.214925759963,
    "output_throughput": 7513.79454092827,
    "total_throughput": 15975.009466688232,
    "itl": 111.31968136911986,
    "ttft": 1391739.862082337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32350180247565746,
    "arrivals": 264567,
    "finished_requests": 123238,
    "scheduler_time": 215.35343661150273
}
#Debug simulation 
Total elapsed time: 106.84527230681852. Arrivals time: 0.5408580172806978 Scheduler time: 106.10389477340505 Scheduler overhead time: 0.07918117521330714 Adapter cache time: 0.014629920944571495 Engine time: 0.07713993825018406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 104.36890215519816,
    "estimated_duration": 3600.0409992601712,
    "input_throughput": 8458.974774525677,
    "output_throughput": 7509.772806908573,
    "total_throughput": 15968.74758143425,
    "itl": 111.25414282083149,
    "ttft": 1394278.5056946292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 102,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3371443657949568,
    "arrivals": 264567,
    "finished_requests": 123290,
    "scheduler_time": 215.49456090102603
}
#Debug simulation 
Total elapsed time: 104.36907602706924. Arrivals time: 0.5378531534224749 Scheduler time: 103.63103457540274 Scheduler overhead time: 0.07851056661456823 Adapter cache time: 0.014334228821098804 Engine time: 0.07769346656277776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 106.88112504500896,
    "estimated_duration": 3600.0012493902505,
    "input_throughput": 8448.917345556956,
    "output_throughput": 7502.569062878711,
    "total_throughput": 15951.486408435667,
    "itl": 111.26332698408474,
    "ttft": 1392128.4489173226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 100,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2990050294902179,
    "arrivals": 264567,
    "finished_requests": 123093,
    "scheduler_time": 215.86076818472893
}
#Debug simulation 
Total elapsed time: 106.88130633113906. Arrivals time: 0.5474200062453747 Scheduler time: 106.13059614272788 Scheduler overhead time: 0.08013508515432477 Adapter cache time: 0.014692626427859068 Engine time: 0.07823819294571877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.8150536250323,
    "estimated_duration": 3600.0473576927366,
    "input_throughput": 8458.959834216472,
    "output_throughput": 7509.759543087509,
    "total_throughput": 15968.719377303982,
    "itl": 111.25419412030496,
    "ttft": 1394281.3172220835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 102,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.34192300967872113,
    "arrivals": 264567,
    "finished_requests": 123290,
    "scheduler_time": 215.49474014959785
}
#Debug simulation 
Total elapsed time: 104.81522298324853. Arrivals time: 0.5486461804248393 Scheduler time: 104.06626931484789 Scheduler overhead time: 0.07928067818284035 Adapter cache time: 0.014433609321713448 Engine time: 0.07740198960527778 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 101.68087916914374,
    "estimated_duration": 3600.0587672634924,
    "input_throughput": 8478.323542257338,
    "output_throughput": 7532.467315974582,
    "total_throughput": 16010.79085823192,
    "itl": 112.01104441195234,
    "ttft": 1389028.2339768717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.32441170085687154,
    "arrivals": 262719,
    "finished_requests": 123443,
    "scheduler_time": 214.25541089644923
}
#Debug simulation 
Total elapsed time: 101.68104867497459. Arrivals time: 0.509916580747813 Scheduler time: 100.97765719890594 Scheduler overhead time: 0.0760837709531188 Adapter cache time: 0.0142840719781816 Engine time: 0.07402933575212955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.46600859193131,
    "estimated_duration": 3600.056082016808,
    "input_throughput": 8478.178757399008,
    "output_throughput": 7535.316778954931,
    "total_throughput": 16013.495536353937,
    "itl": 112.03247465859467,
    "ttft": 1384749.3331904663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.33616825734730804,
    "arrivals": 262719,
    "finished_requests": 123416,
    "scheduler_time": 214.2448264602007
}
#Debug simulation 
Total elapsed time: 106.46618865570053. Arrivals time: 0.5096641182899475 Scheduler time: 105.75607822882012 Scheduler overhead time: 0.07987474836409092 Adapter cache time: 0.01462937192991376 Engine time: 0.07614674000069499 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.29176279297099,
    "estimated_duration": 3600.0543591033133,
    "input_throughput": 8478.18281488457,
    "output_throughput": 7535.320385205745,
    "total_throughput": 16013.503200090316,
    "itl": 112.0324175948026,
    "ttft": 1384747.9513675882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3367525185458367,
    "arrivals": 262719,
    "finished_requests": 123416,
    "scheduler_time": 214.24472013426933
}
#Debug simulation 
Total elapsed time: 106.29207149287686. Arrivals time: 0.493049384560436 Scheduler time: 105.60016645258293 Scheduler overhead time: 0.07819530833512545 Adapter cache time: 0.014594821259379387 Engine time: 0.07627370674163103 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 106.83944491576403,
    "estimated_duration": 3600.0399191916977,
    "input_throughput": 8478.216821232627,
    "output_throughput": 7535.350609692918,
    "total_throughput": 16013.567430925545,
    "itl": 112.03222861437814,
    "ttft": 1384742.94751146,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3230932071572171,
    "arrivals": 262719,
    "finished_requests": 123416,
    "scheduler_time": 214.24453976551845
}
#Debug simulation 
Total elapsed time: 106.8396112988703. Arrivals time: 0.4999649105593562 Scheduler time: 106.14053381280974 Scheduler overhead time: 0.07838765997439623 Adapter cache time: 0.014563152100890875 Engine time: 0.07693495228886604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 105.98525056429207,
    "estimated_duration": 3600.0589457700416,
    "input_throughput": 8478.17201322837,
    "output_throughput": 7535.310784806468,
    "total_throughput": 16013.48279803484,
    "itl": 112.03251667119801,
    "ttft": 1384749.6906557174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.340776639711112,
    "arrivals": 262719,
    "finished_requests": 123416,
    "scheduler_time": 214.2447824869373
}
#Debug simulation 
Total elapsed time: 105.98543003108352. Arrivals time: 0.5125033371150494 Scheduler time: 105.27370154252276 Scheduler overhead time: 0.07806853763759136 Adapter cache time: 0.014601437840610743 Engine time: 0.07684952579438686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 101.86388036701828,
    "estimated_duration": 3600.0484377848074,
    "input_throughput": 8478.34786878067,
    "output_throughput": 7532.488928589503,
    "total_throughput": 16010.836797370173,
    "itl": 112.01088377040166,
    "ttft": 1389023.8862269847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.316945331259631,
    "arrivals": 262719,
    "finished_requests": 123443,
    "scheduler_time": 214.25514879042717
}
#Debug simulation 
Total elapsed time: 101.86405256111175. Arrivals time: 0.5059599112719297 Scheduler time: 101.16551343584433 Scheduler overhead time: 0.07569086086004972 Adapter cache time: 0.014062054920941591 Engine time: 0.0740210972726345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.32734949188307,
    "estimated_duration": 3600.061535610612,
    "input_throughput": 8478.165914134335,
    "output_throughput": 7535.305363995356,
    "total_throughput": 16013.471278129691,
    "itl": 112.03247253543859,
    "ttft": 1384749.9736956954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 103,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3454295298084614,
    "arrivals": 262719,
    "finished_requests": 123416,
    "scheduler_time": 214.24482024759882
}
#Debug simulation 
Total elapsed time: 106.32752415817231. Arrivals time: 0.514461359474808 Scheduler time: 105.61275074537843 Scheduler overhead time: 0.07924137404188514 Adapter cache time: 0.014692621771246195 Engine time: 0.07674596924334764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 105.12601584754884,
    "estimated_duration": 3600.065258680152,
    "input_throughput": 8438.008151868027,
    "output_throughput": 7531.355420468196,
    "total_throughput": 15969.363572336222,
    "itl": 112.38041382669618,
    "ttft": 1396302.8906611626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3029882866493423,
    "arrivals": 261754,
    "finished_requests": 122845,
    "scheduler_time": 214.45681887976957
}
#Debug simulation 
Total elapsed time: 105.12618502462283. Arrivals time: 0.49654681608080864 Scheduler time: 104.43591516604647 Scheduler overhead time: 0.07687351433560252 Adapter cache time: 0.01364229153841734 Engine time: 0.07412485033273697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.32039033295587,
    "estimated_duration": 3600.1095000521755,
    "input_throughput": 8439.699959003929,
    "output_throughput": 7532.484775701125,
    "total_throughput": 15972.184734705053,
    "itl": 112.3678715357617,
    "ttft": 1396197.4536787302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3383411170053296,
    "arrivals": 261754,
    "finished_requests": 122856,
    "scheduler_time": 214.4834130982856
}
#Debug simulation 
Total elapsed time: 105.3205639095977. Arrivals time: 0.4840850527398288 Scheduler time: 104.6449845940806 Scheduler overhead time: 0.07576272543519735 Adapter cache time: 0.013927886728197336 Engine time: 0.07309783902019262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.16546178795397,
    "estimated_duration": 3600.11028336091,
    "input_throughput": 8439.698122701655,
    "output_throughput": 7532.48313678991,
    "total_throughput": 15972.181259491566,
    "itl": 112.36789052593942,
    "ttft": 1396197.6721023472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3391215424053376,
    "arrivals": 261754,
    "finished_requests": 122856,
    "scheduler_time": 214.48341598161375
}
#Debug simulation 
Total elapsed time: 105.16562839411199. Arrivals time: 0.477432394400239 Scheduler time: 104.49477996723726 Scheduler overhead time: 0.07583607640117407 Adapter cache time: 0.013950157910585403 Engine time: 0.07472417643293738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 104.94562577269971,
    "estimated_duration": 3600.0706837859216,
    "input_throughput": 8437.995436260271,
    "output_throughput": 7531.344071135549,
    "total_throughput": 15969.33950739582,
    "itl": 112.38030106286868,
    "ttft": 1396304.2738951652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3090900293854067,
    "arrivals": 261754,
    "finished_requests": 122845,
    "scheduler_time": 214.4569614024686
}
#Debug simulation 
Total elapsed time: 104.94578801374882. Arrivals time: 0.4970564707182348 Scheduler time: 104.25422449177131 Scheduler overhead time: 0.07718110363930464 Adapter cache time: 0.013991618528962135 Engine time: 0.0746378074400127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 104.66842766711488,
    "estimated_duration": 3600.115898582266,
    "input_throughput": 8439.684959021799,
    "output_throughput": 7532.471388123655,
    "total_throughput": 15972.156347145456,
    "itl": 112.36794389767255,
    "ttft": 1396200.2668003105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3432714173570277,
    "arrivals": 261754,
    "finished_requests": 122856,
    "scheduler_time": 214.48360784404926
}
#Debug simulation 
Total elapsed time: 104.66859758505598. Arrivals time: 0.500576966907829 Scheduler time: 103.97392438678071 Scheduler overhead time: 0.07601180160418153 Adapter cache time: 0.014090935233980417 Engine time: 0.07492306595668197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 105.10547698708251,
    "estimated_duration": 3600.053971122492,
    "input_throughput": 8438.034608277934,
    "output_throughput": 7531.379034172115,
    "total_throughput": 15969.41364245005,
    "itl": 112.38035596466177,
    "ttft": 1396298.1804698356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 99,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2960149791953157,
    "arrivals": 261754,
    "finished_requests": 122845,
    "scheduler_time": 214.45640531753344
}
#Debug simulation 
Total elapsed time: 105.10564838536084. Arrivals time: 0.5003600809723139 Scheduler time: 104.41214960953221 Scheduler overhead time: 0.07612719852477312 Adapter cache time: 0.013801947236061096 Engine time: 0.07409748248755932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.14186265785247,
    "estimated_duration": 3600.118736462726,
    "input_throughput": 8439.678306236492,
    "output_throughput": 7532.465450471335,
    "total_throughput": 15972.143756707828,
    "itl": 112.36782442633911,
    "ttft": 1396200.6189656237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 104,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3475470460951326,
    "arrivals": 261754,
    "finished_requests": 122856,
    "scheduler_time": 214.48367067448353
}
#Debug simulation 
Total elapsed time: 105.14203758863732. Arrivals time: 0.4922758690081537 Scheduler time: 104.45722500467673 Scheduler overhead time: 0.07563540292903781 Adapter cache time: 0.013966522179543972 Engine time: 0.07392771122977138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 102.9085362101905,
    "estimated_duration": 3600.1039764431525,
    "input_throughput": 8526.252630716122,
    "output_throughput": 7513.968534521617,
    "total_throughput": 16040.22116523774,
    "itl": 111.65839537926352,
    "ttft": 1387162.88895156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2968673111614768,
    "arrivals": 261307,
    "finished_requests": 123614,
    "scheduler_time": 214.99213147871973
}
#Debug simulation 
Total elapsed time: 102.90870855515823. Arrivals time: 0.48948309710249305 Scheduler time: 102.22813235595822 Scheduler overhead time: 0.07543473830446601 Adapter cache time: 0.013673890382051468 Engine time: 0.07344875112175941 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.814552933909,
    "estimated_duration": 3600.12135658111,
    "input_throughput": 8526.211468923975,
    "output_throughput": 7513.932259686187,
    "total_throughput": 16040.143728610163,
    "itl": 111.65907061258989,
    "ttft": 1387166.0020312478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.317002169622574,
    "arrivals": 261307,
    "finished_requests": 123614,
    "scheduler_time": 214.99247795418933
}
#Debug simulation 
Total elapsed time: 102.81483970489353. Arrivals time: 0.48172210389748216 Scheduler time: 102.14065998326987 Scheduler overhead time: 0.0755107244476676 Adapter cache time: 0.014027793891727924 Engine time: 0.07337083108723164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.81562959309667,
    "estimated_duration": 3600.12184240408,
    "input_throughput": 8526.210318343645,
    "output_throughput": 7513.931245709147,
    "total_throughput": 16040.141564052792,
    "itl": 111.65907290226697,
    "ttft": 1387166.2375372648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3174796629697088,
    "arrivals": 261307,
    "finished_requests": 123614,
    "scheduler_time": 214.99248628380394
}
#Debug simulation 
Total elapsed time: 102.81580174528062. Arrivals time: 0.48713231226429343 Scheduler time: 102.13709096750244 Scheduler overhead time: 0.07577014714479446 Adapter cache time: 0.01368077751249075 Engine time: 0.07302150130271912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 102.79354683635756,
    "estimated_duration": 3600.10835120308,
    "input_throughput": 8526.242269831198,
    "output_throughput": 7513.95940373853,
    "total_throughput": 16040.201673569729,
    "itl": 111.65830170687511,
    "ttft": 1387163.3126455154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30310992879560233,
    "arrivals": 261307,
    "finished_requests": 123614,
    "scheduler_time": 214.99216435402084
}
#Debug simulation 
Total elapsed time: 102.79371594917029. Arrivals time: 0.5059034153819084 Scheduler time: 102.09526175586507 Scheduler overhead time: 0.0762386042624712 Adapter cache time: 0.01381826214492321 Engine time: 0.0738010611385107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 102.4163428870961,
    "estimated_duration": 3600.002198343529,
    "input_throughput": 8526.211182349673,
    "output_throughput": 7514.049855982534,
    "total_throughput": 16040.261038332206,
    "itl": 111.65886609322662,
    "ttft": 1387159.135942194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3217552917078138,
    "arrivals": 261307,
    "finished_requests": 123611,
    "scheduler_time": 214.98490441394554
}
#Debug simulation 
Total elapsed time: 102.41650710999966. Arrivals time: 0.4897326794452965 Scheduler time: 101.73541721701622 Scheduler overhead time: 0.07508417731150985 Adapter cache time: 0.013861818239092827 Engine time: 0.07378123793751001 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 102.61971797514707,
    "estimated_duration": 3600.0938685929295,
    "input_throughput": 8526.276569559857,
    "output_throughput": 7513.989631212787,
    "total_throughput": 16040.266200772643,
    "itl": 111.65729489565089,
    "ttft": 1387161.4107546096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 96,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2870448283106092,
    "arrivals": 261307,
    "finished_requests": 123614,
    "scheduler_time": 214.99133768385997
}
#Debug simulation 
Total elapsed time: 102.61988113634288. Arrivals time: 0.4750822368077934 Scheduler time: 101.95389414997771 Scheduler overhead time: 0.07564284699037671 Adapter cache time: 0.013775623869150877 Engine time: 0.07317456556484103 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.82386320689693,
    "estimated_duration": 3600.008911450871,
    "input_throughput": 8526.195283119338,
    "output_throughput": 7514.035844177425,
    "total_throughput": 16040.231127296764,
    "itl": 111.6587706481665,
    "ttft": 1387161.8278876527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 97,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.325779412873089,
    "arrivals": 261307,
    "finished_requests": 123611,
    "scheduler_time": 214.9851227878177
}
#Debug simulation 
Total elapsed time: 102.82403009478003. Arrivals time: 0.484571672976017 Scheduler time: 102.14842326752841 Scheduler overhead time: 0.07507522217929363 Adapter cache time: 0.01384927611798048 Engine time: 0.07338129496201873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 102.53206434193999,
    "estimated_duration": 3600.0835994825707,
    "input_throughput": 8443.147543676134,
    "output_throughput": 7540.610446907881,
    "total_throughput": 15983.757990584014,
    "itl": 112.35876376673859,
    "ttft": 1394404.43573655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2907463356736113,
    "arrivals": 261056,
    "finished_requests": 123083,
    "scheduler_time": 214.132089156753
}
#Debug simulation 
Total elapsed time: 102.53224349720404. Arrivals time: 0.5008473764173687 Scheduler time: 101.8393086520955 Scheduler overhead time: 0.07612022804096341 Adapter cache time: 0.013894474133849144 Engine time: 0.07331248139962554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.59989393083379,
    "estimated_duration": 3600.079368847672,
    "input_throughput": 8445.358250457635,
    "output_throughput": 7541.087353496267,
    "total_throughput": 15986.445603953902,
    "itl": 112.35869141684391,
    "ttft": 1392025.98166863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.29661029594251886,
    "arrivals": 261056,
    "finished_requests": 123076,
    "scheduler_time": 214.10704969786238
}
#Debug simulation 
Total elapsed time: 104.60006263200194. Arrivals time: 0.49697761284187436 Scheduler time: 103.90769006824121 Scheduler overhead time: 0.0767004401423037 Adapter cache time: 0.014207093976438046 Engine time: 0.07528266916051507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.21422644425184,
    "estimated_duration": 3600.080511677037,
    "input_throughput": 8445.355569516645,
    "output_throughput": 7541.084959612006,
    "total_throughput": 15986.440529128651,
    "itl": 112.35870834506812,
    "ttft": 1392026.6683873932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2971950649097564,
    "arrivals": 261056,
    "finished_requests": 123076,
    "scheduler_time": 214.10710756535323
}
#Debug simulation 
Total elapsed time: 104.2143959342502. Arrivals time: 0.48903639102354646 Scheduler time: 103.53117796359584 Scheduler overhead time: 0.0767625174485147 Adapter cache time: 0.01407155953347683 Engine time: 0.07478201715275645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 102.59270372707397,
    "estimated_duration": 3600.088912169936,
    "input_throughput": 8443.135084038504,
    "output_throughput": 7540.59931915331,
    "total_throughput": 15983.734403191815,
    "itl": 112.35865230068863,
    "ttft": 1394405.7007149651,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2954954469320366,
    "arrivals": 261056,
    "finished_requests": 123083,
    "scheduler_time": 214.1322550284923
}
#Debug simulation 
Total elapsed time: 102.59288564929739. Arrivals time: 0.489953450858593 Scheduler time: 101.90945935621858 Scheduler overhead time: 0.07635657675564289 Adapter cache time: 0.013804228976368904 Engine time: 0.07462006714195013 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 104.51807795697823,
    "estimated_duration": 3600.084841088272,
    "input_throughput": 8445.345413251196,
    "output_throughput": 7541.075890809634,
    "total_throughput": 15986.421304060832,
    "itl": 112.35866773473576,
    "ttft": 1392028.489453324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 91,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.30121918607503173,
    "arrivals": 261056,
    "finished_requests": 123076,
    "scheduler_time": 214.10722257810875
}
#Debug simulation 
Total elapsed time: 104.51824228605255. Arrivals time: 0.5107845370657742 Scheduler time: 103.81147937243804 Scheduler overhead time: 0.07679114630445838 Adapter cache time: 0.014221799094229937 Engine time: 0.07552676601335406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 102.47171416413039,
    "estimated_duration": 3600.07610254922,
    "input_throughput": 8443.165126002896,
    "output_throughput": 7540.62614975758,
    "total_throughput": 15983.791275760475,
    "itl": 112.3581692601101,
    "ttft": 1394403.6564478409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 95,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.284054778015707,
    "arrivals": 261056,
    "finished_requests": 123083,
    "scheduler_time": 214.13148385821654
}
#Debug simulation 
Total elapsed time: 102.47188063524663. Arrivals time: 0.46523192012682557 Scheduler time: 101.81325312703848 Scheduler overhead time: 0.07592680444940925 Adapter cache time: 0.013949671294540167 Engine time: 0.0748408273793757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.61625367682427,
    "estimated_duration": 3600.103852877052,
    "input_throughput": 8442.259790842196,
    "output_throughput": 7540.003596925966,
    "total_throughput": 15982.263387768162,
    "itl": 112.36963690373729,
    "ttft": 1394147.065191331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 92,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3077838109433648,
    "arrivals": 261056,
    "finished_requests": 123078,
    "scheduler_time": 214.105125163756
}
#Debug simulation 
Total elapsed time: 102.61640476016328. Arrivals time: 0.44973094295710325 Scheduler time: 101.97370822029188 Scheduler overhead time: 0.07655170699581504 Adapter cache time: 0.014101522509008646 Engine time: 0.07339188363403082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 106.5514807421714,
    "estimated_duration": 3600.074207384244,
    "input_throughput": 8510.620402533788,
    "output_throughput": 7570.515614399688,
    "total_throughput": 16081.136016933477,
    "itl": 112.6355255830795,
    "ttft": 1380099.1134506098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.36113755378406454,
    "arrivals": 258928,
    "finished_requests": 123849,
    "scheduler_time": 212.45213358193365
}
#Debug simulation 
Total elapsed time: 106.55173788126558. Arrivals time: 0.48215188505128026 Scheduler time: 105.8719770484604 Scheduler overhead time: 0.07800298929214478 Adapter cache time: 0.014497126452624798 Engine time: 0.07542139757424593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.33278977731243,
    "estimated_duration": 3600.0312727843775,
    "input_throughput": 8507.699705705987,
    "output_throughput": 7567.648149603104,
    "total_throughput": 16075.347855309092,
    "itl": 112.61172298401331,
    "ttft": 1382511.6741061283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4178656351333484,
    "arrivals": 258928,
    "finished_requests": 123801,
    "scheduler_time": 212.4517301730271
}
#Debug simulation 
Total elapsed time: 104.33295583399013. Arrivals time: 0.4828527201898396 Scheduler time: 103.65176926925778 Scheduler overhead time: 0.08056234149262309 Adapter cache time: 0.014146660454571247 Engine time: 0.07461696909740567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.19469962501898,
    "estimated_duration": 3600.0319894964114,
    "input_throughput": 8507.698011951383,
    "output_throughput": 7567.646642998576,
    "total_throughput": 16075.344654949959,
    "itl": 112.61173114677717,
    "ttft": 1382511.9455687092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4185736971721064,
    "arrivals": 258928,
    "finished_requests": 123801,
    "scheduler_time": 212.4517388230115
}
#Debug simulation 
Total elapsed time: 104.19487182237208. Arrivals time: 0.47008765768259764 Scheduler time: 103.53001589560881 Scheduler overhead time: 0.07655448839068413 Adapter cache time: 0.014739724807441235 Engine time: 0.07435940019786358 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 104.50552768725902,
    "estimated_duration": 3600.062583161955,
    "input_throughput": 8508.44875399268,
    "output_throughput": 7567.635942615052,
    "total_throughput": 16076.08469660773,
    "itl": 112.60925618673654,
    "ttft": 1382496.6011557546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.37039553349139187,
    "arrivals": 258928,
    "finished_requests": 123810,
    "scheduler_time": 212.45458136976387
}
#Debug simulation 
Total elapsed time: 104.50567968515679. Arrivals time: 0.46427483577281237 Scheduler time: 103.8461947622709 Scheduler overhead time: 0.07717541791498661 Adapter cache time: 0.014263537712395191 Engine time: 0.0750422440469265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 104.25759473396465,
    "estimated_duration": 3600.03769536192,
    "input_throughput": 8507.684527709063,
    "output_throughput": 7567.634648686955,
    "total_throughput": 16075.319176396017,
    "itl": 112.61188916886974,
    "ttft": 1382514.0585812794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4234780948422854,
    "arrivals": 258928,
    "finished_requests": 123801,
    "scheduler_time": 212.45184002079563
}
#Debug simulation 
Total elapsed time: 104.2577492762357. Arrivals time: 0.4735800581984222 Scheduler time: 103.58732619415969 Scheduler overhead time: 0.07662379834800959 Adapter cache time: 0.014676498249173164 Engine time: 0.07603477966040373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 106.13311279471964,
    "estimated_duration": 3600.0662837494997,
    "input_throughput": 8510.639134146542,
    "output_throughput": 7570.532276870828,
    "total_throughput": 16081.171411017369,
    "itl": 112.63542018725431,
    "ttft": 1380096.2603488949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 118,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.35282593479845714,
    "arrivals": 258928,
    "finished_requests": 123849,
    "scheduler_time": 212.45202137325023
}
#Debug simulation 
Total elapsed time: 106.13327226787806. Arrivals time: 0.48742916295304894 Scheduler time: 105.44916810747236 Scheduler overhead time: 0.07711943844333291 Adapter cache time: 0.01453740056604147 Engine time: 0.07637185510247946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.53155124699697,
    "estimated_duration": 3600.0428592307385,
    "input_throughput": 8507.672324363557,
    "output_throughput": 7567.623793740468,
    "total_throughput": 16075.296118104023,
    "itl": 112.61196157120338,
    "ttft": 1382515.4515606062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42938852280378315,
    "arrivals": 258928,
    "finished_requests": 123801,
    "scheduler_time": 212.45190853807273
}
#Debug simulation 
Total elapsed time: 106.53171973628923. Arrivals time: 0.4722778224386275 Scheduler time: 105.86439836677164 Scheduler overhead time: 0.07696736371144652 Adapter cache time: 0.014365287497639656 Engine time: 0.07497246051207185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 104.02493356401101,
    "estimated_duration": 3600.0603239351585,
    "input_throughput": 8547.730657569462,
    "output_throughput": 7548.311015604372,
    "total_throughput": 16096.041673173833,
    "itl": 112.14063006332873,
    "ttft": 1381655.859256207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39786340671125753,
    "arrivals": 257941,
    "finished_requests": 123838,
    "scheduler_time": 213.4659794270187
}
#Debug simulation 
Total elapsed time: 104.0250913980417. Arrivals time: 0.4513252223841846 Scheduler time: 103.38105360604823 Scheduler overhead time: 0.0757438326254487 Adapter cache time: 0.014350779354572296 Engine time: 0.07375491363927722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.31734220590442,
    "estimated_duration": 3600.0847011197834,
    "input_throughput": 8547.672778484477,
    "output_throughput": 7548.259903870479,
    "total_throughput": 16095.932682354958,
    "itl": 112.141323274332,
    "ttft": 1381660.640143363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4250715216784738,
    "arrivals": 257941,
    "finished_requests": 123838,
    "scheduler_time": 213.466551441707
}
#Debug simulation 
Total elapsed time: 104.31749835982919. Arrivals time: 0.4645078466273844 Scheduler time: 103.6611016956158 Scheduler overhead time: 0.07540211547166109 Adapter cache time: 0.014236028771847486 Engine time: 0.07332687871530652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.16875012684613,
    "estimated_duration": 3600.085312968042,
    "input_throughput": 8547.67132577482,
    "output_throughput": 7548.258621014858,
    "total_throughput": 16095.929946789678,
    "itl": 112.14133512280466,
    "ttft": 1381660.8977988225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42567247735336533,
    "arrivals": 257941,
    "finished_requests": 123838,
    "scheduler_time": 213.46656233427993
}
#Debug simulation 
Total elapsed time: 104.16891634883359. Arrivals time: 0.4774329410865903 Scheduler time: 103.49758174363524 Scheduler overhead time: 0.07634721603244543 Adapter cache time: 0.014265528880059719 Engine time: 0.07367767998948693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 104.06030531506985,
    "estimated_duration": 3600.068134867401,
    "input_throughput": 8547.712111880188,
    "output_throughput": 7548.294638318253,
    "total_throughput": 16096.006750198441,
    "itl": 112.14065817727347,
    "ttft": 1381658.1554174977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4066847323486584,
    "arrivals": 257941,
    "finished_requests": 123838,
    "scheduler_time": 213.46607109129226
}
#Debug simulation 
Total elapsed time: 104.0604715240188. Arrivals time: 0.4573062998242676 Scheduler time: 103.41185120120645 Scheduler overhead time: 0.07481800112873316 Adapter cache time: 0.01407358655706048 Engine time: 0.07309917407110333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 103.9340126849711,
    "estimated_duration": 3600.092708392055,
    "input_throughput": 8547.65376687873,
    "output_throughput": 7548.243115143876,
    "total_throughput": 16095.896882022605,
    "itl": 112.14143294583471,
    "ttft": 1381663.8990625523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4313313977420334,
    "arrivals": 257941,
    "finished_requests": 123838,
    "scheduler_time": 213.46670890513298
}
#Debug simulation 
Total elapsed time: 103.93416967615485. Arrivals time: 0.453459685202688 Scheduler time: 103.2872806424275 Scheduler overhead time: 0.07554075960069895 Adapter cache time: 0.014265744015574455 Engine time: 0.07420009141787887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 104.54670123429969,
    "estimated_duration": 3600.051658854237,
    "input_throughput": 8547.75123137919,
    "output_throughput": 7548.3291838786,
    "total_throughput": 16096.08041525779,
    "itl": 112.14053166449312,
    "ttft": 1381653.4161413114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3887065383372833,
    "arrivals": 257941,
    "finished_requests": 123838,
    "scheduler_time": 213.46587098295868
}
#Debug simulation 
Total elapsed time: 104.54685512837023. Arrivals time: 0.4592700945213437 Scheduler time: 103.8944050851278 Scheduler overhead time: 0.07583589851856232 Adapter cache time: 0.014160786289721727 Engine time: 0.07484818203374743 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 103.92136314185336,
    "estimated_duration": 3600.0987778612152,
    "input_throughput": 8547.639356240543,
    "output_throughput": 7548.230389429492,
    "total_throughput": 16095.869745670034,
    "itl": 112.14195967754104,
    "ttft": 1381664.366470966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 130,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.43686456434428667,
    "arrivals": 257941,
    "finished_requests": 123838,
    "scheduler_time": 213.46715830358076
}
#Debug simulation 
Total elapsed time: 103.92162069771439. Arrivals time: 0.482211472466588 Scheduler time: 103.24619765160605 Scheduler overhead time: 0.07637174567207694 Adapter cache time: 0.01416385406628251 Engine time: 0.07377853756770492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 104.81305953208357,
    "estimated_duration": 3600.001855777676,
    "input_throughput": 8543.123929405925,
    "output_throughput": 7565.745544349532,
    "total_throughput": 16108.869473755456,
    "itl": 112.47577921755924,
    "ttft": 1379680.9354737275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3948029189673248,
    "arrivals": 257455,
    "finished_requests": 123961,
    "scheduler_time": 212.73225935289955
}
#Debug simulation 
Total elapsed time: 104.8132199938409. Arrivals time: 0.4630985287949443 Scheduler time: 104.15932365786284 Scheduler overhead time: 0.07466368563473225 Adapter cache time: 0.013946222141385078 Engine time: 0.07337849168106914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.06707534287125,
    "estimated_duration": 3600.0401837766017,
    "input_throughput": 8541.082440847573,
    "output_throughput": 7564.151956613221,
    "total_throughput": 16105.234397460794,
    "itl": 112.4783474338389,
    "ttft": 1377547.4000994423,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41745703981490806,
    "arrivals": 257455,
    "finished_requests": 123913,
    "scheduler_time": 212.73613736129866
}
#Debug simulation 
Total elapsed time: 106.06725197797641. Arrivals time: 0.45125525910407305 Scheduler time: 105.42264350876212 Scheduler overhead time: 0.07618098380044103 Adapter cache time: 0.014237803407013416 Engine time: 0.07425722759217024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.13091194117442,
    "estimated_duration": 3600.0409700744244,
    "input_throughput": 8541.080575359212,
    "output_throughput": 7564.150304499741,
    "total_throughput": 16105.230879858953,
    "itl": 112.47837070548289,
    "ttft": 1377547.6869111166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.41823644967749823,
    "arrivals": 257455,
    "finished_requests": 123913,
    "scheduler_time": 212.7361442492492
}
#Debug simulation 
Total elapsed time: 106.13106362009421. Arrivals time: 0.45310569228604436 Scheduler time: 105.48574601393193 Scheduler overhead time: 0.07605723384767771 Adapter cache time: 0.014097841922193766 Engine time: 0.07369416486471891 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 106.28498515719548,
    "estimated_duration": 3600.0699796479767,
    "input_throughput": 8541.650905076813,
    "output_throughput": 7564.125740317622,
    "total_throughput": 16105.776645394435,
    "itl": 112.47554108679878,
    "ttft": 1377588.0254105467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39988744112197333,
    "arrivals": 257455,
    "finished_requests": 123919,
    "scheduler_time": 212.73778520014503
}
#Debug simulation 
Total elapsed time: 106.28513399511576. Arrivals time: 0.4833394340239465 Scheduler time: 105.60853631794453 Scheduler overhead time: 0.07660089246928692 Adapter cache time: 0.01416356023401022 Engine time: 0.07351974351331592 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 106.27079292526469,
    "estimated_duration": 3600.047351809536,
    "input_throughput": 8541.065434748973,
    "output_throughput": 7564.136895675115,
    "total_throughput": 16105.202330424087,
    "itl": 112.47850492443813,
    "ttft": 1377550.012966778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4236438624933366,
    "arrivals": 257455,
    "finished_requests": 123913,
    "scheduler_time": 212.73621985763558
}
#Debug simulation 
Total elapsed time: 106.27094964496791. Arrivals time: 0.4816680271178484 Scheduler time: 105.59535033255816 Scheduler overhead time: 0.07575468299910426 Adapter cache time: 0.014438459184020758 Engine time: 0.07497613737359643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 104.76241579279304,
    "estimated_duration": 3600.115659907632,
    "input_throughput": 8543.093585162513,
    "output_throughput": 7565.999976983755,
    "total_throughput": 16109.093562146269,
    "itl": 112.47625727245759,
    "ttft": 1379624.9991874567,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 129,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3857164880423811,
    "arrivals": 257455,
    "finished_requests": 123968,
    "scheduler_time": 212.73980009431034
}
#Debug simulation 
Total elapsed time: 104.76258204691112. Arrivals time: 0.45994891226291656 Scheduler time: 104.11153521109372 Scheduler overhead time: 0.07535694725811481 Adapter cache time: 0.013831949792802334 Engine time: 0.07284553302451968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 106.1221736157313,
    "estimated_duration": 3600.052205926842,
    "input_throughput": 8541.05391843444,
    "output_throughput": 7564.126696598626,
    "total_throughput": 16105.180615033065,
    "itl": 112.47856935494084,
    "ttft": 1377551.5754065418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.42892552152276014,
    "arrivals": 257455,
    "finished_requests": 123913,
    "scheduler_time": 212.73629414212658
}
#Debug simulation 
Total elapsed time: 106.12232042476535. Arrivals time: 0.44420542381703854 Scheduler time: 105.48680325318128 Scheduler overhead time: 0.07523680431768298 Adapter cache time: 0.013884054962545633 Engine time: 0.07308787759393454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 105.71076510800049,
    "estimated_duration": 3600.0619303087683,
    "input_throughput": 8545.105499716095,
    "output_throughput": 7591.198854086401,
    "total_throughput": 16136.304353802498,
    "itl": 112.96040951792209,
    "ttft": 1373981.0534205134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 117,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3580770660401318,
    "arrivals": 257175,
    "finished_requests": 124138,
    "scheduler_time": 211.57416236019233
}
#Debug simulation 
Total elapsed time: 105.71091012563556. Arrivals time: 0.4511086978018284 Scheduler time: 105.06554403388873 Scheduler overhead time: 0.07733572646975517 Adapter cache time: 0.0135827730409801 Engine time: 0.07455284101888537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 115.1845369990915,
    "estimated_duration": 3600.0892177692085,
    "input_throughput": 8535.150698026351,
    "output_throughput": 7584.98813452199,
    "total_throughput": 16120.13883254834,
    "itl": 112.72409195474935,
    "ttft": 1367038.0291178112,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3889122058870271,
    "arrivals": 257175,
    "finished_requests": 124062,
    "scheduler_time": 211.91377810049764
}
#Debug simulation 
Total elapsed time: 115.18468698207289. Arrivals time: 0.4906134773045778 Scheduler time: 114.49352095788345 Scheduler overhead time: 0.08008065912872553 Adapter cache time: 0.014181095641106367 Engine time: 0.07711241859942675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 122.06770895374939,
    "estimated_duration": 3600.0899166497234,
    "input_throughput": 8535.14904110926,
    "output_throughput": 7584.986662058654,
    "total_throughput": 16120.135703167913,
    "itl": 112.72410507705295,
    "ttft": 1367038.33814045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.38949579006061047,
    "arrivals": 257175,
    "finished_requests": 124062,
    "scheduler_time": 211.91379335824843
}
#Debug simulation 
Total elapsed time: 122.06786165898666. Arrivals time: 0.4390561222098768 Scheduler time: 121.43907583644614 Scheduler overhead time: 0.07427383633330464 Adapter cache time: 0.0131046655587852 Engine time: 0.07367269461974502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 113.48304187040776,
    "estimated_duration": 3600.0720918172615,
    "input_throughput": 8535.191300707906,
    "output_throughput": 7585.024217172281,
    "total_throughput": 16120.215517880186,
    "itl": 112.7232674793871,
    "ttft": 1367034.8354816353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.372159797830973,
    "arrivals": 257175,
    "finished_requests": 124062,
    "scheduler_time": 211.9130962743225
}
#Debug simulation 
Total elapsed time: 113.48319639032707. Arrivals time: 0.41960661206394434 Scheduler time: 112.87428923044354 Scheduler overhead time: 0.07481954991817474 Adapter cache time: 0.013216444756835699 Engine time: 0.07288339408114552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 113.43436014186591,
    "estimated_duration": 3600.09590141442,
    "input_throughput": 8535.134852359832,
    "output_throughput": 7584.974052849998,
    "total_throughput": 16120.108905209829,
    "itl": 112.72428083618665,
    "ttft": 1367039.8854495003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3946516953036192,
    "arrivals": 257175,
    "finished_requests": 124062,
    "scheduler_time": 211.9139219476493
}
#Debug simulation 
Total elapsed time: 113.43450480280444. Arrivals time: 0.41612215247005224 Scheduler time: 112.82558113522828 Scheduler overhead time: 0.07643361762166023 Adapter cache time: 0.013533211313188076 Engine time: 0.07393718184903264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 107.13454675255343,
    "estimated_duration": 3600.0019434065184,
    "input_throughput": 8537.831502089613,
    "output_throughput": 7586.906737654442,
    "total_throughput": 16124.738239744054,
    "itl": 112.73698978023263,
    "ttft": 1373260.0179422565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 121,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.3617960856831637,
    "arrivals": 257175,
    "finished_requests": 124037,
    "scheduler_time": 211.90962587852846
}
#Debug simulation 
Total elapsed time: 107.13473665062338. Arrivals time: 0.4262815532274544 Scheduler time: 106.52315970417112 Scheduler overhead time: 0.07292241230607033 Adapter cache time: 0.0128159886226058 Engine time: 0.07137798378244042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 113.83828620519489,
    "estimated_duration": 3600.10091986898,
    "input_throughput": 8535.122954586026,
    "output_throughput": 7584.963479577617,
    "total_throughput": 16120.086434163644,
    "itl": 112.72437251594523,
    "ttft": 1367041.6159888168,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.39968184676021307,
    "arrivals": 257175,
    "finished_requests": 124062,
    "scheduler_time": 211.91401028934112
}
#Debug simulation 
Total elapsed time: 113.83842764608562. Arrivals time: 0.44326529931277037 Scheduler time: 113.20605986891314 Scheduler overhead time: 0.07492975238710642 Adapter cache time: 0.013273498509079218 Engine time: 0.07278017094358802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 114.12793906824663,
    "estimated_duration": 3600.0889866108914,
    "input_throughput": 8585.788327720802,
    "output_throughput": 7585.845822580418,
    "total_throughput": 16171.634150301219,
    "itl": 112.68785089636687,
    "ttft": 1359581.1758957345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4957990145171055,
    "arrivals": 256011,
    "finished_requests": 124531,
    "scheduler_time": 211.59307142416793
}
#Debug simulation 
Total elapsed time: 114.12808840395883. Arrivals time: 0.43535537365823984 Scheduler time: 113.50045034242794 Scheduler overhead time: 0.07517322525382042 Adapter cache time: 0.013971969485282898 Engine time: 0.07443614164367318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 98.99197130464017,
    "estimated_duration": 3600.1129542440044,
    "input_throughput": 8581.331306169377,
    "output_throughput": 7585.3600559414945,
    "total_throughput": 16166.691362110872,
    "itl": 112.75141731312071,
    "ttft": 1374424.5184828462,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 167,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5446924795955425,
    "arrivals": 256011,
    "finished_requests": 124357,
    "scheduler_time": 211.5781060361116
}
#Debug simulation 
Total elapsed time: 98.99210884887725. Arrivals time: 0.39681321009993553 Scheduler time: 98.41607390670106 Scheduler overhead time: 0.07025454146787524 Adapter cache time: 0.01280016079545021 Engine time: 0.06869469443336129 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 99.18121616588905,
    "estimated_duration": 3600.1140883109306,
    "input_throughput": 8581.328602976151,
    "output_throughput": 7585.357666487785,
    "total_throughput": 16166.686269463937,
    "itl": 112.75144360094723,
    "ttft": 1374424.9930907534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 167,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5457021196372827,
    "arrivals": 256011,
    "finished_requests": 124357,
    "scheduler_time": 211.57813042440148
}
#Debug simulation 
Total elapsed time: 99.18135761097074. Arrivals time: 0.4234561654739082 Scheduler time: 98.57793021015823 Scheduler overhead time: 0.07013598177582026 Adapter cache time: 0.012729431502521038 Engine time: 0.0698330020532012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 101.74079386377707,
    "estimated_duration": 3600.0875455715973,
    "input_throughput": 8576.224774861093,
    "output_throughput": 7584.370561651104,
    "total_throughput": 16160.595336512199,
    "itl": 112.795937212199,
    "ttft": 1372177.7745868645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5171867101942192,
    "arrivals": 256011,
    "finished_requests": 124322,
    "scheduler_time": 211.5548239096491
}
#Debug simulation 
Total elapsed time: 101.74093164782971. Arrivals time: 0.441475294996053 Scheduler time: 101.11501298006624 Scheduler overhead time: 0.07183400634676218 Adapter cache time: 0.013081115670502186 Engine time: 0.07088798563927412 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 98.68114142119884,
    "estimated_duration": 3600.121150575319,
    "input_throughput": 8581.311769206159,
    "output_throughput": 7585.342786488173,
    "total_throughput": 16166.654555694331,
    "itl": 112.75168092676145,
    "ttft": 1374426.562760536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 167,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5531215930357586,
    "arrivals": 256011,
    "finished_requests": 124357,
    "scheduler_time": 211.57810034871113
}
#Debug simulation 
Total elapsed time: 98.68129176320508. Arrivals time: 0.4138818490318954 Scheduler time: 98.08528370968997 Scheduler overhead time: 0.07161248847842216 Adapter cache time: 0.013025239575654268 Engine time: 0.06973484437912703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 114.27613138733432,
    "estimated_duration": 3600.0818081692187,
    "input_throughput": 8585.805447493076,
    "output_throughput": 7585.860948501071,
    "total_throughput": 16171.666395994147,
    "itl": 112.68756356418919,
    "ttft": 1359580.9533551715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 162,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48438814777415307,
    "arrivals": 256011,
    "finished_requests": 124531,
    "scheduler_time": 211.593002190241
}
#Debug simulation 
Total elapsed time: 114.27626240812242. Arrivals time: 0.42161521781235933 Scheduler time: 113.66338139167055 Scheduler overhead time: 0.07550524873659015 Adapter cache time: 0.01399124413728714 Engine time: 0.07320994976907969 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 99.31748141767457,
    "estimated_duration": 3600.0048809032014,
    "input_throughput": 8581.407254161628,
    "output_throughput": 7585.4072156560105,
    "total_throughput": 16166.814469817638,
    "itl": 112.75167442466741,
    "ttft": 1374380.4546239343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 167,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5596607899293307,
    "arrivals": 256011,
    "finished_requests": 124355,
    "scheduler_time": 211.57054779380414
}
#Debug simulation 
Total elapsed time: 99.3176122000441. Arrivals time: 0.4275020742788911 Scheduler time: 98.71060102200136 Scheduler overhead time: 0.06995800510048866 Adapter cache time: 0.013142820913344622 Engine time: 0.06854067463427782 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 102.71205035923049,
    "estimated_duration": 3600.027151491895,
    "input_throughput": 8533.290363454405,
    "output_throughput": 7592.78495682244,
    "total_throughput": 16126.075320276845,
    "itl": 113.07911904347391,
    "ttft": 1365083.5116493169,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4835570635413745,
    "arrivals": 255515,
    "finished_requests": 124311,
    "scheduler_time": 211.12195435170204
}
#Debug simulation 
Total elapsed time: 102.7121991738677. Arrivals time: 0.4355683200992644 Scheduler time: 102.09367347322404 Scheduler overhead time: 0.07168297749012709 Adapter cache time: 0.013602336402982473 Engine time: 0.07038444327190518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.66578732803464,
    "estimated_duration": 3600.0634095011974,
    "input_throughput": 8538.37932934047,
    "output_throughput": 7592.310159833286,
    "total_throughput": 16130.689489173756,
    "itl": 113.04814547775253,
    "ttft": 1365089.3660661788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5177820269414225,
    "arrivals": 255515,
    "finished_requests": 124340,
    "scheduler_time": 211.1487775222977
}
#Debug simulation 
Total elapsed time: 102.66592707205564. Arrivals time: 0.41715101106092334 Scheduler time: 102.06494363024831 Scheduler overhead time: 0.07208631001412868 Adapter cache time: 0.013282875064760447 Engine time: 0.07088663149625063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.26548320380971,
    "estimated_duration": 3600.0639419288923,
    "input_throughput": 8538.378066565781,
    "output_throughput": 7592.309036976508,
    "total_throughput": 16130.68710354229,
    "itl": 113.04815614721991,
    "ttft": 1365089.539713346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5183104499988276,
    "arrivals": 255515,
    "finished_requests": 124340,
    "scheduler_time": 211.14878152692012
}
#Debug simulation 
Total elapsed time: 102.26560821989551. Arrivals time: 0.4034384195692837 Scheduler time: 101.67871777154505 Scheduler overhead time: 0.07182531617581844 Adapter cache time: 0.013062597252428532 Engine time: 0.07072745775803924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 102.12033578567207,
    "estimated_duration": 3600.0409080597638,
    "input_throughput": 8533.257755828263,
    "output_throughput": 7592.7559430794745,
    "total_throughput": 16126.013698907736,
    "itl": 113.07964671663659,
    "ttft": 1365087.1596576476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4940834984718825,
    "arrivals": 255515,
    "finished_requests": 124311,
    "scheduler_time": 211.1225834815065
}
#Debug simulation 
Total elapsed time: 102.12046180292964. Arrivals time: 0.4575059888884425 Scheduler time: 101.4793152930215 Scheduler overhead time: 0.07225187122821808 Adapter cache time: 0.01306471973657608 Engine time: 0.07034638896584511 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 102.36237418605015,
    "estimated_duration": 3600.072164866906,
    "input_throughput": 8538.358564025175,
    "output_throughput": 7592.291695355637,
    "total_throughput": 16130.650259380813,
    "itl": 113.04829873584293,
    "ttft": 1365092.564035648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5256041696108885,
    "arrivals": 255515,
    "finished_requests": 124340,
    "scheduler_time": 211.1489104366913
}
#Debug simulation 
Total elapsed time: 102.36256307084113. Arrivals time: 0.4246553834527731 Scheduler time: 101.7527278708294 Scheduler overhead time: 0.07305407291278243 Adapter cache time: 0.013461209367960691 Engine time: 0.07081719394773245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 102.45147459069267,
    "estimated_duration": 3600.015549179178,
    "input_throughput": 8533.317864975426,
    "output_throughput": 7592.809427234931,
    "total_throughput": 16126.127292210358,
    "itl": 113.07889075080796,
    "ttft": 1365079.5592924037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47242794659454435,
    "arrivals": 255515,
    "finished_requests": 124311,
    "scheduler_time": 211.12178127164253
}
#Debug simulation 
Total elapsed time: 102.45160149876028. Arrivals time: 0.4103013025596738 Scheduler time: 101.8556875619106 Scheduler overhead time: 0.07298061763867736 Adapter cache time: 0.013340642675757408 Engine time: 0.07104169577360153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 102.32106903707609,
    "estimated_duration": 3600.0772794203076,
    "input_throughput": 8538.346433760336,
    "output_throughput": 7592.280909147925,
    "total_throughput": 16130.627342908261,
    "itl": 113.04836414926308,
    "ttft": 1365093.614571109,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5322691202908755,
    "arrivals": 255515,
    "finished_requests": 124340,
    "scheduler_time": 211.14886470139402
}
#Debug simulation 
Total elapsed time: 102.32119994005188. Arrivals time: 0.450268822722137 Scheduler time: 101.68656758638099 Scheduler overhead time: 0.07231975579634309 Adapter cache time: 0.013503999449312687 Engine time: 0.07063040789216757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 105.48031220305711,
    "estimated_duration": 3600.003274432393,
    "input_throughput": 8545.443894033806,
    "output_throughput": 7594.122259322244,
    "total_throughput": 16139.566153356049,
    "itl": 113.06489930597698,
    "ttft": 1364724.341028305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.514161940980702,
    "arrivals": 255262,
    "finished_requests": 124287,
    "scheduler_time": 211.20806275558664
}
#Debug simulation 
Total elapsed time: 105.48045612173155. Arrivals time: 0.4458321174606681 Scheduler time: 104.84828307060525 Scheduler overhead time: 0.0734712784178555 Adapter cache time: 0.01349428715184331 Engine time: 0.07172882510349154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.4407034991309,
    "estimated_duration": 3600.035905648723,
    "input_throughput": 8545.36643696514,
    "output_throughput": 7594.053425162592,
    "total_throughput": 16139.419862127732,
    "itl": 113.06553896937359,
    "ttft": 1364735.011132404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5464567439351236,
    "arrivals": 255262,
    "finished_requests": 124287,
    "scheduler_time": 211.2084040688577
}
#Debug simulation 
Total elapsed time: 105.4408445423469. Arrivals time: 0.4314309526234865 Scheduler time: 104.82281796261668 Scheduler overhead time: 0.07324789417907596 Adapter cache time: 0.013828136958181858 Engine time: 0.07140052458271384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.99241934390739,
    "estimated_duration": 3600.035725312997,
    "input_throughput": 8545.366865026132,
    "output_throughput": 7594.053805569689,
    "total_throughput": 16139.42067059582,
    "itl": 113.06549740835027,
    "ttft": 1364734.5724906365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5477338960021754,
    "arrivals": 255262,
    "finished_requests": 124287,
    "scheduler_time": 211.2083471211752
}
#Debug simulation 
Total elapsed time: 104.99254779098555. Arrivals time: 0.41188749810680747 Scheduler time: 104.39433211786672 Scheduler overhead time: 0.07349569676443934 Adapter cache time: 0.013286169152706861 Engine time: 0.07156866928562522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 105.41319824522361,
    "estimated_duration": 3600.0122517077984,
    "input_throughput": 8545.422584438746,
    "output_throughput": 7594.103322018085,
    "total_throughput": 16139.52590645683,
    "itl": 113.06501753122673,
    "ttft": 1364727.626373091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5223496201471431,
    "arrivals": 255262,
    "finished_requests": 124287,
    "scheduler_time": 211.20825702023828
}
#Debug simulation 
Total elapsed time: 105.41332806600258. Arrivals time: 0.43752025347203016 Scheduler time: 104.78912155004218 Scheduler overhead time: 0.07360992021858692 Adapter cache time: 0.013521871995180845 Engine time: 0.07173646986484528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 105.19696968514472,
    "estimated_duration": 3600.0441835746565,
    "input_throughput": 8545.346787786732,
    "output_throughput": 7594.035963429185,
    "total_throughput": 16139.382751215917,
    "itl": 113.06565250388583,
    "ttft": 1364737.681486938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5551533694006512,
    "arrivals": 255262,
    "finished_requests": 124287,
    "scheduler_time": 211.20848556222657
}
#Debug simulation 
Total elapsed time: 105.19711306924. Arrivals time: 0.4407009813003242 Scheduler time: 104.56870039692149 Scheduler overhead time: 0.07396832900121808 Adapter cache time: 0.013285108841955662 Engine time: 0.07192263519391418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 105.40709966700524,
    "estimated_duration": 3600.1210031269457,
    "input_throughput": 8545.874422908984,
    "output_throughput": 7594.375849104183,
    "total_throughput": 16140.250272013167,
    "itl": 113.06456949556622,
    "ttft": 1364668.4556864034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5023284495435661,
    "arrivals": 255262,
    "finished_requests": 124296,
    "scheduler_time": 211.21588667207266
}
#Debug simulation 
Total elapsed time: 105.40722749894485. Arrivals time: 0.41683998005464673 Scheduler time: 104.80317104887217 Scheduler overhead time: 0.0738504589535296 Adapter cache time: 0.013488056138157845 Engine time: 0.07127795508131385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_64_slots_16_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 101.49520688690245,
    "estimated_duration": 3600.0815188291213,
    "input_throughput": 8544.485684314326,
    "output_throughput": 7596.10827059664,
    "total_throughput": 16140.593954910964,
    "itl": 113.0908984515799,
    "ttft": 1367613.1301435498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5949915023893126,
    "arrivals": 255262,
    "finished_requests": 124241,
    "scheduler_time": 211.1940346785434
}
#Debug simulation 
Total elapsed time: 101.49533527018502. Arrivals time: 0.42447682190686464 Scheduler time: 100.88910570926964 Scheduler overhead time: 0.07143802661448717 Adapter cache time: 0.012939856853336096 Engine time: 0.06981176929548383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 103.73163444502279,
    "estimated_duration": 3600.040641269269,
    "input_throughput": 8546.72962501665,
    "output_throughput": 7593.594274080663,
    "total_throughput": 16140.323899097313,
    "itl": 113.03715133825398,
    "ttft": 1359701.0859944033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6335209629940792,
    "arrivals": 254534,
    "finished_requests": 124227,
    "scheduler_time": 211.27227698626746
}
#Debug simulation 
Total elapsed time: 103.7317751343362. Arrivals time: 0.4426883067935705 Scheduler time: 103.09979793103412 Scheduler overhead time: 0.07509862165898085 Adapter cache time: 0.013949950225651264 Engine time: 0.07190935499966145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.66064436221495,
    "estimated_duration": 3600.0904523304825,
    "input_throughput": 8544.846416313341,
    "output_throughput": 7592.372014515943,
    "total_throughput": 16137.218430829285,
    "itl": 113.04849361562668,
    "ttft": 1359362.653223054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6707021334208566,
    "arrivals": 254534,
    "finished_requests": 124253,
    "scheduler_time": 211.26175664867614
}
#Debug simulation 
Total elapsed time: 104.66079007601365. Arrivals time: 0.4374927654862404 Scheduler time: 104.03593719191849 Scheduler overhead time: 0.07328836247324944 Adapter cache time: 0.013676705304533243 Engine time: 0.07230346882715821 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 105.44451204128563,
    "estimated_duration": 3600.0920260257008,
    "input_throughput": 8544.842681135504,
    "output_throughput": 7592.3686956898,
    "total_throughput": 16137.211376825306,
    "itl": 113.04852357717687,
    "ttft": 1359363.2018510676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6721560471132426,
    "arrivals": 254534,
    "finished_requests": 124253,
    "scheduler_time": 211.261776391604
}
#Debug simulation 
Total elapsed time: 105.44464461691678. Arrivals time: 0.4172809519805014 Scheduler time: 104.83901395648718 Scheduler overhead time: 0.07374827889725566 Adapter cache time: 0.013762473594397306 Engine time: 0.07194963563233614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 104.79419047292322,
    "estimated_duration": 3600.0655317434494,
    "input_throughput": 8544.905565955736,
    "output_throughput": 7592.424570883572,
    "total_throughput": 16137.33013683931,
    "itl": 113.04785247957298,
    "ttft": 1359355.7404611197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6433262470853528,
    "arrivals": 254534,
    "finished_requests": 124253,
    "scheduler_time": 211.26169866564734
}
#Debug simulation 
Total elapsed time: 104.79436595877632. Arrivals time: 0.4414022141136229 Scheduler time: 104.16698040347546 Scheduler overhead time: 0.07268981775268912 Adapter cache time: 0.013469072990119457 Engine time: 0.07159073604270816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 104.51073830295354,
    "estimated_duration": 3600.1020109622477,
    "input_throughput": 8544.818981887063,
    "output_throughput": 7592.347638142142,
    "total_throughput": 16137.166620029206,
    "itl": 113.04880549135027,
    "ttft": 1359366.3659443678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6805815508030376,
    "arrivals": 254534,
    "finished_requests": 124253,
    "scheduler_time": 211.26183606242972
}
#Debug simulation 
Total elapsed time: 104.51087513798848. Arrivals time: 0.4433077499270439 Scheduler time: 103.87963288510218 Scheduler overhead time: 0.07422932051122189 Adapter cache time: 0.014244277495890856 Engine time: 0.07161614298820496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 104.15939764305949,
    "estimated_duration": 3600.025708705102,
    "input_throughput": 8546.76507603808,
    "output_throughput": 7593.625771587329,
    "total_throughput": 16140.39084762541,
    "itl": 113.03684364597692,
    "ttft": 1359696.7679945265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 207,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6189404110447512,
    "arrivals": 254534,
    "finished_requests": 124227,
    "scheduler_time": 211.27212505117203
}
#Debug simulation 
Total elapsed time: 104.1595492111519. Arrivals time: 0.41837066132575274 Scheduler time: 103.554094389081 Scheduler overhead time: 0.0734844347462058 Adapter cache time: 0.013678051065653563 Engine time: 0.0715106762945652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_64_slots_16_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 104.93877662112936,
    "estimated_duration": 3600.1083646240445,
    "input_throughput": 8544.803901538244,
    "output_throughput": 7592.3342387651655,
    "total_throughput": 16137.138140303408,
    "itl": 113.04908857641105,
    "ttft": 1359366.7833768162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6890070544928331,
    "arrivals": 254534,
    "finished_requests": 124253,
    "scheduler_time": 211.2620790590134
}
#Debug simulation 
Total elapsed time: 104.93890401301906. Arrivals time: 0.41724629839882255 Scheduler time: 104.33348745666444 Scheduler overhead time: 0.07434521429240704 Adapter cache time: 0.013923806138336658 Engine time: 0.07143315020948648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 114.11121607990935,
    "estimated_duration": 3600.0650010755835,
    "input_throughput": 8559.628226377414,
    "output_throughput": 7589.56129732013,
    "total_throughput": 16149.189523697543,
    "itl": 112.85315768930592,
    "ttft": 1350284.1001283783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6457629139698102,
    "arrivals": 254308,
    "finished_requests": 124374,
    "scheduler_time": 211.48760639014927
}
#Debug simulation 
Total elapsed time: 114.1113435588777. Arrivals time: 0.454143940936774 Scheduler time: 113.46323313238099 Scheduler overhead time: 0.07721918076276779 Adapter cache time: 0.014335083309561014 Engine time: 0.07341698184609413 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_64_slots_16_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 114.06142672616988,
    "estimated_duration": 3600.1051531371104,
    "input_throughput": 8559.532760632783,
    "output_throughput": 7589.476650755874,
    "total_throughput": 16149.009411388657,
    "itl": 112.85421663277025,
    "ttft": 1350294.4164705144,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6897383380797714,
    "arrivals": 254308,
    "finished_requests": 124374,
    "scheduler_time": 211.48786624466035
}
#Debug simulation 
Total elapsed time: 114.06157438317314. Arrivals time: 0.441630766261369 Scheduler time: 113.42678639385849 Scheduler overhead time: 0.07657928159460425 Adapter cache time: 0.01431217323988676 Engine time: 0.07324484921991825 
