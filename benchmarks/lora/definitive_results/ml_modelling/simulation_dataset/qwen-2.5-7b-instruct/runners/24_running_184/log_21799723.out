INFO 06-01 00:46:59 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:46:59 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.374467404093593,
    "estimated_duration": 3600.1268978922176,
    "input_throughput": 7982.217798162831,
    "output_throughput": 7055.365191396774,
    "total_throughput": 15037.582989559605,
    "itl": 122.11369086812078,
    "ttft": 1454713.2124922976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 256011,
    "finished_requests": 115700,
    "scheduler_time": 73.19026839354262
}
#Debug simulation 
Total elapsed time: 8.374574452172965. Arrivals time: 0.32333704456686974 Scheduler time: 7.920086494181305 Scheduler overhead time: 0.04565747454762459 Adapter cache time: 0.015193301253020763 Engine time: 0.04838032042607665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.466352364048362,
    "estimated_duration": 3600.1012073246343,
    "input_throughput": 8028.520404146979,
    "output_throughput": 7132.4725393156305,
    "total_throughput": 15160.992943462608,
    "itl": 121.24731767287908,
    "ttft": 1442216.9627724267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 255515,
    "finished_requests": 116841,
    "scheduler_time": 74.07667994216594
}
#Debug simulation 
Total elapsed time: 8.466487722005695. Arrivals time: 0.323055412620306 Scheduler time: 8.013481677975506 Scheduler overhead time: 0.04589585121721029 Adapter cache time: 0.014052637852728367 Engine time: 0.04806124046444893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.430052796378732,
    "estimated_duration": 3600.117601606702,
    "input_throughput": 8028.483843722388,
    "output_throughput": 7132.440059330366,
    "total_throughput": 15160.923903052753,
    "itl": 121.24733514967814,
    "ttft": 1442225.4074820548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 255515,
    "finished_requests": 116841,
    "scheduler_time": 74.07668080842735
}
#Debug simulation 
Total elapsed time: 8.430148107931018. Arrivals time: 0.3237071977928281 Scheduler time: 7.976597642991692 Scheduler overhead time: 0.04595101857557893 Adapter cache time: 0.014061248861253262 Engine time: 0.048006415367126465 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.446991330012679,
    "estimated_duration": 3600.009012664809,
    "input_throughput": 8028.344623116928,
    "output_throughput": 7132.588254548012,
    "total_throughput": 15160.93287766494,
    "itl": 121.24835090726765,
    "ttft": 1442186.9839602457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 255515,
    "finished_requests": 116838,
    "scheduler_time": 74.07427367697892
}
#Debug simulation 
Total elapsed time: 8.447113180998713. Arrivals time: 0.3310864884406328 Scheduler time: 7.986030459403992 Scheduler overhead time: 0.04601689614355564 Adapter cache time: 0.014037550892680883 Engine time: 0.04805103177204728 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.40871942602098,
    "estimated_duration": 3600.107489296927,
    "input_throughput": 8028.506394858957,
    "output_throughput": 7132.460093577551,
    "total_throughput": 15160.966488436508,
    "itl": 121.2472261545589,
    "ttft": 1442218.2621148298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 255515,
    "finished_requests": 116841,
    "scheduler_time": 74.07666807113638
}
#Debug simulation 
Total elapsed time: 8.408845557831228. Arrivals time: 0.3416080214083195 Scheduler time: 7.937478386797011 Scheduler overhead time: 0.045672120060771704 Adapter cache time: 0.014111000578850508 Engine time: 0.04795742407441139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.388322019949555,
    "estimated_duration": 3600.0103433031813,
    "input_throughput": 8028.341655674503,
    "output_throughput": 7132.58561819569,
    "total_throughput": 15160.927273870193,
    "itl": 121.24838406250264,
    "ttft": 1442187.0941550669,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 255515,
    "finished_requests": 116838,
    "scheduler_time": 74.07418974281387
}
#Debug simulation 
Total elapsed time: 8.388424723874778. Arrivals time: 0.3418828579597175 Scheduler time: 7.917370996437967 Scheduler overhead time: 0.04566626017913222 Adapter cache time: 0.01397726172581315 Engine time: 0.04773499583825469 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.432129296008497,
    "estimated_duration": 3600.0827143680854,
    "input_throughput": 8028.561645165802,
    "output_throughput": 7132.509177502922,
    "total_throughput": 15161.070822668724,
    "itl": 121.24692431737137,
    "ttft": 1442210.4846339975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 255515,
    "finished_requests": 116841,
    "scheduler_time": 74.07665600700157
}
#Debug simulation 
Total elapsed time: 8.432245212141424. Arrivals time: 0.32822604943066835 Scheduler time: 7.974189784377813 Scheduler overhead time: 0.04585718363523483 Adapter cache time: 0.014222963713109493 Engine time: 0.04788134898990393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 270, 34560, 34560, 66, 34560, 66, 270, 66, 34560, 270, 34560, 66, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 34560, 66, 66, 66, 34560, 270, 66, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 66, 270, 270, 34560, 34560, 270, 34560, 66, 34560, 270, 270, 270, 34560, 270, 66, 66, 270, 270, 66, 270, 66, 66, 66, 34560, 270]
Prompts retrieved: 767376 . Total input tokens: 171049717 . Total output tokens: 153661803
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.414745882153511,
    "estimated_duration": 3600.020703804365,
    "input_throughput": 8028.318550906484,
    "output_throughput": 7132.565091324369,
    "total_throughput": 15160.883642230852,
    "itl": 121.24860590380624,
    "ttft": 1442193.1164429623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2146942614018914,
    "arrivals": 255515,
    "finished_requests": 116838,
    "scheduler_time": 74.07425772014297
}
#Debug simulation 
Total elapsed time: 8.414876427967101. Arrivals time: 0.32141002640128136 Scheduler time: 7.963722236920148 Scheduler overhead time: 0.045963339041918516 Adapter cache time: 0.014040026813745499 Engine time: 0.047958437353372574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.509231346193701,
    "estimated_duration": 3600.09918566055,
    "input_throughput": 8079.683503126307,
    "output_throughput": 7172.416555312839,
    "total_throughput": 15252.100058439146,
    "itl": 120.47358856972262,
    "ttft": 1431600.7116402488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 255262,
    "finished_requests": 117397,
    "scheduler_time": 74.58039264130258
}
#Debug simulation 
Total elapsed time: 8.509329844266176. Arrivals time: 0.3275593309663236 Scheduler time: 8.051957961637527 Scheduler overhead time: 0.04635159345343709 Adapter cache time: 0.01336847897619009 Engine time: 0.048155090771615505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.478904083836824,
    "estimated_duration": 3600.013589656118,
    "input_throughput": 8079.702833226934,
    "output_throughput": 7172.470146832553,
    "total_throughput": 15252.172980059486,
    "itl": 120.47389116213377,
    "ttft": 1431613.8132346834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 255262,
    "finished_requests": 117395,
    "scheduler_time": 74.57797104565981
}
#Debug simulation 
Total elapsed time: 8.479077824857086. Arrivals time: 0.32408782048150897 Scheduler time: 8.02528796158731 Scheduler overhead time: 0.04614696232602 Adapter cache time: 0.013338507153093815 Engine time: 0.04819757770746946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.534518177621067,
    "estimated_duration": 3600.01177646234,
    "input_throughput": 8079.70690267665,
    "output_throughput": 7172.473759342469,
    "total_throughput": 15252.18066201912,
    "itl": 120.47384098330383,
    "ttft": 1431611.8228416455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 255262,
    "finished_requests": 117395,
    "scheduler_time": 74.57791560075937
}
#Debug simulation 
Total elapsed time: 8.534644037950784. Arrivals time: 0.3287013485096395 Scheduler time: 8.075668052770197 Scheduler overhead time: 0.04622148349881172 Adapter cache time: 0.013568537309765816 Engine time: 0.04843121487647295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.42264783475548,
    "estimated_duration": 3600.0013502013558,
    "input_throughput": 8079.7303029825525,
    "output_throughput": 7172.4945321356,
    "total_throughput": 15252.224835118153,
    "itl": 120.47377416938315,
    "ttft": 1431605.1474293973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 255262,
    "finished_requests": 117395,
    "scheduler_time": 74.57806799817583
}
#Debug simulation 
Total elapsed time: 8.422752493992448. Arrivals time: 0.3380244802683592 Scheduler time: 7.955357392784208 Scheduler overhead time: 0.046056884340941906 Adapter cache time: 0.013115921523422003 Engine time: 0.04819063423201442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.520096937660128,
    "estimated_duration": 3600.015027247535,
    "input_throughput": 8079.699606765001,
    "output_throughput": 7172.467282655197,
    "total_throughput": 15252.166889420198,
    "itl": 120.47380781236478,
    "ttft": 1431612.8458971945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 255262,
    "finished_requests": 117395,
    "scheduler_time": 74.57802519104789
}
#Debug simulation 
Total elapsed time: 8.520204847678542. Arrivals time: 0.33253450505435467 Scheduler time: 8.057499470654875 Scheduler overhead time: 0.046211596578359604 Adapter cache time: 0.013458025176078081 Engine time: 0.048414324410259724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.48266758909449,
    "estimated_duration": 3600.0771113661044,
    "input_throughput": 8079.733044651991,
    "output_throughput": 7172.460533824974,
    "total_throughput": 15252.193578476965,
    "itl": 120.47328295193175,
    "ttft": 1431598.1948027494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 255262,
    "finished_requests": 117397,
    "scheduler_time": 74.58024706922406
}
#Debug simulation 
Total elapsed time: 8.482790743932128. Arrivals time: 0.3326461371034384 Scheduler time: 8.01939996285364 Scheduler overhead time: 0.04667377891018987 Adapter cache time: 0.013398355804383755 Engine time: 0.048539447132498026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 270, 34560, 34560, 33, 34560, 33, 270, 33, 34560, 270, 34560, 33, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 34560, 33, 33, 33, 34560, 270, 33, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 33, 270, 270, 34560, 34560, 270, 34560, 33, 34560, 270, 270, 270, 34560, 270, 33, 33, 270, 270, 33, 270, 33, 33, 33, 34560, 270]
Prompts retrieved: 766683 . Total input tokens: 170902724 . Total output tokens: 153518561
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.395443457178771,
    "estimated_duration": 3600.0204247275688,
    "input_throughput": 8079.687492940032,
    "output_throughput": 7172.456529035943,
    "total_throughput": 15252.144021975975,
    "itl": 120.47389013739522,
    "ttft": 1431615.7827765478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 255262,
    "finished_requests": 117395,
    "scheduler_time": 74.57801232491165
}
#Debug simulation 
Total elapsed time: 8.395543355029076. Arrivals time: 0.3253675987944007 Scheduler time: 7.941163228359073 Scheduler overhead time: 0.04598731221631169 Adapter cache time: 0.013093626126646996 Engine time: 0.048024913761764765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.635628234129399,
    "estimated_duration": 3600.102018393894,
    "input_throughput": 8176.378571941728,
    "output_throughput": 7260.594246065639,
    "total_throughput": 15436.972818007367,
    "itl": 119.05903709953469,
    "ttft": 1422098.8361112531,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 254534,
    "finished_requests": 118824,
    "scheduler_time": 75.41739673734543
}
#Debug simulation 
Total elapsed time: 8.635749496053904. Arrivals time: 0.3332937345840037 Scheduler time: 8.171745837666094 Scheduler overhead time: 0.04684460489079356 Adapter cache time: 0.012614010367542505 Engine time: 0.048995216842740774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.531563833821565,
    "estimated_duration": 3600.053371492081,
    "input_throughput": 8176.1348965225325,
    "output_throughput": 7260.46374950458,
    "total_throughput": 15436.598646027112,
    "itl": 119.06020157580518,
    "ttft": 1422098.8065600072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 254534,
    "finished_requests": 118821,
    "scheduler_time": 75.41594735308892
}
#Debug simulation 
Total elapsed time: 8.531686077825725. Arrivals time: 0.33294314704835415 Scheduler time: 8.068704251665622 Scheduler overhead time: 0.04656027164310217 Adapter cache time: 0.012545616831630468 Engine time: 0.048706948291510344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.607548925094306,
    "estimated_duration": 3600.0537197046624,
    "input_throughput": 8176.134105691823,
    "output_throughput": 7260.463047241497,
    "total_throughput": 15436.59715293332,
    "itl": 119.06020587531607,
    "ttft": 1422099.1033185436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 254534,
    "finished_requests": 118821,
    "scheduler_time": 75.41594153465057
}
#Debug simulation 
Total elapsed time: 8.60764595726505. Arrivals time: 0.332101393956691 Scheduler time: 8.144240051973611 Scheduler overhead time: 0.04695679759606719 Adapter cache time: 0.012621745001524687 Engine time: 0.0494031491689384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.638873565010726,
    "estimated_duration": 3600.040087137298,
    "input_throughput": 8176.097289907049,
    "output_throughput": 7260.380820032564,
    "total_throughput": 15436.478109939613,
    "itl": 119.05989218042234,
    "ttft": 1422095.7293342857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 254534,
    "finished_requests": 118820,
    "scheduler_time": 75.41558196238007
}
#Debug simulation 
Total elapsed time: 8.638990645296872. Arrivals time: 0.33448151173070073 Scheduler time: 8.174238783773035 Scheduler overhead time: 0.046700093895196915 Adapter cache time: 0.012475183233618736 Engine time: 0.04876809474080801 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.575984239112586,
    "estimated_duration": 3600.0574332167407,
    "input_throughput": 8176.1256718894965,
    "output_throughput": 7260.455557967306,
    "total_throughput": 15436.581229856803,
    "itl": 119.06018059360939,
    "ttft": 1422101.6806883335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 254534,
    "finished_requests": 118821,
    "scheduler_time": 75.41593134944137
}
#Debug simulation 
Total elapsed time: 8.57608236093074. Arrivals time: 0.3284407230094075 Scheduler time: 8.117486955132335 Scheduler overhead time: 0.04654512694105506 Adapter cache time: 0.012266093399375677 Engine time: 0.049182789865881205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.568951373919845,
    "estimated_duration": 3600.0985806056055,
    "input_throughput": 8176.386379688618,
    "output_throughput": 7260.601179316301,
    "total_throughput": 15436.98755900492,
    "itl": 119.05901560985004,
    "ttft": 1422097.9613008997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 254534,
    "finished_requests": 118824,
    "scheduler_time": 75.41740378505528
}
#Debug simulation 
Total elapsed time: 8.569074979051948. Arrivals time: 0.3326302976347506 Scheduler time: 8.106303135398775 Scheduler overhead time: 0.04651907505467534 Adapter cache time: 0.012442569714039564 Engine time: 0.0489807091653347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 135, 34560, 34560, 66, 34560, 66, 135, 66, 34560, 135, 34560, 66, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 34560, 66, 66, 66, 34560, 135, 66, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 66, 135, 135, 34560, 34560, 135, 34560, 66, 34560, 135, 135, 135, 34560, 135, 66, 66, 135, 135, 66, 135, 66, 66, 66, 34560, 135]
Prompts retrieved: 764541 . Total input tokens: 170414045 . Total output tokens: 153097094
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.600822870619595,
    "estimated_duration": 3600.066268038536,
    "input_throughput": 8176.105607088488,
    "output_throughput": 7260.437740286678,
    "total_throughput": 15436.543347375165,
    "itl": 119.06029878400405,
    "ttft": 1422137.1065420113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 254534,
    "finished_requests": 118821,
    "scheduler_time": 75.41590657131262
}
#Debug simulation 
Total elapsed time: 8.600943708792329. Arrivals time: 0.3263633851893246 Scheduler time: 8.143670979887247 Scheduler overhead time: 0.04674613522365689 Adapter cache time: 0.012757622636854649 Engine time: 0.04918571002781391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.542273766826838,
    "estimated_duration": 3600.1164683315706,
    "input_throughput": 8260.379146506297,
    "output_throughput": 7334.865200135512,
    "total_throughput": 15595.244346641808,
    "itl": 117.81640066858351,
    "ttft": 1411098.2953599966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 254308,
    "finished_requests": 120015,
    "scheduler_time": 76.20946968971154
}
#Debug simulation 
Total elapsed time: 8.542438440956175. Arrivals time: 0.3310372014530003 Scheduler time: 8.081738675478846 Scheduler overhead time: 0.046858133748173714 Adapter cache time: 0.011503024492412806 Engine time: 0.04892859933897853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.68968790397048,
    "estimated_duration": 3600.00128325561,
    "input_throughput": 8260.603999870438,
    "output_throughput": 7335.0304409114,
    "total_throughput": 15595.634440781838,
    "itl": 117.81633667306217,
    "ttft": 1411056.9910554134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 254308,
    "finished_requests": 120014,
    "scheduler_time": 76.20684217459944
}
#Debug simulation 
Total elapsed time: 8.689786832779646. Arrivals time: 0.3400489594787359 Scheduler time: 8.218903772067279 Scheduler overhead time: 0.04717734642326832 Adapter cache time: 0.01161108585074544 Engine time: 0.049579580780118704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.639674122910947,
    "estimated_duration": 3600.001737673739,
    "input_throughput": 8260.602957157547,
    "output_throughput": 7335.029515031066,
    "total_throughput": 15595.632472188614,
    "itl": 117.81633961765762,
    "ttft": 1411057.3369705298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 254308,
    "finished_requests": 120014,
    "scheduler_time": 76.20684007316979
}
#Debug simulation 
Total elapsed time: 8.639796795323491. Arrivals time: 0.3320623473264277 Scheduler time: 8.176293767988682 Scheduler overhead time: 0.04717184277251363 Adapter cache time: 0.011643147096037865 Engine time: 0.0498407119885087 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.69593175733462,
    "estimated_duration": 3600.1250722266295,
    "input_throughput": 8260.359405126788,
    "output_throughput": 7334.847670630513,
    "total_throughput": 15595.207075757302,
    "itl": 117.81645398088286,
    "ttft": 1411101.6544952574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 254308,
    "finished_requests": 120015,
    "scheduler_time": 76.20950613393477
}
#Debug simulation 
Total elapsed time: 8.696031976025552. Arrivals time: 0.3366342759691179 Scheduler time: 8.228560447692871 Scheduler overhead time: 0.04722861433401704 Adapter cache time: 0.011742531321942806 Engine time: 0.04937428794801235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.681565640028566,
    "estimated_duration": 3600.0027128679003,
    "input_throughput": 8260.600719467075,
    "output_throughput": 7335.027528066465,
    "total_throughput": 15595.62824753354,
    "itl": 117.81619917438205,
    "ttft": 1411059.1498541618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 254308,
    "finished_requests": 120014,
    "scheduler_time": 76.20677424990629
}
#Debug simulation 
Total elapsed time: 8.681683151051402. Arrivals time: 0.3391894195228815 Scheduler time: 8.211203104816377 Scheduler overhead time: 0.047286842949688435 Adapter cache time: 0.011611904948949814 Engine time: 0.049921647645533085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.630661567673087,
    "estimated_duration": 3600.1122672999963,
    "input_throughput": 8260.388785681698,
    "output_throughput": 7334.873759313119,
    "total_throughput": 15595.262544994817,
    "itl": 117.81635465766142,
    "ttft": 1411095.3822578476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 254308,
    "finished_requests": 120015,
    "scheduler_time": 76.20956508515705
}
#Debug simulation 
Total elapsed time: 8.630790021736175. Arrivals time: 0.3321809167973697 Scheduler time: 8.168436643201858 Scheduler overhead time: 0.046976867597550154 Adapter cache time: 0.011556173674762249 Engine time: 0.04927708813920617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 135, 34560, 34560, 33, 34560, 33, 135, 33, 34560, 135, 34560, 33, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 34560, 33, 33, 33, 34560, 135, 33, 34560, 34560, 34560, 34560, 135, 135, 135, 34560, 33, 135, 135, 34560, 34560, 135, 34560, 33, 34560, 135, 135, 135, 34560, 135, 33, 33, 135, 135, 33, 135, 33, 33, 33, 34560, 135]
Prompts retrieved: 763848 . Total input tokens: 170252183 . Total output tokens: 152964575
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.677418337669224,
    "estimated_duration": 3600.056477421478,
    "input_throughput": 8260.51679647535,
    "output_throughput": 7334.987427450978,
    "total_throughput": 15595.504223926328,
    "itl": 117.81718248979776,
    "ttft": 1411093.8024562162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 254308,
    "finished_requests": 120015,
    "scheduler_time": 76.20785654321948
}
#Debug simulation 
Total elapsed time: 8.677521329838783. Arrivals time: 0.3312092390842736 Scheduler time: 8.215459013357759 Scheduler overhead time: 0.047140251379460096 Adapter cache time: 0.011682348791509867 Engine time: 0.04954145010560751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.732689068187028,
    "estimated_duration": 3600.0747215209562,
    "input_throughput": 8338.403872716744,
    "output_throughput": 7403.688829197221,
    "total_throughput": 15742.092701913967,
    "itl": 116.80206526263012,
    "ttft": 1397139.3372213766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 253785,
    "finished_requests": 121077,
    "scheduler_time": 76.98020978565303
}
#Debug simulation 
Total elapsed time: 8.732818196061999. Arrivals time: 0.35570042906329036 Scheduler time: 8.246323148254305 Scheduler overhead time: 0.04744861740618944 Adapter cache time: 0.010727086570113897 Engine time: 0.050045250449329615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.75243136100471,
    "estimated_duration": 3600.1153185271787,
    "input_throughput": 8338.30984399712,
    "output_throughput": 7403.605340871189,
    "total_throughput": 15741.915184868309,
    "itl": 116.80246038294108,
    "ttft": 1397173.9859360124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 253785,
    "finished_requests": 121077,
    "scheduler_time": 76.98094220600694
}
#Debug simulation 
Total elapsed time: 8.752531312871724. Arrivals time: 0.33392259245738387 Scheduler time: 8.288268785458058 Scheduler overhead time: 0.0474448804743588 Adapter cache time: 0.010636024177074432 Engine time: 0.04967287788167596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.761210919823498,
    "estimated_duration": 3600.1156697998968,
    "input_throughput": 8338.525967880518,
    "output_throughput": 7403.747391672422,
    "total_throughput": 15742.273359552939,
    "itl": 116.80245251300953,
    "ttft": 1397162.3987434274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 253785,
    "finished_requests": 121078,
    "scheduler_time": 76.9809610310394
}
#Debug simulation 
Total elapsed time: 8.761339535936713. Arrivals time: 0.33202613750472665 Scheduler time: 8.298868370708078 Scheduler overhead time: 0.047413336113095284 Adapter cache time: 0.010672289412468672 Engine time: 0.04980387631803751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.733841300942004,
    "estimated_duration": 3600.084774013143,
    "input_throughput": 8338.380589448421,
    "output_throughput": 7403.668155927346,
    "total_throughput": 15742.048745375767,
    "itl": 116.80212572988427,
    "ttft": 1397143.577577817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 253785,
    "finished_requests": 121077,
    "scheduler_time": 76.98026331055696
}
#Debug simulation 
Total elapsed time: 8.733937280718237. Arrivals time: 0.3296132870018482 Scheduler time: 8.273374668788165 Scheduler overhead time: 0.04773885849863291 Adapter cache time: 0.010696308221668005 Engine time: 0.04976599244400859 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.799941145349294,
    "estimated_duration": 3600.120163028084,
    "input_throughput": 8338.515560755694,
    "output_throughput": 7403.738151223502,
    "total_throughput": 15742.253711979196,
    "itl": 116.80249690949478,
    "ttft": 1397165.7411696676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 253785,
    "finished_requests": 121078,
    "scheduler_time": 76.98101682713285
}
#Debug simulation 
Total elapsed time: 8.800039662979543. Arrivals time: 0.34166307048872113 Scheduler time: 8.327606969047338 Scheduler overhead time: 0.047524608206003904 Adapter cache time: 0.010954745579510927 Engine time: 0.049721842631697655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.739056122023612,
    "estimated_duration": 3600.0467962946955,
    "input_throughput": 8338.372443073853,
    "output_throughput": 7403.436818496912,
    "total_throughput": 15741.809261570765,
    "itl": 116.80107481588344,
    "ttft": 1397142.964619562,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 253785,
    "finished_requests": 121075,
    "scheduler_time": 76.97991990585913
}
#Debug simulation 
Total elapsed time: 8.739176594186574. Arrivals time: 0.35887778038159013 Scheduler time: 8.25024738907814 Scheduler overhead time: 0.047409721184521914 Adapter cache time: 0.010630177333950996 Engine time: 0.0495478268712759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 66, 34560, 34560, 33, 34560, 33, 66, 33, 34560, 66, 34560, 33, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 34560, 33, 33, 33, 34560, 66, 33, 34560, 34560, 34560, 34560, 66, 66, 66, 34560, 33, 66, 66, 34560, 34560, 66, 34560, 33, 34560, 66, 66, 66, 34560, 66, 33, 33, 66, 66, 33, 66, 33, 33, 33, 34560, 66]
Prompts retrieved: 762399 . Total input tokens: 169942585 . Total output tokens: 152667374
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.795029393862933,
    "estimated_duration": 3600.1257902981674,
    "input_throughput": 8338.502804790533,
    "output_throughput": 7403.863518277902,
    "total_throughput": 15742.366323068434,
    "itl": 116.80268906242146,
    "ttft": 1397151.928006177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2146942614018914,
    "arrivals": 253785,
    "finished_requests": 121079,
    "scheduler_time": 76.98125650099246
}
#Debug simulation 
Total elapsed time: 8.795183501206338. Arrivals time: 0.33684041118249297 Scheduler time: 8.32796002458781 Scheduler overhead time: 0.047550381161272526 Adapter cache time: 0.01058737188577652 Engine time: 0.049636180978268385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.844551808666438,
    "estimated_duration": 3600.024748496378,
    "input_throughput": 6474.699933586707,
    "output_throughput": 5702.539408534473,
    "total_throughput": 12177.23934212118,
    "itl": 150.41148819108196,
    "ttft": 1563323.8543313392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 216813,
    "finished_requests": 94100,
    "scheduler_time": 59.49001107809742
}
#Debug simulation 
Total elapsed time: 6.844660687725991. Arrivals time: 0.2912190412171185 Scheduler time: 6.45333004463464 Scheduler overhead time: 0.03687711386010051 Adapter cache time: 0.0065847355872392654 Engine time: 0.03904173243790865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.808061819989234,
    "estimated_duration": 3600.1088906561713,
    "input_throughput": 6474.627770426004,
    "output_throughput": 5702.53973519622,
    "total_throughput": 12177.167505622223,
    "itl": 150.41272013348308,
    "ttft": 1563364.274838646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 216813,
    "finished_requests": 94101,
    "scheduler_time": 59.49133418082275
}
#Debug simulation 
Total elapsed time: 6.808162059634924. Arrivals time: 0.2919250251725316 Scheduler time: 6.415662236511707 Scheduler overhead time: 0.03715474111959338 Adapter cache time: 0.006749989464879036 Engine time: 0.038973198272287846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.820541904773563,
    "estimated_duration": 3600.110607974442,
    "input_throughput": 6474.62468191074,
    "output_throughput": 5702.537014980998,
    "total_throughput": 12177.161696891739,
    "itl": 150.41245105339837,
    "ttft": 1563363.9080325887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 216813,
    "finished_requests": 94101,
    "scheduler_time": 59.491329117490274
}
#Debug simulation 
Total elapsed time: 6.820665417704731. Arrivals time: 0.2891026991419494 Scheduler time: 6.431763545144349 Scheduler overhead time: 0.03688144497573376 Adapter cache time: 0.006661219522356987 Engine time: 0.038615216966718435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.78635405190289,
    "estimated_duration": 3600.0528329589056,
    "input_throughput": 6474.728589147368,
    "output_throughput": 5702.628531461429,
    "total_throughput": 12177.357120608796,
    "itl": 150.41210302856427,
    "ttft": 1563329.3096970194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 216813,
    "finished_requests": 94101,
    "scheduler_time": 59.49046385068856
}
#Debug simulation 
Total elapsed time: 6.786454290151596. Arrivals time: 0.2874486055225134 Scheduler time: 6.396324238739908 Scheduler overhead time: 0.03694395488128066 Adapter cache time: 0.006722817663103342 Engine time: 0.03903644345700741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.095083053223789,
    "estimated_duration": 3600.1160396841346,
    "input_throughput": 6474.614913258492,
    "output_throughput": 5702.528411223442,
    "total_throughput": 12177.143324481935,
    "itl": 150.41223525845126,
    "ttft": 1563366.0268849456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 216813,
    "finished_requests": 94101,
    "scheduler_time": 59.49136519293938
}
#Debug simulation 
Total elapsed time: 7.095184332225472. Arrivals time: 0.5296917264349759 Scheduler time: 6.465465524233878 Scheduler overhead time: 0.03696508100256324 Adapter cache time: 0.0066955722868442535 Engine time: 0.03864451264962554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.8456866280175745,
    "estimated_duration": 3600.0166755083583,
    "input_throughput": 6474.617509017115,
    "output_throughput": 5702.466085688646,
    "total_throughput": 12177.08359470576,
    "itl": 150.4118268482467,
    "ttft": 1563335.1168736066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 216813,
    "finished_requests": 94099,
    "scheduler_time": 59.49001740582812
}
#Debug simulation 
Total elapsed time: 6.8458124259486794. Arrivals time: 0.30002910690382123 Scheduler time: 6.445514497812837 Scheduler overhead time: 0.03702012216672301 Adapter cache time: 0.006673439871519804 Engine time: 0.03895780770108104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 8640, 17280, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 4320, 4320, 17280, 8640, 4320, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 4320, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 17280, 8640]
Prompts retrieved: 652320 . Total input tokens: 145550749 . Total output tokens: 130477523
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.8587226159870625,
    "estimated_duration": 3600.1611492156785,
    "input_throughput": 6474.833218251452,
    "output_throughput": 5702.680838182128,
    "total_throughput": 12177.514056433582,
    "itl": 150.41259007246236,
    "ttft": 1563326.7431922276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 216813,
    "finished_requests": 94105,
    "scheduler_time": 59.492367140382235
}
#Debug simulation 
Total elapsed time: 6.8588181170634925. Arrivals time: 0.30749179143458605 Scheduler time: 6.450712690595537 Scheduler overhead time: 0.037115057464689016 Adapter cache time: 0.006639778148382902 Engine time: 0.03917609015479684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.985367767047137,
    "estimated_duration": 3600.0572200056845,
    "input_throughput": 6550.776434593104,
    "output_throughput": 5827.907924187625,
    "total_throughput": 12378.684358780729,
    "itl": 148.1267343192619,
    "ttft": 1475936.7333009203,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 194341,
    "finished_requests": 95392,
    "scheduler_time": 61.523926906392646
}
#Debug simulation 
Total elapsed time: 6.985479454975575. Arrivals time: 0.29427767591550946 Scheduler time: 6.582759373355657 Scheduler overhead time: 0.037803792860358953 Adapter cache time: 0.012803460005670786 Engine time: 0.03971697390079498 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.959516274742782,
    "estimated_duration": 3600.0546980160493,
    "input_throughput": 6550.781023687342,
    "output_throughput": 5827.912006882088,
    "total_throughput": 12378.69303056943,
    "itl": 148.12485548041826,
    "ttft": 1475934.8310510518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 194341,
    "finished_requests": 95392,
    "scheduler_time": 61.5241058202854
}
#Debug simulation 
Total elapsed time: 6.959642388857901. Arrivals time: 0.29551648208871484 Scheduler time: 6.556497727986425 Scheduler overhead time: 0.03753047529608011 Adapter cache time: 0.012544055469334126 Engine time: 0.039445390459150076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.963735037948936,
    "estimated_duration": 3600.0562910292783,
    "input_throughput": 6550.778124987992,
    "output_throughput": 5827.909428049932,
    "total_throughput": 12378.687553037924,
    "itl": 148.12478713053886,
    "ttft": 1475935.748491887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2092868485860526,
    "arrivals": 194341,
    "finished_requests": 95392,
    "scheduler_time": 61.52405639869793
}
#Debug simulation 
Total elapsed time: 6.9638300482183695. Arrivals time: 0.29271415807306767 Scheduler time: 6.563043551985174 Scheduler overhead time: 0.03754725493490696 Adapter cache time: 0.012697737663984299 Engine time: 0.0397205394692719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.976580748800188,
    "estimated_duration": 3600.0525311580104,
    "input_throughput": 6550.78496657773,
    "output_throughput": 5827.9155146803405,
    "total_throughput": 12378.70048125807,
    "itl": 148.12615094225853,
    "ttft": 1475935.0451458672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942695,
    "arrivals": 194341,
    "finished_requests": 95392,
    "scheduler_time": 61.523961454060625
}
#Debug simulation 
Total elapsed time: 6.97667837375775. Arrivals time: 0.29446658631786704 Scheduler time: 6.573583726771176 Scheduler overhead time: 0.03767468500882387 Adapter cache time: 0.012499637436121702 Engine time: 0.040439301170408726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.9019300397485495,
    "estimated_duration": 3600.139984650051,
    "input_throughput": 6550.858605652932,
    "output_throughput": 5827.774500285001,
    "total_throughput": 12378.633105937934,
    "itl": 148.12647125051416,
    "ttft": 1475978.3860218348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076463,
    "arrivals": 194341,
    "finished_requests": 95393,
    "scheduler_time": 61.52535191950319
}
#Debug simulation 
Total elapsed time: 6.9020544569939375. Arrivals time: 0.2926342017017305 Scheduler time: 6.501720387488604 Scheduler overhead time: 0.0374828283675015 Adapter cache time: 0.012560312170535326 Engine time: 0.039556768257170916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.2251960639841855,
    "estimated_duration": 3600.0392746492794,
    "input_throughput": 6550.809088686263,
    "output_throughput": 5827.936974949802,
    "total_throughput": 12378.746063636065,
    "itl": 148.1266404057818,
    "ttft": 1475930.9425571086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 194341,
    "finished_requests": 95392,
    "scheduler_time": 61.52392463526296
}
#Debug simulation 
Total elapsed time: 7.225322497077286. Arrivals time: 0.5243878229521215 Scheduler time: 6.593343714252114 Scheduler overhead time: 0.037452539429068565 Adapter cache time: 0.012598130851984024 Engine time: 0.039418188855051994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 8640, 17280, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 8640, 1080, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 1080, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 17280, 8640]
Prompts retrieved: 584280 . Total input tokens: 130406296 . Total output tokens: 116914674
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.94968929560855,
    "estimated_duration": 3600.1511633184614,
    "input_throughput": 6550.838264874772,
    "output_throughput": 5827.756404722966,
    "total_throughput": 12378.594669597738,
    "itl": 148.1261113843865,
    "ttft": 1475980.0767449941,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 194341,
    "finished_requests": 95393,
    "scheduler_time": 61.52533286029432
}
#Debug simulation 
Total elapsed time: 6.949789274018258. Arrivals time: 0.29660432506352663 Scheduler time: 6.545525377150625 Scheduler overhead time: 0.03747203154489398 Adapter cache time: 0.01251101354137063 Engine time: 0.039538920391350985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.147198260761797,
    "estimated_duration": 3600.0655687970884,
    "input_throughput": 6784.110326124056,
    "output_throughput": 5993.5644469969275,
    "total_throughput": 12777.674773120983,
    "itl": 143.29534029599697,
    "ttft": 1421891.0244460593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 190487,
    "finished_requests": 98470,
    "scheduler_time": 63.457106203300036
}
#Debug simulation 
Total elapsed time: 7.147326854988933. Arrivals time: 0.3045303947292268 Scheduler time: 6.7304742988199 Scheduler overhead time: 0.03874863963574171 Adapter cache time: 0.0140857994556427 Engine time: 0.04069086862728 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.153419123031199,
    "estimated_duration": 3600.147167557913,
    "input_throughput": 6784.112110774458,
    "output_throughput": 5993.643591697114,
    "total_throughput": 12777.755702471573,
    "itl": 143.2964616784301,
    "ttft": 1421927.727179179,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 190487,
    "finished_requests": 98472,
    "scheduler_time": 63.45798375666872
}
#Debug simulation 
Total elapsed time: 7.15351900132373. Arrivals time: 0.29826299054548144 Scheduler time: 6.7434227261692286 Scheduler overhead time: 0.038663285318762064 Adapter cache time: 0.013851149007678032 Engine time: 0.040699741803109646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.3300359370186925,
    "estimated_duration": 3600.14763650621,
    "input_throughput": 6784.111227089082,
    "output_throughput": 5993.642810976644,
    "total_throughput": 12777.754038065725,
    "itl": 143.29645474344278,
    "ttft": 1421928.095996085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 190487,
    "finished_requests": 98472,
    "scheduler_time": 63.45797985234269
}
#Debug simulation 
Total elapsed time: 7.330101951956749. Arrivals time: 0.5296488422900438 Scheduler time: 6.688382305204868 Scheduler overhead time: 0.03869171580299735 Adapter cache time: 0.014049788936972618 Engine time: 0.04077220614999533 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.120312203653157,
    "estimated_duration": 3600.085778000606,
    "input_throughput": 6784.072243290835,
    "output_throughput": 5993.530801919789,
    "total_throughput": 12777.603045210624,
    "itl": 143.295312589392,
    "ttft": 1421929.396513362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 190487,
    "finished_requests": 98470,
    "scheduler_time": 63.457390598328416
}
#Debug simulation 
Total elapsed time: 7.1204408667981625. Arrivals time: 0.29882920859381557 Scheduler time: 6.708951812237501 Scheduler overhead time: 0.038812847808003426 Adapter cache time: 0.01402039872482419 Engine time: 0.040956652257591486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.16952965175733,
    "estimated_duration": 3600.026180012774,
    "input_throughput": 6784.136497561065,
    "output_throughput": 5993.6086353470455,
    "total_throughput": 12777.74513290811,
    "itl": 143.29653361052385,
    "ttft": 1421898.2200737866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 190487,
    "finished_requests": 98469,
    "scheduler_time": 63.45619946120646
}
#Debug simulation 
Total elapsed time: 7.169629151001573. Arrivals time: 0.29656161461025476 Scheduler time: 6.760983934160322 Scheduler overhead time: 0.038953065406531096 Adapter cache time: 0.013878833036869764 Engine time: 0.04055751534178853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.11332621704787,
    "estimated_duration": 3600.057190063066,
    "input_throughput": 6784.07806059663,
    "output_throughput": 5993.5570078046485,
    "total_throughput": 12777.635068401278,
    "itl": 143.2954903584997,
    "ttft": 1421902.5609831675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 190487,
    "finished_requests": 98469,
    "scheduler_time": 63.457096685458126
}
#Debug simulation 
Total elapsed time: 7.113419510889798. Arrivals time: 0.29960703989490867 Scheduler time: 6.702202964574099 Scheduler overhead time: 0.038600075989961624 Adapter cache time: 0.014026936143636703 Engine time: 0.04044378874823451 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 17280, 8640, 17280, 540, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 540, 540, 17280, 8640, 540, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 540, 8640, 8640, 17280, 17280, 8640, 17280, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 17280, 8640]
Prompts retrieved: 572940 . Total input tokens: 127882704 . Total output tokens: 114636673
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.1156551679596305,
    "estimated_duration": 3600.046397939719,
    "input_throughput": 6784.0983977254145,
    "output_throughput": 5993.574975130445,
    "total_throughput": 12777.67337285586,
    "itl": 143.29720120674642,
    "ttft": 1421899.920936015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 190487,
    "finished_requests": 98469,
    "scheduler_time": 63.456434744889485
}
#Debug simulation 
Total elapsed time: 7.115785375237465. Arrivals time: 0.29491480719298124 Scheduler time: 6.708332241512835 Scheduler overhead time: 0.0387545065023005 Adapter cache time: 0.014180428348481655 Engine time: 0.04087484581395984 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.297686448786408,
    "estimated_duration": 3600.0428752294424,
    "input_throughput": 7031.991250489361,
    "output_throughput": 6171.196224596093,
    "total_throughput": 13203.187475085453,
    "itl": 138.60997641974595,
    "ttft": 1380006.4157371523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 188561,
    "finished_requests": 101795,
    "scheduler_time": 65.37382312530191
}
#Debug simulation 
Total elapsed time: 7.297785888891667. Arrivals time: 0.3069035499356687 Scheduler time: 6.877761899027973 Scheduler overhead time: 0.03964805509895086 Adapter cache time: 0.01273976918309927 Engine time: 0.04160997224971652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.373720888979733,
    "estimated_duration": 3600.015965367046,
    "input_throughput": 7032.043814122062,
    "output_throughput": 6171.242353847414,
    "total_throughput": 13203.286167969476,
    "itl": 138.61189297573773,
    "ttft": 1379990.9724908038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 188561,
    "finished_requests": 101795,
    "scheduler_time": 65.37348625624188
}
#Debug simulation 
Total elapsed time: 7.373823914676905. Arrivals time: 0.3024457679130137 Scheduler time: 6.957240426912904 Scheduler overhead time: 0.04004358919337392 Adapter cache time: 0.012769187334924936 Engine time: 0.04200931405648589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.419264436233789,
    "estimated_duration": 3600.013012133137,
    "input_throughput": 7032.049582787389,
    "output_throughput": 6171.247416363054,
    "total_throughput": 13203.296999150443,
    "itl": 138.61171864708234,
    "ttft": 1379988.8583433505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 188561,
    "finished_requests": 101795,
    "scheduler_time": 65.3734785295517
}
#Debug simulation 
Total elapsed time: 7.419387389905751. Arrivals time: 0.30989370262250304 Scheduler time: 6.995154088363051 Scheduler overhead time: 0.040146156679838896 Adapter cache time: 0.01280119689181447 Engine time: 0.04206552403047681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.348033349961042,
    "estimated_duration": 3600.12702355256,
    "input_throughput": 7031.982714603903,
    "output_throughput": 6171.126978202203,
    "total_throughput": 13203.109692806107,
    "itl": 138.6120738785943,
    "ttft": 1380082.6157503796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 188561,
    "finished_requests": 101797,
    "scheduler_time": 65.37533521602158
}
#Debug simulation 
Total elapsed time: 7.348131860140711. Arrivals time: 0.3070030794478953 Scheduler time: 6.927542412653565 Scheduler overhead time: 0.03978817444294691 Adapter cache time: 0.012789318338036537 Engine time: 0.0417645825073123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.379315506201237,
    "estimated_duration": 3600.0190179885394,
    "input_throughput": 7032.0378513290925,
    "output_throughput": 6171.237120967544,
    "total_throughput": 13203.274972296636,
    "itl": 138.61158149439035,
    "ttft": 1380006.0028025254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 188561,
    "finished_requests": 101795,
    "scheduler_time": 65.37358310920266
}
#Debug simulation 
Total elapsed time: 7.379473130218685. Arrivals time: 0.31077244179323316 Scheduler time: 6.954168989788741 Scheduler overhead time: 0.040156872011721134 Adapter cache time: 0.012858591042459011 Engine time: 0.04228801606222987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.367284128442407,
    "estimated_duration": 3600.0307655109877,
    "input_throughput": 7032.0149045744965,
    "output_throughput": 6171.216983154472,
    "total_throughput": 13203.231887728967,
    "itl": 138.6099724427752,
    "ttft": 1380004.8113478627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 188561,
    "finished_requests": 101795,
    "scheduler_time": 65.3738169088473
}
#Debug simulation 
Total elapsed time: 7.367411851417273. Arrivals time: 0.3130554142408073 Scheduler time: 6.93957274639979 Scheduler overhead time: 0.04046133579686284 Adapter cache time: 0.01274632615968585 Engine time: 0.04205410787835717 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 17280, 8640, 17280, 270, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 270, 270, 17280, 8640, 270, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 270, 8640, 8640, 17280, 17280, 8640, 17280, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 17280, 8640]
Prompts retrieved: 567270 . Total input tokens: 126607987 . Total output tokens: 113514185
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.6282577072270215,
    "estimated_duration": 3600.021193663776,
    "input_throughput": 7032.033601512274,
    "output_throughput": 6171.233391376228,
    "total_throughput": 13203.266992888503,
    "itl": 138.61134060820334,
    "ttft": 1380008.5108394853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 188561,
    "finished_requests": 101795,
    "scheduler_time": 65.37360328638358
}
#Debug simulation 
Total elapsed time: 7.62832883791998. Arrivals time: 0.3129101088270545 Scheduler time: 7.2016701539978385 Scheduler overhead time: 0.039842139929533005 Adapter cache time: 0.012729275040328503 Engine time: 0.041914583183825016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.451530778314918,
    "estimated_duration": 3600.1438043395583,
    "input_throughput": 7133.84058965708,
    "output_throughput": 6307.3197166816,
    "total_throughput": 13441.160306338681,
    "itl": 136.2183379663834,
    "ttft": 1348686.6480639507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 187655,
    "finished_requests": 103898,
    "scheduler_time": 67.02382572908783
}
#Debug simulation 
Total elapsed time: 7.451636040117592. Arrivals time: 0.3129603359848261 Scheduler time: 7.024960706941783 Scheduler overhead time: 0.04061305336654186 Adapter cache time: 0.010889564175158739 Engine time: 0.042738865595310926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.544928626623005,
    "estimated_duration": 3600.064839016911,
    "input_throughput": 7133.9256786867445,
    "output_throughput": 6307.355565907166,
    "total_throughput": 13441.28124459391,
    "itl": 136.21880024130266,
    "ttft": 1348645.0150467006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 187655,
    "finished_requests": 103897,
    "scheduler_time": 67.02273750926169
}
#Debug simulation 
Total elapsed time: 7.5450274287723005. Arrivals time: 0.311386673245579 Scheduler time: 7.119199630338699 Scheduler overhead time: 0.04072797764092684 Adapter cache time: 0.01112662348896265 Engine time: 0.042954639066010714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.493608112912625,
    "estimated_duration": 3600.0655651579236,
    "input_throughput": 7133.924239758501,
    "output_throughput": 6307.354293699904,
    "total_throughput": 13441.278533458406,
    "itl": 136.21880565011318,
    "ttft": 1348645.5883413313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 187655,
    "finished_requests": 103897,
    "scheduler_time": 67.02273763803484
}
#Debug simulation 
Total elapsed time: 7.493709211237729. Arrivals time: 0.30868588062003255 Scheduler time: 7.070956560783088 Scheduler overhead time: 0.040627078153193 Adapter cache time: 0.01093101454898715 Engine time: 0.0428721634671092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.53443414112553,
    "estimated_duration": 3600.008713405493,
    "input_throughput": 7134.036899512137,
    "output_throughput": 6307.453900165706,
    "total_throughput": 13441.490799677842,
    "itl": 136.2185253929017,
    "ttft": 1348612.4695339396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942695,
    "arrivals": 187655,
    "finished_requests": 103897,
    "scheduler_time": 67.02175728446983
}
#Debug simulation 
Total elapsed time: 7.534540176391602. Arrivals time: 0.30589934438467026 Scheduler time: 7.114428097382188 Scheduler overhead time: 0.0406007282435894 Adapter cache time: 0.011157732456922531 Engine time: 0.04286258341744542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.480619796086103,
    "estimated_duration": 3600.071788343116,
    "input_throughput": 7133.911907856722,
    "output_throughput": 6307.343390630145,
    "total_throughput": 13441.255298486867,
    "itl": 136.21848950722074,
    "ttft": 1348658.363394649,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 187655,
    "finished_requests": 103897,
    "scheduler_time": 67.02311354172876
}
#Debug simulation 
Total elapsed time: 7.4807134992443025. Arrivals time: 0.31509652081876993 Scheduler time: 7.052172353491187 Scheduler overhead time: 0.04050825862213969 Adapter cache time: 0.010961773805320263 Engine time: 0.04243592033162713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.456677719950676,
    "estimated_duration": 3600.1362733722526,
    "input_throughput": 7133.855512625592,
    "output_throughput": 6307.332910687317,
    "total_throughput": 13441.18842331291,
    "itl": 136.21858611242092,
    "ttft": 1348684.8835481396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 187655,
    "finished_requests": 103898,
    "scheduler_time": 67.02381766877375
}
#Debug simulation 
Total elapsed time: 7.456805618945509. Arrivals time: 0.31201707664877176 Scheduler time: 7.03061335394159 Scheduler overhead time: 0.040681791957467794 Adapter cache time: 0.010970525909215212 Engine time: 0.04262083861976862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 17280, 8640, 17280, 135, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 135, 135, 17280, 8640, 135, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 135, 8640, 8640, 17280, 17280, 8640, 17280, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 17280, 8640]
Prompts retrieved: 564435 . Total input tokens: 126014313 . Total output tokens: 112914884
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.526966298930347,
    "estimated_duration": 3600.079142444334,
    "input_throughput": 7133.897334979801,
    "output_throughput": 6307.330506235143,
    "total_throughput": 13441.227841214943,
    "itl": 136.21853087834032,
    "ttft": 1348662.734908642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2146942614018914,
    "arrivals": 187655,
    "finished_requests": 103897,
    "scheduler_time": 67.0231358114092
}
#Debug simulation 
Total elapsed time: 7.527068210765719. Arrivals time: 0.3093059784732759 Scheduler time: 7.1035411399789155 Scheduler overhead time: 0.04081961000338197 Adapter cache time: 0.011021208949387074 Engine time: 0.042752837762236595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.858475975226611,
    "estimated_duration": 3600.0882159401776,
    "input_throughput": 7258.774072339075,
    "output_throughput": 6388.251237336391,
    "total_throughput": 13647.025309675466,
    "itl": 134.29140019634653,
    "ttft": 1333037.2927190051,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 187118,
    "finished_requests": 105378,
    "scheduler_time": 67.92647868284556
}
#Debug simulation 
Total elapsed time: 7.858548839110881. Arrivals time: 0.5460424982011318 Scheduler time: 7.198384504765272 Scheduler overhead time: 0.04139514919370413 Adapter cache time: 0.009425067342817783 Engine time: 0.0433430802077055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.637755690142512,
    "estimated_duration": 3600.0214532654163,
    "input_throughput": 7258.858964936669,
    "output_throughput": 6388.099987332073,
    "total_throughput": 13646.958952268742,
    "itl": 134.29194414317948,
    "ttft": 1333057.490543582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 187118,
    "finished_requests": 105375,
    "scheduler_time": 67.92485355995578
}
#Debug simulation 
Total elapsed time: 7.637890176847577. Arrivals time: 0.3144597625359893 Scheduler time: 7.209339293651283 Scheduler overhead time: 0.04144204314798117 Adapter cache time: 0.009201465640217066 Engine time: 0.04348460119217634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.5681280442513525,
    "estimated_duration": 3600.0219303701015,
    "input_throughput": 7258.858002932634,
    "output_throughput": 6388.099140728222,
    "total_throughput": 13646.957143660855,
    "itl": 134.29192617558354,
    "ttft": 1333057.6144441275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 187118,
    "finished_requests": 105375,
    "scheduler_time": 67.92490954922671
}
#Debug simulation 
Total elapsed time: 7.568225583061576. Arrivals time: 0.3226143657229841 Scheduler time: 7.131638452410698 Scheduler overhead time: 0.041478448547422886 Adapter cache time: 0.009307642001658678 Engine time: 0.04333238350227475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.874790394213051,
    "estimated_duration": 3600.125862376085,
    "input_throughput": 7258.990101738283,
    "output_throughput": 6388.337485740225,
    "total_throughput": 13647.327587478509,
    "itl": 134.29251492185375,
    "ttft": 1333008.4046428474,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 187118,
    "finished_requests": 105380,
    "scheduler_time": 67.92704473625788
}
#Debug simulation 
Total elapsed time: 7.874913233332336. Arrivals time: 0.31849754555150867 Scheduler time: 7.441889358218759 Scheduler overhead time: 0.041729049291461706 Adapter cache time: 0.009301362559199333 Engine time: 0.043335835449397564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.550947234965861,
    "estimated_duration": 3600.028690255432,
    "input_throughput": 7258.844372750224,
    "output_throughput": 6388.087145596686,
    "total_throughput": 13646.93151834691,
    "itl": 134.2918528984116,
    "ttft": 1333059.9032898534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 187118,
    "finished_requests": 105375,
    "scheduler_time": 67.92491903781439
}
#Debug simulation 
Total elapsed time: 7.551077159121633. Arrivals time: 0.3115472672507167 Scheduler time: 7.125886249355972 Scheduler overhead time: 0.041354505345225334 Adapter cache time: 0.009213607292622328 Engine time: 0.04320106049999595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.656090063042939,
    "estimated_duration": 3600.070086560479,
    "input_throughput": 7258.805626466793,
    "output_throughput": 6388.192853759779,
    "total_throughput": 13646.998480226572,
    "itl": 134.29144566843692,
    "ttft": 1333042.1573188736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 187118,
    "finished_requests": 105377,
    "scheduler_time": 67.92590262262256
}
#Debug simulation 
Total elapsed time: 7.656193227972835. Arrivals time: 0.3198815658688545 Scheduler time: 7.222054392565042 Scheduler overhead time: 0.04142095660790801 Adapter cache time: 0.009294894523918629 Engine time: 0.04356086999177933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 17280, 8640, 17280, 66, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 66, 66, 17280, 8640, 66, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 66, 8640, 8640, 17280, 17280, 8640, 17280, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 17280, 8640]
Prompts retrieved: 562986 . Total input tokens: 125700281 . Total output tokens: 112613693
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.868017813656479,
    "estimated_duration": 3600.034916960915,
    "input_throughput": 7258.831817681426,
    "output_throughput": 6388.076096610171,
    "total_throughput": 13646.907914291596,
    "itl": 134.2918672208799,
    "ttft": 1333063.6562675084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 187118,
    "finished_requests": 105375,
    "scheduler_time": 67.92493469019718
}
#Debug simulation 
Total elapsed time: 7.8680875529535115. Arrivals time: 0.5467811562120914 Scheduler time: 7.207137153483927 Scheduler overhead time: 0.04144242452457547 Adapter cache time: 0.009402322582900524 Engine time: 0.04342338116839528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.663310506846756,
    "estimated_duration": 3600.0221889103464,
    "input_throughput": 7292.906438431353,
    "output_throughput": 6441.018078007591,
    "total_throughput": 13733.924516438945,
    "itl": 133.45734464156513,
    "ttft": 1324247.829859278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 186916,
    "finished_requests": 106129,
    "scheduler_time": 68.52358463073037
}
#Debug simulation 
Total elapsed time: 7.663437757175416. Arrivals time: 0.31747899763286114 Scheduler time: 7.232150595169514 Scheduler overhead time: 0.04164606751874089 Adapter cache time: 0.008414882235229015 Engine time: 0.04382350808009505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.605119651183486,
    "estimated_duration": 3600.0621471594973,
    "input_throughput": 7292.825492114154,
    "output_throughput": 6440.94658707365,
    "total_throughput": 13733.772079187804,
    "itl": 133.4572646461229,
    "ttft": 1324256.060136973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 186916,
    "finished_requests": 106129,
    "scheduler_time": 68.52395374697893
}
#Debug simulation 
Total elapsed time: 7.605218517128378. Arrivals time: 0.31342731323093176 Scheduler time: 7.178708060178906 Scheduler overhead time: 0.04132366273552179 Adapter cache time: 0.008368329610675573 Engine time: 0.04347824119031429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.654036644846201,
    "estimated_duration": 3600.067311555894,
    "input_throughput": 7292.8150303537395,
    "output_throughput": 6440.937347357149,
    "total_throughput": 13733.752377710887,
    "itl": 133.45718190052963,
    "ttft": 1324256.5456180684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 186916,
    "finished_requests": 106129,
    "scheduler_time": 68.52411334420027
}
#Debug simulation 
Total elapsed time: 7.654134375043213. Arrivals time: 0.3191784997470677 Scheduler time: 7.221663008444011 Scheduler overhead time: 0.04161282442510128 Adapter cache time: 0.008252464700490236 Engine time: 0.04347563860937953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.642192001920193,
    "estimated_duration": 3600.026722523184,
    "input_throughput": 7292.897254273345,
    "output_throughput": 6441.009966656065,
    "total_throughput": 13733.907220929412,
    "itl": 133.457287382352,
    "ttft": 1324251.0231715322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 186916,
    "finished_requests": 106129,
    "scheduler_time": 68.52356188377851
}
#Debug simulation 
Total elapsed time: 7.642320043873042. Arrivals time: 0.3180054519325495 Scheduler time: 7.211180891375989 Scheduler overhead time: 0.04144220473244786 Adapter cache time: 0.008271760772913694 Engine time: 0.04349729651585221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.63676381111145,
    "estimated_duration": 3600.071946587658,
    "input_throughput": 7292.805640977688,
    "output_throughput": 6440.929054759212,
    "total_throughput": 13733.734695736899,
    "itl": 133.4569190549801,
    "ttft": 1324257.3357440808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 186916,
    "finished_requests": 106129,
    "scheduler_time": 68.52411377149424
}
#Debug simulation 
Total elapsed time: 7.636865983251482. Arrivals time: 0.3132492657750845 Scheduler time: 7.210195062682033 Scheduler overhead time: 0.041602402459830046 Adapter cache time: 0.008255680091679096 Engine time: 0.04355737194418907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.661480887327343,
    "estimated_duration": 3600.0153636662985,
    "input_throughput": 7292.920265001863,
    "output_throughput": 6441.030289488893,
    "total_throughput": 13733.950554490755,
    "itl": 133.45778913097226,
    "ttft": 1324246.9331762611,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 186916,
    "finished_requests": 106129,
    "scheduler_time": 68.5234830364234
}
#Debug simulation 
Total elapsed time: 7.661580504383892. Arrivals time: 0.3143051117658615 Scheduler time: 7.23379978723824 Scheduler overhead time: 0.04161678487434983 Adapter cache time: 0.00834360858425498 Engine time: 0.04352038586512208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 17280, 8640, 17280, 33, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 33, 33, 17280, 8640, 33, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 33, 8640, 8640, 17280, 17280, 8640, 17280, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 17280, 8640]
Prompts retrieved: 562293 . Total input tokens: 125542962 . Total output tokens: 112480896
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.650167807936668,
    "estimated_duration": 3600.0801119862545,
    "input_throughput": 7292.789100049961,
    "output_throughput": 6440.914445986233,
    "total_throughput": 13733.703546036193,
    "itl": 133.45692448540117,
    "ttft": 1324260.477200274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 186916,
    "finished_requests": 106129,
    "scheduler_time": 68.52419759440761
}
#Debug simulation 
Total elapsed time: 7.65029883896932. Arrivals time: 0.310907875187695 Scheduler time: 7.225495736114681 Scheduler overhead time: 0.041593569330871105 Adapter cache time: 0.008388858288526535 Engine time: 0.04385849926620722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.829191311728209,
    "estimated_duration": 3600.08999344574,
    "input_throughput": 6532.432534413101,
    "output_throughput": 5766.839728394947,
    "total_throughput": 12299.272262808048,
    "itl": 148.6575847240522,
    "ttft": 1363894.3498902114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 164321,
    "finished_requests": 94659,
    "scheduler_time": 62.075040251416674
}
#Debug simulation 
Total elapsed time: 6.829287976026535. Arrivals time: 0.28367970371618867 Scheduler time: 6.4405467682518065 Scheduler overhead time: 0.03704066388309002 Adapter cache time: 0.011327280662953854 Engine time: 0.03903813241049647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.838858630973846,
    "estimated_duration": 3600.0031936122405,
    "input_throughput": 6532.441427198638,
    "output_throughput": 5766.8632174652885,
    "total_throughput": 12299.304644663927,
    "itl": 148.65769379667142,
    "ttft": 1363909.6553164516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 164321,
    "finished_requests": 94657,
    "scheduler_time": 62.07356709278932
}
#Debug simulation 
Total elapsed time: 6.8389463550411165. Arrivals time: 0.2759615476243198 Scheduler time: 6.458115239161998 Scheduler overhead time: 0.03712987573817372 Adapter cache time: 0.011302807368338108 Engine time: 0.03876963723450899 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.830477529205382,
    "estimated_duration": 3600.0040581873122,
    "input_throughput": 6532.439858370958,
    "output_throughput": 5766.861832498466,
    "total_throughput": 12299.301690869424,
    "itl": 148.65762941466537,
    "ttft": 1363910.2775355587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 164321,
    "finished_requests": 94657,
    "scheduler_time": 62.073548513594254
}
#Debug simulation 
Total elapsed time: 6.830634914338589. Arrivals time: 0.27934051351621747 Scheduler time: 6.446384813636541 Scheduler overhead time: 0.037004764191806316 Adapter cache time: 0.011341464705765247 Engine time: 0.03888138057664037 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.900355571880937,
    "estimated_duration": 3600.10960151372,
    "input_throughput": 6532.396955390408,
    "output_throughput": 5766.808319188579,
    "total_throughput": 12299.205274578986,
    "itl": 148.65815993800948,
    "ttft": 1363897.7647097164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942695,
    "arrivals": 164321,
    "finished_requests": 94659,
    "scheduler_time": 62.07507595223107
}
#Debug simulation 
Total elapsed time: 6.900464712642133. Arrivals time: 0.28244487941265106 Scheduler time: 6.5123575469478965 Scheduler overhead time: 0.037284409161657095 Adapter cache time: 0.01152372406795621 Engine time: 0.03906802833080292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.893492954783142,
    "estimated_duration": 3600.037318920113,
    "input_throughput": 6532.37950518086,
    "output_throughput": 5766.8085524812,
    "total_throughput": 12299.18805766206,
    "itl": 148.6578771849516,
    "ttft": 1363928.967158404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 164321,
    "finished_requests": 94657,
    "scheduler_time": 62.074263490365375
}
#Debug simulation 
Total elapsed time: 6.89360394468531. Arrivals time: 0.2821403625421226 Scheduler time: 6.50470359204337 Scheduler overhead time: 0.03717978624626994 Adapter cache time: 0.011374447494745255 Engine time: 0.038860142696648836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.806571220047772,
    "estimated_duration": 3600.0772176835753,
    "input_throughput": 6532.426550320461,
    "output_throughput": 5766.766862116997,
    "total_throughput": 12299.193412437457,
    "itl": 148.65731996436236,
    "ttft": 1363907.9764278175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 164321,
    "finished_requests": 94658,
    "scheduler_time": 62.07486168283251
}
#Debug simulation 
Total elapsed time: 6.806692461017519. Arrivals time: 0.26666999654844403 Scheduler time: 6.435741509310901 Scheduler overhead time: 0.03709346055984497 Adapter cache time: 0.011144721880555153 Engine time: 0.03843634016811848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 4320, 17280, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 1080, 1080, 17280, 4320, 1080, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 1080, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 17280, 4320]
Prompts retrieved: 493560 . Total input tokens: 110190145 . Total output tokens: 98804840
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.796169812325388,
    "estimated_duration": 3600.039114541221,
    "input_throughput": 6532.495690118903,
    "output_throughput": 5766.827898103463,
    "total_throughput": 12299.323588222367,
    "itl": 148.6579169430519,
    "ttft": 1363912.136085351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 164321,
    "finished_requests": 94658,
    "scheduler_time": 62.074238969795246
}
#Debug simulation 
Total elapsed time: 6.7962939203716815. Arrivals time: 0.27248467737808824 Scheduler time: 6.419519948773086 Scheduler overhead time: 0.03710740897804499 Adapter cache time: 0.011295941658318043 Engine time: 0.03824885422363877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.914233099203557,
    "estimated_duration": 3600.1218354423886,
    "input_throughput": 6707.219117497673,
    "output_throughput": 5942.06751265997,
    "total_throughput": 12649.286630157643,
    "itl": 144.6633228696453,
    "ttft": 1309567.3415438756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 160553,
    "finished_requests": 97301,
    "scheduler_time": 64.39447426437555
}
#Debug simulation 
Total elapsed time: 6.914347450248897. Arrivals time: 0.27037180541083217 Scheduler time: 6.535557401366532 Scheduler overhead time: 0.03798969183117151 Adapter cache time: 0.013328449800610542 Engine time: 0.03912720922380686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.010843710042536,
    "estimated_duration": 3600.1547222249696,
    "input_throughput": 6707.15784822625,
    "output_throughput": 5942.01323291439,
    "total_throughput": 12649.17108114064,
    "itl": 144.6623784360928,
    "ttft": 1309610.7898285668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 160553,
    "finished_requests": 97301,
    "scheduler_time": 64.39477113910114
}
#Debug simulation 
Total elapsed time: 7.010961104184389. Arrivals time: 0.28217566246166825 Scheduler time: 6.619929838459939 Scheduler overhead time: 0.03806393826380372 Adapter cache time: 0.013595155905932188 Engine time: 0.03911064565181732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.008159629069269,
    "estimated_duration": 3600.1550701917017,
    "input_throughput": 6707.1571999575635,
    "output_throughput": 5942.012658599427,
    "total_throughput": 12649.16985855699,
    "itl": 144.662380795914,
    "ttft": 1309611.0638095813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 160553,
    "finished_requests": 97301,
    "scheduler_time": 64.39476507481328
}
#Debug simulation 
Total elapsed time: 7.008252637926489. Arrivals time: 0.27593115251511335 Scheduler time: 6.623052276670933 Scheduler overhead time: 0.038059120532125235 Adapter cache time: 0.01363943750038743 Engine time: 0.03953579906374216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.992314951028675,
    "estimated_duration": 3600.126897239231,
    "input_throughput": 6707.209687113267,
    "output_throughput": 5942.059158082637,
    "total_throughput": 12649.268845195904,
    "itl": 144.66267854900855,
    "ttft": 1309604.8573461005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 160553,
    "finished_requests": 97301,
    "scheduler_time": 64.39465842894249
}
#Debug simulation 
Total elapsed time: 6.992412463296205. Arrivals time: 0.27501847641542554 Scheduler time: 6.607915181200951 Scheduler overhead time: 0.03815327398478985 Adapter cache time: 0.013538431841880083 Engine time: 0.03974754363298416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.981181618757546,
    "estimated_duration": 3600.0350122061855,
    "input_throughput": 6707.33393373371,
    "output_throughput": 5942.081376283801,
    "total_throughput": 12649.415310017512,
    "itl": 144.66236307099922,
    "ttft": 1309578.3089101778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 160553,
    "finished_requests": 97300,
    "scheduler_time": 64.39327504941839
}
#Debug simulation 
Total elapsed time: 6.9812776180915534. Arrivals time: 0.27377887768670917 Scheduler time: 6.598787302151322 Scheduler overhead time: 0.03796572703868151 Adapter cache time: 0.013438982889056206 Engine time: 0.039355363231152296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.990692511200905,
    "estimated_duration": 3600.0696038473607,
    "input_throughput": 6707.316429158629,
    "output_throughput": 5942.153723122017,
    "total_throughput": 12649.470152280646,
    "itl": 144.6617079162654,
    "ttft": 1309563.426683158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 160553,
    "finished_requests": 97301,
    "scheduler_time": 64.39363556684198
}
#Debug simulation 
Total elapsed time: 6.990786589216441. Arrivals time: 0.27438874915242195 Scheduler time: 6.607364854309708 Scheduler overhead time: 0.03823969652876258 Adapter cache time: 0.013338178861886263 Engine time: 0.03929647943004966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 17280, 4320, 17280, 540, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 540, 540, 17280, 4320, 540, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 540, 4320, 4320, 17280, 17280, 4320, 17280, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 17280, 4320]
Prompts retrieved: 482220 . Total input tokens: 107694622 . Total output tokens: 96525025
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.033344558440149,
    "estimated_duration": 3600.0529817252263,
    "input_throughput": 6707.300454347311,
    "output_throughput": 5942.05171662463,
    "total_throughput": 12649.35217097194,
    "itl": 144.66246420562135,
    "ttft": 1309586.633070096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 160553,
    "finished_requests": 97300,
    "scheduler_time": 64.39329936502992
}
#Debug simulation 
Total elapsed time: 7.033434881363064. Arrivals time: 0.2740884404629469 Scheduler time: 6.65086466120556 Scheduler overhead time: 0.037952760234475136 Adapter cache time: 0.013341908808797598 Engine time: 0.03914870973676443 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.185830750036985,
    "estimated_duration": 3600.087844078168,
    "input_throughput": 6874.321703207486,
    "output_throughput": 6117.63100065116,
    "total_throughput": 12991.952703858646,
    "itl": 140.957675685942,
    "ttft": 1258412.8336416015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 158690,
    "finished_requests": 100094,
    "scheduler_time": 66.76015113576025
}
#Debug simulation 
Total elapsed time: 7.18592030601576. Arrivals time: 0.2776235369965434 Scheduler time: 6.797488264273852 Scheduler overhead time: 0.03923489525914192 Adapter cache time: 0.012918864376842976 Engine time: 0.040214665699750185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.152373563032597,
    "estimated_duration": 3600.1151127583234,
    "input_throughput": 6874.269634405812,
    "output_throughput": 6117.5846633208685,
    "total_throughput": 12991.85429772668,
    "itl": 140.95670034544068,
    "ttft": 1258424.8265410897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 158690,
    "finished_requests": 100094,
    "scheduler_time": 66.76093881367257
}
#Debug simulation 
Total elapsed time: 7.1525052869692445. Arrivals time: 0.27800301369279623 Scheduler time: 6.7639157441444695 Scheduler overhead time: 0.03897128300741315 Adapter cache time: 0.012811997439712286 Engine time: 0.040291063487529755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.1321270279586315,
    "estimated_duration": 3600.116135655038,
    "input_throughput": 6874.26768122776,
    "output_throughput": 6117.582925138817,
    "total_throughput": 12991.850606366577,
    "itl": 140.95677784576685,
    "ttft": 1258425.5300983924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 158690,
    "finished_requests": 100094,
    "scheduler_time": 66.76091017806536
}
#Debug simulation 
Total elapsed time: 7.132219988852739. Arrivals time: 0.295471612829715 Scheduler time: 6.726076500955969 Scheduler overhead time: 0.039028077851980925 Adapter cache time: 0.012989974580705166 Engine time: 0.04012931231409311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.215112834703177,
    "estimated_duration": 3600.1070391691364,
    "input_throughput": 6874.285050622159,
    "output_throughput": 6117.598382597782,
    "total_throughput": 12991.883433219942,
    "itl": 140.95805412299697,
    "ttft": 1258411.4826455407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 158690,
    "finished_requests": 100094,
    "scheduler_time": 66.76046463467745
}
#Debug simulation 
Total elapsed time: 7.215215425938368. Arrivals time: 0.27996801771223545 Scheduler time: 6.824500232003629 Scheduler overhead time: 0.039015870075672865 Adapter cache time: 0.012975541409105062 Engine time: 0.04020986845716834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.152719633188099,
    "estimated_duration": 3600.01825843201,
    "input_throughput": 6874.262635206404,
    "output_throughput": 6117.459251329496,
    "total_throughput": 12991.721886535901,
    "itl": 140.95737804537336,
    "ttft": 1258447.4727358322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 158690,
    "finished_requests": 100091,
    "scheduler_time": 66.75912282707719
}
#Debug simulation 
Total elapsed time: 7.152836271096021. Arrivals time: 0.2773911668919027 Scheduler time: 6.765091246459633 Scheduler overhead time: 0.03899814421311021 Adapter cache time: 0.012860383838415146 Engine time: 0.040082362946122885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.169499408919364,
    "estimated_duration": 3600.0271163406837,
    "input_throughput": 6874.437386226063,
    "output_throughput": 6117.707530599555,
    "total_throughput": 12992.144916825617,
    "itl": 140.9563586603643,
    "ttft": 1258425.056754717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 158690,
    "finished_requests": 100093,
    "scheduler_time": 66.75924810180487
}
#Debug simulation 
Total elapsed time: 7.169592599850148. Arrivals time: 0.2799236667342484 Scheduler time: 6.778835197445005 Scheduler overhead time: 0.03912901831790805 Adapter cache time: 0.012883156538009644 Engine time: 0.0402456084266305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 17280, 4320, 17280, 270, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 270, 270, 17280, 4320, 270, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 270, 4320, 4320, 17280, 17280, 4320, 17280, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 17280, 4320]
Prompts retrieved: 476550 . Total input tokens: 106426455 . Total output tokens: 95403918
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.164166615810245,
    "estimated_duration": 3600.022959916167,
    "input_throughput": 6874.253657697864,
    "output_throughput": 6117.4512621755175,
    "total_throughput": 12991.70491987338,
    "itl": 140.95720702140608,
    "ttft": 1258467.3780991596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 158690,
    "finished_requests": 100091,
    "scheduler_time": 66.7591145969553
}
#Debug simulation 
Total elapsed time: 7.16427682293579. Arrivals time: 0.27702286653220654 Scheduler time: 6.776701952330768 Scheduler overhead time: 0.03901394456624985 Adapter cache time: 0.012807830236852169 Engine time: 0.04027784429490566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.364847375079989,
    "estimated_duration": 3600.0726340918372,
    "input_throughput": 7126.8100973946885,
    "output_throughput": 6274.702567410408,
    "total_throughput": 13401.512664805095,
    "itl": 136.31556595029244,
    "ttft": 1205032.0444680613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 157801,
    "finished_requests": 103252,
    "scheduler_time": 68.81762684454232
}
#Debug simulation 
Total elapsed time: 7.3649342828430235. Arrivals time: 0.2864676509052515 Scheduler time: 6.967201820574701 Scheduler overhead time: 0.04009123519062996 Adapter cache time: 0.010995855089277029 Engine time: 0.04125216184183955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.339106151368469,
    "estimated_duration": 3600.1540808864056,
    "input_throughput": 7126.761917279607,
    "output_throughput": 6274.780882277682,
    "total_throughput": 13401.54279955729,
    "itl": 136.31700564405352,
    "ttft": 1205038.908272482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 157801,
    "finished_requests": 103254,
    "scheduler_time": 68.81869469423975
}
#Debug simulation 
Total elapsed time: 7.339190968312323. Arrivals time: 0.28847929555922747 Scheduler time: 6.939186396542937 Scheduler overhead time: 0.040051527321338654 Adapter cache time: 0.011109715793281794 Engine time: 0.041440630331635475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.364809296093881,
    "estimated_duration": 3600.0027795452943,
    "input_throughput": 7126.575886488758,
    "output_throughput": 6274.523766576942,
    "total_throughput": 13401.0996530657,
    "itl": 136.3166048771267,
    "ttft": 1205040.846760371,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 157801,
    "finished_requests": 103246,
    "scheduler_time": 68.81587298462406
}
#Debug simulation 
Total elapsed time: 7.364900941029191. Arrivals time: 0.28983709029853344 Scheduler time: 6.963562035001814 Scheduler overhead time: 0.04004204040393233 Adapter cache time: 0.011106807738542557 Engine time: 0.04136642813682556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.349765578750521,
    "estimated_duration": 3600.098869110664,
    "input_throughput": 7126.846770832758,
    "output_throughput": 6274.795726813026,
    "total_throughput": 13401.642497645784,
    "itl": 136.31603453870457,
    "ttft": 1205013.1008759046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 157801,
    "finished_requests": 103253,
    "scheduler_time": 68.81810167332421
}
#Debug simulation 
Total elapsed time: 7.349857205990702. Arrivals time: 0.2876495416276157 Scheduler time: 6.950845941435546 Scheduler overhead time: 0.040087816305458546 Adapter cache time: 0.010956970509141684 Engine time: 0.04132790956646204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.347825588658452,
    "estimated_duration": 3600.056807280449,
    "input_throughput": 7126.786985170287,
    "output_throughput": 6274.714041822136,
    "total_throughput": 13401.501026992422,
    "itl": 136.317322699879,
    "ttft": 1205001.2694526704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 157801,
    "finished_requests": 103251,
    "scheduler_time": 68.81720820433628
}
#Debug simulation 
Total elapsed time: 7.347911807708442. Arrivals time: 0.2888987259939313 Scheduler time: 6.947580365929753 Scheduler overhead time: 0.039997938089072704 Adapter cache time: 0.010963569860905409 Engine time: 0.04150499822571874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.317551680840552,
    "estimated_duration": 3600.0675291011585,
    "input_throughput": 7126.820203399318,
    "output_throughput": 6274.711465104092,
    "total_throughput": 13401.53166850341,
    "itl": 136.31638729721837,
    "ttft": 1205034.379335803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 157801,
    "finished_requests": 103252,
    "scheduler_time": 68.8174573548205
}
#Debug simulation 
Total elapsed time: 7.317637010943145. Arrivals time: 0.28374423598870635 Scheduler time: 6.92259463109076 Scheduler overhead time: 0.04013516427949071 Adapter cache time: 0.011026701424270868 Engine time: 0.041178306099027395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 17280, 4320, 17280, 135, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 135, 135, 17280, 4320, 135, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 135, 4320, 4320, 17280, 17280, 4320, 17280, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 17280, 4320]
Prompts retrieved: 473715 . Total input tokens: 105816986 . Total output tokens: 94813786
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.29080651095137,
    "estimated_duration": 3600.0588011689974,
    "input_throughput": 7126.783038007271,
    "output_throughput": 6274.710566578768,
    "total_throughput": 13401.493604586038,
    "itl": 136.3171544547663,
    "ttft": 1205001.5884728164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 157801,
    "finished_requests": 103251,
    "scheduler_time": 68.8171602227074
}
#Debug simulation 
Total elapsed time: 7.290897890925407. Arrivals time: 0.2873828182928264 Scheduler time: 6.892209061887115 Scheduler overhead time: 0.03993043536320329 Adapter cache time: 0.01100513944402337 Engine time: 0.041187125258147717 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.5389451291412115,
    "estimated_duration": 3600.1487353364446,
    "input_throughput": 7224.3715224086145,
    "output_throughput": 6372.861147320316,
    "total_throughput": 13597.232669728932,
    "itl": 134.4996442227239,
    "ttft": 1188865.5006588888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 157261,
    "finished_requests": 104581,
    "scheduler_time": 70.08775185390012
}
#Debug simulation 
Total elapsed time: 7.5390625931322575. Arrivals time: 0.2901674951426685 Scheduler time: 7.1367395906709135 Scheduler overhead time: 0.0406528958119452 Adapter cache time: 0.00975736416876316 Engine time: 0.04230714961886406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.53351981099695,
    "estimated_duration": 3600.14023721707,
    "input_throughput": 7224.388575514205,
    "output_throughput": 6372.876190438423,
    "total_throughput": 13597.264765952626,
    "itl": 134.50003678286922,
    "ttft": 1188863.7635160806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 157261,
    "finished_requests": 104581,
    "scheduler_time": 70.08762273923884
}
#Debug simulation 
Total elapsed time: 7.533614327665418. Arrivals time: 0.29019696824252605 Scheduler time: 7.131607740186155 Scheduler overhead time: 0.04072172939777374 Adapter cache time: 0.009700478054583073 Engine time: 0.04215701809152961 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.460181741043925,
    "estimated_duration": 3600.140580889972,
    "input_throughput": 7224.387885867083,
    "output_throughput": 6372.875582077497,
    "total_throughput": 13597.26346794458,
    "itl": 134.50003943438392,
    "ttft": 1188864.040203601,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 157261,
    "finished_requests": 104581,
    "scheduler_time": 70.08761483107969
}
#Debug simulation 
Total elapsed time: 7.4602667251601815. Arrivals time: 0.2867718320339918 Scheduler time: 7.062301667872816 Scheduler overhead time: 0.04048139974474907 Adapter cache time: 0.009667630307376385 Engine time: 0.04177808668464422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.434511733241379,
    "estimated_duration": 3600.0394862193853,
    "input_throughput": 7224.220484123633,
    "output_throughput": 6372.8889885298,
    "total_throughput": 13597.109472653432,
    "itl": 134.5008267348063,
    "ttft": 1188884.4165753818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942695,
    "arrivals": 157261,
    "finished_requests": 104577,
    "scheduler_time": 70.08582566234456
}
#Debug simulation 
Total elapsed time: 7.434594200924039. Arrivals time: 0.2889754925854504 Scheduler time: 7.034441215917468 Scheduler overhead time: 0.040600710548460484 Adapter cache time: 0.009640722069889307 Engine time: 0.041794315446168184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.455112859141082,
    "estimated_duration": 3600.1489461505525,
    "input_throughput": 7224.371099370718,
    "output_throughput": 6372.860774144357,
    "total_throughput": 13597.231873515075,
    "itl": 134.50012540411652,
    "ttft": 1188867.713146598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 157261,
    "finished_requests": 104581,
    "scheduler_time": 70.08766304678933
}
#Debug simulation 
Total elapsed time: 7.455199399963021. Arrivals time: 0.2914303345605731 Scheduler time: 7.052042604889721 Scheduler overhead time: 0.040698662400245667 Adapter cache time: 0.009680908173322678 Engine time: 0.04211153229698539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.471492430195212,
    "estimated_duration": 3600.134157910955,
    "input_throughput": 7224.400774856706,
    "output_throughput": 6372.886951888829,
    "total_throughput": 13597.287726745535,
    "itl": 134.49978193531007,
    "ttft": 1188859.1416797205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 157261,
    "finished_requests": 104581,
    "scheduler_time": 70.08771231042151
}
#Debug simulation 
Total elapsed time: 7.471575612202287. Arrivals time: 0.3138073100708425 Scheduler time: 7.046273179817945 Scheduler overhead time: 0.04074806394055486 Adapter cache time: 0.009696444030851126 Engine time: 0.04178364761173725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 17280, 4320, 17280, 66, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 66, 66, 17280, 4320, 66, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 66, 4320, 4320, 17280, 17280, 4320, 17280, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 17280, 4320]
Prompts retrieved: 472266 . Total input tokens: 105490336 . Total output tokens: 94521237
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.500775491818786,
    "estimated_duration": 3600.00085000172,
    "input_throughput": 7224.224960943432,
    "output_throughput": 6372.929050833151,
    "total_throughput": 13597.154011776584,
    "itl": 134.49973627372654,
    "ttft": 1188889.1967241252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 157261,
    "finished_requests": 104576,
    "scheduler_time": 70.0847526850203
}
#Debug simulation 
Total elapsed time: 7.500866090878844. Arrivals time: 0.29405827913433313 Scheduler time: 7.095179548021406 Scheduler overhead time: 0.04053565673530102 Adapter cache time: 0.00978173315525055 Engine time: 0.04204745823517442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.548056468833238,
    "estimated_duration": 3600.147483646264,
    "input_throughput": 7230.477672996822,
    "output_throughput": 6418.039290045455,
    "total_throughput": 13648.516963042277,
    "itl": 134.43398291813196,
    "ttft": 1175726.9224608107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 157020,
    "finished_requests": 105038,
    "scheduler_time": 70.87221890970277
}
#Debug simulation 
Total elapsed time: 7.548140386119485. Arrivals time: 0.31076042586937547 Scheduler time: 7.1260322467423975 Scheduler overhead time: 0.04082133388146758 Adapter cache time: 0.008492148481309414 Engine time: 0.042694454081356525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.510329862125218,
    "estimated_duration": 3600.098621660323,
    "input_throughput": 7230.575530176701,
    "output_throughput": 6418.0619555760795,
    "total_throughput": 13648.63748575278,
    "itl": 134.4355439894324,
    "ttft": 1175729.1098736522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 157020,
    "finished_requests": 105037,
    "scheduler_time": 70.87143453629444
}
#Debug simulation 
Total elapsed time: 7.510422560852021. Arrivals time: 0.2846584371291101 Scheduler time: 7.115288204047829 Scheduler overhead time: 0.0409659817814827 Adapter cache time: 0.00849329074844718 Engine time: 0.04168991930782795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.513956529088318,
    "estimated_duration": 3600.096464752406,
    "input_throughput": 7230.579862195512,
    "output_throughput": 6418.065800797667,
    "total_throughput": 13648.645662993178,
    "itl": 134.43540867866213,
    "ttft": 1175728.8073625201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 157020,
    "finished_requests": 105037,
    "scheduler_time": 70.87137591876808
}
#Debug simulation 
Total elapsed time: 7.514044985175133. Arrivals time: 0.2843188648112118 Scheduler time: 7.119031643960625 Scheduler overhead time: 0.040820629335939884 Adapter cache time: 0.008448532782495022 Engine time: 0.04210755554959178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.510549536906183,
    "estimated_duration": 3600.0606369025727,
    "input_throughput": 7230.651821019442,
    "output_throughput": 6418.129673471192,
    "total_throughput": 13648.781494490635,
    "itl": 134.43568991574094,
    "ttft": 1175710.3646641776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 157020,
    "finished_requests": 105037,
    "scheduler_time": 70.8706629204898
}
#Debug simulation 
Total elapsed time: 7.510637948289514. Arrivals time: 0.2869288306683302 Scheduler time: 7.113063286989927 Scheduler overhead time: 0.04088130360469222 Adapter cache time: 0.008404978085309267 Engine time: 0.04202515631914139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.500212341081351,
    "estimated_duration": 3600.102628358416,
    "input_throughput": 7230.567482980224,
    "output_throughput": 6418.054812658432,
    "total_throughput": 13648.622295638655,
    "itl": 134.4353974790993,
    "ttft": 1175730.2819626983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 157020,
    "finished_requests": 105037,
    "scheduler_time": 70.87137914991425
}
#Debug simulation 
Total elapsed time: 7.500299582257867. Arrivals time: 0.28849297808483243 Scheduler time: 7.10057042632252 Scheduler overhead time: 0.040862190537154675 Adapter cache time: 0.008513698354363441 Engine time: 0.042172771878540516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.466785409953445,
    "estimated_duration": 3600.12092505115,
    "input_throughput": 7230.530735472492,
    "output_throughput": 6418.02219453829,
    "total_throughput": 13648.552930010781,
    "itl": 134.43350098157273,
    "ttft": 1175727.605225271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 157020,
    "finished_requests": 105037,
    "scheduler_time": 70.87167690274225
}
#Debug simulation 
Total elapsed time: 7.466873156372458. Arrivals time: 0.3018344836309552 Scheduler time: 7.0546970828436315 Scheduler overhead time: 0.04073141980916262 Adapter cache time: 0.008327077142894268 Engine time: 0.04202461056411266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 17280, 4320, 17280, 33, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 33, 33, 17280, 4320, 33, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 33, 4320, 4320, 17280, 17280, 4320, 17280, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 17280, 4320]
Prompts retrieved: 471573 . Total input tokens: 105322266 . Total output tokens: 94392224
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.5400664852932096,
    "estimated_duration": 3600.1071418827814,
    "input_throughput": 7230.558417877097,
    "output_throughput": 6418.046766218246,
    "total_throughput": 13648.605184095342,
    "itl": 134.43524204748113,
    "ttft": 1175731.2657757017,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 157020,
    "finished_requests": 105037,
    "scheduler_time": 70.87137731772754
}
#Debug simulation 
Total elapsed time: 7.540190044324845. Arrivals time: 0.2876647897064686 Scheduler time: 7.1419220780953765 Scheduler overhead time: 0.04077442875131965 Adapter cache time: 0.008471681736409664 Engine time: 0.04201162187382579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_64_slots_64_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_64_slots_64_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.995834623929113,
    "estimated_duration": 3600.1276514338488,
    "input_throughput": 6743.9122583140215,
    "output_throughput": 5953.898604529484,
    "total_throughput": 12697.810862843506,
    "itl": 143.7940232809459,
    "ttft": 1160579.1654055906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 137849,
    "finished_requests": 97460,
    "scheduler_time": 67.37980931606903
}
#Debug simulation 
Total elapsed time: 6.9959203181788325. Arrivals time: 0.26437725266441703 Scheduler time: 6.617992249317467 Scheduler overhead time: 0.03833628026768565 Adapter cache time: 0.017184192780405283 Engine time: 0.03987203724682331 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_64_slots_64_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_64_slots_64_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.060961903072894,
    "estimated_duration": 3600.1013092812127,
    "input_throughput": 6743.961603915939,
    "output_throughput": 5953.942169555116,
    "total_throughput": 12697.903773471056,
    "itl": 143.79686194186613,
    "ttft": 1160593.4360254088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 137849,
    "finished_requests": 97460,
    "scheduler_time": 67.37858017168355
}
#Debug simulation 
Total elapsed time: 7.061053830198944. Arrivals time: 0.2693277862854302 Scheduler time: 6.677801995072514 Scheduler overhead time: 0.038230171892791986 Adapter cache time: 0.017760219518095255 Engine time: 0.03979708347469568 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_64_slots_64_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_64_slots_64_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.020441520027816,
    "estimated_duration": 3600.098317120547,
    "input_throughput": 6743.967209045261,
    "output_throughput": 5953.947118073183,
    "total_throughput": 12697.914327118446,
    "itl": 143.7966853176148,
    "ttft": 1160592.650038852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 137849,
    "finished_requests": 97460,
    "scheduler_time": 67.3785380026566
}
#Debug simulation 
Total elapsed time: 7.0205261958763. Arrivals time: 0.2772710616700351 Scheduler time: 6.629648417234421 Scheduler overhead time: 0.038468596059829 Adapter cache time: 0.017251348588615656 Engine time: 0.039751268457621336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_64_slots_64_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_64_slots_64_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.015415973961353,
    "estimated_duration": 3600.0627940622853,
    "input_throughput": 6743.907089632824,
    "output_throughput": 5953.93114679909,
    "total_throughput": 12697.838236431915,
    "itl": 143.79640256937392,
    "ttft": 1160580.9501794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 137849,
    "finished_requests": 97459,
    "scheduler_time": 67.37871552367834
}
#Debug simulation 
Total elapsed time: 7.015500333160162. Arrivals time: 0.2752420399338007 Scheduler time: 6.62771877553314 Scheduler overhead time: 0.038081077858805656 Adapter cache time: 0.01686730468645692 Engine time: 0.03950682235881686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_64_slots_64_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_64_slots_64_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.029975695069879,
    "estimated_duration": 3600.096123375658,
    "input_throughput": 6743.971318530978,
    "output_throughput": 5953.950746154383,
    "total_throughput": 12697.922064685361,
    "itl": 143.7961240319927,
    "ttft": 1160591.3908448094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 137849,
    "finished_requests": 97460,
    "scheduler_time": 67.3785985342835
}
#Debug simulation 
Total elapsed time: 7.030064191203564. Arrivals time: 0.2734167901799083 Scheduler time: 6.6436173361726105 Scheduler overhead time: 0.03825599979609251 Adapter cache time: 0.017331008799374104 Engine time: 0.039327096194028854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_64_slots_64_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_64_slots_64_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.016420285217464,
    "estimated_duration": 3600.126397202976,
    "input_throughput": 6743.914607793463,
    "output_throughput": 5953.900678779834,
    "total_throughput": 12697.815286573297,
    "itl": 143.79469542947584,
    "ttft": 1160581.1606238822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 137849,
    "finished_requests": 97460,
    "scheduler_time": 67.37966681713327
}
#Debug simulation 
Total elapsed time: 7.016513884067535. Arrivals time: 0.2662268546409905 Scheduler time: 6.63725827075541 Scheduler overhead time: 0.03839589422568679 Adapter cache time: 0.016999306622892618 Engine time: 0.03957842895761132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_64_slots_64_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_64_slots_64_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 17280, 1080, 17280, 540, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 540, 540, 17280, 1080, 540, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 540, 1080, 1080, 17280, 17280, 1080, 17280, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 17280, 1080]
Prompts retrieved: 414180 . Total input tokens: 92449691 . Total output tokens: 82868236
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.944349464960396,
    "estimated_duration": 3600.1113017211715,
    "input_throughput": 6743.942885430381,
    "output_throughput": 5953.925643841143,
    "total_throughput": 12697.868529271524,
    "itl": 143.79626299120343,
    "ttft": 1160592.5479200054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 137849,
    "finished_requests": 97460,
    "scheduler_time": 67.37875222526816
}
#Debug simulation 
Total elapsed time: 6.944435771089047. Arrivals time: 0.26476543955504894 Scheduler time: 6.567010719329119 Scheduler overhead time: 0.03817191906273365 Adapter cache time: 0.0170185430906713 Engine time: 0.039355598855763674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_64_slots_64_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_64_slots_64_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.190492101944983,
    "estimated_duration": 3600.0014870121963,
    "input_throughput": 6946.501852907506,
    "output_throughput": 6135.423021264205,
    "total_throughput": 13081.924874171711,
    "itl": 139.53655075939042,
    "ttft": 1094963.4413053514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 135943,
    "finished_requests": 100830,
    "scheduler_time": 70.53388709415071
}
#Debug simulation 
Total elapsed time: 7.190577986184508. Arrivals time: 0.27395947789773345 Scheduler time: 6.801521761808544 Scheduler overhead time: 0.039299842435866594 Adapter cache time: 0.016250478103756905 Engine time: 0.040925547014921904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_64_slots_64_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_64_slots_64_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.222059574909508,
    "estimated_duration": 3600.0602083411945,
    "input_throughput": 6946.5668774253645,
    "output_throughput": 6135.595996095232,
    "total_throughput": 13082.162873520598,
    "itl": 139.53692677695517,
    "ttft": 1094915.5533895602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 135943,
    "finished_requests": 100833,
    "scheduler_time": 70.53508796885241
}
#Debug simulation 
Total elapsed time: 7.222144062165171. Arrivals time: 0.2707563145086169 Scheduler time: 6.836215573828667 Scheduler overhead time: 0.03958991402760148 Adapter cache time: 0.016259292606264353 Engine time: 0.040657232981175184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_64_slots_64_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_64_slots_64_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.187212786637247,
    "estimated_duration": 3600.0610469791136,
    "input_throughput": 6946.565259215475,
    "output_throughput": 6135.594566801842,
    "total_throughput": 13082.159826017316,
    "itl": 139.53690261205844,
    "ttft": 1094916.2037261694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 135943,
    "finished_requests": 100833,
    "scheduler_time": 70.53508095628104
}
#Debug simulation 
Total elapsed time: 7.187311636749655. Arrivals time: 0.2751530804671347 Scheduler time: 6.796946523245424 Scheduler overhead time: 0.039491810370236635 Adapter cache time: 0.016177303157746792 Engine time: 0.0409034825861454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_64_slots_64_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_64_slots_64_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.156639224849641,
    "estimated_duration": 3600.0228583768494,
    "input_throughput": 6946.582281223445,
    "output_throughput": 6135.584930690962,
    "total_throughput": 13082.167211914406,
    "itl": 139.53674749713153,
    "ttft": 1094912.3254586263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 135943,
    "finished_requests": 100832,
    "scheduler_time": 70.53477001548627
}
#Debug simulation 
Total elapsed time: 7.156740434933454. Arrivals time: 0.27422593627125025 Scheduler time: 6.7669403944164515 Scheduler overhead time: 0.03967092093080282 Adapter cache time: 0.016416870523244143 Engine time: 0.040773303247988224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_64_slots_64_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_64_slots_64_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.147594651672989,
    "estimated_duration": 3600.091049533192,
    "input_throughput": 6946.507367707461,
    "output_throughput": 6135.543433787354,
    "total_throughput": 13082.050801494814,
    "itl": 139.537599724984,
    "ttft": 1094926.1024517305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 135943,
    "finished_requests": 100833,
    "scheduler_time": 70.5350971002302
}
#Debug simulation 
Total elapsed time: 7.147698016837239. Arrivals time: 0.2718137102201581 Scheduler time: 6.760744514409453 Scheduler overhead time: 0.03939042612910271 Adapter cache time: 0.01625888142734766 Engine time: 0.04084983700886369 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_64_slots_64_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_64_slots_64_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.1719684028066695,
    "estimated_duration": 3600.133502144264,
    "input_throughput": 6946.426010342402,
    "output_throughput": 6135.602189986468,
    "total_throughput": 13082.02820032887,
    "itl": 139.5370907430081,
    "ttft": 1094895.9721641894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 135943,
    "finished_requests": 100835,
    "scheduler_time": 70.53619813944051
}
#Debug simulation 
Total elapsed time: 7.172099587041885. Arrivals time: 0.270664578769356 Scheduler time: 6.786601210013032 Scheduler overhead time: 0.03940445324406028 Adapter cache time: 0.015936924144625664 Engine time: 0.040756648406386375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_64_slots_64_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_64_slots_64_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 17280, 1080, 17280, 270, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 270, 270, 17280, 1080, 270, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 270, 1080, 1080, 17280, 17280, 1080, 17280, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 17280, 1080]
Prompts retrieved: 408510 . Total input tokens: 91162926 . Total output tokens: 81773250
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.141188001725823,
    "estimated_duration": 3600.1026103307513,
    "input_throughput": 6946.48533856718,
    "output_throughput": 6135.627617005793,
    "total_throughput": 13082.112955572973,
    "itl": 139.5378724722219,
    "ttft": 1094901.6846763585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 135943,
    "finished_requests": 100834,
    "scheduler_time": 70.53544106109288
}
#Debug simulation 
Total elapsed time: 7.141297639813274. Arrivals time: 0.2767162974923849 Scheduler time: 6.749938906636089 Scheduler overhead time: 0.039510915987193584 Adapter cache time: 0.01605451526120305 Engine time: 0.04049854213371873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.299300713930279,
    "estimated_duration": 3600.0034571262054,
    "input_throughput": 7030.271026518279,
    "output_throughput": 6275.552306839907,
    "total_throughput": 13305.823333358187,
    "itl": 137.46662109712264,
    "ttft": 1062871.2276312646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 135044,
    "finished_requests": 102528,
    "scheduler_time": 72.99408916884434
}
#Debug simulation 
Total elapsed time: 7.299392980989069. Arrivals time: 0.27660277765244246 Scheduler time: 6.908051909413189 Scheduler overhead time: 0.040124631486833096 Adapter cache time: 0.014323176816105843 Engine time: 0.04138105269521475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.283759455196559,
    "estimated_duration": 3600.1152313086786,
    "input_throughput": 7030.149140753973,
    "output_throughput": 6275.444131211172,
    "total_throughput": 13305.593271965146,
    "itl": 137.4691012972671,
    "ttft": 1062898.2516911167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 135044,
    "finished_requests": 102530,
    "scheduler_time": 72.9954247394622
}
#Debug simulation 
Total elapsed time: 7.283844207413495. Arrivals time: 0.277192244771868 Scheduler time: 6.892378989141434 Scheduler overhead time: 0.03995279548689723 Adapter cache time: 0.014263711404055357 Engine time: 0.041082282550632954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.301064915023744,
    "estimated_duration": 3600.11594569455,
    "input_throughput": 7030.147745732453,
    "output_throughput": 6275.442885948883,
    "total_throughput": 13305.590631681336,
    "itl": 137.46907978992303,
    "ttft": 1062898.8507096376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 135044,
    "finished_requests": 102530,
    "scheduler_time": 72.99542618528118
}
#Debug simulation 
Total elapsed time: 7.301148382015526. Arrivals time: 0.2760835662484169 Scheduler time: 6.910348896868527 Scheduler overhead time: 0.04010622017085552 Adapter cache time: 0.014305823016911745 Engine time: 0.041391636710613966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.305022398009896,
    "estimated_duration": 3600.078022669188,
    "input_throughput": 7030.125414125128,
    "output_throughput": 6275.422326333282,
    "total_throughput": 13305.54774045841,
    "itl": 137.46882633984194,
    "ttft": 1062913.282581932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 135044,
    "finished_requests": 102528,
    "scheduler_time": 72.99388332225415
}
#Debug simulation 
Total elapsed time: 7.3051082459278405. Arrivals time: 0.2783134146593511 Scheduler time: 6.91192834591493 Scheduler overhead time: 0.040203629061579704 Adapter cache time: 0.014379571657627821 Engine time: 0.04135244619101286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.3267796081490815,
    "estimated_duration": 3600.1147689056966,
    "input_throughput": 7030.1500437146115,
    "output_throughput": 6275.44493723661,
    "total_throughput": 13305.59498095122,
    "itl": 137.46874412759712,
    "ttft": 1062900.7067938463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 135044,
    "finished_requests": 102530,
    "scheduler_time": 72.99527157523073
}
#Debug simulation 
Total elapsed time: 7.326869084965438. Arrivals time: 0.27783654956147075 Scheduler time: 6.93422064371407 Scheduler overhead time: 0.04017819091677666 Adapter cache time: 0.01417458988726139 Engine time: 0.04143792483955622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.3701493185944855,
    "estimated_duration": 3600.1417064597563,
    "input_throughput": 7030.177993990282,
    "output_throughput": 6275.554642602386,
    "total_throughput": 13305.732636592667,
    "itl": 137.46736525620318,
    "ttft": 1062883.0191454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 135044,
    "finished_requests": 102532,
    "scheduler_time": 72.99667385624959
}
#Debug simulation 
Total elapsed time: 7.370231247972697. Arrivals time: 0.28519937163218856 Scheduler time: 6.969916874542832 Scheduler overhead time: 0.04020518623292446 Adapter cache time: 0.014459685422480106 Engine time: 0.04147034604102373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 17280, 1080, 17280, 135, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 135, 135, 17280, 1080, 135, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 135, 1080, 1080, 17280, 17280, 1080, 17280, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 17280, 1080]
Prompts retrieved: 405675 . Total input tokens: 90534155 . Total output tokens: 81185589
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.328848659992218,
    "estimated_duration": 3600.1192527958588,
    "input_throughput": 7030.221562895311,
    "output_throughput": 6275.560450519638,
    "total_throughput": 13305.782013414948,
    "itl": 137.46880603254718,
    "ttft": 1062878.733319916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 135044,
    "finished_requests": 102531,
    "scheduler_time": 72.99551647326561
}
#Debug simulation 
Total elapsed time: 7.328938296996057. Arrivals time: 0.2774457656778395 Scheduler time: 6.936358369421214 Scheduler overhead time: 0.04031355306506157 Adapter cache time: 0.01432159123942256 Engine time: 0.0414737886749208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.450015098787844,
    "estimated_duration": 3600.0015568543154,
    "input_throughput": 7183.129393587224,
    "output_throughput": 6398.950566046166,
    "total_throughput": 13582.079959633389,
    "itl": 134.65077888835017,
    "ttft": 1032232.6591684367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 134574,
    "finished_requests": 104802,
    "scheduler_time": 75.0468851729482
}
#Debug simulation 
Total elapsed time: 7.450109337922186. Arrivals time: 0.28124875156208873 Scheduler time: 7.052770582027733 Scheduler overhead time: 0.04097784170880914 Adapter cache time: 0.013299665879458189 Engine time: 0.042239544447511435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.485491247847676,
    "estimated_duration": 3600.066855443951,
    "input_throughput": 7183.071603488615,
    "output_throughput": 6398.905055100483,
    "total_throughput": 13581.9766585891,
    "itl": 134.65201492559717,
    "ttft": 1032245.5129611192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 134574,
    "finished_requests": 104803,
    "scheduler_time": 75.04747998759034
}
#Debug simulation 
Total elapsed time: 7.485582080669701. Arrivals time: 0.28061418514698744 Scheduler time: 7.089139653369784 Scheduler overhead time: 0.04104902455583215 Adapter cache time: 0.013155917637050152 Engine time: 0.04213694157078862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.478649881668389,
    "estimated_duration": 3600.0659437847567,
    "input_throughput": 7183.07342248676,
    "output_throughput": 6398.906675520975,
    "total_throughput": 13581.980098007736,
    "itl": 134.65197929640374,
    "ttft": 1032245.404682314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2092868485860526,
    "arrivals": 134574,
    "finished_requests": 104803,
    "scheduler_time": 75.0474637029908
}
#Debug simulation 
Total elapsed time: 7.478750267997384. Arrivals time: 0.2808398501947522 Scheduler time: 7.082114689517766 Scheduler overhead time: 0.040744266007095575 Adapter cache time: 0.013307923451066017 Engine time: 0.042397170793265104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.4078939612954855,
    "estimated_duration": 3600.0379426498876,
    "input_throughput": 7183.056793275269,
    "output_throughput": 6398.885891475819,
    "total_throughput": 13581.94268475109,
    "itl": 134.65168598803436,
    "ttft": 1032260.3689585219,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 134574,
    "finished_requests": 104802,
    "scheduler_time": 75.04727163624514
}
#Debug simulation 
Total elapsed time: 7.407977607101202. Arrivals time: 0.2857473469339311 Scheduler time: 7.006821000948548 Scheduler overhead time: 0.04077325155958533 Adapter cache time: 0.013254929333925247 Engine time: 0.042119585908949375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.440593109931797,
    "estimated_duration": 3600.079676632736,
    "input_throughput": 7183.046021966717,
    "output_throughput": 6398.88226627993,
    "total_throughput": 13581.928288246647,
    "itl": 134.652009293427,
    "ttft": 1032247.3268352118,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2119276781007646,
    "arrivals": 134574,
    "finished_requests": 104803,
    "scheduler_time": 75.04758000142056
}
#Debug simulation 
Total elapsed time: 7.440730688627809. Arrivals time: 0.279089676681906 Scheduler time: 7.045676386915147 Scheduler overhead time: 0.040872314013540745 Adapter cache time: 0.013286048080772161 Engine time: 0.04243395151570439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.461472078226507,
    "estimated_duration": 3600.145297891641,
    "input_throughput": 7182.971758152174,
    "output_throughput": 6398.7714644436455,
    "total_throughput": 13581.743222595818,
    "itl": 134.65130982493972,
    "ttft": 1032247.316143834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 134574,
    "finished_requests": 104804,
    "scheduler_time": 75.04956423723775
}
#Debug simulation 
Total elapsed time: 7.461558782029897. Arrivals time: 0.28126055374741554 Scheduler time: 7.064431608654559 Scheduler overhead time: 0.04139660578221083 Adapter cache time: 0.013100034557282925 Engine time: 0.04202694073319435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 17280, 1080, 17280, 66, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 66, 66, 17280, 1080, 66, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 66, 1080, 1080, 17280, 17280, 1080, 17280, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 17280, 1080]
Prompts retrieved: 404226 . Total input tokens: 90200478 . Total output tokens: 80895868
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.494046280160546,
    "estimated_duration": 3600.0759072063283,
    "input_throughput": 7183.05354290907,
    "output_throughput": 6398.888966170825,
    "total_throughput": 13581.942509079896,
    "itl": 134.65147134000432,
    "ttft": 1032244.653729861,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 134574,
    "finished_requests": 104803,
    "scheduler_time": 75.04771619615148
}
#Debug simulation 
Total elapsed time: 7.494129689410329. Arrivals time: 0.2815056946128607 Scheduler time: 7.096715131308883 Scheduler overhead time: 0.04083904065191746 Adapter cache time: 0.01333919307217002 Engine time: 0.04238967737182975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.567097543273121,
    "estimated_duration": 3600.0045549575907,
    "input_throughput": 7289.879665252023,
    "output_throughput": 6476.669583067978,
    "total_throughput": 13766.549248320001,
    "itl": 132.86082708759128,
    "ttft": 1005868.1819493802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 134344,
    "finished_requests": 106144,
    "scheduler_time": 76.75533706766312
}
#Debug simulation 
Total elapsed time: 7.5671795210801065. Arrivals time: 0.2824231400154531 Scheduler time: 7.168582642450929 Scheduler overhead time: 0.04153024312108755 Adapter cache time: 0.012335957027971745 Engine time: 0.04278671136125922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.5763870580121875,
    "estimated_duration": 3600.0918501341885,
    "input_throughput": 7289.87317338078,
    "output_throughput": 6476.641977657017,
    "total_throughput": 13766.515151037796,
    "itl": 132.86193020094848,
    "ttft": 1005833.1310529214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 134344,
    "finished_requests": 106147,
    "scheduler_time": 76.75606924781202
}
#Debug simulation 
Total elapsed time: 7.576479321811348. Arrivals time: 0.2826213492080569 Scheduler time: 7.17772686900571 Scheduler overhead time: 0.04149934509769082 Adapter cache time: 0.012085604015737772 Engine time: 0.0430229059420526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.59547823201865,
    "estimated_duration": 3600.0876943050507,
    "input_throughput": 7289.881588583386,
    "output_throughput": 6476.649454090852,
    "total_throughput": 13766.531042674236,
    "itl": 132.86160083684632,
    "ttft": 1005831.2031945561,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 134344,
    "finished_requests": 106147,
    "scheduler_time": 76.756120923044
}
#Debug simulation 
Total elapsed time: 7.595569452736527. Arrivals time: 0.2829709225334227 Scheduler time: 7.1962767681106925 Scheduler overhead time: 0.04147677356377244 Adapter cache time: 0.012364679481834173 Engine time: 0.042812017258256674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.631694320123643,
    "estimated_duration": 3600.033027730646,
    "input_throughput": 7289.822009367283,
    "output_throughput": 6476.61835888704,
    "total_throughput": 13766.440368254322,
    "itl": 132.86119550158415,
    "ttft": 1005878.0124442953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 134344,
    "finished_requests": 106144,
    "scheduler_time": 76.75508483768176
}
#Debug simulation 
Total elapsed time: 7.6317832469940186. Arrivals time: 0.2814506534487009 Scheduler time: 7.2337538646534085 Scheduler overhead time: 0.04150376794859767 Adapter cache time: 0.012400157749652863 Engine time: 0.04308772133663297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.575757632963359,
    "estimated_duration": 3600.0915444598018,
    "input_throughput": 7289.873792344904,
    "output_throughput": 6476.642527571801,
    "total_throughput": 13766.516319916705,
    "itl": 132.8612195572042,
    "ttft": 1005832.3523824671,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 134344,
    "finished_requests": 106147,
    "scheduler_time": 76.7561395314689
}
#Debug simulation 
Total elapsed time: 7.575847900938243. Arrivals time: 0.2847351776435971 Scheduler time: 7.1746387598104775 Scheduler overhead time: 0.04159706784412265 Adapter cache time: 0.012241740245372057 Engine time: 0.04298847075551748 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.579258034937084,
    "estimated_duration": 3600.1385043162586,
    "input_throughput": 7289.943975359481,
    "output_throughput": 6476.671098082757,
    "total_throughput": 13766.615073442239,
    "itl": 132.8609835223442,
    "ttft": 1005833.8256067756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 134344,
    "finished_requests": 106150,
    "scheduler_time": 76.75805385537696
}
#Debug simulation 
Total elapsed time: 7.579361369367689. Arrivals time: 0.28901407727971673 Scheduler time: 7.173224400728941 Scheduler overhead time: 0.041564484126865864 Adapter cache time: 0.012516924645751715 Engine time: 0.04329923167824745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 17280, 1080, 17280, 33, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 33, 33, 17280, 1080, 33, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 33, 1080, 1080, 17280, 17280, 1080, 17280, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 17280, 1080]
Prompts retrieved: 403533 . Total input tokens: 90053908 . Total output tokens: 80753743
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.640884481370449,
    "estimated_duration": 3600.1267488397325,
    "input_throughput": 7289.802507219536,
    "output_throughput": 6476.579194750453,
    "total_throughput": 13766.381701969989,
    "itl": 132.862003764831,
    "ttft": 1005870.6519550403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 134344,
    "finished_requests": 106147,
    "scheduler_time": 76.75672278569623
}
#Debug simulation 
Total elapsed time: 7.640989267267287. Arrivals time: 0.286723256111145 Scheduler time: 7.237481391988695 Scheduler overhead time: 0.04148337850347161 Adapter cache time: 0.012417125515639782 Engine time: 0.043201550375670195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_64_slots_64_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_64_slots_64_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.412891129031777,
    "estimated_duration": 3600.005351140699,
    "input_throughput": 7109.020821841486,
    "output_throughput": 6298.977581467972,
    "total_throughput": 13407.998403309459,
    "itl": 136.32489638518152,
    "ttft": 1026018.7347529377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 132130,
    "finished_requests": 103441,
    "scheduler_time": 74.46318212342445
}
#Debug simulation 
Total elapsed time: 7.413002063985914. Arrivals time: 0.27244134433567524 Scheduler time: 7.017452651634812 Scheduler overhead time: 0.04046611115336418 Adapter cache time: 0.019732446875423193 Engine time: 0.0435711364261806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_64_slots_64_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_64_slots_64_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.432566339150071,
    "estimated_duration": 3600.0684553820806,
    "input_throughput": 7108.950098362451,
    "output_throughput": 6298.927445698613,
    "total_throughput": 13407.877544061064,
    "itl": 136.32599209963195,
    "ttft": 1026017.5438834631,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 132130,
    "finished_requests": 103442,
    "scheduler_time": 74.46275476926641
}
#Debug simulation 
Total elapsed time: 7.432652404997498. Arrivals time: 0.2730124765075743 Scheduler time: 7.037803535349667 Scheduler overhead time: 0.0407260456122458 Adapter cache time: 0.019637098535895348 Engine time: 0.042059448547661304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_64_slots_64_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_64_slots_64_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.438871008343995,
    "estimated_duration": 3600.0699185603094,
    "input_throughput": 7108.947209068285,
    "output_throughput": 6298.924885622361,
    "total_throughput": 13407.872094690647,
    "itl": 136.32600555875067,
    "ttft": 1026018.6723058268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 132130,
    "finished_requests": 103442,
    "scheduler_time": 74.46271495920331
}
#Debug simulation 
Total elapsed time: 7.438955869991332. Arrivals time: 0.27572254557162523 Scheduler time: 7.041595515795052 Scheduler overhead time: 0.04058557655662298 Adapter cache time: 0.019595350604504347 Engine time: 0.04215733613818884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_64_slots_64_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_64_slots_64_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.420853253919631,
    "estimated_duration": 3600.0474960893466,
    "input_throughput": 7108.937598129078,
    "output_throughput": 6298.903840750109,
    "total_throughput": 13407.841438879188,
    "itl": 136.32583106186686,
    "ttft": 1026031.7236694277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 132130,
    "finished_requests": 103441,
    "scheduler_time": 74.46297499782516
}
#Debug simulation 
Total elapsed time: 7.42096433788538. Arrivals time: 0.27756850561127067 Scheduler time: 7.022043284960091 Scheduler overhead time: 0.040493028704077005 Adapter cache time: 0.019400271121412516 Engine time: 0.04214395070448518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_64_slots_64_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_64_slots_64_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.438343997113407,
    "estimated_duration": 3600.073496295668,
    "input_throughput": 7108.940144231465,
    "output_throughput": 6298.918625781747,
    "total_throughput": 13407.858770013212,
    "itl": 136.32588608372583,
    "ttft": 1026020.8961434114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 132130,
    "finished_requests": 103442,
    "scheduler_time": 74.46274685518635
}
#Debug simulation 
Total elapsed time: 7.438429024070501. Arrivals time: 0.27827884582802653 Scheduler time: 7.037614732049406 Scheduler overhead time: 0.0410102279856801 Adapter cache time: 0.019871286116540432 Engine time: 0.04230001429095864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_64_slots_64_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_64_slots_64_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.380277945194393,
    "estimated_duration": 3600.004138257703,
    "input_throughput": 7109.0232169527535,
    "output_throughput": 6298.979703666311,
    "total_throughput": 13408.002920619065,
    "itl": 136.32583350191734,
    "ttft": 1025998.8372236296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 132130,
    "finished_requests": 103441,
    "scheduler_time": 74.46150920001514
}
#Debug simulation 
Total elapsed time: 7.380368546117097. Arrivals time: 0.28641631081700325 Scheduler time: 6.972519451752305 Scheduler overhead time: 0.040569806937128305 Adapter cache time: 0.01960217347368598 Engine time: 0.042086204048246145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_64_slots_64_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_64_slots_64_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 17280, 17280, 270, 17280, 270, 540, 270, 17280, 540, 17280, 270, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 17280, 270, 270, 270, 17280, 540, 270, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 270, 540, 540, 17280, 17280, 540, 17280, 270, 17280, 540, 540, 540, 17280, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 17280, 540]
Prompts retrieved: 397170 . Total input tokens: 88631775 . Total output tokens: 79453170
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.422939928714186,
    "estimated_duration": 3600.1218564773635,
    "input_throughput": 7109.049643403626,
    "output_throughput": 6298.859567544914,
    "total_throughput": 13407.909210948541,
    "itl": 136.32768206952326,
    "ttft": 1026039.2238881948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 132130,
    "finished_requests": 103443,
    "scheduler_time": 74.46353275949353
}
#Debug simulation 
Total elapsed time: 7.423023650888354. Arrivals time: 0.2776760277338326 Scheduler time: 7.0233065518550575 Scheduler overhead time: 0.04062282433733344 Adapter cache time: 0.01978709315881133 Engine time: 0.042361561208963394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.639223041012883,
    "estimated_duration": 3600.127210749839,
    "input_throughput": 7359.348558819863,
    "output_throughput": 6480.328231274027,
    "total_throughput": 13839.67679009389,
    "itl": 131.76778228957042,
    "ttft": 930601.9039302976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 131205,
    "finished_requests": 106601,
    "scheduler_time": 78.55140704370288
}
#Debug simulation 
Total elapsed time: 7.639310163911432. Arrivals time: 0.27746253926306963 Scheduler time: 7.238453389611095 Scheduler overhead time: 0.0420445934869349 Adapter cache time: 0.017976739909499884 Engine time: 0.043579540215432644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.618763883598149,
    "estimated_duration": 3600.036912972941,
    "input_throughput": 7359.466761167385,
    "output_throughput": 6480.254387373652,
    "total_throughput": 13839.721148541037,
    "itl": 131.7679874869818,
    "ttft": 930591.7692589859,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 131205,
    "finished_requests": 106599,
    "scheduler_time": 78.54970080521979
}
#Debug simulation 
Total elapsed time: 7.618854326661676. Arrivals time: 0.2802610737271607 Scheduler time: 7.215522721875459 Scheduler overhead time: 0.04177425103262067 Adapter cache time: 0.01802760735154152 Engine time: 0.04358270252123475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.572125727310777,
    "estimated_duration": 3600.0361530831515,
    "input_throughput": 7359.468314591687,
    "output_throughput": 6480.255755215233,
    "total_throughput": 13839.72406980692,
    "itl": 131.76789419279604,
    "ttft": 930591.1423801717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 131205,
    "finished_requests": 106599,
    "scheduler_time": 78.54974435569699
}
#Debug simulation 
Total elapsed time: 7.572221232112497. Arrivals time: 0.27874572528526187 Scheduler time: 7.170177112799138 Scheduler overhead time: 0.04195155901834369 Adapter cache time: 0.01810659747570753 Engine time: 0.04350028093904257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.592445177026093,
    "estimated_duration": 3600.0046969477326,
    "input_throughput": 7359.532620183319,
    "output_throughput": 6480.312378419852,
    "total_throughput": 13839.84499860317,
    "itl": 131.76808803863577,
    "ttft": 930576.0601275794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 131205,
    "finished_requests": 106599,
    "scheduler_time": 78.54842694245644
}
#Debug simulation 
Total elapsed time: 7.59253187617287. Arrivals time: 0.27729742461815476 Scheduler time: 7.192018050234765 Scheduler overhead time: 0.04196529043838382 Adapter cache time: 0.018258010037243366 Engine time: 0.04322295030578971 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.5845164260827005,
    "estimated_duration": 3600.0457436507445,
    "input_throughput": 7359.448708874608,
    "output_throughput": 6480.238491731581,
    "total_throughput": 13839.68720060619,
    "itl": 131.76788859978615,
    "ttft": 930611.4322135261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 131205,
    "finished_requests": 106599,
    "scheduler_time": 78.5497343343992
}
#Debug simulation 
Total elapsed time: 7.584600573871285. Arrivals time: 0.2756766933016479 Scheduler time: 7.185331902466714 Scheduler overhead time: 0.041950123850256205 Adapter cache time: 0.018335616681724787 Engine time: 0.043569655157625675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.557966156862676,
    "estimated_duration": 3600.096100065162,
    "input_throughput": 7359.412155558972,
    "output_throughput": 6480.384231848068,
    "total_throughput": 13839.79638740704,
    "itl": 131.76704371286237,
    "ttft": 930574.0531188815,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 131205,
    "finished_requests": 106601,
    "scheduler_time": 78.55158084184436
}
#Debug simulation 
Total elapsed time: 7.558059380855411. Arrivals time: 0.2765899384394288 Scheduler time: 7.1584331202320755 Scheduler overhead time: 0.04181305132806301 Adapter cache time: 0.01800500275567174 Engine time: 0.04351148894056678 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_64_slots_64_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 17280, 17280, 135, 17280, 135, 540, 135, 17280, 540, 17280, 135, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 17280, 135, 135, 135, 17280, 540, 135, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 135, 540, 540, 17280, 17280, 540, 17280, 135, 17280, 540, 540, 540, 17280, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 17280, 540]
Prompts retrieved: 394335 . Total input tokens: 88025496 . Total output tokens: 78878611
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.638103596866131,
    "estimated_duration": 3600.0552625253495,
    "input_throughput": 7359.429249820701,
    "output_throughput": 6480.221357389713,
    "total_throughput": 13839.650607210415,
    "itl": 131.7681418621627,
    "ttft": 930614.5530495411,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 131205,
    "finished_requests": 106599,
    "scheduler_time": 78.54994088291951
}
#Debug simulation 
Total elapsed time: 7.638201768975705. Arrivals time: 0.276771139819175 Scheduler time: 7.237237045541406 Scheduler overhead time: 0.0422745430842042 Adapter cache time: 0.01828747196123004 Engine time: 0.043586261570453644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.654715827666223,
    "estimated_duration": 3600.0742267337664,
    "input_throughput": 7420.62644198242,
    "output_throughput": 6557.661735052295,
    "total_throughput": 13978.288177034714,
    "itl": 130.46745662735987,
    "ttft": 873912.530797305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 130771,
    "finished_requests": 107718,
    "scheduler_time": 80.48499018506001
}
#Debug simulation 
Total elapsed time: 7.654796756803989. Arrivals time: 0.27920062374323606 Scheduler time: 7.252819514833391 Scheduler overhead time: 0.04238752741366625 Adapter cache time: 0.016518133226782084 Engine time: 0.04397364752367139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.676889677066356,
    "estimated_duration": 3600.121692598257,
    "input_throughput": 7420.528604609351,
    "output_throughput": 6557.575275451795,
    "total_throughput": 13978.103880061146,
    "itl": 130.46774996820034,
    "ttft": 874044.1674596469,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 130771,
    "finished_requests": 107718,
    "scheduler_time": 80.48551716301165
}
#Debug simulation 
Total elapsed time: 7.676971921231598. Arrivals time: 0.27224701596423984 Scheduler time: 7.281795300077647 Scheduler overhead time: 0.04249203484505415 Adapter cache time: 0.016264738980680704 Engine time: 0.04420893779024482 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.627241767942905,
    "estimated_duration": 3600.1227510488998,
    "input_throughput": 7420.5264229439435,
    "output_throughput": 6557.573347498155,
    "total_throughput": 13978.099770442099,
    "itl": 130.46776786440302,
    "ttft": 874044.9845555453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 130771,
    "finished_requests": 107718,
    "scheduler_time": 80.48550790266825
}
#Debug simulation 
Total elapsed time: 7.627345479093492. Arrivals time: 0.26472111977636814 Scheduler time: 7.240367166697979 Scheduler overhead time: 0.0422433284111321 Adapter cache time: 0.016317663714289665 Engine time: 0.04376183031126857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.672012383118272,
    "estimated_duration": 3600.0966184618323,
    "input_throughput": 7420.5802874852,
    "output_throughput": 6557.620947986313,
    "total_throughput": 13978.201235471513,
    "itl": 130.46767711972572,
    "ttft": 873942.681305127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 130771,
    "finished_requests": 107718,
    "scheduler_time": 80.4846151415997
}
#Debug simulation 
Total elapsed time: 7.67209816724062. Arrivals time: 0.26456634839996696 Scheduler time: 7.284449983853847 Scheduler overhead time: 0.042508705984801054 Adapter cache time: 0.016493186354637146 Engine time: 0.044200699776411057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.658729922957718,
    "estimated_duration": 3600.140089711237,
    "input_throughput": 7420.4906848896435,
    "output_throughput": 6557.541765518789,
    "total_throughput": 13978.032450408433,
    "itl": 130.4682867672961,
    "ttft": 874046.586518768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 130771,
    "finished_requests": 107718,
    "scheduler_time": 80.48577127170357
}
#Debug simulation 
Total elapsed time: 7.658827750943601. Arrivals time: 0.2641550903208554 Scheduler time: 7.271873784717172 Scheduler overhead time: 0.042232994455844164 Adapter cache time: 0.016545490827411413 Engine time: 0.04405062412843108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.666790940333158,
    "estimated_duration": 3600.0552544958173,
    "input_throughput": 7420.665548574024,
    "output_throughput": 6557.696293832656,
    "total_throughput": 13978.361842406679,
    "itl": 130.4675401387865,
    "ttft": 873878.927690286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 130771,
    "finished_requests": 107718,
    "scheduler_time": 80.48390576083517
}
#Debug simulation 
Total elapsed time: 7.666891026310623. Arrivals time: 0.26811341056600213 Scheduler time: 7.276236251462251 Scheduler overhead time: 0.04244234971702099 Adapter cache time: 0.01630864618346095 Engine time: 0.04378576949238777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_64_slots_64_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 17280, 17280, 66, 17280, 66, 540, 66, 17280, 540, 17280, 66, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 17280, 66, 66, 66, 17280, 540, 66, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 66, 540, 540, 17280, 17280, 540, 17280, 66, 17280, 540, 540, 540, 17280, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 17280, 540]
Prompts retrieved: 392886 . Total input tokens: 87687812 . Total output tokens: 78601416
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.672340312041342,
    "estimated_duration": 3600.0029022611106,
    "input_throughput": 7420.555406558507,
    "output_throughput": 6557.707768838824,
    "total_throughput": 13978.263175397331,
    "itl": 130.46896715718927,
    "ttft": 873891.0469030532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 130771,
    "finished_requests": 107716,
    "scheduler_time": 80.48272947985886
}
#Debug simulation 
Total elapsed time: 7.672423350159079. Arrivals time: 0.2656371798366308 Scheduler time: 7.284099200740457 Scheduler overhead time: 0.0422453791834414 Adapter cache time: 0.016503851395100355 Engine time: 0.043946897611021996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_64_slots_64_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [21 21 22]
Adapter prompts. [33, 540, 17280, 17280, 33, 17280, 33, 540, 33, 17280, 540, 17280, 33, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 17280, 33, 33, 33, 17280, 540, 33, 17280, 17280, 17280, 17280, 540, 540, 540, 17280, 33, 540, 540, 17280, 17280, 540, 17280, 33, 17280, 540, 540, 540, 17280, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 17280, 540]
Prompts retrieved: 392193 . Total input tokens: 87540453 . Total output tokens: 78455184
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.677584491670132,
    "estimated_duration": 3600.0986165158474,
    "input_throughput": 7445.681036910563,
    "output_throughput": 6621.158901216191,
    "total_throughput": 14066.839938126755,
    "itl": 129.78810464633932,
    "ttft": 833102.3155863627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 130530,
    "finished_requests": 108627,
    "scheduler_time": 81.92711720682269
}
#Debug simulation 
Total elapsed time: 7.677671881858259. Arrivals time: 0.2614324064925313 Scheduler time: 7.294836623128504 Scheduler overhead time: 0.042574066668748856 Adapter cache time: 0.015014564618468285 Engine time: 0.04381531989201903 
